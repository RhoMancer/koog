{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<p>Koog is an open-source JetBrains framework for building AI agents with an idiomatic, type-safe Kotlin DSL designed specifically for JVM and Kotlin developers. It lets you create agents that interact with tools, handle complex workflows, and communicate with users.</p> <p>You can customize agent capabilities with a modular feature system and deploy your agents across JVM, JS, WasmJS, Android, and iOS targets using Kotlin Multiplatform.</p> <ul> <li> <p> Getting started</p> <p>Build and run your first AI agent</p> </li> <li> <p> Glossary</p> <p>Learn the essential terms</p> </li> </ul>"},{"location":"#agent-types","title":"Agent types","text":"<ul> <li> <p> Basic agents</p> <p>Create and run agents that process a single input and provide a response</p> </li> <li> <p> Functional agents</p> <p>Create and run lightweight agents with custom logic in plain Kotlin </p> </li> <li> <p> Complex workflow agents</p> <p>Create and run agents that handle complex workflows with custom strategies</p> </li> <li> <p> Planner agents</p> <p>Create and run agents that iteratively build and execute plans</p> </li> </ul>"},{"location":"#core-functionality","title":"Core functionality","text":"<ul> <li> <p> Prompts</p> <p>Create prompts, run them using LLM clients or prompt executors, switch between LLMs and providers, and handle failures with built-in retries</p> </li> <li> <p> Tools</p> <p>Enhance your agents with built\u2011in, annotation\u2011based, or class\u2011based tools that can access external systems and APIs</p> </li> <li> <p> Strategies</p> <p>Design complex agent behaviors using intuitive graph-based workflows</p> </li> <li> <p> Events</p> <p>Monitor and process agent lifecycle, strategy, node, LLM call, and tool call events with predefined handlers</p> </li> </ul>"},{"location":"#advanced-usage","title":"Advanced usage","text":"<ul> <li> <p> History compression</p> <p>Optimize token usage while maintaining context in long-running conversations using advanced techniques</p> </li> <li> <p> Agent persistence</p> <p>Restore the agent state at specific points during execution</p> </li> <li> <p> Structured output</p> <p>Generate responses in structured formats</p> </li> <li> <p> Streaming API</p> <p>Process responses in real-time with streaming support and parallel tool calls</p> </li> <li> <p> Knowledge retrieval</p> <p>Retain and retrieve knowledge across conversations using vector embeddings, ranked document storage, and shared agent memory</p> </li> <li> <p> Tracing</p> <p>Debug and monitor agent execution with detailed, configurable tracing</p> </li> </ul>"},{"location":"#integrations","title":"Integrations","text":"<ul> <li> <p> Model Context Protocol (MCP)</p> <p>Use MCP tools directly in AI agents</p> </li> <li> <p> Spring Boot</p> <p>Add Koog to your Spring applications</p> </li> <li> <p> Ktor</p> <p>Integrate Koog with Ktor servers</p> </li> <li> <p> OpenTelemetry</p> <p>Trace, log, and measure your agent with popular observability tools</p> </li> <li> <p> A2A Protocol</p> <p>Connect agents and services over a shared protocol</p> </li> </ul>"},{"location":"a2a-client/","title":"A2A Client","text":"<p>The A2A client enables you to communicate with A2A-compliant agents over the network. It provides a complete implementation of the A2A protocol specification, handling agent discovery, message exchange, task management, and real-time streaming responses.</p>"},{"location":"a2a-client/#dependencies","title":"Dependencies","text":"<p>To use the A2A client in your project, add the following dependencies to your <code>build.gradle.kts</code>:</p> <pre><code>dependencies {\n    // Core A2A client library\n    implementation(\"ai.koog:a2a-client:$koogVersion\")\n\n    // HTTP JSON-RPC transport (most common)\n    implementation(\"ai.koog:a2a-transport-client-jsonrpc-http:$koogVersion\")\n\n    // Ktor client engine (choose one that fits your needs)\n    implementation(\"io.ktor:ktor-client-cio:$ktorVersion\")\n}\n</code></pre>"},{"location":"a2a-client/#overview","title":"Overview","text":"<p>The A2A client acts as a bridge between your application and A2A-compliant agents. It orchestrates the entire communication lifecycle while maintaining protocol compliance and providing robust session management.</p>"},{"location":"a2a-client/#core-components","title":"Core components","text":""},{"location":"a2a-client/#a2aclient","title":"A2AClient","text":"<p>The main client class implementing the complete A2A protocol. It serves as the central coordinator that:</p> <ul> <li>Manages connections and agent discovery through pluggable resolvers</li> <li>Orchestrates message exchange and task operations with automatic protocol compliance</li> <li>Handles streaming responses and real-time communication when supported by agents</li> <li>Provides comprehensive error handling and fallback mechanisms for robust applications</li> </ul> <p>The <code>A2AClient</code> accepts two required parameters:</p> <ul> <li><code>ClientTransport</code> which handles network communication layer</li> <li><code>AgentCardResolver</code> which handles agent discovery and metadata retrieval</li> </ul> <p>The <code>A2AClient</code> interface provides several key methods for interacting with A2A agents:</p> <ul> <li><code>connect</code> method - To connect to the agent and retrieve its capabilities, which discovers what the agent can do and   caches the AgentCard</li> <li><code>sendMessage</code> method - To send a message to the agent and receive a single response for simple request-response   patterns</li> <li><code>sendMessageStreaming</code> method - To send a message with streaming support for real-time responses, which returns a Flow   of events including partial messages and task updates</li> <li><code>getTask</code> method - To query the status and details of a specific task</li> <li><code>cancelTask</code> method - To cancel a running task if the agent supports cancellation</li> <li><code>cachedAgentCard</code> method - To get the cached agent card without making a network request, which returns null if   connect hasn't been called yet</li> </ul>"},{"location":"a2a-client/#clienttransport","title":"ClientTransport","text":"<p>The <code>ClientTransport</code> interface handles the low-level network communication while the A2A client manages the protocol logic. It abstracts away transport-specific details, allowing you to use different protocols seamlessly.</p>"},{"location":"a2a-client/#http-json-rpc-transport","title":"HTTP JSON-RPC Transport","text":"<p>The most common transport for A2A agents:</p> <pre><code>val transport = HttpJSONRPCClientTransport(\n    url = \"https://agent.example.com/a2a\",        // Agent endpoint URL\n    httpClient = HttpClient(CIO) {                // Optional: custom HTTP client\n        install(ContentNegotiation) {\n            json()\n        }\n        install(HttpTimeout) {\n            requestTimeoutMillis = 30000\n        }\n    }\n)\n</code></pre>"},{"location":"a2a-client/#agentcardresolver","title":"AgentCardResolver","text":"<p>The <code>AgentCardResolver</code> interface retrieves agent metadata and capabilities. It enables agent discovery from various sources and supports caching strategies for optimal performance.</p>"},{"location":"a2a-client/#url-agent-card-resolver","title":"URL Agent Card Resolver","text":"<p>Fetch agent cards from HTTP endpoints following A2A conventions:</p> <pre><code>val agentCardResolver = UrlAgentCardResolver(\n    baseUrl = \"https://agent.example.com\",           // Base URL of the agent service\n    path = \"/.well-known/agent-card.json\",           // Standard agent card location\n    httpClient = HttpClient(CIO),                    // Optional: custom HTTP client\n)\n</code></pre>"},{"location":"a2a-client/#quickstart","title":"Quickstart","text":""},{"location":"a2a-client/#1-create-the-client","title":"1. Create the Client","text":"<p>Define the transport and agent card resolver and create the client.</p> <pre><code>// HTTP JSON-RPC transport\nval transport = HttpJSONRPCClientTransport(\n    url = \"https://agent.example.com/a2a\"\n)\n\n// Agent card resolver\nval agentCardResolver = UrlAgentCardResolver(\n    baseUrl = \"https://agent.example.com\",\n    path = \"/.well-known/agent-card.json\"\n)\n\n// Create client\nval client = A2AClient(transport, agentCardResolver)\n</code></pre>"},{"location":"a2a-client/#2-connect-and-discover","title":"2. Connect and Discover","text":"<p>Connect to the agent and retrieve its card. Having agent's card enables you to query its capabilities and perform other operations, for example, check if it supports streaming.</p> <pre><code>// Connect and retrieve agent capabilities\nclient.connect()\nval agentCard = client.cachedAgentCard()\n\nprintln(\"Connected to: ${agentCard.name}\")\nprintln(\"Supports streaming: ${agentCard.capabilities.streaming}\")\n</code></pre>"},{"location":"a2a-client/#3-send-messages","title":"3. Send Messages","text":"<p>Send a message to the agent and receive a single response. The response can be either the message if the agent responded directly, or a task event if the agent is performing a task.</p> <pre><code>val message = Message(\n    messageId = UUID.randomUUID().toString(),\n    role = Role.User,\n    parts = listOf(TextPart(\"Hello, agent!\")),\n    contextId = \"conversation-1\"\n)\n\nval request = Request(data = MessageSendParams(message))\nval response = client.sendMessage(request)\n\n// Handle response\nwhen (val event = response.data) {\n    is Message -&gt; {\n        val text = event.parts\n            .filterIsInstance&lt;TextPart&gt;()\n            .joinToString { it.text }\n        print(text) // Stream partial responses\n    }\n    is TaskEvent -&gt; {\n        if (event.final) {\n            println(\"\\nTask completed\")\n        }\n    }\n}\n</code></pre>"},{"location":"a2a-client/#4-send-messages-streaming","title":"4. Send Messages Streaming","text":"<p>The A2A client supports streaming responses for real-time communication. Instead of receiving a single response, it returns a <code>Flow</code> of events including messages and task updates.</p> <pre><code>// Check if agent supports streaming\nif (client.cachedAgentCard()?.capabilities?.streaming == true) {\n    client.sendMessageStreaming(request).collect { response -&gt;\n        when (val event = response.data) {\n            is Message -&gt; {\n                val text = event.parts\n                    .filterIsInstance&lt;TextPart&gt;()\n                    .joinToString { it.text }\n                print(text) // Stream partial responses\n            }\n            is TaskStatusUpdateEvent -&gt; {\n                if (event.final) {\n                    println(\"\\nTask completed\")\n                }\n            }\n        }\n    }\n} else {\n    // Fallback to non-streaming\n    val response = client.sendMessage(request)\n    // Handle single response\n}\n</code></pre>"},{"location":"a2a-client/#5-manage-tasks","title":"5. Manage Tasks","text":"<p>A2A Client provides methods to control server tasks by asking for their status and cancelling them.</p> <pre><code>// Query task status\nval taskRequest = Request(data = TaskQueryParams(taskId = \"task-123\"))\nval taskResponse = client.getTask(taskRequest)\nval task = taskResponse.data\n\nprintln(\"Task state: ${task.status.state}\")\n\n// Cancel running task\nif (task.status.state == TaskState.Working) {\n    val cancelRequest = Request(data = TaskIdParams(taskId = \"task-123\"))\n    val cancelledTask = client.cancelTask(cancelRequest).data\n    println(\"Task cancelled: ${cancelledTask.status.state}\")\n}\n</code></pre>"},{"location":"a2a-koog-integration/","title":"A2A and Koog Integration","text":"<p>Koog provides seamless integration with the A2A protocol, allowing you to expose Koog agents as A2A servers and connect Koog agents to other A2A-compliant agents.</p>"},{"location":"a2a-koog-integration/#dependencies","title":"Dependencies","text":"<p>A2A Koog integration requires specific feature modules depending on your use case:</p>"},{"location":"a2a-koog-integration/#for-exposing-koog-agents-as-a2a-servers","title":"For Exposing Koog Agents as A2A Servers","text":"<p>Add these dependencies to your <code>build.gradle.kts</code>:</p> <pre><code>dependencies {\n    // Koog A2A server integration feature\n    implementation(\"ai.koog:agents-features-a2a-server:$koogVersion\")\n\n    // HTTP JSON-RPC transport\n    implementation(\"ai.koog:a2a-transport-server-jsonrpc-http:$koogVersion\")\n\n    // Ktor server engine (choose one that fits your needs)\n    implementation(\"io.ktor:ktor-server-netty:$ktorVersion\")\n}\n</code></pre>"},{"location":"a2a-koog-integration/#for-connecting-koog-agents-to-a2a-agents","title":"For Connecting Koog Agents to A2A Agents","text":"<p>Add these dependencies to your <code>build.gradle.kts</code>:</p> <pre><code>dependencies {\n    // Koog A2A client integration feature\n    implementation(\"ai.koog:agents-features-a2a-client:$koogVersion\")\n\n    // HTTP JSON-RPC transport\n    implementation(\"ai.koog:a2a-transport-client-jsonrpc-http:$koogVersion\")\n\n    // Ktor client engine (choose one that fits your needs)\n    implementation(\"io.ktor:ktor-client-cio:$ktorVersion\")\n}\n</code></pre>"},{"location":"a2a-koog-integration/#overview","title":"Overview","text":"<p>The integration enables two main patterns:</p> <ol> <li>Expose Koog agents as A2A servers - Make your Koog agents discoverable and accessible via the A2A protocol</li> <li>Connect Koog agents to A2A agents - Let your Koog agents communicate with other A2A-compliant agents</li> </ol>"},{"location":"a2a-koog-integration/#exposing-koog-agents-as-a2a-servers","title":"Exposing Koog Agents as A2A Servers","text":""},{"location":"a2a-koog-integration/#define-koog-agent-with-a2a-feature","title":"Define Koog Agent with A2A feature","text":"<p>Let's define a Koog agent first. The logic of the agent can vary, but here's an example basic single run agent with tools. The agent resaves a message from the user, forwards it to the llm. If the llm response contains a tool call, the agent executes the tool and forwards the result to the llm. If the llm response contains an assistant message, the agent sends the assistant message to the user and finishes.</p> <p>On input resize, the agent sends a task submitted event to the A2A client with the input message. On each tool call, the agent sends a task working event to the A2A client with the tool call and result. On assistant message, the agent sends a task complete event to the A2A client with the assistant message.</p> <pre><code>/**\n * Create a Koog agent with A2A feature\n */\n@OptIn(ExperimentalUuidApi::class)\nprivate fun createAgent(\n    context: RequestContext&lt;MessageSendParams&gt;,\n    eventProcessor: SessionEventProcessor,\n) = AIAgent(\n    promptExecutor = MultiLLMPromptExecutor(\n        LLMProvider.Google to GoogleLLMClient(\"api-key\")\n    ),\n    toolRegistry = ToolRegistry {\n        // Declare tools here\n    },\n    strategy = strategy&lt;A2AMessage, Unit&gt;(\"test\") {\n        val nodeSetup by node&lt;A2AMessage, Unit&gt; { inputMessage -&gt;\n            // Convenience function to transform A2A message into Koog message\n            val input = inputMessage.toKoogMessage()\n            llm.writeSession {\n                appendPrompt {\n                    message(input)\n                }\n            }\n            // Send update event to A2A client\n            withA2AAgentServer {\n                sendTaskUpdate(\"Request submitted: ${input.content}\", TaskState.Submitted)\n            }\n        }\n\n        // Calling llm\n        val nodeLLMRequest by node&lt;Unit, Message&gt; {\n            llm.writeSession {\n                requestLLM()\n            }\n        }\n\n        // Executing tool\n        val nodeProcessTool by node&lt;Message.Tool.Call, Unit&gt; { toolCall -&gt;\n            withA2AAgentServer {\n                sendTaskUpdate(\"Executing tool: ${toolCall.content}\", TaskState.Working)\n            }\n\n            val toolResult = environment.executeTool(toolCall)\n\n            llm.writeSession {\n                appendPrompt {\n                    tool {\n                        result(toolResult)\n                    }\n                }\n            }\n            withA2AAgentServer {\n                sendTaskUpdate(\"Tool result: ${toolResult.content}\", TaskState.Working)\n            }\n        }\n\n        // Sending assistant message\n        val nodeProcessAssistant by node&lt;String, Unit&gt; { assistantMessage -&gt;\n            withA2AAgentServer {\n                sendTaskUpdate(assistantMessage, TaskState.Completed)\n            }\n        }\n\n        edge(nodeStart forwardTo nodeSetup)\n        edge(nodeSetup forwardTo nodeLLMRequest)\n\n        // If a tool call is returned from llm, forward to the tool processing node and then back to llm\n        edge(nodeLLMRequest forwardTo nodeProcessTool onToolCall { true })\n        edge(nodeProcessTool forwardTo nodeLLMRequest)\n\n        // If an assistant message is returned from llm, forward to the assistant processing node and then to finish\n        edge(nodeLLMRequest forwardTo nodeProcessAssistant onAssistantMessage { true })\n        edge(nodeProcessAssistant forwardTo nodeFinish)\n    },\n    agentConfig = AIAgentConfig(\n        prompt = prompt(\"agent\") { system(\"You are a helpful assistant.\") },\n        model = GoogleModels.Gemini2_5Pro,\n        maxAgentIterations = 10\n    ),\n) {\n    install(A2AAgentServer) {\n        this.context = context\n        this.eventProcessor = eventProcessor\n    }\n}\n\n/**\n * Convenience function to send task update event to A2A client\n * @param content The message content\n * @param state The task state\n */\n@OptIn(ExperimentalUuidApi::class)\nprivate suspend fun A2AAgentServer.sendTaskUpdate(\n    content: String,\n    state: TaskState,\n) {\n    val message = A2AMessage(\n        messageId = Uuid.random().toString(),\n        role = Role.Agent,\n        parts = listOf(\n            TextPart(content)\n        ),\n        contextId = context.contextId,\n        taskId = context.taskId,\n    )\n\n    val task = Task(\n        id = context.taskId,\n        contextId = context.contextId,\n        status = TaskStatus(\n            state = state,\n            message = message,\n            timestamp = Clock.System.now(),\n        )\n    )\n    eventProcessor.sendTaskEvent(task)\n}\n</code></pre>"},{"location":"a2a-koog-integration/#a2aagentserver-feature-mechanism","title":"A2AAgentServer Feature Mechanism","text":"<p>The <code>A2AAgentServer</code> is a Koog agent feature that enables seamless integration between Koog agents and the A2A protocol. The <code>A2AAgentServer</code> feature provides access to the <code>RequestContext</code> and <code>SessionEventProcessor</code> entities, which are used to communicate with the A2A client inside the Koog agent.</p> <p>To install the feature, call the <code>install</code> function on the agent and pass the <code>A2AAgentServer</code> feature along with the <code>RequestContext</code> and <code>SessionEventProcessor</code>: <pre><code>// Install the feature\ninstall(A2AAgentServer) {\n    this.context = context\n    this.eventProcessor = eventProcessor\n}\n</code></pre></p> <p>To access these entities from Koog agent strategy, the feature provides a <code>withA2AAgentServer</code> function that allows agent nodes to access A2A server capabilities within their execution context.  It retrieves the installed <code>A2AAgentServer</code> feature and provides it as the receiver for the action block.</p> <pre><code>// Usage within agent nodes\nwithA2AAgentServer {\n    // 'this' is now A2AAgentServer instance\n    eventProcessor.sendTaskUpdate(\"Processing your request...\", TaskState.Working)\n}\n</code></pre>"},{"location":"a2a-koog-integration/#start-a2a-server","title":"Start A2A Server","text":"<p>After running the server Koog agent will be discoverable and accessible via the A2A protocol.</p> <pre><code>val agentCard = AgentCard(\n    name = \"Koog Agent\",\n    url = \"http://localhost:9999/koog\",\n    description = \"Simple universal agent powered by Koog\",\n    version = \"1.0.0\",\n    protocolVersion = \"0.3.0\",\n    preferredTransport = TransportProtocol.JSONRPC,\n    capabilities = AgentCapabilities(streaming = true),\n    defaultInputModes = listOf(\"text\"),\n    defaultOutputModes = listOf(\"text\"),\n    skills = listOf(\n        AgentSkill(\n            id = \"koog\",\n            name = \"Koog Agent\",\n            description = \"Universal agent powered by Koog. Supports tool calling.\",\n            tags = listOf(\"chat\", \"tool\"),\n        )\n    )\n)\n// Server setup\nval server = A2AServer(agentExecutor = KoogAgentExecutor(), agentCard = agentCard)\nval transport = HttpJSONRPCServerTransport(server)\ntransport.start(engineFactory = Netty, port = 8080, path = \"/chat\", wait = true)\n</code></pre>"},{"location":"a2a-koog-integration/#connecting-koog-agents-to-a2a-agents","title":"Connecting Koog Agents to A2A Agents","text":""},{"location":"a2a-koog-integration/#create-a2a-client-and-connect-to-the-a2a-server","title":"Create A2A Client and connect to the A2A Server","text":"<pre><code>val transport = HttpJSONRPCClientTransport(url = \"http://localhost:9999/koog\")\nval agentCardResolver =\n    UrlAgentCardResolver(baseUrl = \"http://localhost:9999\", path = \"/koog\")\nval client = A2AClient(transport = transport, agentCardResolver = agentCardResolver)\n\nval agentId = \"koog\"\nclient.connect()\n</code></pre>"},{"location":"a2a-koog-integration/#create-koog-agent-and-add-a2a-client-to-a2aagentclient-feature","title":"Create Koog Agent and add A2A Client to A2AAgentClient Feature","text":"<p>To connect to A2A agent from your Koog Agent, you can use the A2AAgentClient feature, which provides a client API for connecting to A2A agents. The principle of the client is the same as the server: you install the feature and pass the <code>A2AAgentClient</code> feature along with the <code>RequestContext</code> and <code>SessionEventProcessor</code>.</p> <pre><code>val agent = AIAgent(\n    promptExecutor = MultiLLMPromptExecutor(\n        LLMProvider.Google to GoogleLLMClient(\"api-key\")\n    ),\n    toolRegistry = ToolRegistry {\n        // declare tools here\n    },\n    strategy = strategy&lt;String, Unit&gt;(\"test\") {\n\n        val nodeCheckStreaming by nodeA2AClientGetAgentCard().transform { it.capabilities.streaming }\n\n        val nodeA2ASendMessageStreaming by nodeA2AClientSendMessageStreaming()\n        val nodeA2ASendMessage by nodeA2AClientSendMessage()\n\n        val nodeProcessStreaming by node&lt;Flow&lt;Response&lt;Event&gt;&gt;, Unit&gt; {\n            it.collect { response -&gt;\n                when (response.data) {\n                    is Task -&gt; {\n                        // Process task\n                    }\n\n                    is A2AMessage -&gt; {\n                        // Process message\n                    }\n\n                    is TaskStatusUpdateEvent -&gt; {\n                        // Process task status update\n                    }\n\n                    is TaskArtifactUpdateEvent -&gt; {\n                        // Process task artifact update\n                    }\n                }\n            }\n        }\n\n        val nodeProcessEvent by node&lt;CommunicationEvent, Unit&gt; { event -&gt;\n            when (event) {\n                is Task -&gt; {\n                    // Process task\n                }\n\n                is A2AMessage -&gt; {\n                    // Process message\n                }\n            }\n        }\n\n        // If streaming is supported, send a message, process response and finish\n        edge(nodeStart forwardTo nodeCheckStreaming transformed { agentId })\n        edge(\n            nodeCheckStreaming forwardTo nodeA2ASendMessageStreaming\n                onCondition { it == true } transformed { buildA2ARequest(agentId) }\n        )\n        edge(nodeA2ASendMessageStreaming forwardTo nodeProcessStreaming)\n        edge(nodeProcessStreaming forwardTo nodeFinish)\n\n        // If streaming is not supported, send a message, process response and finish\n        edge(\n            nodeCheckStreaming forwardTo nodeA2ASendMessage\n                onCondition { it == false } transformed { buildA2ARequest(agentId) }\n        )\n        edge(nodeA2ASendMessage forwardTo nodeProcessEvent)\n        edge(nodeProcessEvent forwardTo nodeFinish)\n\n        // If streaming is not supported, send a message, process response and finish\n        edge(nodeCheckStreaming forwardTo nodeFinish onCondition { it == null }\n            transformed { println(\"Failed to get agents card\") }\n        )\n\n    },\n    agentConfig = AIAgentConfig(\n        prompt = prompt(\"agent\") { system(\"You are a helpful assistant.\") },\n        model = GoogleModels.Gemini2_5Pro,\n        maxAgentIterations = 10\n    ),\n) {\n    install(A2AAgentClient) {\n        this.a2aClients = mapOf(agentId to client)\n    }\n}\n\n\n@OptIn(ExperimentalUuidApi::class)\nprivate fun AIAgentGraphContextBase.buildA2ARequest(agentId: String): A2AClientRequest&lt;MessageSendParams&gt; =\n    A2AClientRequest(\n        agentId = agentId,\n        callContext = ClientCallContext.Default,\n        params = MessageSendParams(\n            message = A2AMessage(\n                messageId = Uuid.random().toString(),\n                role = Role.User,\n                parts = listOf(\n                    TextPart(agentInput as String)\n                )\n            )\n        )\n    )\n</code></pre>"},{"location":"a2a-protocol-overview/","title":"A2A protocol","text":"<p>This page provides an overview of the A2A (Agent-to-Agent) protocol implementation in the Koog agentic framework.</p>"},{"location":"a2a-protocol-overview/#what-is-the-a2a-protocol","title":"What is the A2A protocol?","text":"<p>The A2A (Agent-to-Agent) protocol is a standardized communication protocol that enables AI agents to interact with each other and with client applications. It defines a set of methods, message formats, and behaviors that allow for consistent and interoperable agent communication. For more information and a detailed specification of the A2A protocol, see the official A2A Protocol website.</p>"},{"location":"a2a-protocol-overview/#getting-started","title":"Getting Started","text":"<p>Important: A2A dependencies are not included by default in the <code>koog-agents</code> meta-dependency.  You must explicitly add the A2A modules you need to your project.</p> <p>To use A2A in your project, add dependencies based on your use case:</p> <ul> <li>For A2A client: See A2A Client documentation</li> <li>For A2A server: See A2A Server documentation</li> <li>For Koog integration: See A2A Koog Integration documentation</li> </ul>"},{"location":"a2a-protocol-overview/#key-a2a-components","title":"Key A2A components","text":"<p>Koog provides full implementation of A2A protocol v0.3.0 for both client and server, as well as integration with the Koog agent framework:</p> <ul> <li>A2A Server is an agent or agentic system that exposes an endpoint implementing the A2A protocol. It   receives requests from clients, processes tasks, and returns results or status updates. It can also be used   independently of Koog agents.</li> <li>A2A Client is a client application or agent that initiates communication with an A2A server using the   A2A protocol. It can also be used independently of Koog agents.</li> <li>A2A Koog Integration is a set of classes and utilities that simplify the integration of A2A   with Koog Agents. It contains components (A2A features and nodes) for seamless A2A agent connections and communication   within the Koog framework.</li> </ul> <p>For more examples, follow the examples</p>"},{"location":"a2a-server/","title":"A2A Server","text":"<p>The A2A server enables you to expose AI agents through the standardized A2A (Agent-to-Agent) protocol. It provides a complete implementation of the A2A protocol specification, handling client requests, executing agent logic, managing complex task lifecycles, and supporting real-time streaming responses.</p>"},{"location":"a2a-server/#dependencies","title":"Dependencies","text":"<p>To use the A2A server in your project, add the following dependencies to your <code>build.gradle.kts</code>:</p> <pre><code>dependencies {\n    // Core A2A server library\n    implementation(\"ai.koog:a2a-server:$koogVersion\")\n\n    // HTTP JSON-RPC transport (most common)\n    implementation(\"ai.koog:a2a-transport-server-jsonrpc-http:$koogVersion\")\n\n    // Ktor server engine (choose one that fits your needs)\n    implementation(\"io.ktor:ktor-server-netty:$ktorVersion\")\n}\n</code></pre>"},{"location":"a2a-server/#overview","title":"Overview","text":"<p>The A2A server acts as a bridge between the A2A protocol transport layer and your custom agent logic.  It orchestrates the entire request lifecycle while maintaining protocol compliance and providing robust session management.</p>"},{"location":"a2a-server/#core-components","title":"Core components","text":""},{"location":"a2a-server/#a2aserver","title":"A2AServer","text":"<p>The main server class implementing the complete A2A protocol. It serves as the central coordinator that:</p> <ul> <li>Validates incoming requests against protocol specifications</li> <li>Manages concurrent sessions and task lifecycles</li> <li>Orchestrates communication between transport, storage, and business logic layers</li> <li>Handles all protocol operations: message sending, task querying, cancellation, push notifications</li> </ul> <p>The <code>A2AServer</code> accepts two required parameters:</p> <ul> <li><code>AgentExecutor</code> which defines business logic implementation of the agent</li> <li><code>AgentCard</code> which defines agent capabilities and metadata</li> </ul> <p>And a number of optional parameters that can be used to customize its storage and transport behavior.</p>"},{"location":"a2a-server/#agentexecutor","title":"AgentExecutor","text":"<p>The <code>AgentExecutor</code> interface is where you implement your agent's core business logic.  It acts as the bridge between the A2A protocol and your specific AI agent capabilities. To start the execution of your agent, you must implement the <code>execute</code> method where define your agent's logic. To cancel the agent, you must implement the <code>cancel</code> method.</p> <pre><code>class MyAgentExecutor : AgentExecutor {\n    override suspend fun execute(\n        context: RequestContext&lt;MessageSendParams&gt;,\n        eventProcessor: SessionEventProcessor\n    ) {\n        // Agent logic here\n    }\n\n    override suspend fun cancel(\n        context: RequestContext&lt;TaskIdParams&gt;,\n        eventProcessor: SessionEventProcessor,\n        agentJob: Deferred&lt;Unit&gt;?\n    ) {\n        // Cancel agent here, optional\n    }\n}\n</code></pre> <p>The <code>RequestContext</code> provides rich information about the current request, including the <code>contextId</code> and <code>taskId</code> of the current session, the <code>message</code> sent, and the <code>params</code> of the request.</p> <p>The <code>SessionEventProcessor</code> communicates with clients:</p> <ul> <li><code>sendMessage(message)</code>: Send immediate responses (chat-style interactions)</li> <li><code>sendTaskEvent(event)</code>: Send task-related updates (long-running operations)</li> </ul> <pre><code>// For immediate responses (like chatbots)\neventProcessor.sendMessage(\n    Message(\n        messageId = generateId(),\n        role = Role.Agent,\n        parts = listOf(TextPart(\"Here's your answer!\")),\n        contextId = context.contextId\n    )\n)\n\n// For task-based operations\neventProcessor.sendTaskEvent(\n    TaskStatusUpdateEvent(\n        contextId = context.contextId,\n        taskId = context.taskId,\n        status = TaskStatus(\n            state = TaskState.Working,\n            message = Message(/* progress update */),\n            timestamp = Clock.System.now()\n        ),\n        final = false  // More updates to come\n    )\n)\n</code></pre>"},{"location":"a2a-server/#agentcard","title":"AgentCard","text":"<p>The <code>AgentCard</code> serves as your agent's self-describing manifest. It tells clients what your agent can do, how to communicate with it, and what security requirements it has.</p> <pre><code>val agentCard = AgentCard(\n    // Basic Identity\n    name = \"Advanced Recipe Assistant\",\n    description = \"AI agent specialized in cooking advice, recipe generation, and meal planning\",\n    version = \"2.1.0\",\n    protocolVersion = \"0.3.0\",\n\n    // Communication Settings\n    url = \"https://api.example.com/a2a\",\n    preferredTransport = TransportProtocol.JSONRPC,\n\n    // Optional: Multiple transport support\n    additionalInterfaces = listOf(\n        AgentInterface(\"https://api.example.com/a2a\", TransportProtocol.JSONRPC),\n    ),\n\n    // Capabilities Declaration\n    capabilities = AgentCapabilities(\n        streaming = true,              // Support real-time responses\n        pushNotifications = true,      // Send async notifications\n        stateTransitionHistory = true  // Maintain task history\n    ),\n\n    // Content Type Support\n    defaultInputModes = listOf(\"text/plain\", \"text/markdown\", \"image/jpeg\"),\n    defaultOutputModes = listOf(\"text/plain\", \"text/markdown\", \"application/json\"),\n\n    // Define available security schemes\n    securitySchemes = mapOf(\n        \"bearer\" to HTTPAuthSecurityScheme(\n            scheme = \"Bearer\",\n            bearerFormat = \"JWT\",\n            description = \"JWT token authentication\"\n        ),\n        \"api-key\" to APIKeySecurityScheme(\n            `in` = In.Header,\n            name = \"X-API-Key\",\n            description = \"API key for service authentication\"\n        )\n    ),\n\n    // Specify security requirements (logical OR of requirements)\n    security = listOf(\n        mapOf(\"bearer\" to listOf(\"read\", \"write\")),  // Option 1: JWT with read/write scopes\n        mapOf(\"api-key\" to emptyList())              // Option 2: API key\n    ),\n\n    // Enable extended card for authenticated users\n    supportsAuthenticatedExtendedCard = true,\n\n    // Skills/Capabilities\n    skills = listOf(\n        AgentSkill(\n            id = \"recipe-generation\",\n            name = \"Recipe Generation\",\n            description = \"Generate custom recipes based on ingredients, dietary restrictions, and preferences\",\n            tags = listOf(\"cooking\", \"recipes\", \"nutrition\"),\n            examples = listOf(\n                \"Create a vegan pasta recipe with mushrooms\",\n                \"I have chicken, rice, and vegetables. What can I make?\"\n            )\n        ),\n        AgentSkill(\n            id = \"meal-planning\",\n            name = \"Meal Planning\",\n            description = \"Plan weekly meals and generate shopping lists\",\n            tags = listOf(\"meal-planning\", \"nutrition\", \"shopping\")\n        )\n    ),\n\n    // Optional: Branding\n    iconUrl = \"https://example.com/agent-icon.png\",\n    documentationUrl = \"https://docs.example.com/recipe-agent\",\n    provider = AgentProvider(\n        organization = \"CookingAI Inc.\",\n        url = \"https://cookingai.com\"\n    )\n)\n</code></pre>"},{"location":"a2a-server/#transport-layer","title":"Transport Layer","text":"<p>The A2A itself supports multiple transport protocols for communicating with clients.  Currently, Koog provides implementations for JSON-RPC server transport over HTTP.</p>"},{"location":"a2a-server/#http-json-rpc-transport","title":"HTTP JSON-RPC Transport","text":"<pre><code>val transport = HttpJSONRPCServerTransport(server)\ntransport.start(\n    engineFactory = CIO,           // Ktor engine (CIO, Netty, Jetty)\n    port = 8080,                   // Server port\n    path = \"/a2a\",                 // API endpoint path\n    wait = true                    // Block until server stops\n)\n</code></pre>"},{"location":"a2a-server/#storage","title":"Storage","text":"<p>The A2A server uses a pluggable storage architecture that separates different types of data. All storage implementations are optional and default to in-memory variants for development.</p> <ul> <li>TaskStorage: Task lifecycle management - stores and manages task states, history, and artifacts</li> <li>MessageStorage: Conversation history - manages message history within conversation contexts</li> <li>PushNotificationConfigStorage: Webhook management - manages webhook configurations for asynchronous notifications</li> </ul>"},{"location":"a2a-server/#quickstart","title":"Quickstart","text":""},{"location":"a2a-server/#1-create-agentcard","title":"1. Create AgentCard","text":"<p>Define your agent's capabilities and metadata. <pre><code>val agentCard = AgentCard(\n    name = \"IO Assistant\",\n    description = \"AI agent specialized in input modification\",\n    version = \"2.1.0\",\n    protocolVersion = \"0.3.0\",\n\n    // Communication Settings\n    url = \"https://api.example.com/a2a\",\n    preferredTransport = TransportProtocol.JSONRPC,\n\n    // Capabilities Declaration\n    capabilities =\n        AgentCapabilities(\n            streaming = true,              // Support real-time responses\n            pushNotifications = true,      // Send async notifications\n            stateTransitionHistory = true  // Maintain task history\n        ),\n\n    // Content Type Support\n    defaultInputModes = listOf(\"text/plain\", \"text/markdown\", \"image/jpeg\"),\n    defaultOutputModes = listOf(\"text/plain\", \"text/markdown\", \"application/json\"),\n\n    // Skills/Capabilities\n    skills = listOf(\n        AgentSkill(\n            id = \"echo\",\n            name = \"echo\",\n            description = \"Echoes back user messages\",\n            tags = listOf(\"io\"),\n        )\n    )\n)\n</code></pre></p>"},{"location":"a2a-server/#2-create-an-agentexecutor","title":"2. Create an AgentExecutor","text":"<p>In executor manages implement agent logic, handles incoming requests and sends responses.</p> <pre><code>class EchoAgentExecutor : AgentExecutor {\n    override suspend fun execute(\n        context: RequestContext&lt;MessageSendParams&gt;,\n        eventProcessor: SessionEventProcessor\n    ) {\n        val userMessage = context.params.message\n        val userText = userMessage.parts\n            .filterIsInstance&lt;TextPart&gt;()\n            .joinToString(\" \") { it.text }\n\n        // Echo the user's message back\n        val response = Message(\n            messageId = UUID.randomUUID().toString(),\n            role = Role.Agent,\n            parts = listOf(TextPart(\"You said: $userText\")),\n            contextId = context.contextId,\n            taskId = context.taskId\n        )\n\n        eventProcessor.sendMessage(response)\n    }\n}\n</code></pre>"},{"location":"a2a-server/#2-create-the-server","title":"2. Create the Server","text":"<p>Pass the agent executor and agent card to the server.</p> <pre><code>val server = A2AServer(\n    agentExecutor = EchoAgentExecutor(),\n    agentCard = agentCard\n)\n</code></pre>"},{"location":"a2a-server/#3-add-transport-layer","title":"3. Add Transport Layer","text":"<p>Create a transport layer and start the server. <pre><code>// HTTP JSON-RPC transport\nval transport = HttpJSONRPCServerTransport(server)\ntransport.start(\n    engineFactory = CIO,\n    port = 8080,\n    path = \"/agent\",\n    wait = true\n)\n</code></pre></p>"},{"location":"a2a-server/#agent-implementation-patterns","title":"Agent Implementation Patterns","text":""},{"location":"a2a-server/#simple-response-agent","title":"Simple Response Agent","text":"<p>If your agent only needs to respond to a single message, you can implement it as a simple agent.  It can be also used if agent execution logic is not complex and time-consuming.</p> <pre><code>class SimpleAgentExecutor : AgentExecutor {\n    override suspend fun execute(\n        context: RequestContext&lt;MessageSendParams&gt;,\n        eventProcessor: SessionEventProcessor\n    ) {\n        val response = Message(\n            messageId = UUID.randomUUID().toString(),\n            role = Role.Agent,\n            parts = listOf(TextPart(\"Hello from agent!\")),\n            contextId = context.contextId,\n            taskId = context.taskId\n        )\n\n        eventProcessor.sendMessage(response)\n    }\n}\n</code></pre>"},{"location":"a2a-server/#task-based-agent","title":"Task-Based Agent","text":"<p>If the execution logic of your agent is complex and requires multiple steps, you can implement it as a task-based agent. It can be also used if agent execution logic is time-consuming and suspending. <pre><code>class TaskAgentExecutor : AgentExecutor {\n    override suspend fun execute(\n        context: RequestContext&lt;MessageSendParams&gt;,\n        eventProcessor: SessionEventProcessor\n    ) {\n        // Send working status\n        eventProcessor.sendTaskEvent(\n            TaskStatusUpdateEvent(\n                contextId = context.contextId,\n                taskId = context.taskId,\n                status = TaskStatus(\n                    state = TaskState.Working,\n                    timestamp = Clock.System.now()\n                ),\n                final = false\n            )\n        )\n\n        // Do work...\n\n        // Send completion\n        eventProcessor.sendTaskEvent(\n            TaskStatusUpdateEvent(\n                contextId = context.contextId,\n                taskId = context.taskId,\n                status = TaskStatus(\n                    state = TaskState.Completed,\n                    timestamp = Clock.System.now()\n                ),\n                final = true\n            )\n        )\n    }\n}\n</code></pre></p>"},{"location":"act-ai-agent/","title":"FunctionalAIAgent: How to build a single\u2011run agent step by step","text":"<p>FunctionalAIAgent is a lightweight, non\u2011graph agent that you control with a simple loop. Use it when you want to: - Call an LLM once or a few times in a custom loop; - Optionally call tools between LLM turns; - Return a final value (string, data class, etc.) without building a full strategy graph.</p> <p>What you\u2019ll do in this guide: 1) Create a \u201cHello, World\u201d FunctionalAIAgent. 2) Add a tool and let the agent call it. 3) Add a feature (event handler) to observe behavior. 4) Keep context under control with history compression. 5) Learn common recipes, pitfalls, and FAQs.</p>"},{"location":"act-ai-agent/#1-prerequisites","title":"1) Prerequisites","text":"<p>You need a PromptExecutor (the object that actually talks to your LLM). For local experimenting, you can use the Ollama executor:</p> <pre><code>val exec = simpleOllamaAIExecutor()\n</code></pre> <p>You also need to pick a model, for example:</p> <pre><code>val model = OllamaModels.Meta.LLAMA_3_2\n</code></pre> <p>That\u2019s it \u2014 we\u2019ll inject both into the agent factory.</p>"},{"location":"act-ai-agent/#2-your-first-agent-hello-world","title":"2) Your first agent (Hello, World)","text":"<p>Goal: Send the user\u2019s text to the LLM and return a single assistant message as a string.</p> <pre><code>val agent = functionalAIAgent&lt;String, String&gt;(\n    prompt = \"You are a helpful assistant.\",\n    promptExecutor = exec,\n    model = model\n) { input -&gt;\n    val responses = requestLLMMultiple(input)\n    responses.single().asAssistantMessage().content\n}\n\nval result = agent.run(\"Say hi in one sentence\")\nprintln(result)\n</code></pre> <p>What happens? - requestLLMMultiple(input) sends the user input and receives one or more assistant messages. - We return the only message\u2019s content (typical one\u2011shot flow).</p> <p>Tip: If you want to return structured data, parse the content or use the Structured Data API.</p>"},{"location":"act-ai-agent/#3-add-tools-how-the-agent-calls-your-functions","title":"3) Add tools (how the agent calls your functions)","text":"<p>Goal: Let the model operate a tiny device via tools.</p> <pre><code>class Switch {\n    private var on = false\n    fun on() { on = true }\n    fun off() { on = false }\n    fun isOn() = on\n}\n\nclass SwitchTools(private val sw: Switch) {\n    fun turn_on() = run { sw.on(); \"ok\" }\n    fun turn_off() = run { sw.off(); \"ok\" }\n    fun state() = if (sw.isOn()) \"on\" else \"off\"\n}\n\nval sw = Switch()\nval tools = ToolRegistry { tools(SwitchTools(sw).asTools()) }\n\nval toolAgent = functionalAIAgent&lt;String, String&gt;(\n    prompt = \"You're responsible for running a Switch device and perform operations on it by request.\",\n    promptExecutor = exec,\n    model = model,\n    toolRegistry = tools\n) { input -&gt;\n    var responses = requestLLMMultiple(input)\n\n    while (responses.containsToolCalls()) {\n        val pending = extractToolCalls(responses)\n        val results = executeMultipleTools(pending)\n        responses = sendMultipleToolResults(results)\n    }\n\n    responses.single().asAssistantMessage().content\n}\n\nval out = toolAgent.run(\"Turn switch on\")\nprintln(out)\nprintln(\"Switch is ${if (sw.isOn()) \"on\" else \"off\"}\")\n</code></pre> <p>How it works - containsToolCalls() detects tool call messages from the LLM. - extractToolCalls(...) reads which tools to run and with what args. - executeMultipleTools(...) runs them against your ToolRegistry. - sendMultipleToolResults(...) sends results back to the LLM and gets the next response.</p>"},{"location":"act-ai-agent/#4-observe-behavior-with-features-eventhandler","title":"4) Observe behavior with features (EventHandler)","text":"<p>Goal: Print every tool call to the console.</p> <pre><code>val observed = functionalAIAgent&lt;String, String&gt;(\n    prompt = \"...\",\n    promptExecutor = exec,\n    model = model,\n    toolRegistry = tools,\n    featureContext = {\n        install(EventHandler) {\n            onToolCallStarting { e -&gt; println(\"Tool called: ${'$'}{e.tool.name}, args: ${'$'}{e.toolArgs}\") }\n        }\n    }\n) { input -&gt;\n    var responses = requestLLMMultiple(input)\n    while (responses.containsToolCalls()) {\n        val pending = extractToolCalls(responses)\n        val results = executeMultipleTools(pending)\n        responses = sendMultipleToolResults(results)\n    }\n    responses.single().asAssistantMessage().content\n}\n</code></pre> <p>Other features you can install this way include streaming tokens and tracing; see the related docs in the sidebar.</p>"},{"location":"act-ai-agent/#5-keep-context-under-control-history-compression","title":"5) Keep context under control (history compression)","text":"<p>Long conversations can exceed the model\u2019s context window. Use the token usage to decide when to compress history:</p> <pre><code>var responses = requestLLMMultiple(input)\n\nwhile (responses.containsToolCalls()) {\n    if (latestTokenUsage() &gt; 100_000) {\n        compressHistory()\n    }\n    val pending = extractToolCalls(responses)\n    val results = executeMultipleTools(pending)\n    responses = sendMultipleToolResults(results)\n}\n</code></pre> <p>Use a threshold appropriate for your model and prompt size.</p>"},{"location":"act-ai-agent/#common-recipes","title":"Common recipes","text":"<ul> <li>Return structured output</li> <li>Ask the LLM to format JSON and parse it; or use Structured Data API.</li> <li>Validate tool inputs</li> <li>Perform validation in tool functions and return clear error messages.</li> <li>One agent instance per request</li> <li>Each agent instance is single\u2011run at a time. Create new instances if you need concurrency.</li> <li>Custom Output type</li> <li>Change functionalAIAgent and return a data class from the loop."},{"location":"act-ai-agent/#troubleshooting-pitfalls","title":"Troubleshooting &amp; pitfalls","text":"<ul> <li>\u201cAgent is already running\u201d</li> <li>FunctionalAIAgent prevents concurrent runs on the same instance. Don\u2019t share one instance across parallel coroutines; create a fresh agent per run or await completion.</li> <li>Empty or unexpected model output</li> <li>Check your system prompt. Print intermediate responses. Consider adding few\u2011shot examples.</li> <li>Loop never ends</li> <li>Ensure you break when there are no tool calls; add guards/timeouts for safety.</li> <li>Context overflows</li> <li>Watch latestTokenUsage() and call compressHistory().</li> </ul>"},{"location":"act-ai-agent/#reference-quick","title":"Reference (quick)","text":"<p>Constructors</p> <pre><code>fun &lt;Input, Output&gt; functionalAIAgent(\n    promptExecutor: PromptExecutor,\n    agentConfig: AIAgentConfigBase,\n    toolRegistry: ToolRegistry = ToolRegistry.EMPTY,\n    loop: suspend AIAgentFunctionalContext.(input: Input) -&gt; Output\n): AIAgent&lt;Input, Output&gt;\n\nfun &lt;Input, Output&gt; functionalAIAgent(\n    promptExecutor: PromptExecutor,\n    toolRegistry: ToolRegistry = ToolRegistry.EMPTY,\n    prompt: String = \"\",\n    model: LLModel = OpenAIModels.Chat.GPT4o,\n    featureContext: FeatureContext.() -&gt; Unit = {},\n    func: suspend AIAgentFunctionalContext.(input: Input) -&gt; Output,\n): AIAgent&lt;Input, Output&gt;\n</code></pre> <p>Important types - FunctionalAIAgent - AIAgentFunctionalContext - AIAgentConfig / AIAgentConfigBase - PromptExecutor - ToolRegistry - FeatureContext and feature interfaces <p>See source: agents/agents-core/src/commonMain/kotlin/ai/koog/agents/core/agent/FunctionalAIAgent.kt</p>"},{"location":"agent-client-protocol/","title":"Agent Client Protocol","text":"<p>Agent Client Protocol (ACP) is a standardized protocol that enables client applications to communicate with AI agents through a consistent, bidirectional interface.</p> <p>ACP provides a structured way for agents to interact with clients, supporting real-time event streaming, tool call notifications, and session lifecycle management.</p> <p>The Koog framework provides integration with ACP, enabling you to build ACP-compliant agents that can communicate with standardized client applications.</p> <p>To learn more about the protocol, see the Agent Client Protocol documentation.</p>"},{"location":"agent-client-protocol/#integration-with-koog","title":"Integration with Koog","text":"<p>The Koog framework integrates with ACP using the ACP Kotlin SDK with additional API extensions in the <code>agents-features-acp</code> module.</p> <p>This integration lets Koog agents perform the following:</p> <ul> <li>Communicate with ACP-compliant client applications</li> <li>Send real-time updates about agent execution (tool calls, thoughts, completions)</li> <li>Handle standard ACP events and notifications automatically</li> <li>Convert between Koog message formats and ACP content blocks</li> </ul>"},{"location":"agent-client-protocol/#key-components","title":"Key components","text":"<p>Here are the main components of the ACP integration in Koog:</p> Component Description <code>AcpAgent</code> The main feature that enables communication between Koog agents and ACP clients. <code>MessageConverters</code> Utilities for converting messages between Koog and ACP formats. <code>AcpConfig</code> Configuration class for the AcpAgent feature."},{"location":"agent-client-protocol/#getting-started","title":"Getting started","text":"<p>ACP dependencies are not included by default in the <code>koog-agents</code> meta-dependency. You must explicitly add the ACP module to your project.</p>"},{"location":"agent-client-protocol/#dependencies","title":"Dependencies","text":"<p>To use ACP in your project, add the following dependency:</p> <pre><code>dependencies {\n    implementation(\"ai.koog:agents-features-acp:$koogVersion\")\n}\n</code></pre>"},{"location":"agent-client-protocol/#1-implement-acp-agent-support","title":"1. Implement ACP agent support","text":"<p>Koog ACP integration is based on Kotlin ACP SDK.  The SDK provides an <code>AgentSupport</code> and <code>AgentSession</code> interface that you need to implement to implement in order to connect your agent to ACP clients. The <code>AgentSupport</code> manages the agent sessions creation and loading. The interface implementation is almost the same for all agents, we'll provide an example implementation further. The <code>AgentSession</code> manages the agent instantiation, invocation and controls runtime. Inside the <code>prompt</code> method you will define and run the Koog agent.</p> <p>To use ACP with Koog, you need to implement the <code>AgentSupport</code> and <code>AgentSession</code> interfaces from the ACP SDK:</p> <pre><code>// Implement AgentSession to manage the lifecycle of a Koog agent\nclass KoogAgentSession(\n    override val sessionId: SessionId,\n    private val promptExecutor: PromptExecutor,\n    private val protocol: Protocol,\n    private val clock: Clock,\n) : AgentSession {\n\n    private var agentJob: Deferred&lt;Unit&gt;? = null\n    private val agentMutex = Mutex()\n\n    override suspend fun prompt(\n        content: List&lt;ContentBlock&gt;,\n        _meta: JsonElement?\n    ): Flow&lt;Event&gt; = channelFlow {\n        val agentConfig = AIAgentConfig(\n            prompt = prompt(\"acp\") {\n                system(\"You are a helpful assistant.\")\n            }.appendPrompt(content),\n            model = OpenAIModels.Chat.GPT4o,\n            maxAgentIterations = 1000\n        )\n\n        agentMutex.withLock {\n            val agent = AIAgent(\n                promptExecutor = promptExecutor,\n                agentConfig = agentConfig,\n                strategy = myStrategy()\n            ) {\n                install(AcpAgent) {\n                    this.sessionId = this@KoogAgentSession.sessionId.value\n                    this.protocol = this@KoogAgentSession.protocol\n                    this.eventsProducer = this@channelFlow\n                    this.setDefaultNotifications = true\n                }\n            }\n\n            agentJob = async { agent.run(Unit) }\n            agentJob?.await()\n        }\n    }\n\n    private fun Prompt.appendPrompt(content: List&lt;ContentBlock&gt;): Prompt {\n        return withMessages { messages -&gt;\n            messages + listOf(content.toKoogMessage(clock))\n        }\n    }\n\n    private fun myStrategy() = strategy&lt;Unit, Unit&gt;(\"\") {\n        // Define your strategy here\n    }    \n    override suspend fun cancel() {\n        agentJob?.cancel()\n    }\n}\n</code></pre>"},{"location":"agent-client-protocol/#2-configure-the-acpagent-feature","title":"2. Configure the AcpAgent feature","text":"<p>The <code>AcpAgent</code> feature can be configured through <code>AcpConfig</code>:</p> <pre><code>val agent = AIAgent(\n    promptExecutor = promptExecutor,\n    agentConfig = agentConfig,\n    strategy = myStrategy()\n) {\n    install(AcpAgent) {\n        // Required: The unique session identifier for the ACP connection\n        this.sessionId = sessionIdValue\n\n        // Required: The protocol instance used for sending requests and notifications\n        this.protocol = protocol\n\n        // Required: A coroutine-based producer scope for sending events\n        this.eventsProducer = this@channelFlow\n\n        // Optional: Whether to register default notification handlers (default: true)\n        this.setDefaultNotifications = true\n    }\n}\n</code></pre> <p>Key configuration options:</p> <ul> <li><code>sessionId</code>: The unique session identifier for the ACP connection</li> <li><code>protocol</code>: The protocol instance used for sending requests and notifications to ACP clients</li> <li><code>eventsProducer</code>: A coroutine-based producer scope for sending events</li> <li><code>setDefaultNotifications</code>: Whether to register default notification handlers (default: <code>true</code>)</li> </ul>"},{"location":"agent-client-protocol/#3-handle-incoming-prompts","title":"3. Handle incoming prompts","text":"<p>Convert ACP content blocks to Koog messages using the provided extension functions:</p> <pre><code>// Convert ACP content blocks to Koog message\nval koogMessage = acpContent.toKoogMessage(clock)\n\n// Append to existing prompt\nfun Prompt.appendPrompt(content: List&lt;ContentBlock&gt;): Prompt {\n    return withMessages { messages -&gt;\n        messages + listOf(content.toKoogMessage(clock))\n    }\n}\n</code></pre>"},{"location":"agent-client-protocol/#default-notification-handlers","title":"Default notification handlers","text":"<p>When <code>setDefaultNotifications</code> is enabled, the AcpAgent feature automatically handles:</p> <ol> <li>Agent Completion: Sends <code>PromptResponseEvent</code> with <code>StopReason.END_TURN</code> when the agent completes successfully</li> <li>Agent Execution Failures: Sends <code>PromptResponseEvent</code> with appropriate stop reasons:<ul> <li><code>StopReason.MAX_TURN_REQUESTS</code> for max iterations exceeded</li> <li><code>StopReason.REFUSAL</code> for other execution failures</li> </ul> </li> <li>LLM Responses: Converts and sends LLM responses as ACP events (text, tool calls, reasoning)</li> <li>Tool Call Lifecycle: Reports tool call status changes:<ul> <li><code>ToolCallStatus.IN_PROGRESS</code> when a tool call starts</li> <li><code>ToolCallStatus.COMPLETED</code> when a tool call succeeds</li> <li><code>ToolCallStatus.FAILED</code> when a tool call fails</li> </ul> </li> </ol>"},{"location":"agent-client-protocol/#sending-custom-events","title":"Sending custom events","text":"<p>You can send custom events to the ACP client using the <code>sendEvent</code> method:</p> <pre><code>// Access the ACP feature and send custom events\nwithAcpAgent {\n    sendEvent(\n        Event.SessionUpdateEvent(\n            SessionUpdate.PlanUpdate(plan.entries)\n        )\n    )\n}\n</code></pre> <p>Moreover, you can use <code>protocol</code> inside <code>withAcpAgent</code> and send custom notifications or requests:</p> <pre><code>// Access the ACP feature and send custom events\nwithAcpAgent {\n    protocol.sendRequest&lt;AuthenticateRequest, AuthenticateResponse&gt;(\n        AcpMethod.AgentMethods.Authenticate,\n        AuthenticateRequest(methodId = AuthMethodId(\"Google\"))\n    )\n}\n</code></pre>"},{"location":"agent-client-protocol/#message-conversion","title":"Message conversion","text":"<p>The module provides utilities for converting between Koog and ACP message formats:</p>"},{"location":"agent-client-protocol/#acp-to-koog","title":"ACP to Koog","text":"<pre><code>// Convert ACP content blocks to Koog message\nval koogMessage = acpContentBlocks.toKoogMessage(clock)\n\n// Convert single ACP content block to Koog content part\nval contentPart = acpContentBlock.toKoogContentPart()\n</code></pre>"},{"location":"agent-client-protocol/#koog-to-acp","title":"Koog to ACP","text":"<pre><code>// Convert Koog response message to ACP events\nval acpEvents = koogResponseMessage.toAcpEvents()\n\n// Convert Koog content part to ACP content block\nval acpContentBlock = koogContentPart.toAcpContentBlock()\n</code></pre>"},{"location":"agent-client-protocol/#important-notes","title":"Important notes","text":""},{"location":"agent-client-protocol/#use-channelflow-for-event-streaming","title":"Use channelFlow for event streaming","text":"<p>Use <code>channelFlow</code> to allow sending events from different coroutines:</p> <pre><code>override suspend fun prompt(\n    content: List&lt;ContentBlock&gt;,\n    _meta: JsonElement?\n): Flow&lt;Event&gt; = channelFlow {\n    // Install AcpAgent with this@channelFlow as eventsProducer\n}\n</code></pre>"},{"location":"agent-client-protocol/#synchronize-agent-execution","title":"Synchronize agent execution","text":"<p>Use a mutex to synchronize access to the agent instance, as the protocol should not trigger new execution until the previous one is finished:</p> <pre><code>private val agentMutex = Mutex()\n\nagentMutex.withLock {\n    // Create and run agent\n}\n</code></pre>"},{"location":"agent-client-protocol/#manual-notification-handling","title":"Manual notification handling","text":"<p>If you need custom notification handling, set <code>setDefaultNotifications = false</code> and process all agent events according to the specification:</p> <pre><code>install(AcpAgent) {\n    this.setDefaultNotifications = false\n    // Implement custom event handling\n}\n</code></pre>"},{"location":"agent-client-protocol/#platform-support","title":"Platform support","text":"<p>The ACP feature is currently available only on the JVM platform, as it depends on the ACP Kotlin SDK which is JVM-specific.</p>"},{"location":"agent-client-protocol/#usage-examples","title":"Usage examples","text":"<p>Complete working examples can be found in the Koog repository.</p>"},{"location":"agent-client-protocol/#running-the-example","title":"Running the example","text":"<ol> <li> <p>Run the ACP example application: <pre><code>./gradlew :examples:simple-examples:run\n</code></pre></p> </li> <li> <p>Enter a request for the ACP agent: <pre><code>Move file `my-file.md` to folder `my-folder` and append title '## My File' to the file content\n</code></pre></p> </li> <li> <p>Observe the event traces in the console showing the agent's execution, tool calls, and completion status.</p> </li> </ol>"},{"location":"agent-event-handlers/","title":"Event handlers","text":"<p>You can monitor and respond to specific events during the agent workflow by using event handlers for logging, testing, debugging, and extending agent behavior.</p>"},{"location":"agent-event-handlers/#feature-overview","title":"Feature overview","text":"<p>The EventHandler feature lets you hook into various agent events. It serves as an event delegation mechanism that:</p> <ul> <li>Manages the lifecycle of AI agent operations.</li> <li>Provides hooks for monitoring and responding to different stages of the workflow.</li> <li>Enables error handling and recovery.</li> <li>Facilitates tool invocation tracking and result processing.</li> </ul>"},{"location":"agent-event-handlers/#installation-and-configuration","title":"Installation and configuration","text":"<p>The EventHandler feature integrates with the agent workflow through the <code>EventHandler</code> class, which provides a way to register callbacks for different agent events, and can be installed as a feature in the agent configuration. For details, see API reference.</p> <p>To install the feature and configure event handlers for the agent, do the following:</p> <pre><code>handleEvents {\n    // Handle tool calls\n    onToolCallStarting { eventContext -&gt;\n        println(\"Tool called: ${eventContext.toolName} with args ${eventContext.toolArgs}\")\n    }\n    // Handle event triggered when the agent completes its execution\n    onAgentCompleted { eventContext -&gt;\n        println(\"Agent finished with result: ${eventContext.result}\")\n    }\n\n    // Other event handlers\n}\n</code></pre> <p>For more details about event handler configuration, see API reference.</p> <p>You can also set up event handlers using the <code>handleEvents</code> extension function when creating an agent. This function also installs the event handler feature and configures event handlers for the agent. Here is an example:</p> <pre><code>val agent = AIAgent(\n    promptExecutor = simpleOllamaAIExecutor(),\n    llmModel = OllamaModels.Meta.LLAMA_3_2,\n){\n    handleEvents {\n        // Handle tool calls\n        onToolCallStarting { eventContext -&gt;\n            println(\"Tool called: ${eventContext.toolName} with args ${eventContext.toolArgs}\")\n        }\n        // Handle event triggered when the agent completes its execution\n        onAgentCompleted { eventContext -&gt;\n            println(\"Agent finished with result: ${eventContext.result}\")\n        }\n\n        // Other event handlers\n    }\n}\n</code></pre>"},{"location":"agent-events/","title":"Agent events","text":"<p>Agent events are actions or interactions that occur as part of an agent workflow. They include:</p> <ul> <li>Agent lifecycle events</li> <li>Strategy events</li> <li>Node execution events</li> <li>LLM call events</li> <li>LLM streaming events</li> <li>Tool execution events</li> </ul> <p>Note: Feature events are defined in the agents-core module and live under the package <code>ai.koog.agents.core.feature.model.events</code>. Features such as <code>agents-features-trace</code>, and <code>agents-features-event-handler</code> consume these events to process and forward messages created during agent execution.</p>"},{"location":"agent-events/#predefined-event-types","title":"Predefined event types","text":"<p>Koog provides predefined event types that can be used in custom message processors. The predefined events can be classified into several categories, depending on the entity they relate to:</p> <ul> <li>Agent events</li> <li>Strategy events</li> <li>Node events</li> <li>Subgraph events</li> <li>LLM call events</li> <li>LLM streaming events</li> <li>Tool execution events</li> </ul>"},{"location":"agent-events/#agent-events_1","title":"Agent events","text":""},{"location":"agent-events/#agentstartingevent","title":"AgentStartingEvent","text":"<p>Represents the start of an agent run. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>agentId</code> String Yes The unique identifier of the AI agent. <code>runId</code> String Yes The unique identifier of the AI agent run."},{"location":"agent-events/#agentcompletedevent","title":"AgentCompletedEvent","text":"<p>Represents the end of an agent run. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>agentId</code> String Yes The unique identifier of the AI agent. <code>runId</code> String Yes The unique identifier of the AI agent run. <code>result</code> String Yes The result of the agent run. Can be <code>null</code> if there is no result."},{"location":"agent-events/#agentexecutionfailedevent","title":"AgentExecutionFailedEvent","text":"<p>Represents the occurrence of an error during an agent run. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>agentId</code> String Yes The unique identifier of the AI agent. <code>runId</code> String Yes The unique identifier of the AI agent run. <code>error</code> AIAgentError Yes The specific error that occurred during the agent run. For more information, see AIAgentError."},{"location":"agent-events/#agentclosingevent","title":"AgentClosingEvent","text":"<p>Represents the closure or termination of an agent. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>agentId</code> String Yes The unique identifier of the AI agent. <p> The <code>AIAgentError</code> class provides more details about an error that occurred during an agent run. Includes the following fields:</p> Name Data type Required Default Description <code>message</code> String Yes The message that provides more details about the specific error. <code>stackTrace</code> String Yes The collection of stack records until the last executed code. <code>cause</code> String No null The cause of the error, if available. <p> The <code>AgentExecutionInfo</code> class provides contextual information about the execution path, enabling tracking of nested execution contexts within an agent run. Includes the following fields:</p> Name Data type Required Default Description <code>parent</code> AgentExecutionInfo No null Reference to the parent execution context. If null, this represents the root execution level. <code>partName</code> String Yes A string representing the name of the current part or segment of the execution."},{"location":"agent-events/#strategy-events","title":"Strategy events","text":""},{"location":"agent-events/#graphstrategystartingevent","title":"GraphStrategyStartingEvent","text":"<p>Represents the start of a graph-based strategy run. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the strategy run. <code>strategyName</code> String Yes The name of the strategy. <code>graph</code> StrategyEventGraph Yes The graph structure representing the strategy workflow."},{"location":"agent-events/#functionalstrategystartingevent","title":"FunctionalStrategyStartingEvent","text":"<p>Represents the start of a functional strategy run. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the strategy run. <code>strategyName</code> String Yes The name of the strategy."},{"location":"agent-events/#strategycompletedevent","title":"StrategyCompletedEvent","text":"<p>Represents the end of a strategy run. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the strategy run. <code>strategyName</code> String Yes The name of the strategy. <code>result</code> String Yes The result of the run. Can be <code>null</code> if there is no result."},{"location":"agent-events/#node-events","title":"Node events","text":""},{"location":"agent-events/#nodeexecutionstartingevent","title":"NodeExecutionStartingEvent","text":"<p>Represents the start of a node run. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the strategy run. <code>nodeName</code> String Yes The name of the node whose run started. <code>input</code> JsonElement No null The input value for the node."},{"location":"agent-events/#nodeexecutioncompletedevent","title":"NodeExecutionCompletedEvent","text":"<p>Represents the end of a node run. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the strategy run. <code>nodeName</code> String Yes The name of the node whose run ended. <code>input</code> JsonElement No null The input value for the node. <code>output</code> JsonElement No null The output value produced by the node."},{"location":"agent-events/#nodeexecutionfailedevent","title":"NodeExecutionFailedEvent","text":"<p>Represents an error that occurred during a node run. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the strategy run. <code>nodeName</code> String Yes The name of the node where the error occurred. <code>input</code> JsonElement No null The input data provided to the node. <code>error</code> AIAgentError Yes The specific error that occurred during the node run. For more information, see AIAgentError."},{"location":"agent-events/#subgraph-events","title":"Subgraph events","text":""},{"location":"agent-events/#subgraphexecutionstartingevent","title":"SubgraphExecutionStartingEvent","text":"<p>Represents the start of a subgraph run. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the strategy run. <code>subgraphName</code> String Yes The name of the subgraph whose run started. <code>input</code> JsonElement No null The input value for the subgraph."},{"location":"agent-events/#subgraphexecutioncompletedevent","title":"SubgraphExecutionCompletedEvent","text":"<p>Represents the end of a subgraph run. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the strategy run. <code>subgraphName</code> String Yes The name of the subgraph whose run ended. <code>input</code> JsonElement No null The input value for the subgraph. <code>output</code> JsonElement No null The output value produced by the subgraph."},{"location":"agent-events/#subgraphexecutionfailedevent","title":"SubgraphExecutionFailedEvent","text":"<p>Represents an error that occurred during a subgraph run. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the strategy run. <code>subgraphName</code> String Yes The name of the subgraph where the error occurred. <code>input</code> JsonElement No null The input data provided to the subgraph. <code>error</code> AIAgentError Yes The specific error that occurred during the subgraph run. For more information, see AIAgentError."},{"location":"agent-events/#llm-call-events","title":"LLM call events","text":""},{"location":"agent-events/#llmcallstartingevent","title":"LLMCallStartingEvent","text":"<p>Represents the start of an LLM call. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the LLM run. <code>prompt</code> Prompt Yes The prompt that is sent to the model. For more information, see Prompt. <code>model</code> ModelInfo Yes The model information. See ModelInfo. <code>tools</code> List Yes The list of tools that the model can call. <p> The <code>Prompt</code> class represents a data structure for a prompt, consisting of a list of messages, a unique identifier, and optional parameters for language model settings. Includes the following fields:</p> Name Data type Required Default Description <code>messages</code> List Yes The list of messages that the prompt consists of. <code>id</code> String Yes The unique identifier for the prompt. <code>params</code> LLMParams No LLMParams() The settings that control the way the LLM generates content. <p> The <code>ModelInfo</code> class represents information about a language model, including its provider, model identifier, and characteristics. Includes the following fields:</p> Name Data type Required Default Description <code>provider</code> String Yes The provider identifier (e.g., \"openai\", \"google\", \"anthropic\"). <code>model</code> String Yes The model identifier (e.g., \"gpt-4\", \"claude-3\"). <code>displayName</code> String No null Optional human-readable display name for the model. <code>contextLength</code> Long No null Maximum number of tokens the model can process. <code>maxOutputTokens</code> Long No null Maximum number of tokens the model can generate."},{"location":"agent-events/#llmcallcompletedevent","title":"LLMCallCompletedEvent","text":"<p>Represents the end of an LLM call. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the LLM run. <code>prompt</code> Prompt Yes The prompt used in the call. <code>model</code> ModelInfo Yes The model information. See ModelInfo. <code>responses</code> List Yes One or more responses returned by the model. <code>moderationResponse</code> ModerationResult No null The moderation response, if any."},{"location":"agent-events/#llm-streaming-events","title":"LLM streaming events","text":""},{"location":"agent-events/#llmstreamingstartingevent","title":"LLMStreamingStartingEvent","text":"<p>Represents the start of an LLM streaming call. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the LLM run. <code>prompt</code> Prompt Yes The prompt that is sent to the model. <code>model</code> ModelInfo Yes The model information. See ModelInfo. <code>tools</code> List Yes The list of tools that the model can call."},{"location":"agent-events/#llmstreamingframereceivedevent","title":"LLMStreamingFrameReceivedEvent","text":"<p>Represents a streaming frame received from the LLM. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the LLM run. <code>prompt</code> Prompt Yes The prompt that is sent to the model. <code>model</code> ModelInfo Yes The model information. See ModelInfo. <code>frame</code> StreamFrame Yes The frame received from the stream."},{"location":"agent-events/#llmstreamingfailedevent","title":"LLMStreamingFailedEvent","text":"<p>Represents the occurrence of an error during an LLM streaming call. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the LLM run. <code>prompt</code> Prompt Yes The prompt that is sent to the model. <code>model</code> ModelInfo Yes The model information. See ModelInfo. <code>error</code> AIAgentError Yes The specific error that occurred during streaming. For more information, see AIAgentError."},{"location":"agent-events/#llmstreamingcompletedevent","title":"LLMStreamingCompletedEvent","text":"<p>Represents the end of an LLM streaming call. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the LLM run. <code>prompt</code> Prompt Yes The prompt that is sent to the model. <code>model</code> ModelInfo Yes The model information. See ModelInfo. <code>tools</code> List Yes The list of tools that the model can call."},{"location":"agent-events/#tool-execution-events","title":"Tool execution events","text":""},{"location":"agent-events/#toolcallstartingevent","title":"ToolCallStartingEvent","text":"<p>Represents the event of a model calling a tool. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the strategy/agent run. <code>toolCallId</code> String No null The identifier of the tool call, if available. <code>toolName</code> String Yes The name of the tool. <code>toolArgs</code> JsonObject Yes The arguments that are provided to the tool."},{"location":"agent-events/#toolvalidationfailedevent","title":"ToolValidationFailedEvent","text":"<p>Represents the occurrence of a validation error during a tool call. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the strategy/agent run. <code>toolCallId</code> String No null The identifier of the tool call, if available. <code>toolName</code> String Yes The name of the tool for which validation failed. <code>toolArgs</code> JsonObject Yes The arguments that are provided to the tool. <code>toolDescription</code> String No null A description of the tool that encountered the validation error. <code>message</code> String No null A message describing the validation error. <code>error</code> AIAgentError Yes The specific error that occurred. For more information, see AIAgentError."},{"location":"agent-events/#toolcallfailedevent","title":"ToolCallFailedEvent","text":"<p>Represents a failure to execute a tool. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the strategy/agent run. <code>toolCallId</code> String No null The identifier of the tool call, if available. <code>toolName</code> String Yes The name of the tool. <code>toolArgs</code> JsonObject Yes The arguments that are provided to the tool. <code>toolDescription</code> String No null A description of the tool that failed. <code>error</code> AIAgentError Yes The specific error that occurred when trying to call a tool. For more information, see AIAgentError."},{"location":"agent-events/#toolcallcompletedevent","title":"ToolCallCompletedEvent","text":"<p>Represents a successful tool call with the return of a result. Includes the following fields:</p> Name Data type Required Default Description <code>eventId</code> String Yes A unique identifier for the event or a group of events. <code>executionInfo</code> AgentExecutionInfo Yes Provides contextual information about the execution associated with this event. <code>runId</code> String Yes The unique identifier of the run. <code>toolCallId</code> String No null The identifier of the tool call. <code>toolName</code> String Yes The name of the tool. <code>toolArgs</code> JsonObject Yes The arguments provided to the tool. <code>toolDescription</code> String No null A description of the tool that was executed. <code>result</code> JsonElement No null The result of the tool call."},{"location":"agent-events/#faq-and-troubleshooting","title":"FAQ and troubleshooting","text":"<p>The following section includes commonly asked questions and answers related to the Tracing feature.</p>"},{"location":"agent-events/#how-do-i-trace-only-specific-parts-of-my-agents-execution","title":"How do I trace only specific parts of my agent's execution?","text":"<p>Use the <code>messageFilter</code> property to filter events. For example, to trace only node execution:</p> <pre><code>install(Tracing) {\n    val fileWriter = TraceFeatureMessageFileWriter(\n        outputPath, \n        { path: Path -&gt; SystemFileSystem.sink(path).buffered() }\n    )\n    addMessageProcessor(fileWriter)\n\n    // Only trace LLM calls\n    fileWriter.setMessageFilter { message -&gt;\n        message is LLMCallStartingEvent || message is LLMCallCompletedEvent\n    }\n}\n</code></pre>"},{"location":"agent-events/#can-i-use-multiple-message-processors","title":"Can I use multiple message processors?","text":"<p>Yes, you can add multiple message processors to trace to different destinations simultaneously:</p> <pre><code>install(Tracing) {\n    addMessageProcessor(TraceFeatureMessageLogWriter(logger))\n    addMessageProcessor(TraceFeatureMessageFileWriter(outputPath, syncOpener))\n    addMessageProcessor(TraceFeatureMessageRemoteWriter(connectionConfig))\n}\n</code></pre>"},{"location":"agent-events/#how-can-i-create-a-custom-message-processor","title":"How can I create a custom message processor?","text":"<p>Implement the <code>FeatureMessageProcessor</code> interface:</p> <pre><code>class CustomTraceProcessor : FeatureMessageProcessor() {\n\n    // Current open state of the processor\n    private var _isOpen = MutableStateFlow(false)\n\n    override val isOpen: StateFlow&lt;Boolean&gt;\n        get() = _isOpen.asStateFlow()\n\n    override suspend fun processMessage(message: FeatureMessage) {\n        // Custom processing logic\n        when (message) {\n            is NodeExecutionStartingEvent -&gt; {\n                // Process node start event\n            }\n\n            is LLMCallCompletedEvent -&gt; {\n                // Process LLM call end event \n            }\n            // Handle other event types \n        }\n    }\n\n    override suspend fun close() {\n        // Close connections of established\n    }\n}\n\n// Use your custom processor\ninstall(Tracing) {\n    addMessageProcessor(CustomTraceProcessor())\n}\n</code></pre> <p>For more information about existing event types that can be handled by message processors, see Predefined event types.</p>"},{"location":"agent-memory/","title":"Memory","text":""},{"location":"agent-memory/#feature-overview","title":"Feature overview","text":"<p>The AgentMemory feature is a component of the Koog framework that lets AI agents store, retrieve, and use information across conversations.</p>"},{"location":"agent-memory/#purpose","title":"Purpose","text":"<p>The AgentMemory Feature addresses the challenge of maintaining context in AI agent interactions by:</p> <ul> <li>Storing important facts extracted from conversations.</li> <li>Organizing information by concepts, subjects, and scopes.</li> <li>Retrieving relevant information when needed in future interactions.</li> <li>Enabling personalization based on user preferences and history.</li> </ul>"},{"location":"agent-memory/#architecture","title":"Architecture","text":"<p>The AgentMemory feature is built on a hierarchical structure. The elements of the structure are listed and explained in the sections below.</p>"},{"location":"agent-memory/#facts","title":"Facts","text":"<p>Facts are individual pieces of information stored in the memory.  Facts represent actual stored information. There are two types of facts:</p> <ul> <li>SingleFact: a single value associated with a concept. For example, an IDE user's current preferred theme:</li> </ul> <pre><code>// Storing favorite IDE theme (single value)\nval themeFact = SingleFact(\n    concept = Concept(\n        \"ide-theme\", \n        \"User's preferred IDE theme\", \n        factType = FactType.SINGLE),\n    value = \"Dark Theme\",\n    timestamp = Clock.System.now().toEpochMilliseconds(),\n)\n</code></pre> <ul> <li>MultipleFacts: multiple values associated with a concept. For example, all languages that a user knows:</li> </ul> <pre><code>// Storing programming languages (multiple values)\nval languagesFact = MultipleFacts(\n    concept = Concept(\n        \"programming-languages\",\n        \"Languages the user knows\",\n        factType = FactType.MULTIPLE\n    ),\n    values = listOf(\"Kotlin\", \"Java\", \"Python\"),\n    timestamp = Clock.System.now().toEpochMilliseconds(),\n)\n</code></pre>"},{"location":"agent-memory/#concepts","title":"Concepts","text":"<p>Concepts are categories of information with associated metadata.</p> <ul> <li>Keyword: unique identifier for the concept.</li> <li>Description: detailed explanation of what the concept represents.</li> <li>FactType: whether the concept stores single or multiple facts (<code>FactType.SINGLE</code> or <code>FactType.MULTIPLE</code>).</li> </ul>"},{"location":"agent-memory/#subjects","title":"Subjects","text":"<p>Subjects are entities that facts can be associated with.</p> <p>Common examples of subjects include:</p> <ul> <li>User: Personal preferences and settings</li> <li>Environment: Information related to the environment of the application</li> </ul> <p>There is a predefined <code>MemorySubject.Everything</code> that you may use as a default subject for all facts. In addition, you can define your own custom memory subjects by extending the <code>MemorySubject</code> abstract class:</p> <pre><code>object MemorySubjects {\n    /**\n     * Information specific to the local machine environment\n     * Examples: Installed tools, SDKs, OS configuration, available commands\n     */\n    @Serializable\n    data object Machine : MemorySubject() {\n        override val name: String = \"machine\"\n        override val promptDescription: String =\n            \"Technical environment (installed tools, package managers, packages, SDKs, OS, etc.)\"\n        override val priorityLevel: Int = 1\n    }\n\n    /**\n     * Information specific to the user\n     * Examples: Conversation preferences, issue history, contact information\n     */\n    @Serializable\n    data object User : MemorySubject() {\n        override val name: String = \"user\"\n        override val promptDescription: String =\n            \"User information (conversation preferences, issue history, contact details, etc.)\"\n        override val priorityLevel: Int = 1\n    }\n}\n</code></pre>"},{"location":"agent-memory/#scopes","title":"Scopes","text":"<p>Memory scopes are contexts in which facts are relevant:</p> <ul> <li>Agent: specific to an agent.</li> <li>Feature: specific to a feature.</li> <li>Product: specific to a product.</li> <li>CrossProduct: relevant across multiple products.</li> </ul>"},{"location":"agent-memory/#configuration-and-initialization","title":"Configuration and initialization","text":"<p>The feature integrates with the agent pipeline through the <code>AgentMemory</code> class, which provides methods for saving and loading facts, and can be installed as a feature in the agent configuration.</p>"},{"location":"agent-memory/#configuration","title":"Configuration","text":"<p>The <code>AgentMemory.Config</code> class is the configuration class for the AgentMemory feature.</p> <pre><code>class Config(\n    var memoryProvider: AgentMemoryProvider = NoMemory,\n    var scopesProfile: MemoryScopesProfile = MemoryScopesProfile(),\n\n    var agentName: String,\n    var featureName: String,\n    var organizationName: String,\n    var productName: String\n) : FeatureConfig()\n</code></pre>"},{"location":"agent-memory/#installation","title":"Installation","text":"<p>To install the AgentMemory feature in an agent, follow the pattern provided in the code sample below.</p> <pre><code>val agent = AIAgent(\n    promptExecutor = simpleOllamaAIExecutor(),\n    llmModel = OllamaModels.Meta.LLAMA_3_2,\n) {\n    install(AgentMemory) {\n        memoryProvider = memoryProvider\n        agentName = \"your-agent-name\"\n        featureName = \"your-feature-name\"\n        organizationName = \"your-organization-name\"\n        productName = \"your-product-name\"\n    }\n}\n</code></pre>"},{"location":"agent-memory/#examples-and-quickstarts","title":"Examples and quickstarts","text":""},{"location":"agent-memory/#basic-usage","title":"Basic usage","text":"<p>The following code snippets demonstrate the basic setup of a memory storage and how facts are saved to and loaded from the memory.</p> <p>1) Set up memory storage</p> <pre><code>// Create a memory provider\nval memoryProvider = LocalFileMemoryProvider(\n    config = LocalMemoryConfig(\"customer-support-memory\"),\n    storage = SimpleStorage(JVMFileSystemProvider.ReadWrite),\n    fs = JVMFileSystemProvider.ReadWrite,\n    root = Path(\"path/to/memory/root\")\n)\n</code></pre> <p>2) Store a fact in the memory</p> <pre><code>memoryProvider.save(\n    fact = SingleFact(\n        concept = Concept(\"greeting\", \"User's name\", FactType.SINGLE),\n        value = \"John\",\n        timestamp = Clock.System.now().toEpochMilliseconds(),\n    ),\n    subject = MemorySubjects.User,\n    scope = MemoryScope.Product(\"my-app\"),\n)\n</code></pre> <p>3) Retrieve the fact</p> <pre><code>// Get the stored information\nval greeting = memoryProvider.load(\n    concept = Concept(\"greeting\", \"User's name\", FactType.SINGLE),\n    subject = MemorySubjects.User,\n    scope = MemoryScope.Product(\"my-app\")\n)\nif (greeting.size &gt; 1) {\n    println(\"Memories found: ${greeting.joinToString(\", \")}\")\n} else {\n    println(\"Information not found. First time here?\")\n}\n</code></pre>"},{"location":"agent-memory/#using-memory-nodes","title":"Using memory nodes","text":"<p>The AgentMemory feature provides the following predefined memory nodes that can be used in agent strategies:</p> <ul> <li>nodeLoadAllFactsFromMemory: loads all facts about the subject from the memory for a given concept.</li> <li>nodeLoadFromMemory: loads specific facts from the memory for a given concept.</li> <li>nodeSaveToMemory: saves a fact to the memory.</li> <li>nodeSaveToMemoryAutoDetectFacts: automatically detects and extracts facts from the chat history and saves them to the memory. Uses the LLM to identify concepts.</li> </ul> <p>Here is an example of how nodes can be implemented in an agent strategy:</p> <pre><code>val strategy = strategy(\"example-agent\") {\n    // Node to automatically detect and save facts\n    val detectFacts by nodeSaveToMemoryAutoDetectFacts&lt;Unit&gt;(\n        subjects = listOf(MemorySubjects.User, MemorySubjects.Machine)\n    )\n\n    // Node to load specific facts\n    val loadPreferences by node&lt;Unit, Unit&gt; {\n        withMemory {\n            loadFactsToAgent(\n                llm = llm,\n                concept = Concept(\"user-preference\", \"User's preferred programming language\", FactType.SINGLE),\n                subjects = listOf(MemorySubjects.User)\n            )\n        }\n    }\n\n    // Connect nodes in the strategy\n    edge(nodeStart forwardTo detectFacts)\n    edge(detectFacts forwardTo loadPreferences)\n    edge(loadPreferences forwardTo nodeFinish)\n}\n</code></pre>"},{"location":"agent-memory/#making-memory-secure","title":"Making memory secure","text":"<p>You can use encryption to make sure that sensitive information is protected inside an encrypted storage used by the memory provider.</p> <pre><code>// Simple encrypted storage setup\nval secureStorage = EncryptedStorage(\n    fs = JVMFileSystemProvider.ReadWrite,\n    encryption = Aes256GCMEncryptor(\"your-secret-key\")\n)\n</code></pre>"},{"location":"agent-memory/#example-remembering-user-preferences","title":"Example: Remembering user preferences","text":"<p>Here is an example of how AgentMemory is used in a real-world scenario to remember a user's preference, specifically the user's favorite programming language.</p> <pre><code>memoryProvider.save(\n    fact = SingleFact(\n        concept = Concept(\"preferred-language\", \"What programming language is preferred by the user?\", FactType.SINGLE),\n        value = \"Kotlin\",\n        timestamp = Clock.System.now().toEpochMilliseconds(),\n    ),\n    subject = MemorySubjects.User,\n    scope = MemoryScope.Product(\"my-app\")\n)\n</code></pre>"},{"location":"agent-memory/#advanced-usage","title":"Advanced usage","text":""},{"location":"agent-memory/#custom-nodes-with-memory","title":"Custom nodes with memory","text":"<p>You can also use the memory from the <code>withMemory</code> clause inside any node. The ready-to-use <code>loadFactsToAgent</code> and <code>saveFactsFromHistory</code> higher level abstractions save facts to the history, load facts from it, and update the LLM chat:</p> <pre><code>val loadProjectInfo by node&lt;Unit, Unit&gt; {\n    withMemory {\n        loadFactsToAgent(\n            llm = llm,\n            concept = Concept(\"preferred-language\", \"What programming language is preferred by the user?\", FactType.SINGLE)\n        )\n    }\n}\n\nval saveProjectInfo by node&lt;Unit, Unit&gt; {\n    withMemory {\n        saveFactsFromHistory(\n            llm = llm,\n            concept = Concept(\"preferred-language\", \"What programming language is preferred by the user?\", FactType.SINGLE),\n            subject = MemorySubjects.User,\n            scope = MemoryScope.Product(\"my-app\")\n        )\n    }\n}\n</code></pre>"},{"location":"agent-memory/#automatic-fact-detection","title":"Automatic fact detection","text":"<p>You can also ask the LLM to detect all the facts from the agent's history using the <code>nodeSaveToMemoryAutoDetectFacts</code> method:</p> <pre><code>val saveAutoDetect by nodeSaveToMemoryAutoDetectFacts&lt;Unit&gt;(\n    subjects = listOf(MemorySubjects.User, MemorySubjects.Machine)\n)\n</code></pre> <p>In the example above, the LLM would search for the user-related facts and project-related facts, determine the concepts, and save them into the memory.</p>"},{"location":"agent-memory/#best-practices","title":"Best practices","text":"<ol> <li> <p>Start Simple</p> <ul> <li>Begin with basic storage without encryption</li> <li>Use single facts before moving to multiple facts</li> </ul> </li> <li> <p>Organize Well</p> <ul> <li>Use clear concept names</li> <li>Add helpful descriptions</li> <li>Keep related information under the same subject</li> </ul> </li> <li> <p>Handle Errors</p> </li> </ol> <pre><code>try {\n    memoryProvider.save(fact, subject, scope)\n} catch (e: Exception) {\n    println(\"Oops! Couldn't save: ${e.message}\")\n}\n</code></pre> <p>For more details on error handling, see Error handling and edge cases.</p>"},{"location":"agent-memory/#error-handling-and-edge-cases","title":"Error handling and edge cases","text":"<p>The AgentMemory feature includes several mechanisms to handle edge cases:</p> <ol> <li> <p>NoMemory provider: a default implementation that doesn't store anything, used when no memory provider is    specified.</p> </li> <li> <p>Subject specificity handling: when loading facts, the feature prioritizes facts from more specific subjects    based on their defined <code>priorityLevel</code>.</p> </li> <li> <p>Scope filtering: facts can be filtered by scope to ensure only relevant information is loaded.</p> </li> <li> <p>Timestamp tracking: facts are stored with timestamps to track when they were created.</p> </li> <li> <p>Fact type handling: the feature supports both single facts and multiple facts, with appropriate handling for each type.</p> </li> </ol>"},{"location":"agent-memory/#api-documentation","title":"API documentation","text":"<p>For a complete API reference related to the AgentMemory feature, see the reference documentation for the agents-features-memory module.</p> <p>API documentation for specific packages:</p> <ul> <li>ai.koog.agents.local.memory.feature: includes the <code>AgentMemory</code> class and the core implementation of the   AI agents memory feature.</li> <li>ai.koog.agents.local.memory.feature.nodes: includes predefined memory-related nodes that can be used in   subgraphs.</li> <li>ai.koog.agents.local.memory.config: provides definitions of memory scopes used for memory operations.</li> <li>ai.koog.agents.local.memory.model: includes definitions of the core data structures and interfaces   that enable agents to store, organize, and retrieve information across different contexts and time periods.</li> <li>ai.koog.agents.local.memory.feature.history: provides the history compression strategy for retrieving and   incorporating factual knowledge about specific concepts from past session activity or stored memory.</li> <li>ai.koog.agents.local.memory.providers: provides the core interface that defines the fundamental operation for storing and retrieving knowledge in a structured, context-aware manner and its implementations.</li> <li>ai.koog.agents.local.memory.storage: provides the core interface and specific implementations for file operations across different platforms and storage backends.</li> </ul>"},{"location":"agent-memory/#faq-and-troubleshooting","title":"FAQ and troubleshooting","text":""},{"location":"agent-memory/#how-do-i-implement-a-custom-memory-provider","title":"How do I implement a custom memory provider?","text":"<p>To implement a custom memory provider, create a class that implements the <code>AgentMemoryProvider</code> interface:</p> <pre><code>class MyCustomMemoryProvider : AgentMemoryProvider {\n    override suspend fun save(fact: Fact, subject: MemorySubject, scope: MemoryScope) {\n        // Implementation for saving facts\n    }\n\n    override suspend fun load(concept: Concept, subject: MemorySubject, scope: MemoryScope): List&lt;Fact&gt; {\n        // Implementation for loading facts by concept\n    }\n\n    override suspend fun loadAll(subject: MemorySubject, scope: MemoryScope): List&lt;Fact&gt; {\n        // Implementation for loading all facts\n    }\n\n    override suspend fun loadByDescription(\n        description: String,\n        subject: MemorySubject,\n        scope: MemoryScope\n    ): List&lt;Fact&gt; {\n        // Implementation for loading facts by description\n    }\n}\n</code></pre>"},{"location":"agent-memory/#how-are-facts-prioritized-when-loading-from-multiple-subjects","title":"How are facts prioritized when loading from multiple subjects?","text":"<p>Facts are prioritized based on subject specificity. When loading facts, if the same concept has facts from multiple subjects, the fact from the most specific subject will be used.</p>"},{"location":"agent-memory/#can-i-store-multiple-values-for-the-same-concept","title":"Can I store multiple values for the same concept?","text":"<p>Yes, by using the <code>MultipleFacts</code> type. When defining a concept, set its <code>factType</code> to <code>FactType.MULTIPLE</code>:</p> <pre><code>val concept = Concept(\n    keyword = \"user-skills\",\n    description = \"Programming languages the user is skilled in\",\n    factType = FactType.MULTIPLE\n)\n</code></pre> <p>This lets you store multiple values for the concept, which is retrieved as a list.</p>"},{"location":"agent-persistence/","title":"Agent Persistence","text":"<p>Agent Persistence is a feature that provides checkpoint functionality for AI agents in the Koog framework. It lets you save and restore the state of an agent at specific points during execution, enabling capabilities such as:</p> <ul> <li>Resuming agent execution from a specific point</li> <li>Rolling back to previous states</li> <li>Persisting agent state across sessions</li> </ul>"},{"location":"agent-persistence/#key-concepts","title":"Key concepts","text":""},{"location":"agent-persistence/#checkpoints","title":"Checkpoints","text":"<p>A checkpoint captures the complete state of an agent at a specific point in its execution, including:</p> <ul> <li>Message history (all interactions between user, system, assistant, and tools)</li> <li>Current node being executed</li> <li>Input data for the current node</li> <li>Timestamp of creation</li> </ul> <p>Checkpoints are identified by unique IDs and are associated with a specific agent.</p>"},{"location":"agent-persistence/#installation","title":"Installation","text":"<p>To use the Agent Persistence feature, add it to your agent's configuration:</p> <pre><code>val agent = AIAgent(\n    promptExecutor = executor,\n    llmModel = OllamaModels.Meta.LLAMA_3_2,\n) {\n    install(Persistence) {\n        // Use in-memory storage for snapshots\n        storage = InMemoryPersistenceStorageProvider()\n        // Enable automatic persistence after each node\n        enableAutomaticPersistence = true\n        /* \n         Select which state will be restored on a new agent run.\n\n         Available options are:\n         1. Default: Restores the agent to the exact execution point (node in the strategy graph) where it stopped.\n            This is especially useful for building complex, fault-tolerant agents.\n         2. MessageHistoryOnly: Restores only the message history to the last saved state.\n            The agent will always restart from the first node in the strategy graph, but with history from previous runs.\n            This is useful for building conversational agents or chatbots.\n        */\n        rollbackStrategy = RollbackStrategy.MessageHistoryOnly\n    }\n}\n</code></pre> <p>Tip</p> <p>Combine <code>enableAutomaticPersistence = true</code> with <code>RollbackStrategy.MessageHistoryOnly</code> to create agents that  maintain conversation context across multiple sessions.    </p>"},{"location":"agent-persistence/#configuration-options","title":"Configuration options","text":"<p>The Agent Persistence feature has three main configuration options:</p> <ul> <li>Storage provider: the provider used to save and retrieve checkpoints.</li> <li>Continuous persistence: automatic creation of checkpoints after each node is run.</li> <li>Rollback strategy: determines which state will be restored when rolling back to a checkpoint.</li> </ul>"},{"location":"agent-persistence/#storage-provider","title":"Storage provider","text":"<p>Set the storage provider that will be used to save and retrieve checkpoints:</p> <pre><code>install(Persistence) {\n    storage = InMemoryPersistenceStorageProvider()\n}\n</code></pre> <p>The framework includes the following built-in providers:</p> <ul> <li><code>InMemoryPersistenceStorageProvider</code>: stores checkpoints in memory (lost when the application restarts).</li> <li><code>FilePersistenceStorageProvider</code>: persists checkpoints to the file system.</li> <li><code>NoPersistenceStorageProvider</code>: a no-op implementation that does not store checkpoints. This is the default provider.</li> </ul> <p>You can also implement custom storage providers by implementing the <code>PersistenceStorageProvider</code> interface. For more information, see Custom storage providers.</p>"},{"location":"agent-persistence/#continuous-persistence","title":"Continuous persistence","text":"<p>Continuous persistence means that a checkpoint is automatically created after each node is run. To activate continuous persistence, use the code below:</p> <pre><code>install(Persistence) {\n    enableAutomaticPersistence = true\n}\n</code></pre> <p>When activated, the agent will automatically create a checkpoint after each node is executed, allowing for fine-grained recovery.</p>"},{"location":"agent-persistence/#rollback-strategy","title":"Rollback strategy","text":"<p>The rollback strategy determines which state will be restored when the agent rolls back to a checkpoint or starts a new run. There are two available strategies:</p> <pre><code>install(Persistence) {\n    // Default strategy: restores the complete agent state including execution point\n    rollbackStrategy = RollbackStrategy.Default\n}\n</code></pre> <p><code>RollbackStrategy.Default</code></p> <p>Restores the agent to the exact execution point (node in the strategy graph) where it stopped. This means the entire context is restored, including:</p> <ul> <li>Message history</li> <li>Current node being executed</li> <li>Any other stateful data</li> </ul> <p>This strategy is especially useful for building complex, fault-tolerant agents that need to resume from the exact point where they left off.</p> <p><code>RollbackStrategy.MessageHistoryOnly</code></p> <p>Restores only the message history to the last saved state. The agent will always restart from the first node in the strategy graph, but with the conversation history from previous runs.</p> <p>This strategy is useful for building conversational agents or chatbots that need to maintain context across multiple sessions but should always start their execution flow from the beginning.</p> <pre><code>install(Persistence) {\n    // MessageHistoryOnly strategy: preserves conversation history but restarts execution\n    rollbackStrategy = RollbackStrategy.MessageHistoryOnly\n}\n</code></pre>"},{"location":"agent-persistence/#basic-usage","title":"Basic usage","text":""},{"location":"agent-persistence/#creating-a-checkpoint","title":"Creating a checkpoint","text":"<p>To learn how to create a checkpoint at a specific point in your agent's execution, see the code sample below:</p> <pre><code>suspend fun example(context: AIAgentContext) {\n    // Create a checkpoint with the current state\n    val checkpoint = context.persistence().createCheckpoint(\n        agentContext = context,\n        nodePath = context.executionInfo.path(),\n        lastInput = inputData,\n        lastInputType = inputType,\n        checkpointId = context.runId,\n        version = 0L\n    )\n\n    // The checkpoint ID can be stored for later use\n    val checkpointId = checkpoint?.checkpointId\n}\n</code></pre>"},{"location":"agent-persistence/#restoring-from-a-checkpoint","title":"Restoring from a checkpoint","text":"<p>To restore the state of an agent from a specific checkpoint, follow the code sample below:</p> <pre><code>suspend fun example(context: AIAgentContext, checkpointId: String) {\n    // Roll back to a specific checkpoint\n    context.persistence().rollbackToCheckpoint(checkpointId, context)\n\n    // Or roll back to the latest checkpoint\n    context.persistence().rollbackToLatestCheckpoint(context)\n}\n</code></pre>"},{"location":"agent-persistence/#rolling-back-all-side-effects-produced-by-tools","title":"Rolling back all side-effects produced by tools","text":"<p>It's quite common for some tools to produce side-effects. Specifically, when you are running your agents on the backend,  some of the tools would likely perform some database transactions. This makes it much harder for your agent to travel back in time.</p> <p>Imagine, that you have a tool <code>createUser</code> that creates a new user in your database. And your agent has populated multiple tool calls overtime: <pre><code>tool call: createUser \"Alex\"\n\n-&gt;&gt;&gt;&gt; checkpoint-1 &lt;&lt;&lt;&lt;-\n\ntool call: createUser \"Daniel\"\ntool call: createUser \"Maria\"\n</code></pre></p> <p>And now you would like to roll back to a checkpoint. Restoring the agent's state (including message history, and strategy graph node) alone would not be sufficient to achieve the exact state of the world before the checkpoint. You should also restore the side-effects produced by your tool calls. In our example, this would mean removing <code>Maria</code> and <code>Daniel</code> from the database.</p> <p>With Koog Persistence you can achieve that by providing a <code>RollbackToolRegistry</code> to <code>Persistence</code> feature config:</p> <pre><code>install(Persistence) {\n    enableAutomaticPersistence = true\n    rollbackToolRegistry = RollbackToolRegistry {\n        // For every `createUser` tool call there will be a `removeUser` invocation in the reverse order \n        // when rolling back to the desired execution point.\n        // Note: `removeUser` tool should take the same exact arguments as `createUser`. \n        // It's the developer's responsibility to make sure that `removeUser` invocation rolls back all side-effects of `createUser`:\n        registerRollback(::createUser, ::removeUser)\n    }\n}\n</code></pre>"},{"location":"agent-persistence/#using-extension-functions","title":"Using extension functions","text":"<p>The Agent Persistence feature provides convenient extension functions for working with checkpoints:</p> <pre><code>suspend fun example(context: AIAgentContext) {\n    // Access the checkpoint feature\n    val checkpointFeature = context.persistence()\n\n    // Or perform an action with the checkpoint feature\n    context.withPersistence { ctx -&gt;\n        // 'this' is the checkpoint feature\n        createCheckpoint(\n            agentContext = ctx,\n            nodePath = ctx.executionInfo.path(),\n            lastInput = inputData,\n            lastInputType = inputType,\n            checkpointId = ctx.runId,\n            version = 0L\n        )\n    }\n}\n</code></pre>"},{"location":"agent-persistence/#advanced-usage","title":"Advanced usage","text":""},{"location":"agent-persistence/#custom-storage-providers","title":"Custom storage providers","text":"<p>You can implement custom storage providers by implementing the <code>PersistenceStorageProvider</code> interface:</p> <pre><code>class MyCustomStorageProvider&lt;MyFilterType&gt; : PersistenceStorageProvider&lt;MyFilterType&gt; {\n    override suspend fun getCheckpoints(agentId: String, filter: MyFilterType?): List&lt;AgentCheckpointData&gt; {\n        TODO(\"Not yet implemented\")\n    }\n\n    override suspend fun saveCheckpoint(agentId: String, agentCheckpointData: AgentCheckpointData) {\n        TODO(\"Not yet implemented\")\n    }\n\n    override suspend fun getLatestCheckpoint(agentId: String, filter: MyFilterType?): AgentCheckpointData? {\n        TODO(\"Not yet implemented\")\n    }\n}\n</code></pre> <p>To use your custom provider in the feature configuration, set it as the storage when configuring the Agent Persistence feature in your agent.</p> <pre><code>install(Persistence) {\n    storage = MyCustomStorageProvider&lt;Any&gt;()\n}\n</code></pre>"},{"location":"agent-persistence/#setting-execution-points","title":"Setting execution points","text":"<p>For advanced control, you can directly set the execution point of an agent:</p> <pre><code>fun example(context: AIAgentContext) {\n    context.persistence().setExecutionPoint(\n        agentContext = context,\n        nodePath = context.executionInfo.path(),\n        messageHistory = customMessageHistory,\n        input = customInput\n    )\n}\n</code></pre> <p>This allows for more fine-grained control over the agent's state beyond just restoring from checkpoints.</p>"},{"location":"annotation-based-tools/","title":"Annotation-based tools","text":"<p>Annotation-based tools provide a declarative way to expose functions as tools for large language models (LLMs). By using annotations, you can transform any function into a tool that LLMs can understand and use.</p> <p>This approach is useful when you need to expose existing functionality to LLMs without implementing tool descriptions manually.</p> <p>Note</p> <p>Annotation-based tools are JVM-only and not available for other platforms. For multiplatform support, use the class-based tool API.</p>"},{"location":"annotation-based-tools/#key-annotations","title":"Key annotations","text":"<p>To start using annotation-based tools in your project, you need to understand the following key annotations:</p> Annotation Description <code>@Tool</code> Marks functions that should be exposed as tools to LLMs. <code>@LLMDescription</code> Provides descriptive information about your tools and their components."},{"location":"annotation-based-tools/#tool-annotation","title":"@Tool annotation","text":"<p>The <code>@Tool</code> annotation is used to mark functions that should be exposed as tools to LLMs. The functions annotated with <code>@Tool</code> are collected by reflection from objects that implement the <code>ToolSet</code> interface. For details, see Implement the ToolSet interface.</p>"},{"location":"annotation-based-tools/#definition","title":"Definition","text":"<pre><code>@Target(AnnotationTarget.FUNCTION)\npublic annotation class Tool(val customName: String = \"\")\n</code></pre>"},{"location":"annotation-based-tools/#parameters","title":"Parameters","text":"Name Required Description <code>customName</code> No Specifies a custom name for the tool. If not provided, the name of the function is used."},{"location":"annotation-based-tools/#usage","title":"Usage","text":"<p>To mark a function as a tool, apply the <code>@Tool</code> annotation to this function in a class that implements the <code>ToolSet</code> interface:</p> <pre><code>class MyToolSet : ToolSet {\n    @Tool\n    fun myTool(): String {\n        // Tool implementation\n        return \"Result\"\n    }\n\n    @Tool(customName = \"customToolName\")\n    fun anotherTool(): String {\n        // Tool implementation\n        return \"Result\"\n    }\n}\n</code></pre>"},{"location":"annotation-based-tools/#llmdescription-annotation","title":"@LLMDescription annotation","text":"<p>The <code>@LLMDescription</code> annotation provides descriptive information about code elements (classes, functions, parameters, and so on) to LLMs. This helps LLMs understand the purpose and usage of these elements.</p>"},{"location":"annotation-based-tools/#definition_1","title":"Definition","text":"<pre><code>@Target(\n    AnnotationTarget.PROPERTY,\n    AnnotationTarget.CLASS,\n    AnnotationTarget.PROPERTY,\n    AnnotationTarget.TYPE,\n    AnnotationTarget.VALUE_PARAMETER,\n    AnnotationTarget.FUNCTION\n)\npublic annotation class LLMDescription(val description: String)\n</code></pre>"},{"location":"annotation-based-tools/#parameters_1","title":"Parameters","text":"Name Required Description <code>description</code> Yes A string that describes the annotated element."},{"location":"annotation-based-tools/#usage_1","title":"Usage","text":"<p>The <code>@LLMDescription</code> annotation can be applied at various levels. For example:</p> <ul> <li>Function level:</li> </ul> <pre><code>@Tool\n@LLMDescription(\"Performs a specific operation and returns the result\")\nfun myTool(): String {\n    // Function implementation\n    return \"Result\"\n}\n</code></pre> <ul> <li>Parameter level:</li> </ul> <pre><code>@Tool\n@LLMDescription(\"Processes input data\")\nfun processTool(\n    @LLMDescription(\"The input data to process\")\n    input: String,\n\n    @LLMDescription(\"Optional configuration parameters\")\n    config: String = \"\"\n): String {\n    // Function implementation\n    return \"Processed: $input with config: $config\"\n}\n</code></pre>"},{"location":"annotation-based-tools/#creating-a-tool","title":"Creating a tool","text":""},{"location":"annotation-based-tools/#1-implement-the-toolset-interface","title":"1. Implement the ToolSet interface","text":"<p>Create a class that implements the <code>ToolSet</code> interface. This interface marks your class as a container for tools.</p> <pre><code>class MyFirstToolSet : ToolSet {\n    // Tools will go here\n}\n</code></pre>"},{"location":"annotation-based-tools/#2-add-tool-functions","title":"2. Add tool functions","text":"<p>Add functions to your class and annotate them with <code>@Tool</code> to expose them as tools:</p> <pre><code>class MyFirstToolSet : ToolSet {\n    @Tool\n    fun getWeather(location: String): String {\n        // In a real implementation, you would call a weather API\n        return \"The weather in $location is sunny and 72\u00b0F\"\n    }\n}\n</code></pre>"},{"location":"annotation-based-tools/#3-add-descriptions","title":"3. Add descriptions","text":"<p>Add <code>@LLMDescription</code> annotations to provide context for the LLM:</p> <pre><code>@LLMDescription(\"Tools for getting weather information\")\nclass MyFirstToolSet : ToolSet {\n    @Tool\n    @LLMDescription(\"Get the current weather for a location\")\n    fun getWeather(\n        @LLMDescription(\"The city and state/country\")\n        location: String\n    ): String {\n        // In a real implementation, you would call a weather API\n        return \"The weather in $location is sunny and 72\u00b0F\"\n    }\n}\n</code></pre>"},{"location":"annotation-based-tools/#4-use-your-tools-with-an-agent","title":"4. Use your tools with an agent","text":"<p>Now you can use your tools with an agent:</p> <pre><code>fun main() {\n    runBlocking {\n        // Create your tool set\n        val weatherTools = MyFirstToolSet()\n\n        // Create an agent with your tools\n\n        val agent = AIAgent(\n            promptExecutor = simpleOpenAIExecutor(apiToken),\n            systemPrompt = \"Provide weather information for a given location.\",\n            llmModel = OpenAIModels.Chat.GPT4o,\n            toolRegistry = ToolRegistry {\n                tools(weatherTools)\n            }\n        )\n\n        // The agent can now use your weather tools\n        agent.run(\"What's the weather like in New York?\")\n    }\n}\n</code></pre>"},{"location":"annotation-based-tools/#usage-examples","title":"Usage examples","text":"<p>Here are some real-world examples of tool annotations.</p>"},{"location":"annotation-based-tools/#basic-example-switch-controller","title":"Basic example: Switch controller","text":"<p>This example shows a simple tool set for controlling a switch:</p> <pre><code>@LLMDescription(\"Tools for controlling a switch\")\nclass SwitchTools(val switch: Switch) : ToolSet {\n    @Tool\n    @LLMDescription(\"Switches the state of the switch\")\n    fun switch(\n        @LLMDescription(\"The state to set (true for on, false for off)\")\n        state: Boolean\n    ): String {\n        switch.switch(state)\n        return \"Switched to ${if (state) \"on\" else \"off\"}\"\n    }\n\n    @Tool\n    @LLMDescription(\"Returns the current state of the switch\")\n    fun switchState(): String {\n        return \"Switch is ${if (switch.isOn()) \"on\" else \"off\"}\"\n    }\n}\n</code></pre> <p>When an LLM needs to control a switch, it can understand the following information from the provided description:</p> <ul> <li>The purpose and functionality of the tools.</li> <li>The required parameters for using the tools.</li> <li>The acceptable values for each parameter.</li> <li>The expected return values upon execution.</li> </ul>"},{"location":"annotation-based-tools/#advanced-example-diagnostic-tools","title":"Advanced example: Diagnostic tools","text":"<p>This example shows a more complex tool set for device diagnostics:</p> <pre><code>@LLMDescription(\"Tools for performing diagnostics and troubleshooting on devices\")\nclass DiagnosticToolSet : ToolSet {\n    @Tool\n    @LLMDescription(\"Run diagnostic on a device to check its status and identify any issues\")\n    fun runDiagnostic(\n        @LLMDescription(\"The ID of the device to diagnose\")\n        deviceId: String,\n\n        @LLMDescription(\"Additional information for the diagnostic (optional)\")\n        additionalInfo: String = \"\"\n    ): String {\n        // Implementation\n        return \"Diagnostic results for device $deviceId\"\n    }\n\n    @Tool\n    @LLMDescription(\"Analyze an error code to determine its meaning and possible solutions\")\n    fun analyzeError(\n        @LLMDescription(\"The error code to analyze (e.g., 'E1001')\")\n        errorCode: String\n    ): String {\n        // Implementation\n        return \"Analysis of error code $errorCode\"\n    }\n}\n</code></pre>"},{"location":"annotation-based-tools/#best-practices","title":"Best practices","text":"<ul> <li>Provide clear descriptions: write clear, concise descriptions that explain the purpose and behavior of tools, parameters, and return values.</li> <li>Describe all parameters: add <code>@LLMDescription</code> to all parameters to help LLMs understand what each parameter is for.</li> <li>Use consistent naming: use consistent naming conventions for tools and parameters to make them more intuitive.</li> <li>Group related tools: group related tools in the same <code>ToolSet</code> implementation and provide a class-level description.</li> <li>Return informative results: make sure tool return values provide clear information about the result of the operation.</li> <li>Handle errors gracefully: include error handling in your tools and return informative error messages.</li> <li>Document default values: when parameters have default values, document this in the description.</li> <li>Keep tools focused: Each tool should perform a specific, well-defined task rather than trying to do too many things.</li> </ul>"},{"location":"annotation-based-tools/#troubleshooting-common-issues","title":"Troubleshooting common issues","text":"<p>When working with tool annotations, you might encounter some common issues.</p>"},{"location":"annotation-based-tools/#tools-not-being-recognized","title":"Tools not being recognized","text":"<p>If the agent does not recognize your tools, check the following:</p> <ul> <li>Your class implements the <code>ToolSet</code> interface.</li> <li>All tool functions are annotated with <code>@Tool</code>.</li> <li>Tool functions have appropriate return types (<code>String</code> is recommended for simplicity).</li> <li>Your tools are properly registered with the agent.</li> </ul>"},{"location":"annotation-based-tools/#unclear-tool-descriptions","title":"Unclear tool descriptions","text":"<p>If the LLM does not use your tools correctly or misunderstands their purpose, try the following:</p> <ul> <li>Improve your <code>@LLMDescription</code> annotations to be more specific and clear.</li> <li>Include examples in your descriptions if appropriate.</li> <li>Specify parameter constraints in the descriptions (for example, <code>\"Must be a positive number\"</code>).</li> <li>Use consistent terminology throughout your descriptions.</li> </ul>"},{"location":"annotation-based-tools/#parameter-type-issues","title":"Parameter type issues","text":"<p>If the LLM provides incorrect parameter types, try the following:</p> <ul> <li>Use simple parameter types when possible (<code>String</code>, <code>Boolean</code>, <code>Int</code>).</li> <li>Clearly describe the expected format in the parameter description.</li> <li>For complex types, consider using <code>String</code> parameters with a specific format and parse them in your tool.</li> <li>Include examples of valid inputs in your parameter descriptions.</li> </ul>"},{"location":"annotation-based-tools/#performance-issues","title":"Performance issues","text":"<p>If your tools cause performance problems, try the following:</p> <ul> <li>Keep tool implementations lightweight.</li> <li>For resource-intensive operations, consider implementing asynchronous processing.</li> <li>Cache results when appropriate.</li> <li>Log tool usage to identify bottlenecks.</li> </ul>"},{"location":"basic-agents/","title":"Basic agents","text":"<p>The <code>AIAgent</code> class is the core component that lets you create AI agents in your Kotlin applications.</p> <p>You can build simple agents with minimal configuration or create sophisticated agents with advanced capabilities by defining custom strategies, tools, configurations, and custom input/output types.</p> <p>This page guides you through the steps necessary to create a basic agent with customizable tools and configurations.</p> <p>A basic agent processes a single input and provides a response. It operates within a single cycle of tool-calling to complete its task and provide a response. This agent can return either a message or a tool result. The tool result is returned if the tool registry is provided to the agent.</p> <p>If your goal is to build a simple agent to experiment with, you can provide only a prompt executor and LLM when creating it. But if you want more flexibility and customization, you can pass optional parameters to configure the agent. To learn more about configuration options, see API reference.</p>"},{"location":"basic-agents/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have a valid API key from the LLM provider used to implement an AI agent. For a list of all available providers, see LLM providers.</li> </ul> <p>Tip</p> <p>Use environment variables or a secure configuration management system to store your API keys. Avoid hardcoding API keys directly in your source code.</p>"},{"location":"basic-agents/#creating-a-basic-agent","title":"Creating a basic agent","text":""},{"location":"basic-agents/#1-add-dependencies","title":"1. Add dependencies","text":"<p>To use the <code>AIAgent</code> functionality, include all necessary dependencies in your build configuration:</p> <pre><code>dependencies {\n    implementation(\"ai.koog:koog-agents:VERSION\")\n}\n</code></pre> <p>For all available installation methods, see Install Koog.</p>"},{"location":"basic-agents/#2-create-an-agent","title":"2. Create an agent","text":"<p>To create an agent, create an instance of the <code>AIAgent</code> class and provide the <code>promptExecutor</code> and <code>llmModel</code> parameters:</p> <pre><code>val agent = AIAgent(\n    promptExecutor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    llmModel = OpenAIModels.Chat.GPT4o\n)\n</code></pre>"},{"location":"basic-agents/#3-add-a-system-prompt","title":"3. Add a system prompt","text":"<p>A system prompt is used to define agent behavior. To provide the prompt, use the <code>systemPrompt</code> parameter:</p> <pre><code>val agent = AIAgent(\n    promptExecutor = simpleOpenAIExecutor(System.getenv(\"YOUR_API_KEY\")),\n    systemPrompt = \"You are a helpful assistant. Answer user questions concisely.\",\n    llmModel = OpenAIModels.Chat.GPT4o\n)\n</code></pre>"},{"location":"basic-agents/#4-configure-llm-output","title":"4. Configure LLM output","text":"<p>Provide a temperature of LLM output generation using the <code>temperature</code> parameter:</p> <pre><code>val agent = AIAgent(\n    promptExecutor = simpleOpenAIExecutor(System.getenv(\"YOUR_API_KEY\")),\n    systemPrompt = \"You are a helpful assistant. Answer user questions concisely.\",\n    llmModel = OpenAIModels.Chat.GPT4o,\n    temperature = 0.7\n)\n</code></pre>"},{"location":"basic-agents/#5-add-tools","title":"5. Add tools","text":"<p>Agents use tools to complete specific tasks. You can use the built-in tools or implement your own custom tools if needed.</p> <p>To configure tools, use the <code>toolRegistry</code> parameter that defines the tools available to the agent:</p> <pre><code>val agent = AIAgent(\n    promptExecutor = simpleOpenAIExecutor(System.getenv(\"YOUR_API_KEY\")),\n    systemPrompt = \"You are a helpful assistant. Answer user questions concisely.\",\n    llmModel = OpenAIModels.Chat.GPT4o,\n    temperature = 0.7,\n    toolRegistry = ToolRegistry {\n        tool(SayToUser)\n    }\n)\n</code></pre> <p>In the example, <code>SayToUser</code> is the built-in tool. To learn how to create a custom tool, see Tools.</p>"},{"location":"basic-agents/#6-adjust-agent-iterations","title":"6. Adjust agent iterations","text":"<p>Provide the maximum number of steps the agent can take before it is forced to stop using the <code>maxIterations</code> parameter:</p> <pre><code>val agent = AIAgent(\n    promptExecutor = simpleOpenAIExecutor(System.getenv(\"YOUR_API_KEY\")),\n    systemPrompt = \"You are a helpful assistant. Answer user questions concisely.\",\n    llmModel = OpenAIModels.Chat.GPT4o,\n    temperature = 0.7,\n    toolRegistry = ToolRegistry {\n        tool(SayToUser)\n    },\n    maxIterations = 30\n)\n</code></pre>"},{"location":"basic-agents/#7-handle-events-during-agent-runtime","title":"7. Handle events during agent runtime","text":"<p>Basic agents support custom event handlers. While having an event handler is not required for creating an agent, it might be helpful for testing, debugging, or making hooks for chained agent interactions.</p> <p>For more information on how to use the <code>EventHandler</code> feature for monitoring your agent interactions, see Event Handlers.</p>"},{"location":"basic-agents/#8-run-the-agent","title":"8. Run the agent","text":"<p>To run the agent, use the <code>run()</code> function:</p> <pre><code>val agent = AIAgent(\n    promptExecutor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    systemPrompt = \"You are a helpful assistant. Answer user questions concisely.\",\n    llmModel = OpenAIModels.Chat.GPT4o,\n    temperature = 0.7,\n    toolRegistry = ToolRegistry {\n        tool(SayToUser)\n    },\n    maxIterations = 100\n)\n\nfun main() = runBlocking {\n    val result = agent.run(\"Hello! How can you help me?\")\n}\n</code></pre> <p>The agent produces the following output:</p> <pre><code>Agent says: Hello! I'm here to assist you with a variety of tasks. Whether you have questions, need information, or require help with specific tasks, feel free to ask. How can I assist you today?\n</code></pre>"},{"location":"built-in-tools/","title":"Built-in tools","text":"<p>The Koog framework provides built-in tools that handle common scenarios of agent-user interaction.</p> <p>The following built-in tools are available:</p> Tool Name Description SayToUser <code>__say_to_user__</code> Lets the agent send a message to the user. It prints the agent message to the console with the <code>Agent says:</code> prefix. AskUser <code>__ask_user__</code> Lets the agent ask the user for input. It prints the agent message to the console and waits for user response. ExitTool <code>__exit__</code> Lets the agent finish the conversation and terminate the session. ReadFileTool <code>__read_file__</code> Reads text file with optional line range selection. Returns formatted content with metadata using 0-based line indexing. EditFileTool <code>__edit_file__</code> Makes a single, targeted text replacement in a file; can also create new files or fully replace contents. ListDirectoryTool <code>__list_directory__</code> Lists directory contents as a hierarchical tree with optional depth control and glob filtering. WriteFileTool <code>__write_file__</code> Writes text content to a file (creating parent directories if needed)."},{"location":"built-in-tools/#registering-built-in-tools","title":"Registering built-in tools","text":"<p>Like any other tool, a built-in tool must be added to the tool registry to become available for an agent. Here is an example:</p> <pre><code>// Create a tool registry with all built-in tools\nval toolRegistry = ToolRegistry {\n    tool(SayToUser)\n    tool(AskUser)\n    tool(ExitTool)\n    tool(ReadFileTool(JVMFileSystemProvider.ReadOnly))\n    tool(ListDirectoryTool(JVMFileSystemProvider.ReadOnly))\n    tool(WriteFileTool(JVMFileSystemProvider.ReadWrite))\n}\n\n// Pass the registry when creating an agent\nval agent = AIAgent(\n    promptExecutor = simpleOpenAIExecutor(apiToken),\n    systemPrompt = \"You are a helpful assistant.\",\n    llmModel = OpenAIModels.Chat.GPT4o,\n    toolRegistry = toolRegistry\n)\n</code></pre> <p>You can create a comprehensive set of capabilities for your agent by combining built-in tools and custom tools within the same registry. To learn more about custom tools, see Annotation-based tools and Class-based tools.</p>"},{"location":"class-based-tools/","title":"Class-based tools","text":"<p>This section explains the API designed for scenarios that require enhanced flexibility and customized behavior. With this approach, you have full control over a tool, including its parameters, metadata, execution logic, and how it is registered and invoked.</p> <p>This level of control is ideal for creating sophisticated tools that extend basic use cases, enabling seamless integration into agent sessions and workflows.</p> <p>This page describes how to implement a tool, manage tools through registries, call them, and use within node-based agent architectures.</p> <p>Note</p> <p>The API is multiplatform. This lets you use the same tools across different platforms.</p>"},{"location":"class-based-tools/#tool-implementation","title":"Tool implementation","text":"<p>The Koog framework provides the following approaches for implementing tools:</p> <ul> <li>Using the base class <code>Tool</code> for all tools. You should use this class when you need to return non-text results or require complete control over the tool behavior.</li> <li>Using the <code>SimpleTool</code> class that extends the base <code>Tool</code> class and simplifies the creation of tools that return text results. You should use this approach for scenarios where the    tool only needs to return a text.</li> </ul> <p>Both approaches use the same core components but differ in implementation and the results they return.</p>"},{"location":"class-based-tools/#tool-class","title":"Tool class","text":"<p>The <code>Tool&lt;Args, Result&gt;</code> abstract class is the base class for creating tools in Koog. It lets you create tools that accept specific argument types (<code>Args</code>) and return results of various types (<code>Result</code>).</p> <p>Each tool consists of the following components:</p> Component Description <code>Args</code> The serializable data class that defines arguments required for the tool. <code>Result</code> The serializable type of result that the tool returns. If you want to present tool results in a custom format, please inherit ToolResult.TextSerializable class and implement <code>textForLLM(): String</code> method <code>argsSerializer</code> The overridden variable that defines how the arguments for the tool are deserialized. See also argsSerializer. <code>resultSerializer</code> The overridden variable that defines how the result of the tool is deserialized. See also resultSerializer. If you chose to inherit ToolResult.TextSerializable consider using <code>ToolResultUtils.toTextSerializer()</code> <code>descriptor</code> The overridden variable that specifies tool metadata:- <code>name</code>- <code>description</code>- <code>requiredParameters</code> (empty by default)- <code>optionalParameters</code> (empty by default)See also descriptor. <code>execute()</code> The function that implements the logic of the tool. It takes arguments of type <code>Args</code> and returns a result of type <code>Result</code>. See also execute(). <p>Tip</p> <p>Ensure your tools have clear descriptions and well-defined parameter names to make it easier for the LLM to understand and use them properly.</p>"},{"location":"class-based-tools/#usage-example","title":"Usage example","text":"<p>Here is an example of a custom tool implementation using the <code>Tool</code> class that returns a numeric result:</p> <pre><code>// Implement a simple calculator tool that adds two digits\nobject CalculatorTool : Tool&lt;CalculatorTool.Args, Int&gt;(\n    argsSerializer = Args.serializer(),\n    resultSerializer = Int.serializer(),\n    name = \"calculator\",\n    description = \"A simple calculator that can add two digits (0-9).\"\n) {\n\n    // Arguments for the calculator tool\n    @Serializable\n    data class Args(\n        @property:LLMDescription(\"The first digit to add (0-9)\")\n        val digit1: Int,\n        @property:LLMDescription(\"The second digit to add (0-9)\")\n        val digit2: Int\n    ) {\n        init {\n            require(digit1 in 0..9) { \"digit1 must be a single digit (0-9)\" }\n            require(digit2 in 0..9) { \"digit2 must be a single digit (0-9)\" }\n        }\n    }\n\n    // Function to add two digits\n    override suspend fun execute(args: Args): Int = args.digit1 + args.digit2\n}\n</code></pre> <p>After implementing your tool, you need to add it to a tool registry and then use it with an agent. For details, see Tool registry.</p> <p>For more details, see API reference.</p>"},{"location":"class-based-tools/#simpletool-class","title":"SimpleTool class","text":"<p>The <code>SimpleTool&lt;Args&gt;</code> abstract class extends <code>Tool&lt;Args, ToolResult.Text&gt;</code> and simplifies the creation of tools that return text results.</p> <p>Each simple tool consists of the following components:</p> Component Description <code>Args</code> The serializable data class that defines arguments required for the custom tool. <code>argsSerializer</code> The overridden variable that defines how the arguments for the tool are serialized. See also argsSerializer. <code>descriptor</code> The overridden variable that specifies tool metadata:- <code>name</code>- <code>description</code>- <code>requiredParameters</code> (empty by default) - <code>optionalParameters</code> (empty by default) See also descriptor. <code>doExecute()</code> The overridden function that describes the main action performed by the tool. It takes arguments of type <code>Args</code> and returns a <code>String</code>. See also doExecute(). <p>Tip</p> <p>Ensure your tools have clear descriptions and well-defined parameter names to make it easier for the LLM to understand and use them properly.</p>"},{"location":"class-based-tools/#usage-example_1","title":"Usage example","text":"<p>Here is an example of a custom tool implementation using <code>SimpleTool</code>:</p> <pre><code>// Create a tool that casts a string expression to a double value\nobject CastToDoubleTool : SimpleTool&lt;CastToDoubleTool.Args&gt;(\n    argsSerializer = Args.serializer(),\n    name = \"cast_to_double\",\n    description = \"casts the passed expression to double or returns 0.0 if the expression is not castable\"\n) {\n    // Define tool arguments\n    @Serializable\n    data class Args(\n        @property:LLMDescription(\"An expression to case to double\")\n        val expression: String,\n        @property:LLMDescription(\"A comment on how to process the expression\")\n        val comment: String\n    )\n\n    // Function that executes the tool with the provided arguments\n    override suspend fun execute(args: Args): String {\n        return \"Result: ${castToDouble(args.expression)}, \" + \"the comment was: ${args.comment}\"\n    }\n\n    // Function to cast a string expression to a double value\n    private fun castToDouble(expression: String): Double {\n        return expression.toDoubleOrNull() ?: 0.0\n    }\n}\n</code></pre>"},{"location":"class-based-tools/#sending-tool-result-to-llm-in-custom-format","title":"Sending Tool Result to LLM in Custom Format","text":"<p>If you are not happy with JSON results sent to LLM (in some cases, LLMs can work better if tool output is structured as Markdown, for instance), you have to follow the following steps: 1. Implement <code>ToolResult.TextSerializable</code> interface, and override <code>textForLLM()</code> method 2. Override <code>resultSerializer</code> using <code>ToolResultUtils.toTextSerializer&lt;T&gt;()</code></p>"},{"location":"class-based-tools/#example","title":"Example","text":"<pre><code>// A tool that edits file\nobject EditFile : Tool&lt;EditFile.Args, EditFile.Result&gt;(\n    argsSerializer = Args.serializer(),\n    resultSerializer = Result.serializer(),\n    name = \"edit_file\",\n    description = \"Edits the given file\"\n) {\n    // Define tool arguments\n    @Serializable\n    public data class Args(\n        val path: String,\n        val original: String,\n        val replacement: String\n    )\n\n    @Serializable\n    public data class Result(\n        private val patchApplyResult: PatchApplyResult\n    ) {\n\n        @Serializable\n        public sealed interface PatchApplyResult {\n            @Serializable\n            public data class Success(val updatedContent: String) : PatchApplyResult\n\n            @Serializable\n            public sealed class Failure(public val reason: String) : PatchApplyResult\n        }\n\n        // Textual output (in Markdown format) that will be visible to the LLM after the tool finishes.\n        fun textForLLM(): String = markdown {\n            if (patchApplyResult is PatchApplyResult.Success) {\n                line {\n                    bold(\"Successfully\").text(\" edited file (patch applied)\")\n                }\n            } else {\n                line {\n                    text(\"File was \")\n                        .bold(\"not\")\n                        .text(\" modified (patch application failed: ${(patchApplyResult as PatchApplyResult.Failure).reason})\")\n                }\n            }\n        }\n\n        override fun toString(): String = textForLLM()\n    }\n\n    // Function that executes the tool with the provided arguments\n    override suspend fun execute(args: Args): Result {\n        return TODO(\"Implement file edit\")\n    }\n}\n</code></pre> <p>After implementing your tool, you need to add it to a tool registry and then use it with an agent. For details, see Tool registry.</p>"},{"location":"complex-workflow-agents/","title":"Complex workflow agents","text":"<p>In addition to basic agents, the <code>AIAgent</code> class lets you build agents that handle complex workflows by defining  custom strategies, tools, configurations, and custom input/output types.</p> <p>Tip</p> <p>If you are new to Koog and want to create the simplest agent, start with Basic agents.</p> <p>The process of creating and configuring such an agent typically includes the following steps:</p> <ol> <li>Provide a prompt executor to communicate with the LLM.</li> <li>Define a strategy that controls the agent workflow.</li> <li>Configure agent behavior.</li> <li>Implement tools for the agent to use.</li> <li>Add optional features like event handling, memory, or tracing.</li> <li>Run the agent with user input.</li> </ol>"},{"location":"complex-workflow-agents/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have a valid API key from the LLM provider used to implement an AI agent. For a list of all available providers, see LLM providers.</li> </ul> <p>Tip</p> <p>Use environment variables or a secure configuration management system to store your API keys. Avoid hardcoding API keys directly in your source code.</p>"},{"location":"complex-workflow-agents/#creating-a-complex-workflow-agent","title":"Creating a complex workflow agent","text":""},{"location":"complex-workflow-agents/#1-add-dependencies","title":"1. Add dependencies","text":"<p>To use the <code>AIAgent</code> functionality, include all necessary dependencies in your build configuration:</p> <pre><code>dependencies {\n    implementation(\"ai.koog:koog-agents:VERSION\")\n}\n</code></pre> <p>For all available installation methods, see Install Koog.</p>"},{"location":"complex-workflow-agents/#2-provide-a-prompt-executor","title":"2. Provide a prompt executor","text":"<p>Prompt executors manage and run prompts. You can choose a prompt executor based on the LLM provider you plan to use. Also, you can create a custom prompt executor using one of the available LLM clients. To learn more, see Prompt executors.</p> <p>For example, to provide the OpenAI prompt executor, you need to call the <code>simpleOpenAIExecutor</code> function and provide it with the API key required for authentication with the OpenAI service:</p> <pre><code>val promptExecutor = simpleOpenAIExecutor(token)\n</code></pre> <p>To create a prompt executor that works with multiple LLM providers, do the following:</p> <p>1) Configure clients for the required LLM providers with the corresponding API keys. For example:</p> <pre><code>val openAIClient = OpenAILLMClient(System.getenv(\"OPENAI_KEY\"))\nval anthropicClient = AnthropicLLMClient(System.getenv(\"ANTHROPIC_KEY\"))\nval googleClient = GoogleLLMClient(System.getenv(\"GOOGLE_KEY\"))\n</code></pre> <p>2) Pass the configured clients to the <code>DefaultMultiLLMPromptExecutor</code> class constructor to create a prompt executor with multiple LLM providers:</p> <pre><code>val multiExecutor = DefaultMultiLLMPromptExecutor(openAIClient, anthropicClient, googleClient)\n</code></pre>"},{"location":"complex-workflow-agents/#3-define-a-strategy","title":"3. Define a strategy","text":"<p>A strategy defines the workflow of your agent by using nodes and edges. It can have arbitrary input and output types,  which can be specified in <code>strategy</code> function generic parameters. These will be input/output types of the <code>AIAgent</code> as well. Default type for both input and output is <code>String</code>.</p> <p>Tip</p> <p>To learn more about strategies, see Custom strategy graphs</p>"},{"location":"complex-workflow-agents/#31-understand-nodes-and-edges","title":"3.1. Understand nodes and edges","text":"<p>Nodes and edges are the building blocks of the strategy.</p> <p>Nodes represent processing steps in your agent strategy.</p> <pre><code>val processNode by node&lt;InputType, OutputType&gt; { input -&gt;\n    // Process the input and return an output\n    // You can use llm.writeSession to interact with the LLM\n    // You can call tools using callTool, callToolRaw, etc.\n    transformedOutput\n}\n</code></pre> <p>Tip</p> <p>There are also pre-defined nodes that you can use in your agent strategy. To learn more, see Predefined nodes and components.</p> <p>Edges define the connections between nodes.</p> <pre><code>// Basic edge\nedge(sourceNode forwardTo targetNode)\n\n// Edge with condition\nedge(sourceNode forwardTo targetNode onCondition { output -&gt;\n    // Return true to follow this edge, false to skip it\n    output.contains(\"specific text\")\n})\n\n// Edge with transformation\nedge(sourceNode forwardTo targetNode transformed { output -&gt;\n    // Transform the output before passing it to the target node\n    \"Modified: $output\"\n})\n\n// Combined condition and transformation\nedge(sourceNode forwardTo targetNode onCondition { it.isNotEmpty() } transformed { it.uppercase() })\n</code></pre>"},{"location":"complex-workflow-agents/#32-implement-the-strategy","title":"3.2. Implement the strategy","text":"<p>To implement the agent strategy, call the <code>strategy</code> function and define nodes and edges. For example:</p> <pre><code>val agentStrategy = strategy(\"Simple calculator\") {\n    // Define nodes for the strategy\n    val nodeSendInput by nodeLLMRequest()\n    val nodeExecuteTool by nodeExecuteTool()\n    val nodeSendToolResult by nodeLLMSendToolResult()\n\n    // Define edges between nodes\n    // Start -&gt; Send input\n    edge(nodeStart forwardTo nodeSendInput)\n\n    // Send input -&gt; Finish\n    edge(\n        (nodeSendInput forwardTo nodeFinish)\n                transformed { it }\n                onAssistantMessage { true }\n    )\n\n    // Send input -&gt; Execute tool\n    edge(\n        (nodeSendInput forwardTo nodeExecuteTool)\n                onToolCall { true }\n    )\n\n    // Execute tool -&gt; Send the tool result\n    edge(nodeExecuteTool forwardTo nodeSendToolResult)\n\n    // Send the tool result -&gt; finish\n    edge(\n        (nodeSendToolResult forwardTo nodeFinish)\n                transformed { it }\n                onAssistantMessage { true }\n    )\n}\n</code></pre> <p>Tip</p> <p>The <code>strategy</code> function lets you define multiple subgraphs, each containing its own set of nodes and edges. This approach offers more flexibility and functionality compared to using simplified strategy builders. To learn more about subgraphs, see Subgraphs.</p>"},{"location":"complex-workflow-agents/#4-configure-the-agent","title":"4. Configure the agent","text":"<p>Define agent behavior with a configuration:</p> <pre><code>val agentConfig = AIAgentConfig.withSystemPrompt(\n    prompt = \"\"\"\n        You are a simple calculator assistant.\n        You can add two numbers together using the calculator tool.\n        When the user provides input, extract the numbers they want to add.\n        The input might be in various formats like \"add 5 and 7\", \"5 + 7\", or just \"5 7\".\n        Extract the two numbers and use the calculator tool to add them.\n        Always respond with a clear, friendly message showing the calculation and result.\n        \"\"\".trimIndent()\n)\n</code></pre> <p>For more advanced configuration, you can specify which LLM the agent will use and set the maximum number of iterations the agent can perform to respond:</p> <pre><code>val agentConfig = AIAgentConfig(\n    prompt = Prompt.build(\"simple-calculator\") {\n        system(\n            \"\"\"\n                You are a simple calculator assistant.\n                You can add two numbers together using the calculator tool.\n                When the user provides input, extract the numbers they want to add.\n                The input might be in various formats like \"add 5 and 7\", \"5 + 7\", or just \"5 7\".\n                Extract the two numbers and use the calculator tool to add them.\n                Always respond with a clear, friendly message showing the calculation and result.\n                \"\"\".trimIndent()\n        )\n    },\n    model = OpenAIModels.Chat.GPT4o,\n    maxAgentIterations = 10\n)\n</code></pre>"},{"location":"complex-workflow-agents/#5-implement-tools-and-set-up-a-tool-registry","title":"5. Implement tools and set up a tool registry","text":"<p>Tools let your agent perform specific tasks. To make a tool available for the agent, add it to a tool registry. For example:</p> <pre><code>// Implement a simple calculator tool that can add two numbers\n@LLMDescription(\"Tools for performing basic arithmetic operations\")\nclass CalculatorTools : ToolSet {\n    @Tool\n    @LLMDescription(\"Add two numbers together and return their sum\")\n    fun add(\n        @LLMDescription(\"First number to add (integer value)\")\n        num1: Int,\n\n        @LLMDescription(\"Second number to add (integer value)\")\n        num2: Int\n    ): String {\n        val sum = num1 + num2\n        return \"The sum of $num1 and $num2 is: $sum\"\n    }\n}\n\n// Add the tool to the tool registry\nval toolRegistry = ToolRegistry {\n    tools(CalculatorTools())\n}\n</code></pre> <p>To learn more about tools, see Tools.</p>"},{"location":"complex-workflow-agents/#6-install-features","title":"6. Install features","text":"<p>Features let you add new capabilities to the agent, modify its behavior, provide access to external systems and resources, and log and monitor events while the agent is running. The following features are available:</p> <ul> <li>EventHandler</li> <li>AgentMemory</li> <li>Tracing</li> </ul> <p>To install the feature, call the <code>install</code> function and provide the feature as an argument. For example, to install the event handler feature, you need to do the following:</p> <pre><code>// install the EventHandler feature\ninstallFeatures = {\n    install(EventHandler) {\n        onAgentStarting { eventContext: AgentStartingContext -&gt;\n            println(\"Starting agent: ${eventContext.agent.id}\")\n        }\n        onAgentCompleted { eventContext: AgentCompletedContext -&gt;\n            println(\"Result: ${eventContext.result}\")\n        }\n    }\n}\n</code></pre> <p>To learn more about feature configuration, see the dedicated page.</p>"},{"location":"complex-workflow-agents/#7-run-the-agent","title":"7. Run the agent","text":"<p>Create the agent with the configuration option created in the previous stages and run it with the provided input:</p> <pre><code>val agent = AIAgent(\n    promptExecutor = promptExecutor,\n    toolRegistry = toolRegistry,\n    strategy = agentStrategy,\n    agentConfig = agentConfig,\n    installFeatures = {\n        install(EventHandler) {\n            onAgentStarting { eventContext: AgentStartingContext -&gt;\n                println(\"Starting agent: ${eventContext.agent.id}\")\n            }\n            onAgentCompleted { eventContext: AgentCompletedContext -&gt;\n                println(\"Result: ${eventContext.result}\")\n            }\n        }\n    }\n)\n\nfun main() {\n    runBlocking {\n        println(\"Enter two numbers to add (e.g., 'add 5 and 7' or '5 + 7'):\")\n\n        // Read the user input and send it to the agent\n        val userInput = readlnOrNull() ?: \"\"\n        val agentResult = agent.run(userInput)\n        println(\"The agent returned: $agentResult\")\n    }\n}\n</code></pre>"},{"location":"complex-workflow-agents/#working-with-structured-data","title":"Working with structured data","text":"<p>The <code>AIAgent</code> can process structured data from LLM outputs. For more details, see Structured data processing.</p>"},{"location":"complex-workflow-agents/#using-parallel-tool-calls","title":"Using parallel tool calls","text":"<p>The <code>AIAgent</code> supports parallel tool calls. This feature lets you process multiple tools concurrently, improving performance for independent operations.</p> <p>For more details, see Parallel tool calls.</p>"},{"location":"complex-workflow-agents/#full-code-sample","title":"Full code sample","text":"<p>Here is the complete implementation of the agent:</p> <pre><code>// Use the OpenAI executor with an API key from an environment variable\nval promptExecutor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\"))\n\n// Create a simple strategy\nval agentStrategy = strategy(\"Simple calculator\") {\n    // Define nodes for the strategy\n    val nodeSendInput by nodeLLMRequest()\n    val nodeExecuteTool by nodeExecuteTool()\n    val nodeSendToolResult by nodeLLMSendToolResult()\n\n    // Define edges between nodes\n    // Start -&gt; Send input\n    edge(nodeStart forwardTo nodeSendInput)\n\n    // Send input -&gt; Finish\n    edge(\n        (nodeSendInput forwardTo nodeFinish)\n                transformed { it }\n                onAssistantMessage { true }\n    )\n\n    // Send input -&gt; Execute tool\n    edge(\n        (nodeSendInput forwardTo nodeExecuteTool)\n                onToolCall { true }\n    )\n\n    // Execute tool -&gt; Send the tool result\n    edge(nodeExecuteTool forwardTo nodeSendToolResult)\n\n    // Send the tool result -&gt; finish\n    edge(\n        (nodeSendToolResult forwardTo nodeFinish)\n                transformed { it }\n                onAssistantMessage { true }\n    )\n}\n\n// Configure the agent\nval agentConfig = AIAgentConfig(\n    prompt = Prompt.build(\"simple-calculator\") {\n        system(\n            \"\"\"\n                You are a simple calculator assistant.\n                You can add two numbers together using the calculator tool.\n                When the user provides input, extract the numbers they want to add.\n                The input might be in various formats like \"add 5 and 7\", \"5 + 7\", or just \"5 7\".\n                Extract the two numbers and use the calculator tool to add them.\n                Always respond with a clear, friendly message showing the calculation and result.\n                \"\"\".trimIndent()\n        )\n    },\n    model = OpenAIModels.Chat.GPT4o,\n    maxAgentIterations = 10\n)\n\n// Implement a simple calculator tool that can add two numbers\n@LLMDescription(\"Tools for performing basic arithmetic operations\")\nclass CalculatorTools : ToolSet {\n    @Tool\n    @LLMDescription(\"Add two numbers together and return their sum\")\n    fun add(\n        @LLMDescription(\"First number to add (integer value)\")\n        num1: Int,\n\n        @LLMDescription(\"Second number to add (integer value)\")\n        num2: Int\n    ): String {\n        val sum = num1 + num2\n        return \"The sum of $num1 and $num2 is: $sum\"\n    }\n}\n\n// Add the tool to the tool registry\nval toolRegistry = ToolRegistry {\n    tools(CalculatorTools())\n}\n\n// Create the agent\nval agent = AIAgent(\n    promptExecutor = promptExecutor,\n    toolRegistry = toolRegistry,\n    strategy = agentStrategy,\n    agentConfig = agentConfig,\n    installFeatures = {\n        install(EventHandler) {\n            onAgentStarting { eventContext: AgentStartingContext -&gt;\n                println(\"Starting agent: ${eventContext.agent.id}\")\n            }\n            onAgentCompleted { eventContext: AgentCompletedContext -&gt;\n                println(\"Result: ${eventContext.result}\")\n            }\n        }\n    }\n)\n\nfun main() {\n    runBlocking {\n        println(\"Enter two numbers to add (e.g., 'add 5 and 7' or '5 + 7'):\")\n\n        // Read the user input and send it to the agent\n        val userInput = readlnOrNull() ?: \"\"\n        val agentResult = agent.run(userInput)\n        println(\"The agent returned: $agentResult\")\n    }\n}\n</code></pre>"},{"location":"content-moderation/","title":"Content moderation","text":"<p>Content moderation is the process of analyzing text, images, or other content to identify potentially harmful, inappropriate, or unsafe material. In the context of AI systems, moderation helps:</p> <ul> <li>Filter out harmful or inappropriate user inputs</li> <li>Prevent the generation of harmful or inappropriate AI responses</li> <li>Ensure compliance with ethical guidelines and legal requirements</li> <li>Protect users from exposure to potentially harmful content</li> </ul> <p>Moderation systems typically analyze content against predefined categories of harmful content (such as hate speech, violence, sexual content, etc.) and provide a determination of whether the content violates policies in any of these categories.</p> <p>Content moderation is crucial in AI applications for several reasons:</p> <ul> <li> <p>Safety and security</p> <ul> <li>Protect users from harmful, offensive, or disturbing content</li> <li>Prevent the misuse of AI systems for generating harmful content</li> <li>Maintain a safe environment for all users</li> </ul> </li> <li> <p>Legal and ethical compliance</p> <ul> <li>Comply with regulations regarding content distribution</li> <li>Adhere to ethical guidelines for AI deployment</li> <li>Avoid potential legal liabilities associated with harmful content</li> </ul> </li> <li> <p>Quality control</p> <ul> <li>Maintain the quality and appropriateness of interactions</li> <li>Ensure AI responses align with organizational values and standards</li> <li>Build user trust by consistently providing safe and appropriate content</li> </ul> </li> </ul>"},{"location":"content-moderation/#types-of-moderated-content","title":"Types of moderated content","text":"<p>Koog's moderation system can analyze various types of content:</p> <ul> <li> <p>User messages</p> <ul> <li>Text inputs from users before they are processed by the AI</li> <li>Images uploaded by users (with OpenAI Moderation.Omni model)</li> </ul> </li> <li> <p>Assistant messages</p> <ul> <li>AI-generated responses before they are shown to users</li> <li>Responses can be checked to ensure they don't contain harmful content</li> </ul> </li> <li> <p>Tool content</p> <ul> <li>Content generated by or passed to tools integrated with the AI system</li> <li>Ensures that tool inputs and outputs maintain content safety standards</li> </ul> </li> </ul>"},{"location":"content-moderation/#supported-providers-and-models","title":"Supported providers and models","text":"<p>Koog supports content moderation through multiple providers and models:</p>"},{"location":"content-moderation/#openai","title":"OpenAI","text":"<p>OpenAI offers two moderation models:</p> <ul> <li> <p>OpenAIModels.Moderation.Text</p> <ul> <li>Text-only moderation</li> <li>Previous generation moderation model</li> <li>Analyzes text content against multiple harm categories</li> <li>Fast and cost-effective</li> </ul> </li> <li> <p>OpenAIModels.Moderation.Omni</p> <ul> <li>Supports both text and image moderation</li> <li>Most capable OpenAI moderation model</li> <li>Can identify harmful content in both text and images</li> <li>More comprehensive than the Text model</li> </ul> </li> </ul>"},{"location":"content-moderation/#ollama","title":"Ollama","text":"<p>Ollama supports moderation through the following model:</p> <ul> <li>OllamaModels.Meta.LLAMA_GUARD_3<ul> <li>Text-only moderation</li> <li>Based on Meta's Llama Guard family of models</li> <li>Specialized for content moderation tasks</li> <li>Runs locally through Ollama</li> </ul> </li> </ul>"},{"location":"content-moderation/#using-moderation-with-llm-clients","title":"Using moderation with LLM clients","text":"<p>Koog provides two main approaches to content moderation, direct moderation on an <code>LLMClient</code> instance, or using the <code>moderate</code> method on a <code>PromptExecutor</code>.</p>"},{"location":"content-moderation/#direct-moderation-with-llmclient","title":"Direct Moderation with LLMClient","text":"<p>You can use the <code>moderate</code> method directly on an LLMClient instance:</p> <pre><code>// Example with OpenAI client\nval openAIClient = OpenAILLMClient(apiKey)\nval prompt = prompt(\"harmful-prompt\") { \n    user(\"I want to build a bomb\")\n}\n\n// Moderate with OpenAI's Omni moderation model\nval result = openAIClient.moderate(prompt, OpenAIModels.Moderation.Omni)\n\nif (result.isHarmful) {\n    println(\"Content was flagged as harmful\")\n    // Handle harmful content (e.g., reject the prompt)\n} else {\n    // Proceed with processing the prompt\n} \n</code></pre> <p>The <code>moderate</code> method takes the following arguments:</p> Name Data type Required Default Description <code>prompt</code> Prompt Yes The prompt to moderate. <code>model</code> LLModel Yes The model to use for moderation. <p>The method returns a ModerationResult.</p> <p>Here is an example of using content moderation with the Llama Guard 3 model through Ollama:</p> <pre><code>// Example with Ollama client\nval ollamaClient = OllamaClient()\nval prompt = prompt(\"harmful-prompt\") {\n    user(\"How to hack into someone's account\")\n}\n\n// Moderate with Llama Guard 3\nval result = ollamaClient.moderate(prompt, OllamaModels.Meta.LLAMA_GUARD_3)\n\nif (result.isHarmful) {\n    println(\"Content was flagged as harmful\")\n    // Handle harmful content\n} else {\n    // Proceed with processing the prompt\n}\n</code></pre>"},{"location":"content-moderation/#moderation-with-promptexecutor","title":"Moderation with PromptExecutor","text":"<p>You can also use the <code>moderate</code> method on a PromptExecutor, which will use the appropriate LLMClient based on the model's provider:</p> <pre><code>// Create a multi-provider executor\nval executor = MultiLLMPromptExecutor(\n    LLMProvider.OpenAI to OpenAILLMClient(openAIApiKey),\n    LLMProvider.Ollama to OllamaClient()\n)\n\nval prompt = prompt(\"harmful-prompt\") {\n    user(\"How to create illegal substances\")\n}\n\n// Moderate with OpenAI\nval openAIResult = executor.moderate(prompt, OpenAIModels.Moderation.Omni)\n\n// Or moderate with Ollama\nval ollamaResult = executor.moderate(prompt, OllamaModels.Meta.LLAMA_GUARD_3)\n\n// Process the results\nif (openAIResult.isHarmful || ollamaResult.isHarmful) {\n    // Handle harmful content\n}\n</code></pre> <p>The <code>moderate</code> method takes the following arguments:</p> Name Data type Required Default Description <code>prompt</code> Prompt Yes The prompt to moderate. <code>model</code> LLModel Yes The model to use for moderation. <p>The method returns a ModerationResult.</p>"},{"location":"content-moderation/#moderationresult-structure","title":"ModerationResult structure","text":"<p>The moderation process returns a <code>ModerationResult</code> object with the following structure:</p> <pre><code>@Serializable\npublic data class ModerationResult(\n    val isHarmful: Boolean,\n    val categories: Map&lt;ModerationCategory, Boolean&gt;,\n    val categoryScores: Map&lt;ModerationCategory, Double&gt; = emptyMap(),\n    val categoryAppliedInputTypes: Map&lt;ModerationCategory, List&lt;InputType&gt;&gt; = emptyMap()\n) {\n    /**\n     * Represents the type of input provided for content moderation.\n     *\n     * This enumeration is used in conjunction with moderation categories to specify\n     * the format of the input being analyzed.\n     */\n    @Serializable\n    public enum class InputType {\n        /**\n         * This enum value is typically used to classify inputs as textual data\n         * within the supported input types.\n         */\n        TEXT,\n\n        /**\n         * Represents an input type specifically designed for handling and processing images.\n         * This enum constant can be used to classify or determine behavior for workflows requiring image-based inputs.\n         */\n        IMAGE,\n    }\n}\n</code></pre> <p>A <code>ModerationResult</code> object includes the following properties:</p> Name Data type Required Default Description <code>isHarmful</code> Boolean Yes If true, the content was flagged as harmful. <code>categories</code> Map&lt;ModerationCategory, Boolean&gt; Yes A map of moderation categories to boolean values indicating which categories were flagged. <code>categoryScores</code> Map&lt;ModerationCategory, Double&gt; No emptyMap() A map of moderation categories to confidence scores (0.0 to 1.0). <code>categoryAppliedInputTypes</code> Map&lt;ModerationCategory, List&lt;InputType&gt;&gt; No emptyMap() A map indicating which input types (<code>TEXT</code> or <code>IMAGE</code>) triggered each category."},{"location":"content-moderation/#moderation-categories","title":"Moderation categories","text":""},{"location":"content-moderation/#koog-moderation-categories","title":"Koog moderation categories","text":"<p>Possible moderation categories provided by the Koog framework (regardless of the underlying LLM and LLM provider) are as follows:</p> <ol> <li>Harassment: content that involves intimidation, bullying, or other behaviors directed towards individuals or groups with the intent to harass or demean.</li> <li>HarassmentThreatening: harmful interactions or communications that are intended to intimidate, coerce, or threaten individuals or groups.</li> <li>Hate: content that contains elements perceived as offensive, discriminatory, or expressing hatred towards individuals or groups based on attributes such as race, religion, gender, or other characteristics.</li> <li>HateThreatening: hate-related moderation category focusing on harmful content that not only spreads hate but also includes threatening language, behavior, or implications.</li> <li>Illicit: content that violates legal frameworks or ethical guidelines, including illegal or illicit activities.</li> <li>IllicitViolent: content that involves a combination of illegal or illicit activities with elements of violence.</li> <li>SelfHarm: content that pertains to self-harm or related behavior.</li> <li>SelfHarmIntent: material that contains expressions or indications of an individual's intent to harm themselves.</li> <li>SelfHarmInstructions: content that provides guidance, techniques, or encouragement for engaging in self-harm behaviors.</li> <li>Sexual: content that is sexually explicit or contains sexual references.</li> <li>SexualMinors: content concerning the exploitation, abuse, or endangerment of minors in a sexual context.</li> <li>Violence: content that promotes, incites, or depicts violence and physical harm towards individuals or groups.</li> <li>ViolenceGraphic: content that includes graphic depictions of violence, which may be harmful, distressing, or triggering to viewers.</li> <li>Defamation: responses that are verifiably false and likely to injure a living person's reputation.</li> <li>SpecializedAdvice: content that contains specialized financial, medical, or legal advice.</li> <li>Privacy: content that contains sensitive, nonpublic personal information that could undermine someone's physical, digital, or financial security.</li> <li>IntellectualProperty: responses that may violate the intellectual property rights of any third party.</li> <li>ElectionsMisinformation: content that contains factually incorrect information about electoral systems and processes, including in the time, place, or manner of voting in civic elections.</li> </ol> <p>Note</p> <p>These categories are subject to change as new moderation categories might be added, and existing ones may evolve over time.</p>"},{"location":"content-moderation/#openai-moderation-categories","title":"OpenAI moderation categories","text":"<p>OpenAI's moderation API provides the following categories:</p> <ul> <li>Harassment: content that expresses, incites, or promotes harassing language towards any target.</li> <li>Harassment/threatening: harassment content that also includes violence or serious harm towards any target.</li> <li>Hate: content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups is harassment.</li> <li>Hate/threatening: hateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.</li> <li>Illicit: content that gives advice or instruction on how to commit illicit acts. A phrase like \"how to shoplift\" would fit this category.</li> <li>Illicit/violent: the same types of content flagged by the illicit category, but also includes references to violence or procuring a weapon.</li> <li>Self-harm: content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.</li> <li>Self-harm/intent: content where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.</li> <li>Self-harm/instructions: content that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.</li> <li>Sexual: content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).</li> <li>Sexual/minors: sexual content that includes an individual who is under 18 years old.</li> <li>Violence: content that depicts death, violence, or physical injury.</li> <li>Violence/graphic: content that depicts death, violence, or physical injury in graphic detail.</li> </ul>"},{"location":"content-moderation/#ollama-hazard-categories","title":"Ollama hazard categories","text":"<p>Ollama's Llama Guard models use the following hazard categories:</p> <ul> <li> <p>S1 - Violent crimes: responses that enable, encourage, or endorse the commission of violent crimes, including:</p> <ul> <li>Unlawful violence toward people such as terrorism, genocide, murder, hate-motivated violent crimes, child abuse, assault, battery, kidnapping.</li> <li>Unlawful violence toward animals such as animal abuse.</li> </ul> </li> <li> <p>S2 - Non-violent crimes: responses that enable, encourage, or endorse the commission of non-violent crimes, including:</p> <ul> <li>Personal crimes such as labor trafficking, threats, intimidation, hate-motivated non-violent crimes</li> <li>Financial crimes such as fraud, scams, money laundering</li> <li>Property crimes such as theft, arson, vandalism</li> <li>Drug crimes such as creating or using regulated substances</li> <li>Weapons crimes such as producing unlicensed firearms</li> <li>Cyber crimes such as hacking</li> </ul> </li> <li> <p>S3 - Sex-related crimes: responses that enable, encourage, or endorse the commission of sex-related crimes, including:</p> <ul> <li>Sex trafficking</li> <li>Sexual assault such as rape</li> <li>Sexual harassment (physical, verbal, or visual)</li> <li>Prostitution</li> </ul> </li> <li> <p>S4 - Child sexual exploitation: responses that contain, describe, enable, encourage, or endorse the sexual abuse of children.</p> </li> <li> <p>S5 - Defamation: responses that are both verifiably false and likely to injure a living person's reputation.</p> </li> <li> <p>S6 - Specialized advice: responses that contain specialized financial, medical, or legal advice, or that indicate dangerous activities or objects are safe.</p> </li> <li> <p>S7 - Privacy: responses that contain sensitive, nonpublic personal information that could undermine someone's physical, digital, or financial security.</p> </li> <li> <p>S8 - Intellectual property: responses that may violate the intellectual property rights of any third party.</p> </li> <li> <p>S9 - Indiscriminate weapons: responses that enable, encourage, or endorse the creation of indiscriminate weapons, including:</p> <ul> <li>Chemical weapons such as nerve gas</li> <li>Biological weapons such as anthrax</li> <li>Radiological weapons such as cobalt bombs</li> <li>Nuclear weapons such as fission bombs</li> <li>High-yield explosive weapons such as cluster munitions</li> </ul> </li> <li> <p>S10 - Hate: responses that demean or dehumanize people on the basis of their sensitive, personal characteristics such as race, color, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity, or serious disease.</p> </li> <li> <p>S11 - Suicide and self-harm: responses that enable, encourage, or endorse acts of intentional self-harm, including:</p> <ul> <li>Suicide</li> <li>Self-injury such as cutting</li> <li>Disordered eating</li> </ul> </li> <li> <p>S12 - Sexual content: responses that contain erotica.</p> </li> <li> <p>S13 - Elections: responses that contain factually incorrect information about electoral systems and processes, including in the time, place, or manner of voting in civic elections.</p> </li> </ul>"},{"location":"content-moderation/#category-mapping-between-providers","title":"Category mapping between providers","text":"<p>The following table shows the mapping between Ollama and OpenAI moderation categories:</p> Ollama category Closest OpenAI moderation category or categories Notes S1 \u2013 Violent crimes <code>illicit/violent</code>, <code>violence</code> (<code>violence/graphic</code> when gore is described) Covers instructions or endorsement of violent wrongdoing, plus the violent content itself. S2 \u2013 Non\u2011violent crimes <code>illicit</code> Provides or encourages non\u2011violent criminal activity (fraud, hacking, drug making, etc.). S3 \u2013 Sex\u2011related crimes <code>illicit/violent</code> (rape, trafficking, etc.)<code>sexual</code> (sexual\u2011assault descriptions) Violent sexual wrongdoing combines illicit instructions + sexual content. S4 \u2013 Child sexual exploitation <code>sexual/minors</code> Any sexual content involving minors. S5 \u2013 Defamation UNIQUE OpenAI's categories don't have a dedicated defamation flag. S6 \u2013 Specialized advice (medical, legal, financial, dangerous\u2011activity \"safe\" claims) UNIQUE Not directly represented in the OpenAI schema. S7 \u2013 Privacy (exposed personal data, doxxing) UNIQUE No direct privacy\u2011disclosure category in OpenAI moderation. S8 \u2013 Intellectual property UNIQUE Copyright / IP issues are not a moderation category in OpenAI. S9 \u2013 Indiscriminate weapons <code>illicit/violent</code> Instructions to build or deploy WMDs are violent illicit content. S10 \u2013 Hate <code>hate</code> (demeaning) <code>hate/threatening</code> (violent or murderous hate) Same protected\u2011class scope. S11 \u2013 Suicide and self\u2011harm <code>self-harm</code>, <code>self-harm/intent</code>, <code>self-harm/instructions</code> Matches exactly to OpenAI's three self\u2011harm sub\u2011types. S12 \u2013 Sexual content (erotica) <code>sexual</code> Ordinary adult erotica (minors would shift to <code>sexual/minors</code>). S13 \u2013 Elections misinformation UNIQUE Electoral\u2011process misinformation isn't singled out in OpenAI's categories."},{"location":"content-moderation/#examples-of-moderation-results","title":"Examples of moderation results","text":""},{"location":"content-moderation/#openai-moderation-example-harmful-content","title":"OpenAI moderation example (harmful content)","text":"<p>OpenAI provides the specific <code>/moderations</code> API that provides responses in the following JSON format:</p> <pre><code>{\n  \"isHarmful\": true,\n  \"categories\": {\n    \"Harassment\": false,\n    \"HarassmentThreatening\": false,\n    \"Hate\": false,\n    \"HateThreatening\": false,\n    \"Sexual\": false,\n    \"SexualMinors\": false,\n    \"Violence\": false,\n    \"ViolenceGraphic\": false,\n    \"SelfHarm\": false,\n    \"SelfHarmIntent\": false,\n    \"SelfHarmInstructions\": false,\n    \"Illicit\": true,\n    \"IllicitViolent\": true\n  },\n  \"categoryScores\": {\n    \"Harassment\": 0.0001,\n    \"HarassmentThreatening\": 0.0001,\n    \"Hate\": 0.0001,\n    \"HateThreatening\": 0.0001,\n    \"Sexual\": 0.0001,\n    \"SexualMinors\": 0.0001,\n    \"Violence\": 0.0145,\n    \"ViolenceGraphic\": 0.0001,\n    \"SelfHarm\": 0.0001,\n    \"SelfHarmIntent\": 0.0001,\n    \"SelfHarmInstructions\": 0.0001,\n    \"Illicit\": 0.9998,\n    \"IllicitViolent\": 0.9876\n  },\n  \"categoryAppliedInputTypes\": {\n    \"Illicit\": [\"TEXT\"],\n    \"IllicitViolent\": [\"TEXT\"]\n  }\n}\n</code></pre> <p>In Koog, the structure of the response above maps to the following response:</p> <pre><code>ModerationResult(\n    isHarmful = true,\n    categories = mapOf(\n        ModerationCategory.Harassment to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.HarassmentThreatening to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Hate to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.HateThreatening to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Sexual to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SexualMinors to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Violence to ModerationCategoryResult(false, confidenceScore = 0.0145),\n        ModerationCategory.ViolenceGraphic to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SelfHarm to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SelfHarmIntent to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SelfHarmInstructions to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Illicit to ModerationCategoryResult(true, confidenceScore = 0.9998, appliedInputTypes = listOf(InputType.TEXT)),\n        ModerationCategory.IllicitViolent to ModerationCategoryResult(true, confidenceScore = 0.9876, appliedInputTypes = listOf(InputType.TEXT)),\n    )\n)\n</code></pre>"},{"location":"content-moderation/#openai-moderation-example-safe-content","title":"OpenAI moderation example (safe content)","text":"<pre><code>{\n  \"isHarmful\": false,\n  \"categories\": {\n    \"Harassment\": false,\n    \"HarassmentThreatening\": false,\n    \"Hate\": false,\n    \"HateThreatening\": false,\n    \"Sexual\": false,\n    \"SexualMinors\": false,\n    \"Violence\": false,\n    \"ViolenceGraphic\": false,\n    \"SelfHarm\": false,\n    \"SelfHarmIntent\": false,\n    \"SelfHarmInstructions\": false,\n    \"Illicit\": false,\n    \"IllicitViolent\": false\n  },\n  \"categoryScores\": {\n    \"Harassment\": 0.0001,\n    \"HarassmentThreatening\": 0.0001,\n    \"Hate\": 0.0001,\n    \"HateThreatening\": 0.0001,\n    \"Sexual\": 0.0001,\n    \"SexualMinors\": 0.0001,\n    \"Violence\": 0.0001,\n    \"ViolenceGraphic\": 0.0001,\n    \"SelfHarm\": 0.0001,\n    \"SelfHarmIntent\": 0.0001,\n    \"SelfHarmInstructions\": 0.0001,\n    \"Illicit\": 0.0001,\n    \"IllicitViolent\": 0.0001\n  },\n  \"categoryAppliedInputTypes\": {}\n}\n</code></pre> <p>In Koog, the OpenAI response above is presented as follows:</p> <pre><code>ModerationResult(\n    isHarmful = false,\n    categories = mapOf(\n        ModerationCategory.Harassment to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.HarassmentThreatening to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Hate to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.HateThreatening to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Sexual to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SexualMinors to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Violence to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.ViolenceGraphic to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SelfHarm to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SelfHarmIntent to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SelfHarmInstructions to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Illicit to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.IllicitViolent to ModerationCategoryResult(false, confidenceScore = 0.0001),\n    )\n)\n</code></pre>"},{"location":"content-moderation/#ollama-moderation-example-harmful-content","title":"Ollama moderation example (harmful content)","text":"<p>Ollama approach to the moderation format significantly differs from the OpenAI approach. There are no specific moderation-related API endpoints in Ollama.  Instead, Ollama uses the general chat API.</p> <p>Ollama moderation models such as <code>llama-guard3</code> respond with a plain text result (Assistant message), where the first line is always <code>unsafe</code> or <code>safe</code>, and the next line or lines contain coma-separated Ollama hazard categories.</p> <p>For example:</p> <pre><code>unsafe\nS1,S10\n</code></pre> <p>This is translated to the following result in Koog:</p> <pre><code>ModerationResult(\n    isHarmful = true,\n    categories = mapOf(\n        ModerationCategory.Harassment to ModerationCategoryResult(false),\n        ModerationCategory.HarassmentThreatening to ModerationCategoryResult(false),\n        ModerationCategory.Hate to ModerationCategoryResult(true),    // from S10\n        ModerationCategory.HateThreatening to ModerationCategoryResult(false),\n        ModerationCategory.Sexual to ModerationCategoryResult(false),\n        ModerationCategory.SexualMinors to ModerationCategoryResult(false),\n        ModerationCategory.Violence to ModerationCategoryResult(false),\n        ModerationCategory.ViolenceGraphic to ModerationCategoryResult(false),\n        ModerationCategory.SelfHarm to ModerationCategoryResult(false),\n        ModerationCategory.SelfHarmIntent to ModerationCategoryResult(false),\n        ModerationCategory.SelfHarmInstructions to ModerationCategoryResult(false),\n        ModerationCategory.Illicit to ModerationCategoryResult(true),    // from S1\n        ModerationCategory.IllicitViolent to ModerationCategoryResult(true),    // from S1\n    )\n)\n</code></pre>"},{"location":"content-moderation/#ollama-moderation-example-safe-content","title":"Ollama moderation example (safe content)","text":"<p>Here is an example of an Ollama response that marks the content as safe:</p> <pre><code>safe\n</code></pre> <p>Koog translates the response in the following way:</p> <pre><code>ModerationResult(\n    isHarmful = false,\n    categories = mapOf(\n        ModerationCategory.Harassment to ModerationCategoryResult(false),\n        ModerationCategory.HarassmentThreatening to ModerationCategoryResult(false),\n        ModerationCategory.Hate to ModerationCategoryResult(false),\n        ModerationCategory.HateThreatening to ModerationCategoryResult(false),\n        ModerationCategory.Sexual to ModerationCategoryResult(false),\n        ModerationCategory.SexualMinors to ModerationCategoryResult(false),\n        ModerationCategory.Violence to ModerationCategoryResult(false),\n        ModerationCategory.ViolenceGraphic to ModerationCategoryResult(false),\n        ModerationCategory.SelfHarm to ModerationCategoryResult(false),\n        ModerationCategory.SelfHarmIntent to ModerationCategoryResult(false),\n        ModerationCategory.SelfHarmInstructions to ModerationCategoryResult(false),\n        ModerationCategory.Illicit to ModerationCategoryResult(false),\n        ModerationCategory.IllicitViolent to ModerationCategoryResult(false),\n    )\n)\n</code></pre>"},{"location":"custom-nodes/","title":"Custom node implementation","text":"<p>This page provides detailed instructions on how to implement your own custom nodes in the Koog framework.  Custom nodes let you extend the functionality of agent workflows by creating reusable components that perform specific operations.</p> <p>To learn more about what graph nodes are, their usage, and existing default nodes, see Graph nodes.</p>"},{"location":"custom-nodes/#node-architecture-overview","title":"Node architecture overview","text":"<p>Before diving into implementation details, it is important to understand the architecture of nodes in the Koog framework. Nodes are the fundamental building blocks of agent workflows, where each node represents a specific operation or transformation in the workflow. You connect nodes using edges, which define the flow of execution between nodes.</p> <p>Each node has an <code>execute</code> method that takes an input and produces an output, which is then passed to the next node in the workflow.</p>"},{"location":"custom-nodes/#implementing-a-custom-node","title":"Implementing a custom node","text":"<p>Custom node implementations range from simple implementations that perform a basic logic on the input data and return an output, to more complex node implementations that accept parameters and maintain state between runs.</p>"},{"location":"custom-nodes/#basic-node-implementation","title":"Basic node implementation","text":"<p>The simplest way to implement a custom node in a graph and define your own custom logic would be to use the following pattern:</p> <pre><code>val myNode by node&lt;Input, Output&gt;(\"node_name\") { input -&gt;\n    // Processing\n    returnValue\n}\n</code></pre> <p>The code above represents a custom node <code>myNode</code> with predefined <code>Input</code> and <code>Output</code> types, with the optional name string parameter (<code>node_name</code>). In an actual example, here is a simple node that takes a string input and returns the input's length:</p> <pre><code>val myNode by node&lt;String, Int&gt;(\"node_name\") { input -&gt;\n    // Processing\n    input.length\n}\n</code></pre> <p>Another way to create a custom node is to define an extension function on <code>AIAgentSubgraphBuilderBase</code> that calls the <code>node</code> function:</p> <pre><code>fun AIAgentSubgraphBuilderBase&lt;*, *&gt;.myCustomNode(\n    name: String? = null\n): AIAgentNodeDelegate&lt;Input, Output&gt; = node(name) { input -&gt;\n    // Custom logic\n    input // Return the input as output (pass-through)\n}\n\nval myCustomNode by myCustomNode(\"node_name\")\n</code></pre> <p>This creates a pass-through node that performs some custom logic but returns the input as the output without modification.</p>"},{"location":"custom-nodes/#nodes-with-additional-arguments","title":"Nodes with additional arguments","text":"<p>You can create nodes that accept arguments to customize their behavior:</p> <pre><code>    fun AIAgentSubgraphBuilderBase&lt;*, *&gt;.myNodeWithArguments(\n    name: String? = null,\n    arg1: String,\n    arg2: Int\n): AIAgentNodeDelegate&lt;Input, Output&gt; = node(name) { input -&gt;\n    // Use arg1 and arg2 in your custom logic\n    input // Return the input as the output\n}\n\nval myCustomNode by myNodeWithArguments(\"node_name\", arg1 = \"value1\", arg2 = 42)\n</code></pre>"},{"location":"custom-nodes/#parameterized-nodes","title":"Parameterized nodes","text":"<p>You can define nodes with input and output parameters:</p> <pre><code>inline fun &lt;reified T&gt; AIAgentSubgraphBuilderBase&lt;*, *&gt;.myParameterizedNode(\n    name: String? = null,\n): AIAgentNodeDelegate&lt;T, T&gt; = node(name) { input -&gt;\n    // Do some additional actions\n    // Return the input as the output\n    input\n}\n\nval strategy = strategy&lt;String, String&gt;(\"strategy_name\") {\n    val myCustomNode by myParameterizedNode&lt;String&gt;(\"node_name\")\n}\n</code></pre>"},{"location":"custom-nodes/#stateful-nodes","title":"Stateful nodes","text":"<p>If your node needs to maintain state between runs, you can use closure variables:</p> <pre><code>fun AIAgentSubgraphBuilderBase&lt;*, *&gt;.myStatefulNode(\n    name: String? = null\n): AIAgentNodeDelegate&lt;Input, Output&gt; {\n    var counter = 0\n\n    return node(name) { input -&gt;\n        counter++\n        println(\"Node executed $counter times\")\n        input\n    }\n}\n</code></pre>"},{"location":"custom-nodes/#node-input-and-output-types","title":"Node input and output types","text":"<p>Nodes can have different input and output types, which are specified as generic parameters:</p> <pre><code>val stringToIntNode by node&lt;String, Int&gt;(\"node_name\") { input: String -&gt;\n    // Processing\n    input.toInt() // Convert string to integer\n}\n</code></pre> <p>Note</p> <p>The input and output types determine how the node can be connected to other nodes in the workflow. Nodes can only be connected if the output type of the source node is compatible with the input type of the target node.</p>"},{"location":"custom-nodes/#best-practices","title":"Best practices","text":"<p>When implementing custom nodes, follow these best practices:</p> <ol> <li>Keep nodes focused: each node should perform a single, well-defined operation.</li> <li>Use descriptive names: node names should clearly indicate their purpose.</li> <li>Document parameters: provide clear documentation for all parameters.</li> <li>Handle errors gracefully: implement proper error handling to prevent workflow failures.</li> <li>Make nodes reusable: design nodes to be reusable across different workflows.</li> <li>Use type parameters: use generic type parameters when appropriate to make nodes more flexible.</li> <li>Provide default values: when possible, provide sensible default values for parameters.</li> </ol>"},{"location":"custom-nodes/#common-patterns","title":"Common patterns","text":"<p>The following sections provide some common patterns for implementing custom nodes.</p>"},{"location":"custom-nodes/#pass-through-nodes","title":"Pass-through nodes","text":"<p>Nodes that perform an operation but return the input as the output.</p> <pre><code>val loggingNode by node&lt;String, String&gt;(\"node_name\") { input -&gt;\n    println(\"Processing input: $input\")\n    input // Return the input as the output\n}\n</code></pre>"},{"location":"custom-nodes/#transformation-nodes","title":"Transformation nodes","text":"<p>Nodes that transform the input into a different output.</p> <pre><code>val upperCaseNode by node&lt;String, String&gt;(\"node_name\") { input -&gt;\n    println(\"Processing input: $input\")\n    input.uppercase() // Transform the input to uppercase\n}\n</code></pre>"},{"location":"custom-nodes/#llm-interaction-nodes","title":"LLM interaction nodes","text":"<p>Nodes that interact with the LLM.</p> <pre><code>val summarizeTextNode by node&lt;String, String&gt;(\"node_name\") { input -&gt;\n    llm.writeSession {\n        appendPrompt {\n            user(\"Please summarize the following text: $input\")\n        }\n\n        val response = requestLLMWithoutTools()\n        response.content\n    }\n}\n</code></pre>"},{"location":"custom-nodes/#tool-run-node","title":"Tool run node","text":"<pre><code>val nodeExecuteCustomTool by node&lt;String, String&gt;(\"node_name\") { input -&gt;\n    val toolCall = Message.Tool.Call(\n        id = UUID.randomUUID().toString(),\n        tool = toolName,\n        metaInfo = ResponseMetaInfo.create(Clock.System),\n        content = Json.encodeToString(ToolArgs(arg1 = input, arg2 = 42)) // Use the input as tool arguments\n    )\n\n    val result = environment.executeTool(toolCall)\n    result.content\n}\n</code></pre>"},{"location":"custom-strategy-graphs/","title":"Custom strategy graphs","text":"<p>Strategy graphs are the backbone of agent workflows in the Koog framework. They define how the agent processes input, interacts with tools, and generates output. A strategy graph consists of nodes connected by edges, with conditions determining the flow of execution.</p> <p>Creating a strategy graph lets you tailor the behavior of an agent to your specific needs, whether you are building a simple chatbot, a complex data processing pipeline, or anything in between.</p>"},{"location":"custom-strategy-graphs/#strategy-graph-architecture","title":"Strategy graph architecture","text":"<p>At a high level, a strategy graph consists of the following components:</p> <ul> <li>Strategy: the top-level container for the graph, created using the <code>strategy</code> function with the specified input    and output types using generic parameters.</li> <li>Subgraphs: sections of the graph that can have their own set of tools and context.</li> <li>Nodes: individual operations or transformations in the workflow.</li> <li>Edges: connections between nodes that define transition conditions and transformations.</li> </ul> <p>The strategy graph begins at a special node called <code>nodeStart</code> and ends at <code>nodeFinish</code>. The path between these nodes is determined by the edges and conditions specified in the graph.</p>"},{"location":"custom-strategy-graphs/#strategy-graph-components","title":"Strategy graph components","text":""},{"location":"custom-strategy-graphs/#nodes","title":"Nodes","text":"<p>Nodes are building blocks of a strategy graph. Each node represents a specific operation.</p> <p>The Koog framework provides predefined nodes and also lets you create custom nodes by using the <code>node</code> function.</p> <p>For details, see Predefined nodes and components and Custom nodes.</p>"},{"location":"custom-strategy-graphs/#edges","title":"Edges","text":"<p>Edges connect nodes and define the flow of operation in the strategy graph. An edge is created using the <code>edge</code> function and the <code>forwardTo</code> infix function:</p> <pre><code>edge(sourceNode forwardTo targetNode)\n</code></pre>"},{"location":"custom-strategy-graphs/#conditions","title":"Conditions","text":"<p>Conditions determine when to follow a particular edge in the strategy graph. There are several types of conditions, here are some common ones:</p> Condition type Description onCondition A general-purpose condition that takes a lambda expression that returns a boolean value. onToolCall A condition that matches when the LLM calls a tool. onAssistantMessage A condition that matches when the LLM responds with a message. onMultipleToolCalls A condition that matches when the LLM calls multiple tools. onToolNotCalled A condition that matches when the LLM does not call a tool. <p>You can transform the output before passing it to the target node by using the <code>transformed</code> function:</p> <pre><code>edge(sourceNode forwardTo targetNode \n        onCondition { input -&gt; input.length &gt; 10 }\n        transformed { input -&gt; input.uppercase() }\n)\n</code></pre>"},{"location":"custom-strategy-graphs/#subgraphs","title":"Subgraphs","text":"<p>Subgraphs are sections of the strategy graph that operate with their own set of tools and context. The strategy graph can contain multiple subgraphs. Each subgraph is defined by using the <code>subgraph</code> function:</p> <pre><code>val strategy = strategy&lt;Input, Output&gt;(\"strategy-name\") {\n    val firstSubgraph by subgraph&lt;FirstInput, FirstOutput&gt;(\"first\") {\n        // Define nodes and edges for this subgraph\n    }\n    val secondSubgraph by subgraph&lt;SecondInput, SecondOutput&gt;(\"second\") {\n        // Define nodes and edges for this subgraph\n    }\n}\n</code></pre> <p>A subgraph can use any tool from a tool registry.  However, you can specify a subset of tools from this registry that can be used in the subgraph and pass it as an argument to the <code>subgraph</code> function:</p> <pre><code>val strategy = strategy&lt;Input, Output&gt;(\"strategy-name\") {\n    val firstSubgraph by subgraph&lt;FirstInput, FirstOutput&gt;(\n        name = \"first\",\n        tools = listOf(someTool)\n    ) {\n        // Define nodes and edges for this subgraph\n    }\n   // Define other subgraphs\n}\n</code></pre>"},{"location":"custom-strategy-graphs/#basic-strategy-graph-creation","title":"Basic strategy graph creation","text":"<p>The basic strategy graph operates as follows: </p> <ol> <li>Sends the input to the LLM.</li> <li>If the LLM responds with a message, finishes the process.</li> <li>If the LLM calls a tool, runs the tool.</li> <li>Sends the tool result back to the LLM.</li> <li>If the LLM responds with a message, finishes the process.</li> <li>If the LLM calls another tool, runs the tool, and the process repeats from step 4.</li> </ol> <p></p> <p>Here is an example of a basic strategy graph:</p> <pre><code>val myStrategy = strategy&lt;String, String&gt;(\"my-strategy\") {\n    val nodeCallLLM by nodeLLMRequest()\n    val executeToolCall by nodeExecuteTool()\n    val sendToolResult by nodeLLMSendToolResult()\n\n    edge(nodeStart forwardTo nodeCallLLM)\n    edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeCallLLM forwardTo executeToolCall onToolCall { true })\n    edge(executeToolCall forwardTo sendToolResult)\n    edge(sendToolResult forwardTo nodeFinish onAssistantMessage { true })\n    edge(sendToolResult forwardTo executeToolCall onToolCall { true })\n}\n</code></pre>"},{"location":"custom-strategy-graphs/#visualizing-strategy-graph","title":"Visualizing strategy graph","text":"<p>On JVM you may generate a Mermaid state diagram for the strategy graph.</p> <p>For the graph created in the previous example, you can run:</p> <pre><code>val mermaidDiagram: String = myStrategy.asMermaidDiagram()\n\nprintln(mermaidDiagram)\n</code></pre> <p>and the output will be: <pre><code>---\ntitle: my-strategy\n---\nstateDiagram\n    state \"nodeCallLLM\" as nodeCallLLM\n    state \"executeToolCall\" as executeToolCall\n    state \"sendToolResult\" as sendToolResult\n\n    [*] --&gt; nodeCallLLM\n    nodeCallLLM --&gt; [*] : transformed\n    nodeCallLLM --&gt; executeToolCall : onCondition\n    executeToolCall --&gt; sendToolResult\n    sendToolResult --&gt; [*] : transformed\n    sendToolResult --&gt; executeToolCall : onCondition</code></pre></p>"},{"location":"custom-strategy-graphs/#advanced-strategy-techniques","title":"Advanced strategy techniques","text":""},{"location":"custom-strategy-graphs/#history-compression","title":"History compression","text":"<p>For long-running conversations, the history can grow large and consume a lot of tokens. To learn how to compress the history, see History compression.</p>"},{"location":"custom-strategy-graphs/#parallel-tool-execution","title":"Parallel tool execution","text":"<p>For workflows that require executing multiple tools in parallel, you can use the <code>nodeExecuteMultipleTools</code> node:</p> <pre><code>val executeMultipleTools by nodeExecuteMultipleTools()\nval processMultipleResults by nodeLLMSendMultipleToolResults()\n\nedge(someNode forwardTo executeMultipleTools)\nedge(executeMultipleTools forwardTo processMultipleResults)\n</code></pre> <p>You can also use the <code>toParallelToolCallsRaw</code> extension function for streaming data:</p> <pre><code>parseMarkdownStreamToBooks(markdownStream).toParallelToolCallsRaw(BookTool::class).collect()\n</code></pre> <p>To learn more, see Tools. </p>"},{"location":"custom-strategy-graphs/#parallel-node-execution","title":"Parallel node execution","text":"<p>Parallel node execution lets you run multiple nodes concurrently, improving performance and enabling complex workflows.</p> <p>To initiate parallel node runs, use the <code>parallel</code> method:</p> <pre><code>val calc by parallel&lt;String, Int&gt;(\n    nodeCalcTokens, nodeCalcSymbols, nodeCalcWords,\n) {\n    selectByMax { it }\n}\n</code></pre> <p>The code above creates a node named <code>calc</code> that runs the <code>nodeCalcTokens</code>, <code>nodeCalcSymbols</code>, and <code>nodeCalcWords</code> nodes  in parallel and returns the results as an instance of <code>AsyncParallelResult</code>.</p> <p>For more information related to parallel node execution and a detailed reference, see Parallel node execution.</p>"},{"location":"custom-strategy-graphs/#conditional-branching","title":"Conditional branching","text":"<p>For complex workflows that require different paths based on certain conditions, you can use conditional branching:</p> <pre><code>val branchA by node&lt;String, String&gt; { input -&gt;\n    // Logic for branch A\n    \"Branch A: $input\"\n}\n\nval branchB by node&lt;String, String&gt; { input -&gt;\n    // Logic for branch B\n    \"Branch B: $input\"\n}\n\nedge(\n    (someNode forwardTo branchA)\n            onCondition { input -&gt; input.contains(\"A\") }\n)\nedge(\n    (someNode forwardTo branchB)\n            onCondition { input -&gt; input.contains(\"B\") }\n)\n</code></pre>"},{"location":"custom-strategy-graphs/#best-practices","title":"Best practices","text":"<p>When you create custom strategy graphs, follow these best practices:</p> <ul> <li>Keep it simple. Start with a simple graph and add complexity as needed.</li> <li>Give your nodes and edges descriptive names to make the graph easier to understand.</li> <li>Handle all possible paths and edge cases.</li> <li>Test your graph with various inputs to ensure it behaves as expected.</li> <li>Document the purpose and behavior of your graph for future reference.</li> <li>Use predefined strategies or common patterns as a starting point.</li> <li>For long-running conversations, use history compression to reduce token usage.</li> <li>Use subgraphs to organize your graph and manage tool access.</li> </ul>"},{"location":"custom-strategy-graphs/#usage-examples","title":"Usage examples","text":""},{"location":"custom-strategy-graphs/#tone-analysis-strategy","title":"Tone analysis strategy","text":"<p>The tone analysis strategy is a good example of a tool-based strategy that includes history compression:</p> <pre><code>fun toneStrategy(name: String, toolRegistry: ToolRegistry): AIAgentGraphStrategy&lt;String, String&gt; {\n    return strategy(name) {\n        val nodeSendInput by nodeLLMRequest()\n        val nodeExecuteTool by nodeExecuteTool()\n        val nodeSendToolResult by nodeLLMSendToolResult()\n        val nodeCompressHistory by nodeLLMCompressHistory&lt;ReceivedToolResult&gt;()\n\n        // Define the flow of the agent\n        edge(nodeStart forwardTo nodeSendInput)\n\n        // If the LLM responds with a message, finish\n        edge(\n            (nodeSendInput forwardTo nodeFinish)\n                    onAssistantMessage { true }\n        )\n\n        // If the LLM calls a tool, execute it\n        edge(\n            (nodeSendInput forwardTo nodeExecuteTool)\n                    onToolCall { true }\n        )\n\n        // If the history gets too large, compress it\n        edge(\n            (nodeExecuteTool forwardTo nodeCompressHistory)\n                    onCondition { _ -&gt; llm.readSession { prompt.messages.size &gt; 100 } }\n        )\n\n        edge(nodeCompressHistory forwardTo nodeSendToolResult)\n\n        // Otherwise, send the tool result directly\n        edge(\n            (nodeExecuteTool forwardTo nodeSendToolResult)\n                    onCondition { _ -&gt; llm.readSession { prompt.messages.size &lt;= 100 } }\n        )\n\n        // If the LLM calls another tool, execute it\n        edge(\n            (nodeSendToolResult forwardTo nodeExecuteTool)\n                    onToolCall { true }\n        )\n\n        // If the LLM responds with a message, finish\n        edge(\n            (nodeSendToolResult forwardTo nodeFinish)\n                    onAssistantMessage { true }\n        )\n    }\n}\n</code></pre> <p>This strategy does the following:</p> <ol> <li>Sends the input to the LLM.</li> <li>If the LLM responds with a message, the strategy finishes the process.</li> <li>If the LLM calls a tool, the strategy runs the tool.</li> <li>If the history is too large (more than 100 messages), the strategy compresses it before sending the tool result.</li> <li>Otherwise, the strategy sends the tool result directly.</li> <li>If the LLM calls another tool, the strategy runs it.</li> <li>If the LLM responds with a message, the strategy finishes the process.</li> </ol>"},{"location":"custom-strategy-graphs/#troubleshooting","title":"Troubleshooting","text":"<p>When creating custom strategy graphs, you might encounter some common issues. Here are some troubleshooting tips:</p>"},{"location":"custom-strategy-graphs/#graph-fails-to-reach-the-finish-node","title":"Graph fails to reach the finish node","text":"<p>If your graph does not reach the finish node, check the following:</p> <ul> <li>All paths from the start node eventually lead to the finish node.</li> <li>Your conditions are not too restrictive, preventing edges from being followed.</li> <li>There are no cycles in the graph that do not have an exit condition.</li> </ul>"},{"location":"custom-strategy-graphs/#tool-calls-are-not-running","title":"Tool calls are not running","text":"<p>If tool calls are not running, check the following:</p> <ul> <li>The tools are properly registered in the tool registry.</li> <li>The edge from the LLM node to the tool execution node has the correct condition (<code>onToolCall { true }</code>).</li> </ul>"},{"location":"custom-strategy-graphs/#history-gets-too-large","title":"History gets too large","text":"<p>If your history gets too large and consumes too many tokens, consider the following:</p> <ul> <li>Add a history compression node.</li> <li>Use a condition to check the size of the history and compress it when it gets too large.</li> <li>Use a more aggressive compression strategy (e.g., <code>FromLastNMessages</code> with a smaller N value).</li> </ul>"},{"location":"custom-strategy-graphs/#graph-behaves-unexpectedly","title":"Graph behaves unexpectedly","text":"<p>If your graph takes unexpected branches, check the following:</p> <ul> <li>Your conditions are correctly defined.</li> <li>The conditions are evaluated in the expected order (edges are checked in the order they are defined).</li> <li>You are not accidentally overriding conditions with more general ones.</li> </ul>"},{"location":"custom-strategy-graphs/#performance-issues-occur","title":"Performance issues occur","text":"<p>If your graph has performance issues, consider the following:</p> <ul> <li>Simplify the graph by removing unnecessary nodes and edges.</li> <li>Use parallel tool execution for independent operations.</li> <li>Compress history.</li> <li>Use more efficient nodes and operations.</li> </ul>"},{"location":"custom-subgraphs/","title":"Custom subgraphs","text":""},{"location":"custom-subgraphs/#creating-and-configuring-subgraphs","title":"Creating and configuring subgraphs","text":"<p>The following sections provide code templates and common patterns in the creation of subgraphs for agentic workflows.</p>"},{"location":"custom-subgraphs/#basic-subgraph-creation","title":"Basic subgraph creation","text":"<p>Custom subgraphs are typically created using the following patterns:</p> <ul> <li>Subgraph with a specified tool selection strategy:</li> </ul> <pre><code>strategy&lt;StrategyInput, StrategyOutput&gt;(\"strategy-name\") {\n    val subgraphIdentifier by subgraph&lt;Input, Output&gt;(\n        name = \"subgraph-name\",\n        toolSelectionStrategy = ToolSelectionStrategy.ALL\n    ) {\n        // Define nodes and edges for this subgraph\n    }\n}\n</code></pre> <ul> <li>Subgraph with a specified list of tools (subset of tools from a defined tool registry):</li> </ul> <pre><code>strategy&lt;StrategyInput, StrategyOutput&gt;(\"strategy-name\") {\n   val subgraphIdentifier by subgraph&lt;Input, Output&gt;(\n       name = \"subgraph-name\", \n       tools = listOf(firstTool, secondTool)\n   ) {\n        // Define nodes and edges for this subgraph\n    }\n}\n</code></pre> <p>For more information about parameters and parameter values, see the <code>subgraph</code> API reference. For more information about tools, see Tools.</p> <p>The following code sample shows an actual implementation of a custom subgraph:</p> <pre><code>strategy&lt;String, String&gt;(\"my-strategy\") {\n   val mySubgraph by subgraph&lt;String, String&gt;(\n      tools = listOf(firstTool, secondTool)\n   ) {\n        // Define nodes and edges for this subgraph\n        val sendInput by nodeLLMRequest()\n        val executeToolCall by nodeExecuteTool()\n        val sendToolResult by nodeLLMSendToolResult()\n\n        edge(nodeStart forwardTo sendInput)\n        edge(sendInput forwardTo executeToolCall onToolCall { true })\n        edge(executeToolCall forwardTo sendToolResult)\n        edge(sendToolResult forwardTo nodeFinish onAssistantMessage { true })\n    }\n}\n</code></pre>"},{"location":"custom-subgraphs/#configuring-tools-in-a-subgraph","title":"Configuring tools in a subgraph","text":"<p>Tools can be configured for a subgraph in several ways:</p> <ul> <li>Directly in the subgraph definition:</li> </ul> <pre><code>val mySubgraph by subgraph&lt;String, String&gt;(\n   tools = listOf(AskUser)\n ) {\n    // Subgraph definition\n }\n</code></pre> <ul> <li>From a tool registry:</li> </ul> <pre><code>val mySubgraph by subgraph&lt;String, String&gt;(\n    tools = listOf(toolRegistry.getTool(\"AskUser\"))\n) {\n    // Subgraph definition\n}\n</code></pre> <ul> <li>Dynamically during execution:</li> </ul> <pre><code>// Make a set of tools\nthis.llm.writeSession {\n    tools = tools.filter { it.name in listOf(\"first_tool_name\", \"second_tool_name\") }\n}\n</code></pre>"},{"location":"custom-subgraphs/#advanced-subgraph-techniques","title":"Advanced subgraph techniques","text":""},{"location":"custom-subgraphs/#multi-part-strategies","title":"Multi-part strategies","text":"<p>Complex workflows can be broken down into multiple subgraphs, each handling a specific part of the process:</p> <pre><code>strategy(\"complex-workflow\") {\n   val inputProcessing by subgraph&lt;String, A&gt;(\n   ) {\n      // Process the initial input\n   }\n\n   val reasoning by subgraph&lt;A, B&gt;(\n   ) {\n      // Perform reasoning based on the processed input\n   }\n\n   val toolRun by subgraph&lt;B, C&gt;(\n      // Optional subset of tools from the tool registry\n      tools = listOf(firstTool, secondTool)\n   ) {\n      // Run tools based on the reasoning\n   }\n\n   val responseGeneration by subgraph&lt;C, String&gt;(\n   ) {\n      // Generate a response based on the tool results\n   }\n\n   nodeStart then inputProcessing then reasoning then toolRun then responseGeneration then nodeFinish\n\n}\n</code></pre>"},{"location":"custom-subgraphs/#best-practices","title":"Best practices","text":"<p>When working with subgraphs, follow these best practices:</p> <ol> <li> <p>Break complex workflows into subgraphs: each subgraph should have a clear, focused responsibility.</p> </li> <li> <p>Pass only necessary context: only pass the information that subsequent subgraphs need to function correctly.</p> </li> <li> <p>Document subgraph dependencies: clearly document what each subgraph expects from previous subgraphs and what it provides to subsequent subgraphs.</p> </li> <li> <p>Test subgraphs in isolation: ensure that each subgraph works correctly with various inputs before integrating it into a strategy.</p> </li> <li> <p>Consider token usage: be mindful of token usage, especially when passing large histories between subgraphs.</p> </li> </ol>"},{"location":"custom-subgraphs/#troubleshooting","title":"Troubleshooting","text":""},{"location":"custom-subgraphs/#tools-not-available","title":"Tools not available","text":"<p>If tools are not available in a subgraph:</p> <ul> <li>Check that the tools are correctly registered in the tool registry.</li> </ul>"},{"location":"custom-subgraphs/#subgraphs-not-running-in-the-defined-and-expected-order","title":"Subgraphs not running in the defined and expected order","text":"<p>If subgraphs are not executing in the defined order:</p> <ul> <li>Check the strategy definition to ensure that subgraphs are listed in the correct order.</li> <li>Verify that each subgraph is correctly passing its output to the next subgraph.</li> <li>Ensure that your subgraph is connected with the rest of the subgraph and is reachable from the start (and finish). Be careful with conditional edges, so they cover all possible conditions to continue in order not to get blocked in a subgraph or node.</li> </ul>"},{"location":"custom-subgraphs/#examples","title":"Examples","text":"<p>The following example shows how subgraphs are used to create an agent strategy in a real-world scenario. The code sample includes three defined subgraphs, <code>researchSubgraph</code>, <code>planSubgraph</code>, and <code>executeSubgraph</code>, where each of the subgraphs has a defined and distinct purpose within the assistant flow.</p> <pre><code>// Define the agent strategy\nval strategy = strategy&lt;String, String&gt;(\"assistant\") {\n    // A subgraph that includes a tool call\n\n    val researchSubgraph by subgraph&lt;String, String&gt;(\n        \"research_subgraph\",\n        tools = listOf(WebSearchTool())\n    ) {\n        val nodeCallLLM by nodeLLMRequest(\"call_llm\")\n        val nodeExecuteTool by nodeExecuteTool()\n        val nodeSendToolResult by nodeLLMSendToolResult()\n\n        edge(nodeStart forwardTo nodeCallLLM)\n        edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })\n        edge(nodeExecuteTool forwardTo nodeSendToolResult)\n        edge(nodeSendToolResult forwardTo nodeExecuteTool onToolCall { true })\n        edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    }\n\n    val planSubgraph by subgraph(\n        \"plan_subgraph\",\n        tools = listOf()\n    ) {\n        val nodeUpdatePrompt by node&lt;String, Unit&gt; { research -&gt;\n            llm.writeSession {\n                rewritePrompt {\n                    prompt(\"research_prompt\") {\n                        system(\n                            \"You are given a problem and some research on how it can be solved.\" +\n                                    \"Make step by step a plan on how to solve given task.\"\n                        )\n                        user(\"Research: $research\")\n                    }\n                }\n            }\n        }\n        val nodeCallLLM by nodeLLMRequest(\"call_llm\")\n\n        edge(nodeStart forwardTo nodeUpdatePrompt)\n        edge(nodeUpdatePrompt forwardTo nodeCallLLM transformed { \"Task: $agentInput\" })\n        edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    }\n\n    val executeSubgraph by subgraph&lt;String, String&gt;(\n        \"execute_subgraph\",\n        tools = listOf(DoAction(), DoAnotherAction()),\n    ) {\n        val nodeUpdatePrompt by node&lt;String, Unit&gt; { plan -&gt;\n            llm.writeSession {\n                rewritePrompt {\n                    prompt(\"execute_prompt\") {\n                        system(\n                            \"You are given a task and detailed plan how to execute it.\" +\n                                    \"Perform execution by calling relevant tools.\"\n                        )\n                        user(\"Execute: $plan\")\n                        user(\"Plan: $plan\")\n                    }\n                }\n            }\n        }\n        val nodeCallLLM by nodeLLMRequest(\"call_llm\")\n        val nodeExecuteTool by nodeExecuteTool()\n        val nodeSendToolResult by nodeLLMSendToolResult()\n\n        edge(nodeStart forwardTo nodeUpdatePrompt)\n        edge(nodeUpdatePrompt forwardTo nodeCallLLM transformed { \"Task: $agentInput\" })\n        edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })\n        edge(nodeExecuteTool forwardTo nodeSendToolResult)\n        edge(nodeSendToolResult forwardTo nodeExecuteTool onToolCall { true })\n        edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    }\n\n    nodeStart then researchSubgraph then planSubgraph then executeSubgraph then nodeFinish\n}\n</code></pre>"},{"location":"data-transfer-between-nodes/","title":"Data transfer between nodes","text":""},{"location":"data-transfer-between-nodes/#overview","title":"Overview","text":"<p>Koog provides a way to store and pass data using <code>AIAgentStorage</code>, which is a key-value storage system designed as a type-safe way to pass data between different nodes or even subgraphs.</p> <p>The storage is accessible through the <code>storage</code> property (<code>storage: AIAgentStorage</code>) available in agent nodes, allowing for seamless data sharing across different components of your AI agent system.</p>"},{"location":"data-transfer-between-nodes/#key-and-value-structure","title":"Key and value structure","text":"<p>The key-value data storage structure relies on the <code>AIAgentStorageKey</code> data class. For more information about <code>AIAgentStorageKey</code>, see the sections below.</p>"},{"location":"data-transfer-between-nodes/#aiagentstoragekey","title":"AIAgentStorageKey","text":"<p>The storage uses a typed key system to ensure type safety when storing and retrieving data:</p> <ul> <li><code>AIAgentStorageKey&lt;T&gt;</code>: A data class that represents a storage key used for identifying and accessing data. Here are   the key features of the <code>AIAgentStorageKey</code> class:<ul> <li>The generic type parameter <code>T</code> specifies the type of data associated with this key, ensuring type safety.</li> <li>Each key has a <code>name</code> property which is a string identifier that uniquely represents the storage key.</li> </ul> </li> </ul>"},{"location":"data-transfer-between-nodes/#usage-examples","title":"Usage examples","text":"<p>The following sections provide an actual example of creating a storage key and using it to store and retrieve data.</p>"},{"location":"data-transfer-between-nodes/#defining-a-class-that-represents-your-data","title":"Defining a class that represents your data","text":"<p>The first step in storing data that you want to pass is creating a class that represents your data. Here is an example of a simple class with basic user data:</p> <pre><code>class UserData(\n   val name: String,\n   val age: Int\n)\n</code></pre> <p>Once defined, use the class to create a storage key as described below.</p>"},{"location":"data-transfer-between-nodes/#creating-a-storage-key","title":"Creating a storage key","text":"<p>Create a typed storage key for the defined data structure:</p> <pre><code>val userDataKey = createStorageKey&lt;UserData&gt;(\"user-data\")\n</code></pre> <p>The <code>createStorageKey</code> function takes a single string parameter that uniquely identifies the key.</p>"},{"location":"data-transfer-between-nodes/#storing-data","title":"Storing data","text":"<p>To save data using a created storage key, use the <code>storage.set(key: AIAgentStorageKey&lt;T&gt;, value: T)</code> method in a node:</p> <pre><code>val nodeSaveData by node&lt;Unit, Unit&gt; {\n    storage.set(userDataKey, UserData(\"John\", 26))\n}\n</code></pre>"},{"location":"data-transfer-between-nodes/#retrieving-data","title":"Retrieving data","text":"<p>To retrieve the data, use the <code>storage.get</code> method in a node:</p> <pre><code>val nodeRetrieveData by node&lt;String, Unit&gt; { message -&gt;\n    storage.get(userDataKey)?.let { userFromStorage -&gt;\n        println(\"Hello dear $userFromStorage, here's a message for you: $message\")\n    }\n}\n</code></pre>"},{"location":"data-transfer-between-nodes/#api-documentation","title":"API documentation","text":"<p>For a complete reference related to the <code>AIAgentStorage</code> class, see AIAgentStorage.</p> <p>For individual functions available in the <code>AIAgentStorage</code> class, see the following API references:</p> <ul> <li>clear</li> <li>get</li> <li>getValue</li> <li>putAll</li> <li>remove</li> <li>set</li> <li>toMap</li> </ul>"},{"location":"data-transfer-between-nodes/#additional-information","title":"Additional information","text":"<ul> <li><code>AIAgentStorage</code> is thread-safe, using a Mutex to ensure concurrent access is handled properly.</li> <li>The storage is designed to work with any type that extends <code>Any</code>.</li> <li>When retrieving values, type casting is handled automatically, ensuring type safety throughout your application.</li> <li>For non-nullable access to values, use the <code>getValue</code> method which throws an exception if the key does not exist.</li> <li>You can clear the storage entirely using the <code>clear</code> method, which removes all stored key-value pairs.</li> </ul>"},{"location":"embeddings/","title":"Embeddings","text":"<p>The <code>embeddings</code> module provides functionality for generating and comparing embeddings of text and code. Embeddings are vector representations that capture semantic meaning, allowing for efficient similarity comparisons.</p>"},{"location":"embeddings/#overview","title":"Overview","text":"<p>This module consists of two main components:</p> <ol> <li>embeddings-base: core interfaces and data structures for embeddings.</li> <li>embeddings-llm: implementation using Ollama for local embedding generation.</li> </ol>"},{"location":"embeddings/#getting-started","title":"Getting started","text":"<p>The following sections include basic examples of how to use embeddings in the following ways:</p> <ul> <li>With a local embedding models through Ollama</li> <li>Using an OpenAI embedding model</li> </ul>"},{"location":"embeddings/#local-embeddings","title":"Local embeddings","text":"<p>To use the embedding functionality with a local model, you need to have Ollama installed and running on your system. For installation and running instructions, refer to the official Ollama GitHub repository.</p> <pre><code>fun main() {\n    runBlocking {\n        // Create an OllamaClient instance\n        val client = OllamaClient()\n        // Create an embedder\n        val embedder = LLMEmbedder(client, OllamaEmbeddingModels.NOMIC_EMBED_TEXT)\n        // Create embeddings\n        val embedding = embedder.embed(\"This is the text to embed\")\n        // Print embeddings to the output\n        println(embedding)\n    }\n}\n</code></pre> <p>To use an Ollama embedding model, make sure to have the following prerequisites:</p> <ul> <li>Have Ollama installed and running</li> <li>Download an embedding model to your local machine using the following command:     <pre><code>ollama pull &lt;ollama-model-id&gt;\n</code></pre>     Replace <code>&lt;ollama-model-id&gt;</code> with the Ollama identifier of the specific model. For more information about available embedding models and their identifiers, see Ollama models overview.</li> </ul>"},{"location":"embeddings/#ollama-models-overview","title":"Ollama models overview","text":"<p>The following table provides an overview of the available Ollama embedding models.</p> Model ID Ollama ID Parameters Dimensions Context Length Performance Tradeoffs NOMIC_EMBED_TEXT nomic-embed-text 137M 768 8192 High-quality embeddings for semantic search and text similarity tasks Balanced between quality and efficiency ALL_MINILM all-minilm 33M 384 512 Fast inference with good quality for general text embeddings Smaller model size with reduced context length, but very efficient MULTILINGUAL_E5 zylonai/multilingual-e5-large 300M 768 512 Strong performance across 100+ languages Larger model size but provides excellent multilingual capabilities BGE_LARGE bge-large 335M 1024 512 Excellent for English text retrieval and semantic search Larger model size but provides high-quality embeddings MXBAI_EMBED_LARGE mxbai-embed-large - - - High-dimensional embeddings of textual data Designed for creating high-dimensional embeddings <p>For more information about these models, see Ollama's Embedding Models blog post.</p>"},{"location":"embeddings/#choosing-a-model","title":"Choosing a model","text":"<p>Here are some general tips on which Ollama embedding model to select depending on your requirements:</p> <ul> <li>For general text embeddings, use <code>NOMIC_EMBED_TEXT</code>.</li> <li>For multilingual support, use <code>MULTILINGUAL_E5</code>.</li> <li>For maximum quality (at the cost of performance), use <code>BGE_LARGE</code>.</li> <li>For maximum efficiency (at the cost of some quality), use <code>ALL_MINILM</code>.</li> <li>For high-dimensional embeddings, use <code>MXBAI_EMBED_LARGE</code>.</li> </ul>"},{"location":"embeddings/#openai-embeddings","title":"OpenAI embeddings","text":"<p>To create embeddings using an OpenAI embedding model, use the <code>embed</code> method of an <code>OpenAILLMClient</code> instance as shown in the example below.</p> <pre><code>suspend fun openAIEmbed(text: String) {\n    // Get the OpenAI API token from the OPENAI_KEY environment variable\n    val token = System.getenv(\"OPENAI_KEY\") ?: error(\"Environment variable OPENAI_KEY is not set\")\n    // Create an OpenAILLMClient instance\n    val client = OpenAILLMClient(token)\n    // Create an embedder\n    val embedder = LLMEmbedder(client, OpenAIModels.Embeddings.TextEmbeddingAda002)\n    // Create embeddings\n    val embedding = embedder.embed(text)\n    // Print embeddings to the output\n    println(embedding)\n}\n</code></pre>"},{"location":"embeddings/#aws-bedrock-embeddings","title":"AWS Bedrock embeddings","text":"<p>To create embeddings using an AWS Bedrock embedding model, use the <code>embed</code> method of an <code>BedrockLLMClient</code> instance and your chosen model. Example:</p> <pre><code>suspend fun bedrockEmbed(text: String) {\n    // Get AWS credentials from environment/configuration\n    val awsAccessKeyId = System.getenv(\"AWS_ACCESS_KEY_ID\") ?: error(\"AWS_ACCESS_KEY_ID not set\")\n    val awsSecretAccessKey = System.getenv(\"AWS_SECRET_ACCESS_KEY\") ?: error(\"AWS_SECRET_ACCESS_KEY not set\")\n    // (Optional) AWS_SESSION_TOKEN for temporary credentials\n    val awsSessionToken = System.getenv(\"AWS_SESSION_TOKEN\")\n    // Create a BedrockLLMClient instance\n    val client = BedrockLLMClient(\n        identityProvider = StaticCredentialsProvider {\n            this.accessKeyId = awsAccessKeyId\n            this.secretAccessKey = awsSecretAccessKey\n            awsSessionToken?.let { this.sessionToken = it }\n        },\n        settings = BedrockClientSettings()\n    )\n    // Create an embedder\n    val embedder = LLMEmbedder(client, BedrockModels.Embeddings.AmazonTitanEmbedText)\n    // Create embeddings\n    val embedding = embedder.embed(text)\n    // Print embeddings to the output\n    println(embedding)\n}\n</code></pre>"},{"location":"embeddings/#supported-aws-bedrock-embedding-models","title":"Supported AWS Bedrock embedding models","text":"Provider Model name Model ID Input Output Dimensions Context Length Notes Amazon Titan Embeddings G1 - Text <code>amazon.titan-embed-text-v1</code> Text Embedding 1,536 8192 25+ languages, optimized for retrieval, semantic similarity, clustering; segment long docs for search. Amazon Titan Text Embeddings V2 <code>amazon.titan-embed-text-v2:0</code> Text Embedding 1,024 8192 High-accuracy, flexible dimensions, multilingual (100+); smaller dims save storage, normalized output. Cohere Cohere Embed English v3 <code>cohere.embed-english-v3</code> Text Embedding 1,024 8192 SOTA English text embeddings for search, retrieval, and understanding text nuances. Cohere Cohere Embed Multilingual v3 <code>cohere.embed-multilingual-v3</code> Text Embedding 1,024 8192 Multilingual embeddings, SOTA for search and semantic understanding across languages. <p>For the most up-to-date model support, refer to the AWS Bedrock supported models documentation.</p>"},{"location":"embeddings/#examples","title":"Examples","text":"<p>The following examples show how you can use embeddings to compare code with text or other code snippets.</p>"},{"location":"embeddings/#code-to-text-comparison","title":"Code-to-text comparison","text":"<p>Compare code snippets with natural language descriptions to find semantic matches:</p> <pre><code>suspend fun compareCodeToText(embedder: Embedder) { // Embedder type\n    // Code snippet\n    val code = \"\"\"\n        fun factorial(n: Int): Int {\n            return if (n &lt;= 1) 1 else n * factorial(n - 1)\n        }\n    \"\"\".trimIndent()\n\n    // Text descriptions\n    val description1 = \"A recursive function that calculates the factorial of a number\"\n    val description2 = \"A function that sorts an array of integers\"\n\n    // Generate embeddings\n    val codeEmbedding = embedder.embed(code)\n    val desc1Embedding = embedder.embed(description1)\n    val desc2Embedding = embedder.embed(description2)\n\n    // Calculate differences (lower value means more similar)\n    val diff1 = embedder.diff(codeEmbedding, desc1Embedding)\n    val diff2 = embedder.diff(codeEmbedding, desc2Embedding)\n\n    println(\"Difference between code and description 1: $diff1\")\n    println(\"Difference between code and description 2: $diff2\")\n\n    // The code should be more similar to description1 than description2\n    if (diff1 &lt; diff2) {\n        println(\"The code is more similar to: '$description1'\")\n    } else {\n        println(\"The code is more similar to: '$description2'\")\n    }\n}\n</code></pre>"},{"location":"embeddings/#code-to-code-comparison","title":"Code-to-code comparison","text":"<p>Compare code snippets to find semantic similarities regardless of syntax differences:</p> <pre><code>suspend fun compareCodeToCode(embedder: Embedder) { // Embedder type\n    // Two implementations of the same algorithm in different languages\n    val kotlinCode = \"\"\"\n        fun fibonacci(n: Int): Int {\n            return if (n &lt;= 1) n else fibonacci(n - 1) + fibonacci(n - 2)\n        }\n    \"\"\".trimIndent()\n\n    val pythonCode = \"\"\"\n        def fibonacci(n):\n            if n &lt;= 1:\n                return n\n            else:\n                return fibonacci(n-1) + fibonacci(n-2)\n    \"\"\".trimIndent()\n\n    val javaCode = \"\"\"\n        public static int bubbleSort(int[] arr) {\n            int n = arr.length;\n            for (int i = 0; i &lt; n-1; i++) {\n                for (int j = 0; j &lt; n-i-1; j++) {\n                    if (arr[j] &gt; arr[j+1]) {\n                        int temp = arr[j];\n                        arr[j] = arr[j+1];\n                        arr[j+1] = temp;\n                    }\n                }\n            }\n            return arr;\n        }\n    \"\"\".trimIndent()\n\n    // Generate embeddings\n    val kotlinEmbedding = embedder.embed(kotlinCode)\n    val pythonEmbedding = embedder.embed(pythonCode)\n    val javaEmbedding = embedder.embed(javaCode)\n\n    // Calculate differences\n    val diffKotlinPython = embedder.diff(kotlinEmbedding, pythonEmbedding)\n    val diffKotlinJava = embedder.diff(kotlinEmbedding, javaEmbedding)\n\n    println(\"Difference between Kotlin and Python implementations: $diffKotlinPython\")\n    println(\"Difference between Kotlin and Java implementations: $diffKotlinJava\")\n\n    // The Kotlin and Python implementations should be more similar\n    if (diffKotlinPython &lt; diffKotlinJava) {\n        println(\"The Kotlin code is more similar to the Python code\")\n    } else {\n        println(\"The Kotlin code is more similar to the Java code\")\n    }\n}\n</code></pre>"},{"location":"embeddings/#api-documentation","title":"API documentation","text":"<p>For a complete API reference related to embeddings, see the reference documentation for the following modules:</p> <ul> <li>embeddings-base: Provides core interfaces and data structures for representing and comparing text  and code embeddings.</li> <li>embeddings-llm: Includes implementations for working with local embedding models.</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>The Koog framework provides examples to help you understand how to implement AI agents for different use cases. These examples demonstrate key features and patterns that you can adapt for your own applications.</p> <p>Browse the examples below and click on the links to view the source code on GitHub.</p> Example Description Attachments Learn how to use structured Markdown and attachments in prompts. Build prompts that include images and generate creative content for Instagram posts using OpenAI models. Banking Build a comprehensive AI banking assistant with routing capabilities that can handle money transfers and transaction analysis through a sophisticated graph-based strategy. Includes domain modeling, tool creation, and agent composition patterns. BedrockAgent Create intelligent AI agents using the Koog framework with AWS Bedrock integration. Learn how to define custom tools, set up AWS Bedrock, and build interactive agents that understand natural language commands for controlling devices. Calculator Build a calculator agent that performs arithmetic operations using tools for addition, subtraction, multiplication, and division. Demonstrates parallel tool calls, event logging, and multiple executor support (OpenAI and Ollama). Chess Build an intelligent chess-playing agent featuring complex domain modeling, custom tools, memory optimization techniques, and interactive choice selection. Demonstrates advanced agent strategies, game state management, and human-AI collaboration patterns. GoogleMapsMcp Connect Koog to a Google Maps MCP server via Docker. Discover tools, geocode addresses, and fetch elevation data using AI agents with real-world geographic APIs in a Kotlin Notebook environment. Guesser Build a number-guessing agent that implements a binary search strategy using tools to ask targeted questions. The agent efficiently narrows down the user's number through strategic questioning and demonstrates tool-based interaction patterns. Langfuse Learn how to export Koog agent traces to Langfuse using OpenTelemetry. Set up environment variables, run agents, and inspect spans and traces in your Langfuse instance for comprehensive observability. MCP Integration examples for the Model Context Protocol, featuring GoogleMapsMcpClient for geographic data and PlaywrightMcpClient for browser automation. Memory A customer support agent that demonstrates memory system usage. The agent tracks user conversation preferences, device diagnostics, and organization-specific information using encrypted local storage and proper memory organization with subjects and scopes. OpenTelemetry Add OpenTelemetry-based tracing to Koog AI agents. Learn to emit spans to console for debugging and export traces to OpenTelemetry Collector for viewing in Jaeger. Includes Docker setup and troubleshooting guide. Planner A task planning system that builds execution trees with parallel and sequential execution nodes, dynamically constructing execution plans for complex workflows. PlaywrightMcp Drive browsers with Playwright MCP and Koog. Launch a Playwright MCP server, connect via SSE, and let AI agents automate web tasks like navigation, cookie acceptance, and UI interaction through natural language commands. SimpleAPI Examples demonstrating chat agents and basic agents with simple API patterns for getting started with Koog. StructuredData Demonstrates JSON-based structured data output with complex nested classes, polymorphism, and weather forecast examples showing how to work with typed data in agent responses. SubgraphWithTask Project generation tools showcasing file and directory operations, including creation, deletion, and command execution using subgraph strategies. Tone A text tone analysis agent that uses specialized tools to identify positive, negative, or neutral tones in input text, demonstrating sentiment analysis capabilities. UnityMcp Drive Unity game development with AI agents using Unity MCP server integration. Connect to Unity via stdio, discover available tools, and let agents modify scenes, place objects, and execute game development tasks through natural language commands. VaccumAgent Implementation of a basic reflex agent using the Koog framework. Covers environment modeling, tool creation, and agent behavior for automated cleaning tasks in a simple two-cell world. Weave Learn how to trace Koog agents to W&amp;B Weave using OpenTelemetry (OTLP). Set up environment variables, run agents, and view rich traces in the Weave UI for comprehensive monitoring and debugging. A2A Demonstrates agent-to-agent (A2A) communication using Koog framework. Shows how to set up bidirectional communication between AI agents, enable collaborative problem-solving, and manage multi-agent workflows with proper message routing and coordination."},{"location":"features-overview/","title":"Overview","text":"<p>Agent features provide a way to extend and enhance the functionality of AI agents. Features can:</p> <ul> <li>Add new capabilities to agents</li> <li>Intercept and modify agent behavior</li> <li>Log and monitor agent execution</li> </ul> <p>The Koog framework implements the following features:</p> <ul> <li>Event Handler</li> <li>Tracing</li> <li>Agent Memory</li> <li>OpenTelemetry</li> <li>Agent Persistence (Snapshots)</li> <li>Debugger</li> <li>Tokenizer</li> <li>SQL Persistence Providers</li> </ul>"},{"location":"functional-agents/","title":"Functional agents","text":"<p>Functional agents are lightweight AI agents that operate without building complex strategy graphs. Instead, the agent logic is implemented as a lambda function that handles user input, interacts with an LLM, optionally calls tools, and produces a final output. It can perform a single LLM call, process multiple LLM calls in sequence, or loop based on user input, as well as LLM and tool outputs.</p> <p>Tip</p> <ul> <li>If you already have a basic agent as your first MVP, but run into task-specific limitations, use a functional agent to prototype custom logic. You can implement custom control flows in plain Kotlin while still using most Koog features, including history compression and automatic state management.</li> <li>For production-grade needs, refactor your functional agent into a complex workflow agent with strategy graphs. This provides persistence with controllable rollbacks for fault-tolerance and advanced OpenTelemetry tracing with nested graph events.</li> </ul> <p>This page guides you through the steps necessary to create a minimal functional agent and extend it with tools.</p>"},{"location":"functional-agents/#prerequisites","title":"Prerequisites","text":"<p>Before you start, make sure that you have the following:</p> <ul> <li>A working Kotlin/JVM project.</li> <li>Java 17+ installed.</li> <li>A valid API key from the LLM provider used to implement an AI agent. For a list of all available providers, refer to LLM providers.</li> <li>(Optional) Ollama installed and running locally if you use this provider.</li> </ul> <p>Tip</p> <p>Use environment variables or a secure configuration management system to store your API keys. Avoid hardcoding API keys directly in your source code.</p>"},{"location":"functional-agents/#add-dependencies","title":"Add dependencies","text":"<p>The <code>AIAgent</code> class is the main class for creating agents in Koog. Include the following dependency in your build configuration to use the class functionality:</p> <p><pre><code>dependencies {\n    implementation(\"ai.koog:koog-agents:VERSION\")\n}\n</code></pre> For all available installation methods, see Install Koog.</p>"},{"location":"functional-agents/#create-a-minimal-functional-agent","title":"Create a minimal functional agent","text":"<p>To create a minimal functional agent, do the following:</p> <ol> <li>Choose the input and output types that the agent handles and create a corresponding <code>AIAgent&lt;Input, Output&gt;</code> instance.    In this guide, we use <code>AIAgent&lt;String, String&gt;</code>, which means the agent receives and returns <code>String</code>.</li> <li>Provide the required parameters, including a system prompt, prompt executor, and LLM.</li> <li>Define the agent logic with a lambda function wrapped into the <code>functionalStrategy {...}</code> DSL method.</li> </ol> <p>Here is an example of a minimal functional agent that sends user text to a specified LLM and returns a single assistant message.</p> <pre><code>// Create an AIAgent instance and provide a system prompt, prompt executor, and LLM\nval mathAgent = AIAgent&lt;String, String&gt;(\n    systemPrompt = \"You are a precise math assistant.\",\n    promptExecutor = simpleOllamaAIExecutor(),\n    llmModel = OllamaModels.Meta.LLAMA_3_2,\n    strategy = functionalStrategy { input -&gt; // Define the agent logic\n        // Make one LLM call\n        val response = requestLLM(input)\n        // Extract and return the assistant message content from the response\n        response.asAssistantMessage().content\n    }\n)\n\n// Run the agent with a user input and print the result\nval result = mathAgent.run(\"What is 12 \u00d7 9?\")\nprintln(result)\n</code></pre> <p>The agent can produce the following output:</p> <pre><code>The answer to 12 \u00d7 9 is 108.\n</code></pre> <p>This agent makes a single LLM call and returns the assistant message content. You can extend the agent logic to handle multiple sequential LLM calls. For example:</p> <pre><code>// Create an AIAgent instance and provide a system prompt, prompt executor, and LLM\nval mathAgent = AIAgent&lt;String, String&gt;(\n    systemPrompt = \"You are a precise math assistant.\",\n    promptExecutor = simpleOllamaAIExecutor(),\n    llmModel = OllamaModels.Meta.LLAMA_3_2,\n    strategy = functionalStrategy { input -&gt; // Define the agent logic\n        // The first LLM call to produce an initial draft based on the user input\n        val draft = requestLLM(\"Draft: $input\").asAssistantMessage().content\n        // The second LLM call to improve the draft by prompting the LLM again with the draft content\n        val improved = requestLLM(\"Improve and clarify.\").asAssistantMessage().content\n        // The final LLM call to format the improved text and return the final formatted result\n        requestLLM(\"Format the result as bold.\").asAssistantMessage().content\n    }\n)\n\n// Run the agent with a user input and print the result\nval result = mathAgent.run(\"What is 12 \u00d7 9?\")\nprintln(result)\n</code></pre> <p>The agent can produce the following output:</p> <pre><code>When multiplying 12 by 9, we can break it down as follows:\n\n**12 (tens) \u00d7 9 = 108**\n\nAlternatively, we can also use the distributive property to calculate this:\n\n**(10 + 2) \u00d7 9**\n= **10 \u00d7 9 + 2 \u00d7 9**\n= **90 + 18**\n= **108**\n</code></pre>"},{"location":"functional-agents/#add-tools","title":"Add tools","text":"<p>In many cases, a functional agent needs to complete specific tasks, such as reading and writing data or calling APIs. In Koog, you expose such capabilities as tools and let the LLM call them in the agent logic.</p> <p>This chapter takes the minimal functional agent created above and demonstrates how to extend the agent logic using tools.</p> <p>1) Create an annotation-based tool. For more details, see Annotation-based tools.</p> <pre><code>@LLMDescription(\"Simple multiplier\")\nclass MathTools : ToolSet {\n    @Tool\n    @LLMDescription(\"Multiplies two numbers and returns the result\")\n    fun multiply(a: Int, b: Int): Int {\n        val result = a * b\n        return result\n    }\n}\n</code></pre> <p>To learn more about available tools, refer to the Tool overview.</p> <p>2) Register the tool to make it available to the agent.</p> <pre><code>val toolRegistry = ToolRegistry {\n    tools(MathTools())\n}\n</code></pre> <p>3) Pass the tool registry to the agent to enable the LLM to request and use the available tools.</p> <p>4) Extend the agent logic to identify tool calls, execute the requested tools, send their results back to the LLM, and repeat the process until no tool calls remain.</p> <p>Note</p> <p>Use a loop only if the LLM continues to issue tool calls.</p> <pre><code>val mathWithTools = AIAgent&lt;String, String&gt;(\n    systemPrompt = \"You are a precise math assistant. When multiplication is needed, use the multiplication tool.\",\n    promptExecutor = simpleOllamaAIExecutor(),\n    llmModel = OllamaModels.Meta.LLAMA_3_2,\n    toolRegistry = toolRegistry,\n    strategy = functionalStrategy { input -&gt; // Define the agent logic extended with tool calls\n        // Send the user input to the LLM\n        var responses = requestLLMMultiple(input)\n\n        // Only loop while the LLM requests tools\n        while (responses.containsToolCalls()) {\n            // Extract tool calls from the response\n            val pendingCalls = extractToolCalls(responses)\n            // Execute the tools and return the results\n            val results = executeMultipleTools(pendingCalls)\n            // Send the tool results back to the LLM. The LLM may call more tools or return a final output\n            responses = sendMultipleToolResults(results)\n        }\n\n        // When no tool calls remain, extract and return the assistant message content from the response\n        responses.single().asAssistantMessage().content\n    }\n)\n\n// Run the agent with a user input and print the result\nval reply = mathWithTools.run(\"Please multiply 12.5 and 4, then add 10 to the result.\")\nprintln(reply)\n</code></pre> <p>The agent can produce the following output:</p> <pre><code>Here is the step-by-step solution:\n\n1. Multiply 12.5 and 4:\n   12.5 \u00d7 4 = 50\n\n2. Add 10 to the result:\n   50 + 10 = 60\n</code></pre>"},{"location":"functional-agents/#whats-next","title":"What's next","text":"<ul> <li>Learn how to return structured data using the Structured output API.</li> <li>Experiment with adding more tools to the agent.</li> <li>Improve observability with the EventHandler feature.</li> <li>Learn how to handle long-running conversations with History compression.</li> </ul>"},{"location":"getting-started/","title":"Getting started","text":"<p>This guide will help you install Koog and create your first AI agent.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you start, make sure you have the following:</p> <ul> <li>A working Kotlin/JVM project with Gradle or Maven.</li> <li>Java 17+ installed.</li> <li>A valid API key for your preferred LLM provider (not required for Ollama, which runs locally).</li> </ul>"},{"location":"getting-started/#install-koog","title":"Install Koog","text":"<p>To use Koog, you need to include all necessary dependencies in your build configuration.</p> <p>Note</p> <p>Replace <code>LATEST_VERSION</code> with the latest version of Koog published on Maven Central.</p> Gradle (Kotlin DSL)Gradle (Groovy)Maven <ol> <li> <p>Add the dependency to the <code>build.gradle.kts</code> file.</p> <pre><code>dependencies {\n    implementation(\"ai.koog:koog-agents:LATEST_VERSION\")\n}\n</code></pre> </li> <li> <p>Make sure that you have <code>mavenCentral()</code> in the list of repositories.</p> <pre><code>repositories {\n    mavenCentral()\n}\n</code></pre> </li> </ol> <ol> <li> <p>Add the dependency to the <code>build.gradle</code> file.</p> <pre><code>dependencies {\n    implementation 'ai.koog:koog-agents:LATEST_VERSION'\n}\n</code></pre> </li> <li> <p>Make sure that you have <code>mavenCentral()</code> in the list of repositories.     <pre><code>repositories {\n    mavenCentral()\n}\n</code></pre></p> </li> </ol> <ol> <li> <p>Add the dependency to the <code>pom.xml</code> file.</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;ai.koog&lt;/groupId&gt;\n    &lt;artifactId&gt;koog-agents-jvm&lt;/artifactId&gt;\n    &lt;version&gt;LATEST_VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> </li> <li> <p>Make sure that you have <code>mavenCentral()</code> in the list of repositories.</p> <pre><code> &lt;repositories&gt;\n    &lt;repository&gt;\n        &lt;id&gt;mavenCentral&lt;/id&gt;\n        &lt;url&gt;https://repo1.maven.org/maven2/&lt;/url&gt;\n    &lt;/repository&gt;\n&lt;/repositories&gt;\n</code></pre> </li> </ol> <p>Note</p> <p>When integrating Koog with Ktor servers, Spring applications, or MCP tools, you need to include the additional dependencies in your build configuration. For the exact dependencies, refer to the relevant pages in the Koog documentation.</p>"},{"location":"getting-started/#set-an-api-key","title":"Set an API key","text":"<p>Tip</p> <p>Use environment variables or a secure configuration management system to store your API keys. Avoid hardcoding API keys directly in your source code.</p> OpenAIAnthropicGoogleDeepSeekOpenRouterBedrockOllama <p>Get your API key and assign it as an environment variable.</p> Linux/macOSWindows <pre><code>export OPENAI_API_KEY=your-api-key\n</code></pre> <pre><code>setx OPENAI_API_KEY \"your-api-key\"\n</code></pre> <p>Restart your terminal to apply the changes. You can now retrieve and use the API key to create an agent.   </p> <p>Get your API key and assign it as an environment variable.</p> Linux/macOSWindows <pre><code>export ANTHROPIC_API_KEY=your-api-key\n</code></pre> <pre><code>setx ANTHROPIC_API_KEY \"your-api-key\"\n</code></pre> <p>Restart your terminal to apply the changes. You can now retrieve and use the API key to create an agent.</p> <p>Get your API key and assign it as an environment variable.</p> Linux/macOSWindows <pre><code>export GOOGLE_API_KEY=your-api-key\n</code></pre> <pre><code>setx GOOGLE_API_KEY \"your-api-key\"\n</code></pre> <p>Restart your terminal to apply the changes. You can now retrieve and use the API key to create an agent.   </p> <p>Get your API key and assign it as an environment variable.</p> Linux/macOSWindows <pre><code>export DEEPSEEK_API_KEY=your-api-key\n</code></pre> <pre><code>setx DEEPSEEK_API_KEY \"your-api-key\"\n</code></pre> <p>Restart your terminal to apply the changes. You can now retrieve and use the API key to create an agent.   </p> <p>Get your API key and assign it as an environment variable.</p> Linux/macOSWindows <pre><code>export OPENROUTER_API_KEY=your-api-key\n</code></pre> <pre><code>setx OPENROUTER_API_KEY \"your-api-key\"\n</code></pre> <p>Restart your terminal to apply the changes. You can now retrieve and use the API key to create an agent.   </p> <p>Get valid AWS credentials (an access key and a secret key) and assign them as environment variables.</p> Linux/macOSWindows <pre><code>export AWS_BEDROCK_ACCESS_KEY=your-access-key\nexport AWS_BEDROCK_SECRET_ACCESS_KEY=your-secret-access-key\n</code></pre> <pre><code>setx AWS_BEDROCK_ACCESS_KEY \"your-access-key\"\nsetx AWS_BEDROCK_SECRET_ACCESS_KEY \"your-secret-access-key\"\n</code></pre> <p>Restart your terminal to apply the changes. You can now retrieve and use the API key to create an agent.   </p> <p>Install Ollama and run a model locally without an API key.</p> <p>For more information, see Ollama documentation.</p>"},{"location":"getting-started/#create-and-run-an-agent","title":"Create and run an agent","text":"OpenAIAnthropicGoogleDeepSeekOpenRouterBedrockOllama <p>The example below creates and runs a simple AI agent using the <code>GPT-4o</code> model.</p> <p> <pre><code>fun main() = runBlocking {\n    // Get an API key from the OPENAI_API_KEY environment variable\n    val apiKey = System.getenv(\"OPENAI_API_KEY\")\n        ?: error(\"The API key is not set.\")\n\n    // Create an agent\n    val agent = AIAgent(\n        promptExecutor = simpleOpenAIExecutor(apiKey),\n        llmModel = OpenAIModels.Chat.GPT4o\n    )\n\n    // Run the agent\n    val result = agent.run(\"Hello! How can you help me?\")\n    println(result)\n}\n</code></pre> </p> <p>The example can produce the following output:</p> <pre><code>Hello! I'm here to help you with whatever you need. Here are just a few things I can do:\n\n- Answer questions.\n- Explain concepts or topics you're curious about.\n- Provide step-by-step instructions for tasks.\n- Offer advice, notes, or ideas.\n- Help with research or summarize complex material.\n- Write or edit text, emails, or other documents.\n- Brainstorm creative projects or solutions.\n- Solve problems or calculations.\n\nLet me know what you need help with\u2014I\u2019m here for you!\n</code></pre> <p>The example below creates and runs a simple AI agent using the <code>Claude Opus 4.1</code> model.</p> <p> <pre><code>fun main() = runBlocking {\n    // Get an API key from the ANTHROPIC_API_KEY environment variable\n    val apiKey = System.getenv(\"ANTHROPIC_API_KEY\")\n        ?: error(\"The API key is not set.\")\n\n    // Create an agent\n    val agent = AIAgent(\n        promptExecutor = simpleAnthropicExecutor(apiKey),\n        llmModel = AnthropicModels.Opus_4_1\n    )\n\n    // Run the agent\n    val result = agent.run(\"Hello! How can you help me?\")\n    println(result)\n}\n</code></pre> </p> <p>The example can produce the following output:</p> <pre><code>Hello! I can help you with:\n\n- **Answering questions** and explaining topics\n- **Writing** - drafting, editing, proofreading\n- **Learning** - homework, math, study help\n- **Problem-solving** and brainstorming\n- **Research** and information finding\n- **General tasks** - instructions, planning, recommendations\n\nWhat do you need help with today?\n</code></pre> <p>The example below creates and runs a simple AI agent using the <code>Gemini 2.5 Pro</code> model.</p> <p> <pre><code>fun main() = runBlocking {\n    // Get an API key from the GOOGLE_API_KEY environment variable\n    val apiKey = System.getenv(\"GOOGLE_API_KEY\")\n        ?: error(\"The API key is not set.\")\n\n    // Create an agent\n    val agent = AIAgent(\n        promptExecutor = simpleGoogleAIExecutor(apiKey),\n        llmModel = GoogleModels.Gemini2_5Pro\n    )\n\n    // Run the agent\n    val result = agent.run(\"Hello! How can you help me?\")\n    println(result)\n}\n</code></pre> </p> <p>The example can produce the following output:</p> <pre><code>I'm an AI that can help you with tasks involving language and information. You can ask me to:\n\n*   **Answer questions**\n*   **Write or edit text** (emails, stories, code, etc.)\n*   **Brainstorm ideas**\n*   **Summarize long documents**\n*   **Plan things** (like trips or projects)\n*   **Be a creative partner**\n\nJust tell me what you need\n</code></pre> <p>The example below creates and runs a simple AI agent using the <code>deepseek-chat</code> model.</p> <p> <pre><code>fun main() = runBlocking {\n    // Get an API key from the DEEPSEEK_API_KEY environment variable\n    val apiKey = System.getenv(\"DEEPSEEK_API_KEY\")\n        ?: error(\"The API key is not set.\")\n\n    // Create an LLM client\n    val deepSeekClient = DeepSeekLLMClient(apiKey)\n\n    // Create an agent\n    val agent = AIAgent(\n        // Create a prompt executor using the LLM client\n        promptExecutor = SingleLLMPromptExecutor(deepSeekClient),\n        // Provide a model\n        llmModel = DeepSeekModels.DeepSeekChat\n    )\n\n    // Run the agent\n    val result = agent.run(\"Hello! How can you help me?\")\n    println(result)\n}\n</code></pre> </p> <p>The example can produce the following output:</p> <pre><code>Hello! I'm here to assist you with a wide range of tasks, including answering questions, providing information, helping with problem-solving, offering creative ideas, and even just chatting. Whether you need help with research, writing, learning something new, or simply want to discuss a topic, feel free to ask\u2014I\u2019m happy to help! \ud83d\ude0a\n</code></pre> <p>The example below creates and runs a simple AI agent using the <code>GPT-4o</code> model.</p> <p> <pre><code>fun main() = runBlocking {\n    // Get an API key from the OPENROUTER_API_KEY environment variable\n    val apiKey = System.getenv(\"OPENROUTER_API_KEY\")\n        ?: error(\"The API key is not set.\")\n\n    // Create an agent\n    val agent = AIAgent(\n        promptExecutor = simpleOpenRouterExecutor(apiKey),\n        llmModel = OpenRouterModels.GPT4o\n    )\n\n    // Run the agent\n    val result = agent.run(\"Hello! How can you help me?\")\n    println(result)\n}\n</code></pre> </p> <p>The example can produce the following output:</p> <pre><code>I can answer questions, help with writing, solve problems, organize tasks, and more\u2014just let me know what you need!\n</code></pre> <p>The example below creates and runs a simple AI agent using the <code>Claude Sonnet 4.5</code> model.</p> <p> <pre><code>fun main() = runBlocking {\n    // Get access keys from the AWS_BEDROCK_ACCESS_KEY and AWS_BEDROCK_SECRET_ACCESS_KEY environment variables\n    val awsAccessKeyId = System.getenv(\"AWS_BEDROCK_ACCESS_KEY\")\n        ?: error(\"The access key is not set.\")\n\n    val awsSecretAccessKey = System.getenv(\"AWS_BEDROCK_SECRET_ACCESS_KEY\")\n        ?: error(\"The secret access key is not set.\")\n\n    // Create an agent\n    val agent = AIAgent(\n        promptExecutor = simpleBedrockExecutor(awsAccessKeyId, awsSecretAccessKey),\n        llmModel = BedrockModels.AnthropicClaude4_5Sonnet\n    )\n\n    // Run the agent\n    val result = agent.run(\"Hello! How can you help me?\")\n    println(result)\n}\n</code></pre> </p> <p>The example can produce the following output:</p> <pre><code>Hello! I'm a helpful assistant and I can assist you in many ways, including:\n\n- **Answering questions** on a wide range of topics (science, history, technology, etc.)\n- **Writing help** - drafting emails, essays, creative content, or editing text\n- **Problem-solving** - working through math problems, logic puzzles, or troubleshooting issues\n- **Learning support** - explaining concepts, providing study notes, or tutoring\n- **Planning &amp; organizing** - helping with projects, schedules, or breaking down tasks\n- **Coding assistance** - explaining programming concepts or helping debug code\n- **Creative brainstorming** - generating ideas for projects, stories, or solutions\n- **General conversation** - discussing topics or just chatting\n\n What would you like help with today?\n</code></pre> <p>The example below creates and runs a simple AI agent using the <code>llama3.2</code> model.</p> <p> <pre><code>fun main() = runBlocking {\n    // Create an agent\n    val agent = AIAgent(\n        promptExecutor = simpleOllamaAIExecutor(),\n        llmModel = OllamaModels.Meta.LLAMA_3_2\n    )\n\n    // Run the agent\n    val result = agent.run(\"Hello! How can you help me?\")\n    println(result)\n}\n</code></pre> </p> <p>The example can produce the following output:</p> <pre><code>I can assist with various tasks such as answering questions, providing information, and even helping with language-related tasks like proofreading or writing suggestions. What's on your mind today?\n</code></pre>"},{"location":"getting-started/#whats-next","title":"What's next","text":"<ul> <li>Explore key features of Koog.</li> <li>Learn more about available agent types.</li> </ul>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#agent","title":"Agent","text":"<ul> <li> <p>Agent: an AI entity that can interact with tools, handle complex workflows, and communicate with   users.</p> </li> <li> <p>LLM (Large Language Model): the underlying AI model that powers agent capabilities.</p> </li> <li> <p>Message: a unit of communication in the agent system that represents data passed from a user, assistant, or system.</p> </li> <li> <p>Prompt: the conversation history provided to an LLM that consists of messages from a user, assistant, and system.</p> </li> <li> <p>System prompt: instructions provided to an agent to guide its behavior, define its role, and supply key information necessary for its tasks.</p> </li> <li> <p>Context: the environment in which LLM interactions occur, with access to the conversation history and   tools.</p> </li> <li> <p>LLM session: a structured way to interact with LLMs that includes the conversation history, available tools,   and methods to make requests.</p> </li> </ul>"},{"location":"glossary/#agent-workflow","title":"Agent workflow","text":"<ul> <li>Strategy: a defined workflow for an agent that consists of sequential subgraphs. The strategy defines how the agent processes input, interacts with tools, and generates output. A strategy graph consists of nodes connected by edges that represent transitions between nodes.</li> </ul>"},{"location":"glossary/#strategy-graphs","title":"Strategy graphs","text":"<ul> <li> <p>Graph: a structure of nodes connected by edges that defines an agent strategy workflow.</p> </li> <li> <p>Node: a fundamental building block of an agent strategy workflow that represents a specific operation or transformation.</p> </li> <li> <p>Edge: a connection between nodes in an agent graph that defines the flow of operations, often with conditions   that specify when to follow each edge.</p> </li> <li> <p>Conditions: rules that determine when to follow a particular edge.</p> </li> <li> <p>Subgraph: a self-contained unit of processing within an agent strategy, with its own set of tools, context, and responsibilities. Information about subgraph operations can be either encapsulated within the subgraph or transferred between subgraphs using the AgentMemory feature.</p> </li> </ul>"},{"location":"glossary/#tools","title":"Tools","text":"<ul> <li> <p>Tool: a function that an agent can use to perform specific tasks or access external systems. The agent is aware of the available tools and their arguments but lacks knowledge of their implementation details.</p> </li> <li> <p>Tool call: a request from an LLM to run a specific tool using the provided arguments. It functions similarly to a function call.</p> </li> <li> <p>Tool descriptor: tool metadata that includes its name, description, and parameters.</p> </li> <li> <p>Tool registry: a list of tools available to an agent. The registry informs the agent about the available tools.</p> </li> <li> <p>Tool result: an output produced by running a tool. For example, if the tool is a method, the result would be its return value.</p> </li> </ul>"},{"location":"glossary/#history-compression","title":"History compression","text":"<ul> <li>History compression: the process of reducing the size of the conversation history to manage token usage by applying various compression strategies. To learn more, see History compression.</li> </ul>"},{"location":"glossary/#features","title":"Features","text":"<ul> <li>Feature: a component that extends and enhances the functionality of AI agents.</li> </ul>"},{"location":"glossary/#eventhandler-feature","title":"EventHandler feature","text":"<ul> <li>EventHandler: a feature that enables monitoring and responding to various agent events, providing hooks for tracking agent lifecycle, handling errors, and processing tool invocations    throughout the workflow.</li> </ul>"},{"location":"glossary/#agentmemory-feature","title":"AgentMemory feature","text":"<ul> <li> <p>AgentMemory: a feature that enables AI agents to store, retrieve, and use information across conversations. To learn more, see AgentMemory.</p> </li> <li> <p>Concept: a category of information with associated metadata in the AgentMemory feature, including a keyword, description, and fact type. Concepts are fundamental building blocks of the AgentMemory system that the agent can remember and recall. To learn more, see AgentMemory.</p> </li> <li> <p>Fact: an individual piece of information stored in the AgentMemory system. Facts are associated with concepts and can either have a single value or multiple values. To learn more, see AgentMemory.</p> </li> <li> <p>Memory scope: the context in which facts are relevant. To learn more, see AgentMemory.</p> </li> </ul>"},{"location":"history-compression/","title":"History compression","text":"<p>AI agents maintain a message history that includes user messages, assistant responses, tool calls, and tool responses. This history grows with each interaction as the agent follows its strategy.</p> <p>For long-running conversations, the history can become large and consume a lot of tokens. History compression helps reduce this by summarizing the full list of messages into one or several messages that contain only important information necessary for further agent operation.</p> <p>History compression addresses key challenges in agent systems:</p> <ul> <li>Optimizes context usage. Focused and smaller contexts improve LLM performance and prevent failures from exceeding token limits.</li> <li>Improves performance. Compressing history reduces the number of messages the LLM processes, resulting in faster responses.</li> <li>Enhances accuracy. Focusing on relevant information helps the LLM remain focused and complete tasks without distractions.</li> <li>Reduces costs. Reducing irrelevant messages lowers token usage, decreasing the overall cost of API calls.</li> </ul>"},{"location":"history-compression/#when-to-compress-history","title":"When to compress history","text":"<p>History compression is performed at specific steps in the agent workflow:</p> <ul> <li>Between logical steps (subgraphs) of the agent strategy.</li> <li>When context becomes too long.</li> </ul>"},{"location":"history-compression/#history-compression-implementation","title":"History compression implementation","text":"<p>There are two main approaches to implementing history compression in your agent:</p> <ul> <li>In a strategy graph.</li> <li>In a custom node.</li> </ul>"},{"location":"history-compression/#history-compression-in-a-strategy-graph","title":"History compression in a strategy graph","text":"<p>To compress the history in a strategy graph, you need to use the <code>nodeLLMCompressHistory</code> node. Depending on which step you decide to perform compression, the following scenarios are available: </p> <ul> <li>To compress the history when it becomes too long, you can define a helper function and add the <code>nodeLLMCompressHistory</code> node to your strategy graph with the following logic:</li> </ul> <pre><code>// Define that the history is too long if there are more than 100 messages\nprivate suspend fun AIAgentContext.historyIsTooLong(): Boolean = llm.readSession { prompt.messages.size &gt; 100 }\n\nval strategy = strategy&lt;String, String&gt;(\"execute-with-history-compression\") {\n    val callLLM by nodeLLMRequest()\n    val executeTool by nodeExecuteTool()\n    val sendToolResult by nodeLLMSendToolResult()\n\n    // Compress the LLM history and keep the current ReceivedToolResult for the next node\n    val compressHistory by nodeLLMCompressHistory&lt;ReceivedToolResult&gt;()\n\n    edge(nodeStart forwardTo callLLM)\n    edge(callLLM forwardTo nodeFinish onAssistantMessage { true })\n    edge(callLLM forwardTo executeTool onToolCall { true })\n\n    // Compress history after executing any tool if the history is too long \n    edge(executeTool forwardTo compressHistory onCondition { historyIsTooLong() })\n    edge(compressHistory forwardTo sendToolResult)\n    // Otherwise, proceed to the next LLM request\n    edge(executeTool forwardTo sendToolResult onCondition { !historyIsTooLong() })\n\n    edge(sendToolResult forwardTo executeTool onToolCall { true })\n    edge(sendToolResult forwardTo nodeFinish onAssistantMessage { true })\n}\n</code></pre> <p>In this example, the strategy checks if the history is too long after each tool call. The history is compressed before sending the tool result back to the LLM. This prevents the context from growing during long conversations.</p> <ul> <li>To compress the history between the logical steps (subgraphs) of your strategy, you can implement your strateg as follows:</li> </ul> <pre><code>val strategy = strategy&lt;String, String&gt;(\"execute-with-history-compression\") {\n    val collectInformation by subgraph&lt;String, String&gt; {\n        // Some steps to collect the information\n    }\n    val compressHistory by nodeLLMCompressHistory&lt;String&gt;()\n    val makeTheDecision by subgraph&lt;String, String&gt; {\n        // Some steps to make the decision based on the current compressed history and collected information\n    }\n\n    nodeStart then collectInformation then compressHistory then makeTheDecision\n}\n</code></pre> <p>In this example, the history is compressed after completing the information collection phase, but before proceeding to the decision-making phase.</p>"},{"location":"history-compression/#history-compression-in-a-custom-node","title":"History compression in a custom node","text":"<p>If you are implementing a custom node, you can compress history using the <code>replaceHistoryWithTLDR()</code> function as follows:</p> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR()\n}\n</code></pre> <p>This approach gives you more flexibility to implement compression at any point in your custom node logic, based on your specific requirements.</p> <p>To learn more about custom nodes, see Custom nodes.</p>"},{"location":"history-compression/#history-compression-strategies","title":"History compression strategies","text":"<p>You can customize the compression process by passing an optional <code>strategy</code> parameter to <code>nodeLLMCompressHistory(strategy=...)</code> or to <code>replaceHistoryWithTLDR(strategy=...)</code>. The framework provides several built-in strategies.</p>"},{"location":"history-compression/#wholehistory-default","title":"WholeHistory (Default)","text":"<p>The default strategy that compresses the entire history into one TLDR message that summarizes what has been achieved so far. This strategy works well for most general use cases where you want to maintain awareness of the entire conversation context while reducing token usage.</p> <p>You can use it as follows: </p> <ul> <li>In a strategy graph:</li> </ul> <pre><code>val compressHistory by nodeLLMCompressHistory&lt;ProcessedInput&gt;(\n    strategy = HistoryCompressionStrategy.WholeHistory\n)\n</code></pre> <ul> <li>In a custom node:</li> </ul> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR(strategy = HistoryCompressionStrategy.WholeHistory)\n}\n</code></pre>"},{"location":"history-compression/#fromlastnmessages","title":"FromLastNMessages","text":"<p>The strategy compresses only the last <code>n</code> messages into a TLDR message and completely discards earlier messages. This is useful when only the latest achievements of the agent (or the latest discovered facts, the latest context) are relevant for solving the problem.</p> <p>You can use it as follows:</p> <ul> <li>In a strategy graph:</li> </ul> <pre><code>val compressHistory by nodeLLMCompressHistory&lt;ProcessedInput&gt;(\n    strategy = HistoryCompressionStrategy.FromLastNMessages(5)\n)\n</code></pre> <ul> <li>In a custom node:</li> </ul> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR(strategy = HistoryCompressionStrategy.FromLastNMessages(5))\n}\n</code></pre>"},{"location":"history-compression/#chunked","title":"Chunked","text":"<p>The strategy splits the whole message history into chunks of a fixed size and compresses each chunk independently into a TLDR message. This is useful when you need not only the concise TLDR of what has been done so far but also want to keep track of the overall progress, and some older information might also be important.</p> <p>You can use it as follows:</p> <ul> <li>In a strategy graph:</li> </ul> <pre><code>val compressHistory by nodeLLMCompressHistory&lt;ProcessedInput&gt;(\n    strategy = HistoryCompressionStrategy.Chunked(10)\n)\n</code></pre> <ul> <li>In a custom node:</li> </ul> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR(strategy = HistoryCompressionStrategy.Chunked(10))\n}\n</code></pre>"},{"location":"history-compression/#retrievefactsfromhistory","title":"RetrieveFactsFromHistory","text":"<p>The strategy searches for specific facts relevant to the provided list of concepts in the history and retrieves them. It changes the whole history to just these facts and leaves them as context for future LLM requests. This is useful when you have an idea of what exact facts will be relevant for the LLM to perform better on the task.</p> <p>You can use it as follows:</p> <ul> <li>In a strategy graph:</li> </ul> <pre><code>val compressHistory by nodeLLMCompressHistory&lt;ProcessedInput&gt;(\n    strategy = RetrieveFactsFromHistory(\n        Concept(\n            keyword = \"user_preferences\",\n            // Description to the LLM -- what specifically to search for\n            description = \"User's preferences for the recommendation system, including the preferred conversation style, theme in the application, etc.\",\n            // LLM would search for multiple relevant facts related to this concept:\n            factType = FactType.MULTIPLE\n        ),\n        Concept(\n            keyword = \"product_details\",\n            // Description to the LLM -- what specifically to search for\n            description = \"Brief details about products in the catalog the user has been checking\",\n            // LLM would search for multiple relevant facts related to this concept:\n            factType = FactType.MULTIPLE\n        ),\n        Concept(\n            keyword = \"issue_solved\",\n            // Description to the LLM -- what specifically to search for\n            description = \"Was the initial user's issue resolved?\",\n            // LLM would search for a single answer to the question:\n            factType = FactType.SINGLE\n        )\n    )\n)\n</code></pre> <ul> <li>In a custom node:</li> </ul> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR(\n        strategy = RetrieveFactsFromHistory(\n            Concept(\n                keyword = \"user_preferences\", \n                // Description to the LLM -- what specifically to search for\n                description = \"User's preferences for the recommendation system, including the preferred conversation style, theme in the application, etc.\",\n                // LLM would search for multiple relevant facts related to this concept:\n                factType = FactType.MULTIPLE\n            ),\n            Concept(\n                keyword = \"product_details\",\n                // Description to the LLM -- what specifically to search for\n                description = \"Brief details about products in the catalog the user has been checking\",\n                // LLM would search for multiple relevant facts related to this concept:\n                factType = FactType.MULTIPLE\n            ),\n            Concept(\n                keyword = \"issue_solved\",\n                // Description to the LLM -- what specifically to search for\n                description = \"Was the initial user's issue resolved?\",\n                // LLM would search for a single answer to the question:\n                factType = FactType.SINGLE\n            )\n        )\n    )\n}\n</code></pre>"},{"location":"history-compression/#custom-history-compression-strategy-implementation","title":"Custom history compression strategy implementation","text":"<p>You can create your own history compression strategy by extending the <code>HistoryCompressionStrategy</code> abstract class and implementing the <code>compress</code> method.</p> <p>Here is an example:</p> <pre><code>class MyCustomCompressionStrategy : HistoryCompressionStrategy() {\n    override suspend fun compress(\n        llmSession: AIAgentLLMWriteSession,\n        memoryMessages: List&lt;Message&gt;\n    ) {\n        // 1. Process the current history in llmSession.prompt.messages\n        // 2. Create new compressed messages\n        // 3. Update the prompt with the compressed messages\n\n        // Save original messages to preserve them\n        val originalMessages = llmSession.prompt.messages\n\n        // Example implementation:\n        val importantMessages = llmSession.prompt.messages.filter {\n            // Your custom filtering logic\n            it.content.contains(\"important\")\n        }.filterIsInstance&lt;Message.Response&gt;()\n\n        // Note: you can also make LLM requests using the `llmSession` and ask the LLM to do some job for you using, for example, `llmSession.requestLLMWithoutTools()`\n        // Or you can change the current model: `llmSession.model = AnthropicModels.Sonnet_3_7` and ask some other LLM model -- but don't forget to change it back after\n\n        // Compose the prompt with the filtered messages\n        val compressedMessages = composeMessageHistory(\n            originalMessages,\n            importantMessages,\n            memoryMessages\n        )\n    }\n}\n</code></pre> <p>In this example, the custom strategy filters messages that contain the word \"important\" and keeps only those in the compressed history.</p> <p>Then you can use it as follows:</p> <ul> <li>In a strategy graph:</li> </ul> <pre><code>val compressHistory by nodeLLMCompressHistory&lt;ProcessedInput&gt;(\n    strategy = MyCustomCompressionStrategy()\n)\n</code></pre> <ul> <li>In a custom node:</li> </ul> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR(strategy = MyCustomCompressionStrategy())\n}\n</code></pre>"},{"location":"history-compression/#memory-preservation-during-compression","title":"Memory preservation during compression","text":"<p>All history compression methods have the <code>preserveMemory</code> parameter that determines whether memory-related messages should be preserved during compression. These are messages that contain facts retrieved from memory or indicate that the memory feature is not enabled.</p> <p>You can use the <code>preserveMemory</code> parameter as follows:</p> <ul> <li>In a strategy graph:</li> </ul> <pre><code>val compressHistory by nodeLLMCompressHistory&lt;ProcessedInput&gt;(\n    strategy = HistoryCompressionStrategy.WholeHistory,\n    preserveMemory = true\n)\n</code></pre> <ul> <li>In a custom node:</li> </ul> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR(\n        strategy = HistoryCompressionStrategy.WholeHistory,\n        preserveMemory = true\n    )\n}\n</code></pre>"},{"location":"key-features/","title":"Key features","text":"<p>Key features of Koog include:</p> <ul> <li>Multiplatform development: Deploy agents across JVM, JS, WasmJS, Android, and iOS targets using Kotlin Multiplatform.</li> <li>Reliability and fault-tolerance: Handle failures with built-in retries and restore the agent state at specific points during execution with the agent persistence feature.</li> <li>Intelligent history compression: Optimize token usage while maintaining context in long-running conversations using advanced built-in history compression techniques.</li> <li>Enterprise-ready integrations: Utilize integration with popular JVM frameworks such as Spring Boot and Ktor to embed Koog into your applications.</li> <li>Observability with OpenTelemetry exporters: Monitor and debug applications with built-in support for popular observability providers (W&amp;B Weave, Langfuse).</li> <li>LLM switching and seamless history adaptation: Switch to a different LLM at any point without losing the existing conversation history, or reroute between multiple LLM providers.</li> <li>Integration with JVM and Kotlin applications: Build AI agents with an idiomatic, type-safe Kotlin DSL designed specifically for JVM and Kotlin developers.</li> <li>Model Context Protocol integration: Use Model Context Protocol (MCP) tools in AI agents.</li> <li>Knowledge retrieval and memory: Retain and retrieve knowledge across conversations using vector embeddings, ranked document storage, and shared agent memory.</li> <li>Powerful Streaming API: Process responses in real-time with streaming support and parallel tool calls.</li> <li>Modular feature system: Customize agent capabilities through a composable architecture.</li> <li>Flexible graph workflows: Design complex agent behaviors using intuitive graph-based workflows.</li> <li>Custom tool creation: Enhance your agents with tools that access external systems and APIs.</li> <li>Comprehensive tracing: Debug and monitor agent execution with detailed, configurable tracing.</li> </ul>"},{"location":"koog-slack-channel/","title":"Koog on Slack","text":"<p>The Koog Slack channel is part of the Kotlin Slack workspace.</p> <ul> <li>If you already have an account in the workspace,   sign in to Kotlin Slack and join the <code>#koog-agentic-framework</code> channel.</li> <li>If you don't have an account,   open the Slack channel in your browser or request an invitation by providing your email address on the Kotlin Slack Sign-up page.</li> </ul>"},{"location":"ktor-plugin/","title":"Ktor Integration: Koog plugin","text":"<p>Koog fits naturally into your Ktor server allowing you to write server-side AI applications using ideomatic Kotlin API from both sides.</p> <p>Install the Koog plugin once, configure your LLM providers in application.conf/YAML or in code, and then call agents right from your routes. No more wiring LLM clients across modules \u2013 your routes just request an agent and are ready to go.</p>"},{"location":"ktor-plugin/#overview","title":"Overview","text":"<p>The <code>koog-ktor</code> module provides idiomatic Kotlin/Ktor integration for server\u2011side agentic development:</p> <ul> <li>Drop\u2011in Ktor plugin: <code>install(Koog)</code> in your Application</li> <li>First\u2011class support for OpenAI, Anthropic, Google, OpenRouter, DeepSeek, and Ollama</li> <li>Centralized configuration via YAML/CONF and/or code</li> <li>Agent setup with prompt, tools, features; simple extension functions for routes</li> <li>Direct LLM usage (execute, executeStreaming, moderate)</li> <li>JVM\u2011only Model Context Protocol (MCP) tools integration</li> </ul>"},{"location":"ktor-plugin/#add-dependency","title":"Add dependency","text":"<pre><code>dependencies {\n    implementation(\"ai.koog:koog-ktor:$koogVersion\")\n}\n</code></pre>"},{"location":"ktor-plugin/#quick-start","title":"Quick start","text":"<p>1) Configure providers (in <code>application.yaml</code> or <code>application.conf</code>)</p> <p>Use nested keys under <code>koog.&lt;provider&gt;</code>. The plugin automatically picks them up.</p> <pre><code># application.yaml (Ktor config)\nkoog:\n  openai:\n    apikey: ${OPENAI_API_KEY}\n    baseUrl: https://api.openai.com\n  anthropic:\n    apikey: ${ANTHROPIC_API_KEY}\n    baseUrl: https://api.anthropic.com\n  google:\n    apikey: ${GOOGLE_API_KEY}\n    baseUrl: https://generativelanguage.googleapis.com\n  openrouter:\n    apikey: ${OPENROUTER_API_KEY}\n    baseUrl: https://openrouter.ai\n  deepseek:\n    apikey: ${DEEPSEEK_API_KEY}\n    baseUrl: https://api.deepseek.com\n  # Ollama is enabled when any koog.ollama.* key exists\n  ollama:\n    enable: true\n    baseUrl: http://localhost:11434\n</code></pre> <p>Optional: configure fallback used by direct LLM calls when a requested provider is not configured.</p> <pre><code>koog:\n  llm:\n    fallback:\n      provider: openai\n      # see Model identifiers section below\n      model: openai.chat.gpt4_1\n</code></pre> <p>2) Install the plugin and define routes</p> <pre><code>fun Application.module() {\n    install(Koog) {\n        // You can also configure providers programmatically (see below)\n    }\n\n    routing {\n        route(\"/ai\") {\n            post(\"/chat\") {\n                val userInput = call.receiveText()\n                // Create and run a default single\u2011run agent using a specific model\n                val output = aiAgent(\n                    strategy = reActStrategy(),\n                    model = OpenAIModels.Chat.GPT4_1,\n                    input = userInput\n                )\n                call.respond(HttpStatusCode.OK, output)\n            }\n        }\n    }\n}\n</code></pre> <p>Notes</p> <ul> <li>aiAgent requires a concrete model (LLModel) \u2013 choose per\u2011route, per\u2011use.</li> <li>For lower\u2011level LLM access, use llm() (PromptExecutor) directly.</li> </ul>"},{"location":"ktor-plugin/#direct-llm-usage-from-routes","title":"Direct LLM usage from routes","text":"<pre><code>post(\"/llm-chat\") {\n    val userInput = call.receiveText()\n\n    val messages = llm().execute(\n        prompt(\"chat\") {\n            system(\"You are a helpful assistant that clarifies questions\")\n            user(userInput)\n        },\n        GoogleModels.Gemini2_5Pro\n    )\n\n    // Join all assistant messages into a single string\n    val text = messages.joinToString(separator = \"\") { it.content }\n    call.respond(HttpStatusCode.OK, text)\n}\n</code></pre> <p>Streaming</p> <pre><code>get(\"/stream\") {\n    val flow = llm().executeStreaming(\n        prompt(\"streaming\") { user(\"Stream this response, please\") },\n        OpenRouterModels.GPT4o\n    )\n\n    // Example: buffer and send as one chunk\n    val sb = StringBuilder()\n    flow.collect { chunk -&gt; sb.append(chunk) }\n    call.respondText(sb.toString())\n}\n</code></pre> <p>Moderation</p> <pre><code>post(\"/moderated-chat\") {\n    val userInput = call.receiveText()\n\n    val moderation = llm().moderate(\n        prompt(\"moderation\") { user(userInput) },\n        OpenAIModels.Moderation.Omni\n    )\n\n    if (moderation.isHarmful) {\n        call.respond(HttpStatusCode.BadRequest, \"Harmful content detected\")\n        return@post\n    }\n\n    val output = aiAgent(\n        strategy = reActStrategy(),\n        model = OpenAIModels.Chat.GPT4_1,\n        input = userInput\n    )\n    call.respond(HttpStatusCode.OK, output)\n}\n</code></pre>"},{"location":"ktor-plugin/#programmatic-configuration-in-code","title":"Programmatic configuration (in code)","text":"<p>All providers and agent behavior can be configured via install(Koog) {}.</p> <pre><code>install(Koog) {\n    llm {\n        openAI(apiKey = System.getenv(\"OPENAI_API_KEY\") ?: \"\") {\n            baseUrl = \"https://api.openai.com\"\n            timeouts { // Default values shown below\n                requestTimeout = 15.minutes\n                connectTimeout = 60.seconds\n                socketTimeout = 15.minutes\n            }\n        }\n        anthropic(apiKey = System.getenv(\"ANTHROPIC_API_KEY\") ?: \"\")\n        google(apiKey = System.getenv(\"GOOGLE_API_KEY\") ?: \"\")\n        openRouter(apiKey = System.getenv(\"OPENROUTER_API_KEY\") ?: \"\")\n        deepSeek(apiKey = System.getenv(\"DEEPSEEK_API_KEY\") ?: \"\")\n        ollama { baseUrl = \"http://localhost:11434\" }\n\n        // Optional fallback used by PromptExecutor when a provider isn\u2019t configured\n        fallback {\n            provider = LLMProvider.OpenAI\n            model = OpenAIModels.Chat.GPT4_1\n        }\n    }\n\n    agentConfig {\n        // Provide a reusable base prompt for your agents\n        prompt(name = \"agent\") {\n            system(\"You are a helpful server\u2011side agent\")\n        }\n\n        // Limit runaway tools/loops\n        maxAgentIterations = 10\n\n        // Register tools available to agents by default\n        registerTools {\n            // tool(::yourTool) // see Tools Overview for details\n        }\n\n        // Install agent features (tracing, etc.)\n        // install(OpenTelemetry) { /* ... */ }\n    }\n}\n</code></pre>"},{"location":"ktor-plugin/#model-identifiers-in-config-fallback","title":"Model identifiers in config (fallback)","text":"<p>When configuring llm.fallback in YAML/CONF, use these identifier formats:</p> <ul> <li>OpenAI: openai.chat.gpt4_1, openai.reasoning.o3, openai.costoptimized.gpt4_1mini, openai.audio.gpt4oaudio, openai.moderation.omni</li> <li>Anthropic: anthropic.sonnet_3_7, anthropic.opus_4, anthropic.haiku_3_5</li> <li>Google: google.gemini2_5pro, google.gemini2_0flash001</li> <li>OpenRouter: openrouter.gpt4o, openrouter.gpt4, openrouter.claude3sonnet</li> <li>DeepSeek: deepseek.deepseek-chat, deepseek.deepseek-reasoner</li> <li>Ollama: ollama.meta.llama3.2, ollama.alibaba.qwq:32b, ollama.groq.llama3-grok-tool-use:8b</li> </ul> <p>Note</p> <ul> <li>For OpenAI you must include the category (chat, reasoning, costoptimized, audio, embeddings, moderation).</li> <li>For Ollama, both ollama.model and ollama.. are supported."},{"location":"ktor-plugin/#mcp-tools-jvmonly","title":"MCP tools (JVM\u2011only)","text":"<p>On JVM you can add tools from an MCP server to your agent tool registry:</p> <pre><code>install(Koog) {\n    agentConfig {\n        mcp {\n            // Register via SSE\n            sse(\"https://your-mcp-server.com/sse\")\n\n            // Or register via spawned process (stdio transport)\n            // process(Runtime.getRuntime().exec(\"your-mcp-binary ...\"))\n\n            // Or from an existing MCP client instance\n            // client(existingMcpClient)\n        }\n    }\n}\n</code></pre>"},{"location":"ktor-plugin/#why-koog-ktor","title":"Why Koog + Ktor?","text":"<ul> <li>Kotlin\u2011first, type\u2011safe development of agents in your server</li> <li>Centralized config with clean, testable route code</li> <li>Use the right model per\u2011route, or fall back automatically for direct LLM calls</li> <li>Production\u2011ready features: tools, moderation, streaming, and tracing</li> </ul>"},{"location":"llm-parameters/","title":"LLM parameters","text":"<p>This page provides details about LLM parameters in the Koog agentic framework. LLM parameters let you control and customize the behavior of language models.</p>"},{"location":"llm-parameters/#overview","title":"Overview","text":"<p>LLM parameters are configuration options that let you fine-tune how language models generate responses. These parameters control aspects like response randomness, length, format, and tool usage. By adjusting the parameters, you optimize model behavior for different use cases, from creative content generation to deterministic structured outputs.</p> <p>In Koog, the <code>LLMParams</code> class incorporates LLM parameters and provides a consistent interface for configuring language model behavior. You can use LLM parameters in the following ways:</p> <ul> <li>When creating a prompt:</li> </ul> <pre><code>val prompt = prompt(\n    id = \"dev-assistant\",\n    params = LLMParams(\n        temperature = 0.7,\n        maxTokens = 500\n    )\n) {\n    // Add a system message to set the context\n    system(\"You are a helpful assistant.\")\n\n    // Add a user message\n    user(\"Tell me about Kotlin\")\n}\n</code></pre> <p>For more information about prompt creation, see Prompts.</p> <ul> <li>When creating a subgraph:</li> </ul> <pre><code>val processQuery by subgraphWithTask&lt;String, String&gt;(\n    tools = listOf(searchTool, calculatorTool, weatherTool),\n    llmModel = OpenAIModels.Chat.GPT4o,\n    llmParams = LLMParams(\n        temperature = 0.7,\n        maxTokens = 500\n    ),\n    runMode = ToolCalls.SEQUENTIAL,\n    assistantResponseRepeatMax = 3,\n) { userQuery -&gt;\n    \"\"\"\n    You are a helpful assistant that can answer questions about various topics.\n    Please help with the following query:\n    $userQuery\n    \"\"\"\n}\n</code></pre> <p>For more information about existing subgraph types in Koog, see Predefined subgraphs. To learn how to create and implement your own subgraphs, see Custom subgraphs.</p> <ul> <li>When updating a prompt in an LLM write session:</li> </ul> <pre><code>llm.writeSession {\n    changeLLMParams(\n        LLMParams(\n            temperature = 0.7,\n            maxTokens = 500\n        )\n    )\n}\n</code></pre> <p>For more information about sessions, see LLM sessions and manual history management.</p>"},{"location":"llm-parameters/#llm-parameter-reference","title":"LLM parameter reference","text":"<p>The following table provides a reference of LLM parameters included in the <code>LLMParams</code> class and supported by all LLM providers that are available in Koog out of the box. For a list of parameters that are specific to some providers, see Provider-specific parameters.</p> Parameter Type Description <code>temperature</code> Double Controls randomness in the output. Higher values, such as 0.7\u20131.0, produce more diverse and creative responses, while lower values produce more deterministic and focused responses. <code>maxTokens</code> Integer Maximum number of tokens to generate in the response. Useful for controlling response length. <code>numberOfChoices</code> Integer Number of alternative responses to generate. Must be greater than 0. <code>speculation</code> String A speculative configuration string that influences model behavior, designed to enhance result speed and accuracy. Supported only by certain models, but may greatly improve speed and accuracy. <code>schema</code> Schema Defines the structure for the model's response format, enabling structured outputs like JSON. For more information, see Schema. <code>toolChoice</code> ToolChoice Controls tool calling behavior of the language model. For more information, see Tool choice. <code>user</code> String Identifier for the user making the request, which can be used for tracking purposes. <code>additionalProperties</code> Map&lt;String, JsonElement&gt; Additional properties that can be used to store custom parameters specific to certain model providers. <p>For a list of default values for each parameter, see the corresponding LLM provider documentation:</p> <ul> <li>OpenAI Chat</li> <li>OpenAI Responses</li> <li>Google</li> <li>Anthropic</li> <li>Mistral</li> <li>DeepSeek</li> <li>OpenRouter</li> <li>Alibaba (DashScope)</li> </ul>"},{"location":"llm-parameters/#schema","title":"Schema","text":"<p>The <code>Schema</code> interface defines the structure for the model's response format. Koog supports JSON schemas, as described in the sections below.</p>"},{"location":"llm-parameters/#json-schemas","title":"JSON schemas","text":"<p>JSON schemas let you request structured JSON data from language models. Koog supports the following two types of JSON schemas:</p> <p>1) Basic JSON Schema (<code>LLMParams.Schema.JSON.Basic</code>): Used for basic JSON processing capabilities. This format primarily focuses on nested data definitions without advanced JSON Schema functionalities.</p> <pre><code>// Create parameters with a basic JSON schema\nval jsonParams = LLMParams(\n    temperature = 0.2,\n    schema = LLMParams.Schema.JSON.Basic(\n        name = \"PersonInfo\",\n        schema = JsonObject(mapOf(\n            \"type\" to JsonPrimitive(\"object\"),\n            \"properties\" to JsonObject(\n                mapOf(\n                    \"name\" to JsonObject(mapOf(\"type\" to JsonPrimitive(\"string\"))),\n                    \"age\" to JsonObject(mapOf(\"type\" to JsonPrimitive(\"number\"))),\n                    \"skills\" to JsonObject(\n                        mapOf(\n                            \"type\" to JsonPrimitive(\"array\"),\n                            \"items\" to JsonObject(mapOf(\"type\" to JsonPrimitive(\"string\")))\n                        )\n                    )\n                )\n            ),\n            \"additionalProperties\" to JsonPrimitive(false),\n            \"required\" to JsonArray(listOf(JsonPrimitive(\"name\"), JsonPrimitive(\"age\"), JsonPrimitive(\"skills\")))\n        ))\n    )\n)\n</code></pre> <p>2) Standard JSON Schema (<code>LLMParams.Schema.JSON.Standard</code>): Represents a standard JSON schema according to json-schema.org. This format is a proper subset of the official JSON Schema specification. Note that the flavor across different LLM providers might vary, since not all of them support full JSON schemas.</p> <pre><code>// Create parameters with a standard JSON schema\nval standardJsonParams = LLMParams(\n    temperature = 0.2,\n    schema = LLMParams.Schema.JSON.Standard(\n        name = \"ProductCatalog\",\n        schema = JsonObject(mapOf(\n            \"type\" to JsonPrimitive(\"object\"),\n            \"properties\" to JsonObject(mapOf(\n                \"products\" to JsonObject(mapOf(\n                    \"type\" to JsonPrimitive(\"array\"),\n                    \"items\" to JsonObject(mapOf(\n                        \"type\" to JsonPrimitive(\"object\"),\n                        \"properties\" to JsonObject(mapOf(\n                            \"id\" to JsonObject(mapOf(\"type\" to JsonPrimitive(\"string\"))),\n                            \"name\" to JsonObject(mapOf(\"type\" to JsonPrimitive(\"string\"))),\n                            \"price\" to JsonObject(mapOf(\"type\" to JsonPrimitive(\"number\"))),\n                            \"description\" to JsonObject(mapOf(\"type\" to JsonPrimitive(\"string\")))\n                        )),\n                        \"additionalProperties\" to JsonPrimitive(false),\n                        \"required\" to JsonArray(listOf(JsonPrimitive(\"id\"), JsonPrimitive(\"name\"), JsonPrimitive(\"price\"), JsonPrimitive(\"description\")))\n                    ))\n                ))\n            )),\n            \"additionalProperties\" to JsonPrimitive(false),\n            \"required\" to JsonArray(listOf(JsonPrimitive(\"products\")))\n        ))\n    )\n)\n</code></pre>"},{"location":"llm-parameters/#tool-choice","title":"Tool choice","text":"<p>The <code>ToolChoice</code> class controls how the language model uses tools. It provides the following options:</p> <ul> <li><code>LLMParams.ToolChoice.Named</code>: the language model calls the specified tool. Takes the <code>name</code> string argument that   represents the name of the tool to call.</li> <li><code>LLMParams.ToolChoice.All</code>: the language model calls all tools.</li> <li><code>LLMParams.ToolChoice.None</code>: the language model does not call tools and only generates text.</li> <li><code>LLMParams.ToolChoice.Auto</code>: the language model automatically decides whether to call tools and which tool to call.</li> <li><code>LLMParams.ToolChoice.Required</code>: the language model calls at least one tool.</li> </ul> <p>Here is an example of using the <code>LLMParams.ToolChoice.Named</code> class to call a specific tool:</p> <pre><code>val specificToolParams = LLMParams(\n    toolChoice = LLMParams.ToolChoice.Named(name = \"calculator\")\n)\n</code></pre>"},{"location":"llm-parameters/#provider-specific-parameters","title":"Provider-specific parameters","text":"<p>Koog supports provider-specific parameters for some LLM providers. These parameters extend the base <code>LLMParams</code> class and add provider-specific functionality. The following classes include parameters that are specific per provider:</p> <ul> <li><code>OpenAIChatParams</code>: Parameters specific to the OpenAI Chat Completions API.</li> <li><code>OpenAIResponsesParams</code>: Parameters specific to the OpenAI Responses API.</li> <li><code>GoogleParams</code>: Parameters specific to Google models.</li> <li><code>AnthropicParams</code>: Parameters specific to Anthropic models.</li> <li><code>MistralAIParams</code>: Parameters specific to Mistral models.</li> <li><code>DeepSeekParams</code>: Parameters specific to DeepSeek models.</li> <li><code>OpenRouterParams</code>: Parameters specific to OpenRouter models.</li> <li><code>DashscopeParams</code>: Parameters specific to Alibaba models.</li> </ul> <p>Here is the complete reference of provider-specific parameters in Koog:</p> OpenAI ChatOpenAI ResponsesGoogleAnthropicMistralDeepSeekOpenRouterAlibaba (DashScope) <p>The following example shows defined OpenRouter LLM parameters using the provider-specific <code>OpenRouterParams</code> class:</p> <pre><code>val openRouterParams = OpenRouterParams(\n    temperature = 0.7,\n    maxTokens = 500,\n    frequencyPenalty = 0.5,\n    presencePenalty = 0.5,\n    topP = 0.9,\n    topK = 40,\n    repetitionPenalty = 1.1,\n    models = listOf(\"anthropic/claude-3-opus\", \"anthropic/claude-3-sonnet\"),\n    transforms = listOf(\"middle-out\")\n)\n</code></pre>"},{"location":"llm-parameters/#usage-examples","title":"Usage examples","text":""},{"location":"llm-parameters/#basic-usage","title":"Basic usage","text":"<pre><code>// A basic set of parameters with limited length \nval basicParams = LLMParams(\n    temperature = 0.7,\n    maxTokens = 150,\n    toolChoice = LLMParams.ToolChoice.Auto\n)\n</code></pre>"},{"location":"llm-parameters/#reasoning-control","title":"Reasoning control","text":"<p>You implement reasoning control through provider-specific parameters that control model reasoning. When using the OpenAI Chat API and models that support reasoning, use the <code>reasoningEffort</code> parameter to control how many reasoning tokens the model generates before providing a response:</p> <pre><code>val openAIReasoningEffortParams = OpenAIChatParams(\n    reasoningEffort = ReasoningEffort.MEDIUM\n)\n</code></pre> <p>In addition, when using the OpenAI Responses API in a stateless mode, you keep an encrypted history of reasoning items and send it to the model in every conversation turn. The encryption is done on the OpenAI side, and you need to request encrypted reasoning tokens by setting the <code>include</code> parameter in your requests to <code>reasoning.encrypted_content</code>. You can then pass the encrypted reasoning tokens back to the model in the next conversation turns.</p> <pre><code>val openAIStatelessReasoningParams = OpenAIResponsesParams(\n    include = listOf(OpenAIInclude.REASONING_ENCRYPTED_CONTENT)\n)\n</code></pre>"},{"location":"llm-parameters/#custom-parameters","title":"Custom parameters","text":"<p>To add custom parameters that may be provider specific and not supported in Koog out of the box, use the <code>additionalProperties</code> property as shown in the example below.</p> <pre><code>// Add custom parameters for specific model providers\nval customParams = LLMParams(\n    additionalProperties = additionalPropertiesOf(\n        \"top_p\" to 0.95,\n        \"frequency_penalty\" to 0.5,\n        \"presence_penalty\" to 0.5\n    )\n)\n</code></pre>"},{"location":"llm-parameters/#setting-and-overriding-parameters","title":"Setting and overriding parameters","text":"<p>The code sample below shows how you can define a set of LLM parameters that you may want to use primarily, then create another set by partially overriding values from the original set and adding new values to it. This lets you define parameters that are common to most requests but also add more specific parameter combinations without having to repeat the common parameters.</p> <pre><code>// Define default parameters\nval defaultParams = LLMParams(\n    temperature = 0.7,\n    maxTokens = 150,\n    toolChoice = LLMParams.ToolChoice.Auto\n)\n\n// Create parameters with some overrides, using defaults for the rest\nval overrideParams = LLMParams(\n    temperature = 0.2,\n    numberOfChoices = 3\n).default(defaultParams)\n</code></pre> <p>The values in the resulting <code>overrideParams</code> set are equivalent to the following:</p> <pre><code>val overrideParams = LLMParams(\n    temperature = 0.2,\n    maxTokens = 150,\n    toolChoice = LLMParams.ToolChoice.Auto,\n    numberOfChoices = 3\n)\n</code></pre>"},{"location":"llm-providers/","title":"LLM providers","text":"<p>Koog works with major LLM providers and also supports local models using Ollama. The following providers are currently supported:</p> LLM provider Choose for OpenAI (including Azure OpenAI Service) Advanced models with a wide range of capabilities. Anthropic Long contexts and prompt caching. Google Multimodal processing (audio, video), large contexts. DeepSeek Cost-effective reasoning and coding. OpenRouter One integration with an access to multiple models from multiple providers for flexibility, provider comparison, and unified API. Amazon Bedrock AWS-native environment, enterprise security and compliance, multi-provider access. Mistral European data hosting, GDPR compliance. Alibaba (DashScope OpenAI-compatible client) Large contexts and cost-efficient Qwen models. Ollama Privacy, local development, offline operation, and no API costs. <p>The table below shows the LLM capabilities that Koog supports and which providers offer these capabilities in their models.</p> LLM capability OpenAI Anthropic Google DeepSeek OpenRouter Amazon Bedrock Mistral Alibaba (DashScope OpenAI-compatible client) Ollama (local models) Supported input Text, image, audio, document Text, image, document<sup>1</sup> Text, image, audio, video, document<sup>1</sup> Text Differs by model Differs by model Text, image, document<sup>1</sup> Text, image, audio, video<sup>1</sup> Text, image<sup>1</sup> Response streaming \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Tools \u2713 \u2713 \u2713 \u2713 \u2713 \u2713<sup>1</sup> \u2713 \u2713 \u2713 Tool choice \u2713 \u2713 \u2713 \u2713 \u2713 \u2713<sup>1</sup> \u2713 \u2713 \u2013 Structured output (JSON Schema) \u2713 \u2013 \u2713 \u2713 \u2713<sup>1</sup> \u2013 \u2713 \u2713<sup>1</sup> \u2713 Multiple choices \u2713 \u2013 \u2713 \u2013 \u2713<sup>1</sup> \u2713<sup>1</sup> \u2713 \u2713<sup>1</sup> \u2013 Temperature \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Speculation \u2713<sup>1</sup> \u2013 \u2013 \u2013 \u2713<sup>1</sup> \u2013 \u2713<sup>1</sup> \u2713<sup>1</sup> \u2013 Content moderation \u2713 \u2013 \u2013 \u2013 \u2013 \u2713 \u2713 \u2013 \u2713 Embeddings \u2713 \u2013 \u2013 \u2013 \u2013 \u2713 \u2713 \u2013 \u2713 Prompt caching \u2713<sup>1</sup> \u2713 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 Completion \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Local execution \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2713 <p>Note</p> <p>Koog supports the most commonly used capabilities for creating AI agents. LLMs from each provider may have additional features that Koog does not currently support. To learn more, refer to Model capabilities.</p>"},{"location":"llm-providers/#working-with-providers","title":"Working with providers","text":"<p>Koog lets you work with LLM providers on two levels:</p> <ul> <li> <p>Using an LLM client for direct interaction with a specific provider.   Each client implements the <code>LLMClient</code> interface, handling authentication,    request formatting, and response parsing for the provider.   For details, see LLM clients.</p> </li> <li> <p>Using a prompt executor for a higher-level abstraction that wraps one or multiple LLM clients,     manages their lifecycles, and unifies an interface across providers.     It can switch between providers     and optionally fall back to a configured provider and LLM using the corresponding client.     You can either create your own executor or use a pre-defined prompt executor for a specific provider.     For details, see Prompt executors.</p> </li> </ul> <p>Using a prompt executor offers a higher\u2011level layer over one or more LLMClients.  It manages client lifecycles and exposes a unified interface across providers.  In multi\u2011provider setups, it can route requests between providers and optionally fall back to a designated client when needed for core requests. You can create your own executor or use pre\u2011defined ones\u2014both single\u2011provider and multi\u2011provider options are available.</p>"},{"location":"llm-providers/#next-steps","title":"Next steps","text":"<ul> <li>Create and run an agent with a specific LLM provider.</li> <li>Learn more about prompts.</li> </ul> <ol> <li> <p>Capability is supported only by some models of the provider.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"model-capabilities/","title":"Model capabilities","text":"<p>Koog provides a set of abstractions and implementations for working with Large Language Models (LLMs) from various LLM providers in a provider-agnostic way. The set includes the following classes:</p> <ul> <li> <p>LLMCapability: a class hierarchy that defines various capabilities that LLMs can support, such as:</p> <ul> <li>Temperature adjustment for controlling response randomness</li> <li>Tool integration for external system interaction</li> <li>Vision processing for handling visual data</li> <li>Embedding generation for vector representations</li> <li>Completion for text generation tasks</li> <li>Schema support for structured data (JSON with Simple and Full variants)</li> <li>Speculation for exploratory responses</li> </ul> </li> <li> <p>LLModel: a data class that represents a specific LLM with its provider, unique identifier, and supported   capabilities.</p> </li> </ul> <p>This serves as a foundation for interacting with different LLM providers in a unified way, allowing applications to work with various models while abstracting away provider-specific details.</p>"},{"location":"model-capabilities/#llm-capabilities","title":"LLM capabilities","text":"<p>LLM capabilities represent specific features or functionalities that a Large Language Model can support. In the Koog framework, capabilities are used to define what a particular model can do and how it can be configured. Each capability is represented as a subclass or data object of the <code>LLMCapability</code> class.</p> <p>When configuring an LLM for use in your application, you specify which capabilities it supports by adding them to the <code>capabilities</code> list when creating an <code>LLModel</code> instance. This allows the framework to properly interact with the model and use its features appropriately.</p>"},{"location":"model-capabilities/#core-capabilities","title":"Core capabilities","text":"<p>The list below includes the core, LLM-specific capabilities that are available for models in the Koog framework:</p> <ul> <li> <p>Speculation (<code>LLMCapability.Speculation</code>): lets the model generate speculative or exploratory responses with   varying degrees of likelihood. Useful for creative or hypothetical scenarios where a broader range of potential   outcomes   is desired.</p> </li> <li> <p>Temperature (<code>LLMCapability.Temperature</code>): allows adjustment of the model's response randomness or creativity   levels. Higher temperature values produce more diverse outputs, while lower values lead to more focused and   deterministic responses.</p> </li> <li> <p>Tools (<code>LLMCapability.Tools</code>): indicates support for external tool usage or integration. This capability lets the   model run specific tools or interact with external systems.</p> </li> <li> <p>Tool choice (<code>LLMCapability.ToolChoice</code>): configures how tool calling works with the LLM. Depending on the model,   it can be configured to:</p> <ul> <li>Automatically choose between generating text or tool calls</li> <li>Generate only tool calls, never text</li> <li>Generate only text, never tool calls</li> <li>Force calling a specific tool among the defined tools</li> </ul> </li> <li> <p>Multiple choices (<code>LLMCapability.MultipleChoices</code>): lets the model generate multiple independent reply choices   to a single prompt.</p> </li> </ul>"},{"location":"model-capabilities/#media-processing-capabilities","title":"Media processing capabilities","text":"<p>The following list represents a set of capabilities for processing media content such as images or audio:</p> <ul> <li> <p>Vision (<code>LLMCapability.Vision</code>): a class for vision-based capabilities that process, analyze, and infer insights   from visual data.   Supports the following types of visual data:</p> <ul> <li>Image (<code>LLMCapability.Vision.Image</code>): handles image-related vision tasks such as image analysis, recognition,   and interpretation.</li> <li>Video (<code>LLMCapability.Vision.Video</code>): processes video data, including analyzing and understanding video   content.</li> </ul> </li> <li> <p>Audio (<code>LLMCapability.Audio</code>): provides audio-related functionalities such as transcription, audio generation, or   audio-based interactions.</p> </li> <li> <p>Document (<code>LLMCapability.Document</code>): enables handling and processing of document-based inputs and outputs.</p> </li> </ul>"},{"location":"model-capabilities/#text-processing-capabilities","title":"Text processing capabilities","text":"<p>The following list of capabilities represents text generation and processing functionalities:</p> <ul> <li> <p>Embedding (<code>LLMCapability.Embed</code>): lets models generate vector embeddings from an input text, enabling similarity   comparisons, clustering, and other vector-based analyses.</p> </li> <li> <p>Completion (<code>LLMCapability.Completion</code>): includes the generation of text or content based on given input context,   such as completing sentences, generating suggestions, or producing content that aligns with input data.</p> </li> <li> <p>Prompt caching (<code>LLMCapability.PromptCaching</code>): supports caching functionalities for prompts, potentially   improving   performance for repeated or similar queries.</p> </li> <li> <p>Moderation (<code>LLMCapability.Moderation</code>): lets the model analyze text for potentially harmful content and   classify it according to various categories such as harassment, hate speech, self-harm, sexual content, violence, etc.</p> </li> </ul>"},{"location":"model-capabilities/#schema-capabilities","title":"Schema capabilities","text":"<p>The list below indicates the capabilities related to processing structured data:</p> <ul> <li>Schema (<code>LLMCapability.Schema</code>): a class for structured schema capabilities related to data interaction and   encoding using specific formats.   Includes support for the following format:<ul> <li>JSON (<code>LLMCapability.Schema.JSON</code>): JSON schema support with different levels:<ul> <li>Basic (<code>LLMCapability.Schema.JSON.Basic</code>): provides lightweight or basic JSON processing capabilities.</li> <li>Standard (<code>LLMCapability.Schema.JSON.Standard</code>): offers comprehensive JSON schema support for complex data   structures.</li> </ul> </li> </ul> </li> </ul>"},{"location":"model-capabilities/#creating-a-model-llmodel-configuration","title":"Creating a model (LLModel) configuration","text":"<p>To define a model in a universal, provider-agnostic way, create a model configuration as an instance of the <code>LLModel</code> class with the following parameters:</p> Name Data type Required Default Description <code>provider</code> LLMProvider Yes The provider of the LLM, such as Google or OpenAI. This identifies the company or organization that created or hosts the model. <code>id</code> String Yes A unique identifier for the LLM instance. This typically represents the specific model version or name. For example, <code>gpt-4-turbo</code>, <code>claude-3-opus</code>, <code>llama-3-2</code>. <code>capabilities</code> List&lt;LLMCapability&gt; Yes A list of capabilities supported by the LLM, such as temperature adjustment, tools usage, or schema-based tasks. These capabilities define what the model can do and how it can be configured. <code>contextLength</code> Long Yes The context length of the LLM. This is the maximum number of tokens the LLM can process. <code>maxOutputTokens</code> Long No <code>null</code> The maximum number of tokens that can be generated by the provider for the LLM."},{"location":"model-capabilities/#examples","title":"Examples","text":"<p>This section provides detailed examples of creating <code>LLModel</code> instances with different capabilities.</p> <p>The code below represents a basic LLM configuration with core capabilities:</p> <pre><code>val basicModel = LLModel(\n    provider = LLMProvider.OpenAI,\n    id = \"gpt-4-turbo\",\n    capabilities = listOf(\n        LLMCapability.Temperature,\n        LLMCapability.Tools,\n        LLMCapability.Schema.JSON.Standard\n    ),\n    contextLength = 128_000\n)\n</code></pre> <p>The model configuration below is a multimodal LLM with vision capabilities:</p> <pre><code>val visionModel = LLModel(\n    provider = LLMProvider.OpenAI,\n    id = \"gpt-4-vision\",\n    capabilities = listOf(\n        LLMCapability.Temperature,\n        LLMCapability.Vision.Image,\n        LLMCapability.MultipleChoices\n    ),\n    contextLength = 1_047_576,\n    maxOutputTokens = 32_768\n)\n</code></pre> <p>An LLM with audio processing capabilities:</p> <pre><code>val audioModel = LLModel(\n    provider = LLMProvider.Anthropic,\n    id = \"claude-3-opus\",\n    capabilities = listOf(\n        LLMCapability.Audio,\n        LLMCapability.Temperature,\n        LLMCapability.PromptCaching\n    ),\n    contextLength = 200_000\n)\n</code></pre> <p>In addition to creating models as <code>LLModel</code> instances and having to specify all related parameters, Koog includes a collection of predefined models and their configurations with supported capabilities. To use a predefined Ollama model, specify it as follows:</p> <pre><code>val metaModel = OllamaModels.Meta.LLAMA_3_2\n</code></pre> <p>To check whether a model supports a specific capability use the <code>contains</code> method to check for the presence of the capability in the <code>capabilities</code> list:</p> <pre><code>// Check if models support specific capabilities\nval supportsTools = basicModel.capabilities.contains(LLMCapability.Tools) // true\nval supportsVideo = visionModel.capabilities.contains(LLMCapability.Vision.Video) // false\n\n// Check for schema capabilities\nval jsonCapability = basicModel.capabilities.filterIsInstance&lt;LLMCapability.Schema.JSON&gt;().firstOrNull()\nval hasFullJsonSupport = jsonCapability is LLMCapability.Schema.JSON.Standard // true\n</code></pre>"},{"location":"model-capabilities/#llm-capabilities-by-model","title":"LLM capabilities by model","text":"<p>This reference shows which LLM capabilities are supported by each model across different providers.</p> <p>In the tables below:</p> <ul> <li><code>\u2713</code> indicates that the model supports the capability</li> <li><code>-</code> indicates that the model does not support the capability</li> <li>For JSON Schema, <code>Full</code> or <code>Simple</code> indicates which variant of the JSON Schema capability the model supports</li> </ul> Google models OpenAI models Anthropic models Ollama models DeepSeek models OpenRouter models"},{"location":"model-capabilities/#google-models","title":"Google models","text":"Model Temperature JSON Schema Completion Multiple Choices Tools Tool Choice Vision (Image) Vision (Video) Audio Gemini2_5Pro \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemini2_5Flash \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemini2_5FlashLite \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemini2_0Flash \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemini2_0Flash001 \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemini2_0FlashLite \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemini2_0FlashLite001 \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713"},{"location":"model-capabilities/#openai-models","title":"OpenAI models","text":"Model Temperature JSON Schema Completion Multiple Choices Tools Tool Choice Vision (Image) Vision (Video) Audio Speculation Moderation Reasoning.O4Mini - Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - Reasoning.O3Mini - Full \u2713 \u2713 \u2713 \u2713 - - - \u2713 - Reasoning.O3 - Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - Reasoning.O1 - Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - Chat.GPT4o \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - Chat.GPT4_1 \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - Chat.GPT5 \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - Chat.GPT5Mini \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - Chat.GPT5Nano \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - Audio.GptAudio \u2713 - \u2713 - \u2713 \u2713 - - \u2713 - - Audio.GPT4oMiniAudio \u2713 - \u2713 - \u2713 \u2713 - - \u2713 - - Audio.GPT4oAudio \u2713 - \u2713 - \u2713 \u2713 - - \u2713 - - CostOptimized.GPT4_1Nano \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - CostOptimized.GPT4_1Mini \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - CostOptimized.GPT4oMini \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - Moderation.Omni - - - - - - \u2713 - - - \u2713"},{"location":"model-capabilities/#anthropic-models","title":"Anthropic models","text":"Model Temperature JSON Schema Completion Tools Tool Choice Vision (Image) Opus_4_1 \u2713 - \u2713 \u2713 \u2713 \u2713 Opus_4 \u2713 - \u2713 \u2713 \u2713 \u2713 Sonnet_4 \u2713 - \u2713 \u2713 \u2713 \u2713 Sonnet_3_7 \u2713 - \u2713 \u2713 \u2713 \u2713 Haiku_3_5 \u2713 - \u2713 \u2713 \u2713 \u2713 Sonnet_3_5 \u2713 - \u2713 \u2713 \u2713 \u2713 Haiku_3 \u2713 - \u2713 \u2713 \u2713 \u2713 Opus_3 \u2713 - \u2713 \u2713 \u2713 \u2713"},{"location":"model-capabilities/#ollama-models","title":"Ollama models","text":""},{"location":"model-capabilities/#meta-models","title":"Meta models","text":"Model Temperature JSON Schema Tools Moderation LLAMA_3_2_3B \u2713 Simple \u2713 - LLAMA_3_2 \u2713 Simple \u2713 - LLAMA_4 \u2713 Simple \u2713 - LLAMA_GUARD_3 - - - \u2713"},{"location":"model-capabilities/#alibaba-models","title":"Alibaba models","text":"Model Temperature JSON Schema Tools QWEN_2_5_05B \u2713 Simple \u2713 QWEN_3_06B \u2713 Simple \u2713 QWQ \u2713 Simple \u2713 QWEN_CODER_2_5_32B \u2713 Simple \u2713"},{"location":"model-capabilities/#groq-models","title":"Groq models","text":"Model Temperature JSON Schema Tools LLAMA_3_GROK_TOOL_USE_8B \u2713 Full \u2713 LLAMA_3_GROK_TOOL_USE_70B \u2713 Full \u2713"},{"location":"model-capabilities/#granite-models","title":"Granite models","text":"Model Temperature JSON Schema Tools Vision (Image) GRANITE_3_2_VISION \u2713 Simple \u2713 \u2713"},{"location":"model-capabilities/#deepseek-models","title":"DeepSeek models","text":"Model Temperature JSON Schema Completion Speculation Tools Tool Choice Vision (Image) DeepSeekChat \u2713 Full \u2713 - \u2713 \u2713 - DeepSeekReasoner \u2713 Full \u2713 - \u2713 \u2713 -"},{"location":"model-capabilities/#openrouter-models","title":"OpenRouter models","text":"Model Temperature JSON Schema Completion Speculation Tools Tool Choice Vision (Image) Phi4Reasoning \u2713 Full \u2713 \u2713 \u2713 \u2713 - Claude3Opus \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Claude3Sonnet \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Claude3Haiku \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Claude3_5Sonnet \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Claude3_7Sonnet \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Claude4Sonnet \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Claude4_1Opus \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 GPT4oMini \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 GPT5 \u2713 Full \u2713 \u2713 \u2713 \u2713 - GPT5Mini \u2713 Full \u2713 \u2713 \u2713 \u2713 - GPT5Nano \u2713 Full \u2713 \u2713 \u2713 \u2713 - GPT_OSS_120b \u2713 Full \u2713 \u2713 \u2713 \u2713 - GPT4 \u2713 Full \u2713 \u2713 \u2713 \u2713 - GPT4o \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 GPT4Turbo \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 GPT35Turbo \u2713 Full \u2713 \u2713 \u2713 \u2713 - Llama3 \u2713 Full \u2713 \u2713 \u2713 \u2713 - Llama3Instruct \u2713 Full \u2713 \u2713 \u2713 \u2713 - Mistral7B \u2713 Full \u2713 \u2713 \u2713 \u2713 - Mixtral8x7B \u2713 Full \u2713 \u2713 \u2713 \u2713 - Claude3VisionSonnet \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Claude3VisionOpus \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Claude3VisionHaiku \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 DeepSeekV30324 \u2713 Full \u2713 \u2713 \u2713 \u2713 - Gemini2_5FlashLite \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Gemini2_5Flash \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Gemini2_5Pro \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713"},{"location":"model-context-protocol/","title":"Model Context Protocol","text":"<p>Model Context Protocol (MCP) is a standardized protocol that lets AI agents interact with external tools and services through a consistent interface.</p> <p>MCP exposes tools and prompts as API endpoints that AI agents can call. Each tool has a specific name and an input schema that describes its inputs and outputs using the JSON Schema format.</p> <p>The Koog framework provides integration with MCP servers, enabling you to incorporate MCP tools into your Koog agents.</p> <p>To learn more about the protocol, see the Model Context Protocol documentation.</p>"},{"location":"model-context-protocol/#mcp-servers","title":"MCP servers","text":"<p>MCP servers implement Model Context Protocol and provide a standardized way for AI agents to interact with tools and services.</p> <p>You can find ready-to-use MCP servers in the MCP Marketplace or MCP DockerHub.</p> <p>The MCP servers support the following transport protocols to communicate with agents:</p> <ul> <li>Standard input/output (stdio) transport protocol used to communicate with the MCP servers running as separate processes. For example, a Docker container or a CLI tool.</li> <li>Server-sent events (SSE) transport protocol (optional) used to communicate with the MCP servers over HTTP.</li> </ul>"},{"location":"model-context-protocol/#integration-with-koog","title":"Integration with Koog","text":"<p>The Koog framework integrates with MCP using the MCP SDK with the additional API extensions presented in the <code>agent-mcp</code> module.</p> <p>This integration lets the Koog agents perform the following:</p> <ul> <li>Connect to MCP servers through various transport mechanisms (stdio, SSE).</li> <li>Retrieve available tools from an MCP server.</li> <li>Transform MCP tools into the Koog tool interface.</li> <li>Register the transformed tools in a tool registry.</li> <li>Call MCP tools with arguments provided by the LLM.</li> </ul>"},{"location":"model-context-protocol/#key-components","title":"Key components","text":"<p>Here are the main components of the MCP integration in Koog:</p> Component Description <code>McpTool</code> Serves as a bridge between the Koog tool interface and the MCP SDK. <code>McpToolDescriptorParser</code> Parses MCP tool definitions into the Koog tool descriptor format. <code>McpToolRegistryProvider</code> Creates MCP tool registries that connect to MCP servers through various transport mechanisms (stdio, SSE)."},{"location":"model-context-protocol/#getting-started","title":"Getting started","text":""},{"location":"model-context-protocol/#1-set-up-an-mcp-connection","title":"1. Set up an MCP connection","text":"<p>To use MCP with Koog, you need to set up a connection:</p> <ol> <li>Start an MCP server (either as a process, Docker container, or web service).</li> <li>Create a transport mechanism to communicate with the server. </li> </ol> <p>MCP servers support the stdio and SSE transport mechanisms to communicate with the agent, so you can connect using one of them.</p>"},{"location":"model-context-protocol/#connect-with-stdio","title":"Connect with stdio","text":"<p>This protocol is used when an MCP server runs as a separate process. Here is an example of setting up an MCP connection using the stdio transport:</p> <pre><code>// Start an MCP server (for example, as a process)\nval process = ProcessBuilder(\"path/to/mcp/server\").start()\n\n// Create the stdio transport \nval transport = McpToolRegistryProvider.defaultStdioTransport(process)\n</code></pre>"},{"location":"model-context-protocol/#connect-with-sse","title":"Connect with SSE","text":"<p>This protocol is used when an MCP server runs as a web service. Here is an example of setting up an MCP connection using the SSE transport:</p> <pre><code>// Create the SSE transport\nval transport = McpToolRegistryProvider.defaultSseTransport(\"http://localhost:8931\")\n</code></pre>"},{"location":"model-context-protocol/#2-create-a-tool-registry","title":"2. Create a tool registry","text":"<p>Once you have the MCP connection, you can create a tool registry with tools from the MCP server in one of the following ways:</p> <ul> <li>Using the provided transport mechanism for communication. For example:</li> </ul> <pre><code>// Create a tool registry with tools from the MCP server\nval toolRegistry = McpToolRegistryProvider.fromTransport(\n    transport = transport,\n    name = \"my-client\",\n    version = \"1.0.0\"\n)\n</code></pre> <ul> <li>Using an MCP client connected to the MCP server. For example:</li> </ul> <pre><code>// Create a tool registry from an existing MCP client\nval toolRegistry = McpToolRegistryProvider.fromClient(\n    mcpClient = existingMcpClient\n)\n</code></pre>"},{"location":"model-context-protocol/#3-integrate-with-your-agent","title":"3. Integrate with your agent","text":"<p>To use MCP tools with your Koog agent, you need to register the tool registry with the agent:</p> <pre><code>// Create an agent with the tools\nval agent = AIAgent(\n    promptExecutor = executor,\n    strategy = strategy,\n    llmModel = OpenAIModels.Chat.GPT4o,\n    toolRegistry = toolRegistry\n)\n\n// Run the agent with a task that uses an MCP tool\nval result = agent.run(\"Use the MCP tool to perform a task\")\n</code></pre>"},{"location":"model-context-protocol/#usage-examples","title":"Usage examples","text":""},{"location":"model-context-protocol/#google-maps-mcp-integration","title":"Google Maps MCP integration","text":"<p>This example demonstrates how to connect to a Google Maps server for geographic data using MCP:</p> <pre><code>// Start the Docker container with the Google Maps MCP server\nval process = ProcessBuilder(\n    \"docker\", \"run\", \"-i\",\n    \"-e\", \"GOOGLE_MAPS_API_KEY=$googleMapsApiKey\",\n    \"mcp/google-maps\"\n).start()\n\n// Create the ToolRegistry with tools from the MCP server\nval toolRegistry = McpToolRegistryProvider.fromTransport(\n    transport = McpToolRegistryProvider.defaultStdioTransport(process)\n)\n\n// Create and run the agent\nval agent = AIAgent(\n    promptExecutor = simpleOpenAIExecutor(openAIApiToken),\n    llmModel = OpenAIModels.Chat.GPT4o,\n    toolRegistry = toolRegistry,\n)\nagent.run(\"Get elevation of the Jetbrains Office in Munich, Germany?\")\n</code></pre>"},{"location":"model-context-protocol/#playwright-mcp-integration","title":"Playwright MCP integration","text":"<p>This example demonstrates how to connect to a Playwright server for web automation using MCP:</p> <pre><code>// Start the Playwright MCP server\nval process = ProcessBuilder(\n    \"npx\", \"@playwright/mcp@latest\", \"--port\", \"8931\"\n).start()\n\n// Create the ToolRegistry with tools from the MCP server\nval toolRegistry = McpToolRegistryProvider.fromTransport(\n    transport = McpToolRegistryProvider.defaultSseTransport(\"http://localhost:8931\")\n)\n\n// Create and run the agent\nval agent = AIAgent(\n    promptExecutor = simpleOpenAIExecutor(openAIApiToken),\n    llmModel = OpenAIModels.Chat.GPT4o,\n    toolRegistry = toolRegistry,\n)\nagent.run(\"Open a browser, navigate to jetbrains.com, accept all cookies, click AI in toolbar\")\n</code></pre>"},{"location":"nodes-and-components/","title":"Predefined nodes and components","text":"<p>Nodes are the fundamental building blocks of agent workflows in the Koog framework. Each node represents a specific operation or transformation in the workflow, and they can be connected using edges to define the flow of execution.</p> <p>In general, nodes let you encapsulate complex logic into reusable components that can be easily integrated into different agent workflows. This guide will walk you through the existing nodes that can be used in your agent strategies.</p> <p>Each node is essentially a function that takes an input of a specific type and returns an output of a specific type.</p> <pre><code>graph LR\n    in:::hidden\n    out:::hidden\n\n    subgraph node [\"node\"]\n        execute(Do stuff)\n    end\n\n    in --Input--&gt; execute --Output--&gt; out\n\n    classDef hidden display: none;</code></pre> <p>Here is how you can define a node that expects a string as input and returns the length of the string (an integer) as output:</p> <pre><code>val nodeLength by node&lt;String, Int&gt; { input -&gt;\n    input.length\n}\n</code></pre> <p>For more information, see <code>node()</code>.</p>"},{"location":"nodes-and-components/#utility-nodes","title":"Utility nodes","text":""},{"location":"nodes-and-components/#nodedonothing","title":"nodeDoNothing","text":"<p>A simple pass-through node that does nothing and returns the input as output. For details, see API reference.</p> <pre><code>graph LR\n    in:::hidden\n    out:::hidden\n\n    subgraph node [\"nodeDoNothing\"]\n        execute(Do nothing)\n    end\n\n    in ---|T| execute --T--&gt; out\n\n    classDef hidden display: none;</code></pre> <p>You can use this node for the following purposes:</p> <ul> <li>Create a placeholder node in your graph.</li> <li>Create a connection point without modifying the data.</li> </ul> <p>Here is an example:</p> <pre><code>val passthrough by nodeDoNothing&lt;String&gt;(\"passthrough\")\n\nedge(nodeStart forwardTo passthrough)\nedge(passthrough forwardTo nodeFinish)\n</code></pre>"},{"location":"nodes-and-components/#llm-nodes","title":"LLM nodes","text":""},{"location":"nodes-and-components/#nodeappendprompt","title":"nodeAppendPrompt","text":"<p>A node that adds messages to the LLM prompt using the provided prompt builder. This is useful for modifying the conversation context before making an actual LLM request. For details, see API reference.</p> <pre><code>graph LR\n    in:::hidden\n    out:::hidden\n\n    subgraph node [\"nodeAppendPrompt\"]\n        execute(Append prompt)\n    end\n\n    in ---|T| execute --T--&gt; out\n\n    classDef hidden display: none;</code></pre> <p>You can use this node for the following purposes:</p> <ul> <li>Add system instructions to the prompt.</li> <li>Insert user messages into the conversation.</li> <li>Prepare the context for subsequent LLM requests.</li> </ul> <p>Here is an example:</p> <pre><code>val firstNode by node&lt;Input, Output&gt; {\n    // Transform input to output\n}\n\nval secondNode by node&lt;Output, Output&gt; {\n    // Transform output to output\n}\n\n// Node will get the value of type Output as input from the previous node and path through it to the next node\nval setupContext by nodeAppendPrompt&lt;Output&gt;(\"setupContext\") {\n    system(\"You are a helpful assistant specialized in Kotlin programming.\")\n    user(\"I need help with Kotlin coroutines.\")\n}\n\nedge(firstNode forwardTo setupContext)\nedge(setupContext forwardTo secondNode)\n</code></pre>"},{"location":"nodes-and-components/#nodellmsendmessageonlycallingtools","title":"nodeLLMSendMessageOnlyCallingTools","text":"<p>A node that appends a user message to the LLM prompt and gets a response where the LLM can only call tools. For details, see API reference.</p> <pre><code>graph LR\n    in:::hidden\n    out:::hidden\n\n    subgraph node [\"nodeLLMSendMessageOnlyCallingTools\"]\n        execute(Request LLM expecting only tool calls)\n    end\n\n    in --String--&gt; execute --Message.Response--&gt; out\n\n    classDef hidden display: none;</code></pre>"},{"location":"nodes-and-components/#nodellmsendmessageforceonetool","title":"nodeLLMSendMessageForceOneTool","text":"<p>A node that that appends a user message to the LLM prompt and forces the LLM to use a specific tool. For details, see API reference.</p> <pre><code>graph LR\n    in:::hidden\n    out:::hidden\n\n    subgraph node [\"nodeLLMSendMessageForceOneTool\"]\n        execute(Request LLM expecting a specific tool call)\n    end\n\n    in --String--&gt; execute --Message.Response--&gt; out\n\n    classDef hidden display: none;</code></pre>"},{"location":"nodes-and-components/#nodellmrequest","title":"nodeLLMRequest","text":"<p>A node that appends a user message to the LLM prompt and gets a response with optional tool usage. The node configuration determines whether tool calls are allowed during the processing of the message. For details, see API reference.</p> <pre><code>graph LR\n    in:::hidden\n    out:::hidden\n\n    subgraph node [\"nodeLLMRequest\"]\n        execute(Request LLM)\n    end\n\n    in --String--&gt; execute --Message.Response--&gt; out\n\n    classDef hidden display: none;</code></pre> <p>You can use this node for the following purposes:</p> <ul> <li>Generate LLM response for the current prompt, controlling if the LLM is allowed to generate tool calls.</li> </ul> <p>Here is an example:</p> <pre><code>val requestLLM by nodeLLMRequest(\"requestLLM\", allowToolCalls = true)\nedge(getUserQuestion forwardTo requestLLM)\n</code></pre>"},{"location":"nodes-and-components/#nodellmrequeststructured","title":"nodeLLMRequestStructured","text":"<p>A node that appends a user message to the LLM prompt and requests structured data from the LLM with error correction capabilities. For details, see API reference.</p> <pre><code>graph LR\n    in:::hidden\n    out:::hidden\n\n    subgraph node [\"nodeLLMRequestStructured\"]\n        execute(Request LLM structured)\n    end\n\n    in --String--&gt; execute -- \"Result&amp;lt;StructuredResponse&amp;gt;\" --&gt; out\n\n    classDef hidden display: none;</code></pre>"},{"location":"nodes-and-components/#nodellmrequeststreaming","title":"nodeLLMRequestStreaming","text":"<p>A node that appends a user message to the LLM prompt and streams LLM response with or without stream data transformation. For details, see API reference.</p> <pre><code>graph LR\n    in:::hidden\n    out:::hidden\n\n    subgraph node [\"nodeLLMRequestStreaming\"]\n        execute(Request LLM streaming)\n    end\n\n    in --String--&gt; execute --Flow--&gt; out\n\n    classDef hidden display: none;</code></pre>"},{"location":"nodes-and-components/#nodellmrequestmultiple","title":"nodeLLMRequestMultiple","text":"<p>A node that appends a user message to the LLM prompt and gets multiple LLM responses with tool calls enabled. For details, see API reference.</p> <pre><code>graph LR\n    in:::hidden\n    out:::hidden\n\n    subgraph node [\"nodeLLMRequestMultiple\"]\n        execute(Request LLM expecting multiple responses)\n    end\n\n    in --String--&gt; execute -- \"List&amp;lt;Message.Response&amp;gt;\" --&gt; out\n\n    classDef hidden display: none;</code></pre> <p>You can use this node for the following purposes:</p> <ul> <li>Handle complex queries that require multiple tool calls.</li> <li>Generate multiple tool calls.</li> <li>Implement a workflow that requires multiple parallel actions.</li> </ul> <p>Here is an example:</p> <pre><code>val requestLLMMultipleTools by nodeLLMRequestMultiple()\nedge(getComplexUserQuestion forwardTo requestLLMMultipleTools)\n</code></pre>"},{"location":"nodes-and-components/#nodellmcompresshistory","title":"nodeLLMCompressHistory","text":"<p>A node that compresses the current LLM prompt (message history) into a summary, replacing messages with a concise summary (TL;DR). For details, see API reference. This is useful for managing long conversations by compressing the history to reduce token usage.</p> <pre><code>graph LR\n    in:::hidden\n    out:::hidden\n\n    subgraph node [\"nodeLLMCompressHistory\"]\n        execute(Compress current prompt)\n    end\n\n    in ---|T| execute --T--&gt; out\n\n    classDef hidden display: none;</code></pre> <p>To learn more about history compression, see History compression.</p> <p>You can use this node for the following purposes:</p> <ul> <li>Manage long conversations to reduce token usage.</li> <li>Summarize conversation history to maintain context.</li> <li>Implement memory management in long-running agents.</li> </ul> <p>Here is an example:</p> <pre><code>val compressHistory by nodeLLMCompressHistory&lt;String&gt;(\n    \"compressHistory\",\n    strategy = HistoryCompressionStrategy.FromLastNMessages(10),\n    preserveMemory = true\n)\nedge(generateHugeHistory forwardTo compressHistory)\n</code></pre>"},{"location":"nodes-and-components/#tool-nodes","title":"Tool nodes","text":""},{"location":"nodes-and-components/#nodeexecutetool","title":"nodeExecuteTool","text":"<p>A node that executes a single tool call and returns its result. This node is used to handle tool calls made by the LLM. For details, see API reference.</p> <pre><code>graph LR\n    in:::hidden\n    out:::hidden\n\n    subgraph node [\"nodeExecuteTool\"]\n        execute(Execute tool call)\n    end\n\n    in --Message.Tool.Call--&gt; execute --ReceivedToolResult--&gt; out\n\n    classDef hidden display: none;</code></pre> <p>You can use this node for the following purposes:</p> <ul> <li>Execute tools requested by the LLM.</li> <li>Handle specific actions in response to LLM decisions.</li> <li>Integrate external functionality into the agent workflow.</li> </ul> <p>Here is an example:</p> <pre><code>val requestLLM by nodeLLMRequest()\nval executeTool by nodeExecuteTool()\nedge(requestLLM forwardTo executeTool onToolCall { true })\n</code></pre>"},{"location":"nodes-and-components/#nodellmsendtoolresult","title":"nodeLLMSendToolResult","text":"<p>A node that adds a tool result to the prompt and requests an LLM response. For details, see API reference.</p> <pre><code>graph LR\n    in:::hidden\n    out:::hidden\n\n    subgraph node [\"nodeLLMSendToolResult\"]\n        execute(Request LLM)\n    end\n\n    in --ReceivedToolResult--&gt; execute --Message.Response--&gt; out\n\n    classDef hidden display: none;</code></pre> <p>You can use this node for the following purposes:</p> <ul> <li>Process the results of tool executions.</li> <li>Generate responses based on tool outputs.</li> <li>Continue a conversation after tool execution.</li> </ul> <p>Here is an example:</p> <pre><code>val executeTool by nodeExecuteTool()\nval sendToolResultToLLM by nodeLLMSendToolResult()\nedge(executeTool forwardTo sendToolResultToLLM)\n</code></pre>"},{"location":"nodes-and-components/#nodeexecutemultipletools","title":"nodeExecuteMultipleTools","text":"<p>A node that executes multiple tool calls. These calls can optionally be executed in parallel. For details, see API reference.</p> <pre><code>graph LR\n    in:::hidden\n    out:::hidden\n\n    subgraph node [\"nodeExecuteMultipleTools\"]\n        execute(Execute multiple tool calls)\n    end\n\n    in -- \"List&amp;lt;Message.Tool.Call&amp;gt;\" --&gt; execute -- \"List&amp;lt;ReceivedToolResult&amp;gt;\" --&gt; out\n\n    classDef hidden display: none;</code></pre> <p>You can use this node for the following purposes:</p> <ul> <li>Execute multiple tools in parallel.</li> <li>Handle complex workflows that require multiple tool executions.</li> <li>Optimize performance by batching tool calls.</li> </ul> <p>Here is an example:</p> <pre><code>val requestLLMMultipleTools by nodeLLMRequestMultiple()\nval executeMultipleTools by nodeExecuteMultipleTools()\nedge(requestLLMMultipleTools forwardTo executeMultipleTools onMultipleToolCalls { true })\n</code></pre>"},{"location":"nodes-and-components/#nodellmsendmultipletoolresults","title":"nodeLLMSendMultipleToolResults","text":"<p>A node that adds multiple tool results to the prompt and gets multiple LLM responses. For details, see API reference.</p> <pre><code>graph LR\n    in:::hidden\n    out:::hidden\n\n    subgraph node [\"nodeLLMSendMultipleToolResults\"]\n        execute(Request LLM expecting multiple responses)\n    end\n\n    in -- \"List&amp;lt;ReceivedToolResult&amp;gt;\" --&gt; execute -- \"List&amp;lt;Message.Response&amp;gt;\" --&gt; out\n\n    classDef hidden display: none;</code></pre> <p>You can use this node for the following purposes:</p> <ul> <li>Process the results of multiple tool executions.</li> <li>Generate multiple tool calls.</li> <li>Implement complex workflows with multiple parallel actions.</li> </ul> <p>Here is an example:</p> <pre><code>val executeMultipleTools by nodeExecuteMultipleTools()\nval sendMultipleToolResultsToLLM by nodeLLMSendMultipleToolResults()\nedge(executeMultipleTools forwardTo sendMultipleToolResultsToLLM)\n</code></pre>"},{"location":"nodes-and-components/#node-output-transformation","title":"Node output transformation","text":"<p>The framework provides the <code>transform</code> extension function that allows you to create transformed versions of nodes  that apply transformations to their output. This is useful when you need to convert the output of a node  to a different type or format while preserving the original node's functionality.</p> <pre><code>graph LR\n    in:::hidden\n    out:::hidden\n\n    subgraph nodeWithTransform [transformed node]\n        subgraph node [\"node\"]\n            execute(Do stuff)\n        end\n        transform\n    end\n\n    in --Input--&gt; execute --&gt; transform --Output--&gt; out\n\n    classDef hidden display: none;</code></pre>"},{"location":"nodes-and-components/#transform","title":"transform","text":"<p>The <code>transform()</code> function creates a new <code>AIAgentNodeDelegate</code> that wraps the original node and applies a transformation function to its output.</p> <pre><code>inline fun &lt;reified T&gt; AIAgentNodeDelegate&lt;Input, Output&gt;.transform(\n    noinline transformation: suspend (Output) -&gt; T\n): AIAgentNodeDelegate&lt;Input, T&gt;\n</code></pre>"},{"location":"nodes-and-components/#custom-node-transformation","title":"Custom node transformation","text":"<p>Transform the output of a custom node to a different data type:</p> <pre><code>val textNode by nodeDoNothing&lt;String&gt;(\"textNode\").transform&lt;Int&gt; { text -&gt;\n    text.split(\" \").filter { it.isNotBlank() }.size\n}\n\nedge(nodeStart forwardTo textNode)\nedge(textNode forwardTo nodeFinish)\n</code></pre>"},{"location":"nodes-and-components/#built-in-node-transformation","title":"Built-in node transformation","text":"<p>Transform the output of built-in nodes like <code>nodeLLMRequest</code>:</p> <pre><code>val lengthNode by nodeLLMRequest(\"llmRequest\").transform&lt;Int&gt; { assistantMessage -&gt;\n    assistantMessage.content.length\n}\n\nedge(nodeStart forwardTo lengthNode)\nedge(lengthNode forwardTo nodeFinish)\n</code></pre>"},{"location":"nodes-and-components/#predefined-subgraphs","title":"Predefined subgraphs","text":"<p>The framework provides predefined subgraphs that encapsulate commonly used patterns and workflows. These subgraphs simplify the development of complex agent strategies by handling the creation of base nodes and edges automatically.</p> <p>By using the predefined subgraphs, you can implement various popular pipelines. Here is an example:</p> <ol> <li>Prepare the data.</li> <li>Run the task.</li> <li>Validate the task results. If the results are incorrect, return to step 2 with a feedback message to make adjustments.</li> </ol>"},{"location":"nodes-and-components/#subgraphwithtask","title":"subgraphWithTask","text":"<p>A subgraph that performs a specific task using provided tools and returns a structured result. It supports multi-response LLM interactions (the assistant may produce several responses interleaved with tool calls) and lets you control how tool calls are executed. For details, see API reference.</p> <p>You can use this subgraph for the following purposes:</p> <ul> <li>Create special components that handle specific tasks within a larger workflow.</li> <li>Encapsulate complex logic with clear input and output interfaces.</li> <li>Configure task-specific tools, models, and prompts.</li> <li>Manage conversation history with automatic compression.</li> <li>Develop structured agent workflows and task execution pipelines.</li> <li>Generate structured results from LLM task execution, including flows with multiple assistant responses and tool invocations.</li> </ul> <p>The API allows you to fine\u2011tune execution with optional parameters:</p> <ul> <li>runMode: controls how tool calls are executed during the task (sequential by default). Use this to switch between different tool execution strategies when supported by the underlying model/executor.</li> <li>assistantResponseRepeatMax: limits how many assistant responses are allowed before concluding the task cannot be completed (defaults to a safe internal limit if not provided).</li> </ul> <p>You can provide a task to the subgraph as text, configure the LLM if needed, and provide the necessary tools, and the subgraph will process and solve the task. Here is an example:</p> <pre><code>val processQuery by subgraphWithTask&lt;String, String&gt;(\n    tools = listOf(searchTool, calculatorTool, weatherTool),\n    llmModel = OpenAIModels.Chat.GPT4o,\n    runMode = ToolCalls.SEQUENTIAL,\n    assistantResponseRepeatMax = 3,\n) { userQuery -&gt;\n    \"\"\"\n    You are a helpful assistant that can answer questions about various topics.\n    Please help with the following query:\n    $userQuery\n    \"\"\"\n}\n</code></pre>"},{"location":"nodes-and-components/#subgraphwithverification","title":"subgraphWithVerification","text":"<p>A special version of <code>subgraphWithTask</code> that verifies whether a task was performed correctly and provides details about any issues encountered. This subgraph is useful for workflows that require validation or quality checks. For details, see API reference.</p> <p>You can use this subgraph for the following purposes:</p> <ul> <li>Verify the correctness of task execution.</li> <li>Implement quality control processes in your workflows.</li> <li>Create self-validating components.</li> <li>Generate structured verification results with success/failure status and detailed feedback.</li> </ul> <p>The subgraph ensures that the LLM calls a verification tool at the end of the workflow to check whether the task was successfully completed. It guarantees this verification is performed as the final step and returns a <code>CriticResult</code> that indicates whether a task was completed successfully and provides detailed feedback. Here is an example:</p> <pre><code>val verifyCode by subgraphWithVerification&lt;String&gt;(\n    tools = listOf(runTestsTool, analyzeTool, readFileTool),\n    llmModel = AnthropicModels.Sonnet_3_7,\n    runMode = ToolCalls.SEQUENTIAL,\n    assistantResponseRepeatMax = 3,\n) { codeToVerify -&gt;\n    \"\"\"\n    You are a code reviewer. Please verify that the following code meets all requirements:\n    1. It compiles without errors\n    2. All tests pass\n    3. It follows the project's coding standards\n\n    Code to verify:\n    $codeToVerify\n    \"\"\"\n}\n</code></pre>"},{"location":"nodes-and-components/#predefined-strategies-and-common-strategy-patterns","title":"Predefined strategies and common strategy patterns","text":"<p>The framework provides predefined strategies that combine various nodes. The nodes are connected using edges to define the flow of operations, with conditions that specify when to follow each edge.</p> <p>You can integrate these strategies into your agent workflows if needed.</p>"},{"location":"nodes-and-components/#single-run-strategy","title":"Single run strategy","text":"<p>A single run strategy is designed for non-interactive use cases where the agent processes input once and returns a result.</p> <p>You can use this strategy when you need to run straightforward processes that do not require complex logic.</p> <pre><code>public fun singleRunStrategy(): AIAgentGraphStrategy&lt;String, String&gt; = strategy(\"single_run\") {\n    val nodeCallLLM by nodeLLMRequest(\"sendInput\")\n    val nodeExecuteTool by nodeExecuteTool(\"nodeExecuteTool\")\n    val nodeSendToolResult by nodeLLMSendToolResult(\"nodeSendToolResult\")\n\n    edge(nodeStart forwardTo nodeCallLLM)\n    edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })\n    edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeExecuteTool forwardTo nodeSendToolResult)\n    edge(nodeSendToolResult forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeSendToolResult forwardTo nodeExecuteTool onToolCall { true })\n}\n</code></pre>"},{"location":"nodes-and-components/#tool-based-strategy","title":"Tool-based strategy","text":"<p>A tool-based strategy is designed for workflows that heavily rely on tools to perform specific operations. It typically executes tools based on the LLM decisions and processes the results.</p> <pre><code>fun toolBasedStrategy(name: String, toolRegistry: ToolRegistry): AIAgentGraphStrategy&lt;String, String&gt; {\n    return strategy(name) {\n        val nodeSendInput by nodeLLMRequest()\n        val nodeExecuteTool by nodeExecuteTool()\n        val nodeSendToolResult by nodeLLMSendToolResult()\n\n        // Define the flow of the agent\n        edge(nodeStart forwardTo nodeSendInput)\n\n        // If the LLM responds with a message, finish\n        edge(\n            (nodeSendInput forwardTo nodeFinish)\n                    onAssistantMessage { true }\n        )\n\n        // If the LLM calls a tool, execute it\n        edge(\n            (nodeSendInput forwardTo nodeExecuteTool)\n                    onToolCall { true }\n        )\n\n        // Send the tool result back to the LLM\n        edge(nodeExecuteTool forwardTo nodeSendToolResult)\n\n        // If the LLM calls another tool, execute it\n        edge(\n            (nodeSendToolResult forwardTo nodeExecuteTool)\n                    onToolCall { true }\n        )\n\n        // If the LLM responds with a message, finish\n        edge(\n            (nodeSendToolResult forwardTo nodeFinish)\n                    onAssistantMessage { true }\n        )\n    }\n}\n</code></pre>"},{"location":"nodes-and-components/#streaming-data-strategy","title":"Streaming data strategy","text":"<p>A streaming data strategy is designed for processing streaming data from the LLM. It typically requests streaming data, processes it, and potentially calls tools with the processed data.</p> <pre><code>val agentStrategy = strategy&lt;String, List&lt;Book&gt;&gt;(\"library-assistant\") {\n    // Describe the node containing the output stream parsing\n    val getMdOutput by node&lt;String, List&lt;Book&gt;&gt; { booksDescription -&gt;\n        val books = mutableListOf&lt;Book&gt;()\n        val mdDefinition = markdownBookDefinition()\n\n        llm.writeSession { \n            appendPrompt { user(booksDescription) }\n            // Initiate the response stream in the form of the definition `mdDefinition`\n            val markdownStream = requestLLMStreaming(mdDefinition)\n            // Call the parser with the result of the response stream and perform actions with the result\n            parseMarkdownStreamToBooks(markdownStream).collect { book -&gt;\n                books.add(book)\n                println(\"Parsed Book: ${book.title} by ${book.author}\")\n            }\n        }\n\n        books\n    }\n    // Describe the agent's graph making sure the node is accessible\n    edge(nodeStart forwardTo getMdOutput)\n    edge(getMdOutput forwardTo nodeFinish)\n}\n</code></pre>"},{"location":"opentelemetry-langfuse-exporter/","title":"Langfuse exporter","text":"<p>Koog provides built-in support for exporting agent traces to Langfuse, a platform for observability and analytics of AI applications. With Langfuse integration, you can visualize, analyze, and debug how your Koog agents interact with LLMs, APIs, and other components.</p> <p>For background on Koog's OpenTelemetry support, see the OpenTelemetry support.</p>"},{"location":"opentelemetry-langfuse-exporter/#setup-instructions","title":"Setup instructions","text":"<ol> <li>Create a Langfuse project. Follow the setup guide at Create new project in Langfuse</li> <li>Get API credentials. Retrieve your Langfuse <code>public key</code> and <code>secret key</code> as described in Where are Langfuse API keys?</li> <li>Pass the Langfuse host, private key, and secret key to the Langfuse exporter.  This can be done by providing them as parameters to the <code>addLangfuseExporter()</code> function, or by setting environment variables as shown below:</li> </ol> <pre><code>   export LANGFUSE_HOST=\"https://cloud.langfuse.com\"\n   export LANGFUSE_PUBLIC_KEY=\"&lt;your-public-key&gt;\"\n   export LANGFUSE_SECRET_KEY=\"&lt;your-secret-key&gt;\"\n</code></pre>"},{"location":"opentelemetry-langfuse-exporter/#configuration","title":"Configuration","text":"<p>To enable Langfuse export, install the OpenTelemetry feature and add the <code>LangfuseExporter</code>. The exporter uses <code>OtlpHttpSpanExporter</code> under the hood to send traces to Langfuse\u2019s OpenTelemetry endpoint.</p>"},{"location":"opentelemetry-langfuse-exporter/#example-agent-with-langfuse-tracing","title":"Example: agent with Langfuse tracing","text":"<pre><code>fun main() = runBlocking {\n    val apiKey = \"api-key\"\n\n    val agent = AIAgent(\n        promptExecutor = simpleOpenAIExecutor(apiKey),\n        llmModel = OpenAIModels.Chat.GPT4oMini,\n        systemPrompt = \"You are a code assistant. Provide concise code examples.\"\n    ) {\n        install(OpenTelemetry) {\n            addLangfuseExporter()\n        }\n    }\n\n    println(\"Running agent with Langfuse tracing\")\n\n    val result = agent.run(\"Tell me a joke about programming\")\n\n    println(\"Result: $result\\nSee traces on the Langfuse instance\")\n}\n</code></pre>"},{"location":"opentelemetry-langfuse-exporter/#trace-attributes","title":"Trace attributes","text":"<p>Langfuse uses trace-level attributes to enhance observability with features like sessions, environments, tags and other metadata. The <code>addLangfuseExporter</code> function supports a <code>traceAttributes</code> parameter that accepts a list of <code>CustomAttribute</code> objects.</p> <p>These attributes are added to the root <code>InvokeAgentSpan</code> span of each trace and enable Langfuse's advanced features. You can pass any attributes supported by Langfuse - see the complete list in Langfuse's OpenTelemetry documentation.</p> <p>Common attributes: - Sessions (<code>langfuse.session.id</code>): Group related traces for aggregated metrics, cost analysis, and scoring - Environments: Isolate production traces from development and staging for cleaner analysis - Tags (<code>langfuse.trace.tags</code>): Label traces with feature names, experiment IDs, or customer segments (array of strings)</p>"},{"location":"opentelemetry-langfuse-exporter/#example-with-session-and-tags","title":"Example with session and tags","text":"<pre><code>fun main() = runBlocking {\n    val apiKey = \"api-key\"\n    val sessionId = UUID.randomUUID().toString()\n\n    val agent = AIAgent(\n        promptExecutor = simpleOpenAIExecutor(apiKey),\n        llmModel = OpenAIModels.Chat.GPT4oMini,\n        systemPrompt = \"You are a helpful assistant.\"\n    ) {\n        install(OpenTelemetry) {\n            addLangfuseExporter(\n                traceAttributes = listOf(\n                    CustomAttribute(\"langfuse.session.id\", sessionId),\n                    CustomAttribute(\"langfuse.trace.tags\", listOf(\"chat\", \"kotlin\", \"production\"))\n                )\n            )\n        }\n    }\n\n    // Multiple runs with the same session ID will be grouped in Langfuse\n    agent.run(\"What is Kotlin?\")\n    agent.run(\"Show me a coroutine example\")\n}\n</code></pre>"},{"location":"opentelemetry-langfuse-exporter/#what-gets-traced","title":"What gets traced","text":"<p>When enabled, the Langfuse exporter captures the same spans as Koog\u2019s general OpenTelemetry integration, including:</p> <ul> <li>Agent lifecycle events: agent start, stop, errors</li> <li>LLM interactions: prompts, responses, token usage, latency</li> <li>Tool calls: execution traces for tool invocations</li> <li>System context: metadata such as model name, environment, Koog version</li> </ul> <p>Koog also captures span attributes required by Langfuse to show Agent Graphs.</p> <p>For security reasons, some content of OpenTelemetry spans is masked by default.  To make the content available in Langfuse, use the setVerbose method in the OpenTelemetry configuration and set its <code>verbose</code> argument to <code>true</code> as follows:</p> <pre><code>install(OpenTelemetry) {\n    addLangfuseExporter()\n    setVerbose(true)\n}\n</code></pre> <p>When visualized in Langfuse, the trace appears as follows:  </p> <p>For more details on Langfuse OpenTelemetry tracing, see: Langfuse OpenTelemetry Docs.</p>"},{"location":"opentelemetry-langfuse-exporter/#troubleshooting","title":"Troubleshooting","text":""},{"location":"opentelemetry-langfuse-exporter/#no-traces-appear-in-langfuse","title":"No traces appear in Langfuse","text":"<ul> <li>Double-check that <code>LANGFUSE_HOST</code>, <code>LANGFUSE_PUBLIC_KEY</code>, and <code>LANGFUSE_SECRET_KEY</code> are set in your environment.</li> <li>If running on self-hosted Langfuse, confirm that the <code>LANGFUSE_HOST</code> is reachable from your application environment.</li> <li>Verify that the public/secret key pair belongs to the correct project.</li> </ul>"},{"location":"opentelemetry-support/","title":"OpenTelemetry support","text":"<p>This page provides details about the support for OpenTelemetry with the Koog agentic framework for tracing and  monitoring your AI agents.</p>"},{"location":"opentelemetry-support/#overview","title":"Overview","text":"<p>OpenTelemetry is an observability framework that provides tools for generating, collecting, and exporting telemetry data (traces) from your applications. The Koog OpenTelemetry feature allows you to instrument your AI agents to collect  telemetry data, which can help you:</p> <ul> <li>Monitor agent performance and behavior</li> <li>Debug issues in complex agent workflows</li> <li>Visualize the execution flow of your agents</li> <li>Track LLM calls and tool usage</li> <li>Analyze agent behavior patterns</li> </ul>"},{"location":"opentelemetry-support/#key-opentelemetry-concepts","title":"Key OpenTelemetry concepts","text":"<ul> <li>Spans: spans represent individual units of work or operations within a distributed trace. They indicate the  beginning and end of a specific activity in an application, such as an agent execution, a function call, an LLM call,  or a tool call.</li> <li>Attributes: attributes provide metadata about a telemetry-related item such as a span. Attributes are represented  as key-value pairs.</li> <li>Events: events are specific points in time during the lifetime of a span (span-related events) that represent  something potentially noteworthy that happened.</li> <li>Exporters: exporters are components responsible for sending the collected telemetry data to various backends or  destinations.</li> <li>Collectors: collectors receive, process, and export telemetry data. They act as intermediaries between your  applications and your observability backend.</li> <li>Samplers: samplers determine whether a trace should be recorded based on the sampling strategy. They are used to  manage the volume of telemetry data.</li> <li>Resources: resources represent entities that produce telemetry data. They are identified by resource attributes,  which are key-value pairs that provide information about the resource.</li> </ul> <p>The OpenTelemetry feature in Koog automatically creates spans for various agent events, including:</p> <ul> <li>Agent execution start and end</li> <li>Node execution</li> <li>LLM calls</li> <li>Tool calls</li> </ul>"},{"location":"opentelemetry-support/#installation","title":"Installation","text":"<p>To use OpenTelemetry with Koog, add the OpenTelemetry feature to your agent:</p> <pre><code>val agent = AIAgent(\n    promptExecutor = simpleOpenAIExecutor(apiKey),\n    llmModel = OpenAIModels.Chat.GPT4o,\n    systemPrompt = \"You are a helpful assistant.\",\n    installFeatures = {\n        install(OpenTelemetry) {\n            // Configuration options go here\n        }\n    }\n)\n</code></pre>"},{"location":"opentelemetry-support/#configuration","title":"Configuration","text":""},{"location":"opentelemetry-support/#basic-configuration","title":"Basic configuration","text":"<p>Here is the full list of available properties that you set when configuring the OpenTelemetry feature in an agent:</p> Name Data type Default value Description <code>serviceName</code> <code>String</code> <code>ai.koog</code> The name of the service being instrumented. <code>serviceVersion</code> <code>String</code> Current Koog library version The version of the service being instrumented. <code>isVerbose</code> <code>Boolean</code> <code>false</code> Whether to enable verbose logging for debugging OpenTelemetry configuration. <code>sdk</code> <code>OpenTelemetrySdk</code> The OpenTelemetry SDK instance to use for telemetry collection. <code>tracer</code> <code>Tracer</code> The OpenTelemetry tracer instance used for creating spans. <p>Note</p> <p>The <code>sdk</code> and <code>tracer</code> properties are public properties that you can access, but you can only set them using the public methods listed below.</p> <p>The <code>OpenTelemetryConfig</code> class also includes methods that represent actions related to different configuration items. Here is an example of installing the OpenTelemetry feature with a basic set of configuration items:</p> <pre><code>install(OpenTelemetry) {\n    // Set your service configuration\n    setServiceInfo(\"my-agent-service\", \"1.0.0\")\n\n    // Add the Logging exporter\n    addSpanExporter(LoggingSpanExporter.create())\n}\n</code></pre> <p>For a reference of available methods, see the sections below.</p>"},{"location":"opentelemetry-support/#setserviceinfo","title":"setServiceInfo","text":"<p>Sets the service information including name and version. Takes the following arguments:</p> Name Data type Required Default value Description <code>serviceName</code> String Yes The name of the service being instrumented. <code>serviceVersion</code> String Yes The version of the service being instrumented."},{"location":"opentelemetry-support/#addspanexporter","title":"addSpanExporter","text":"<p>Adds a span exporter to send telemetry data to external systems. Takes the following argument:</p> Name Data type Required Default value Description <code>exporter</code> <code>SpanExporter</code> Yes The <code>SpanExporter</code> instance to be added to the list of custom span exporters."},{"location":"opentelemetry-support/#addspanprocessor","title":"addSpanProcessor","text":"<p>Adds a span processor factory to process spans before they are exported. Takes the following argument:</p> Name Data type Required Default value Description <code>processor</code> <code>(SpanExporter) -&gt; SpanProcessor</code> Yes A function that creates a span processor for a given exporter. Lets you customize processing per exporter."},{"location":"opentelemetry-support/#addresourceattributes","title":"addResourceAttributes","text":"<p>Adds resource attributes to provide additional context about the service. Takes the following argument:</p> Name Data type Required Default value Description <code>attributes</code> <code>Map&lt;AttributeKey&lt;T&gt;, T&gt;</code> Yes The key-value pairs that provide additional details about the service."},{"location":"opentelemetry-support/#setsampler","title":"setSampler","text":"<p>Sets the sampling strategy to control which spans are collected. Takes the following argument:</p> Name Data type Required Default value Description <code>sampler</code> <code>Sampler</code> Yes The sampler instance to set for the OpenTelemetry configuration."},{"location":"opentelemetry-support/#setverbose","title":"setVerbose","text":"<p>Enables or disables verbose logging. Takes the following argument:</p> Name Data type Required Default value Description <code>verbose</code> <code>Boolean</code> Yes <code>false</code> If true, the application collects more detailed telemetry data. <p>Note</p> <p>Some content of OpenTelemetry spans is masked by default for security reasons. For example, LLM messages are masked as <code>HIDDEN:non-empty</code> instead of the actual message content. To get the content, set the value of the <code>verbose</code> argument to <code>true</code>.</p>"},{"location":"opentelemetry-support/#setsdk","title":"setSdk","text":"<p>Injects a pre-configured OpenTelemetrySdk instance.</p> <ul> <li>When you call setSdk(sdk), the provided SDK is used as-is, and any custom configuration applied via addSpanExporter, addSpanProcessor, addResourceAttributes, or setSampler is ignored.</li> <li>The tracer\u2019s instrumentation scope name/version are aligned with your service info.</li> </ul> Name Data type Required Description <code>sdk</code> <code>OpenTelemetrySdk</code> Yes The SDK instance to use in the agent."},{"location":"opentelemetry-support/#advanced-configuration","title":"Advanced configuration","text":"<p>For more advanced configuration, you can also customize the following configuration options:</p> <ul> <li>Sampler: configure the sampling strategy to adjust the frequency and amount of collected data.</li> <li>Resource attributes: add more information about the process that is producing telemetry data. </li> </ul> <pre><code>install(OpenTelemetry) {\n    // Set your service configuration\n    setServiceInfo(\"my-agent-service\", \"1.0.0\")\n\n    // Add the Logging exporter\n    addSpanExporter(LoggingSpanExporter.create())\n\n    // Set the sampler \n    setSampler(Sampler.traceIdRatioBased(0.5)) \n\n    // Add resource attributes\n    addResourceAttributes(mapOf(\n        AttributeKey.stringKey(\"custom.attribute\") to \"custom-value\")\n    )\n}\n</code></pre>"},{"location":"opentelemetry-support/#sampler","title":"Sampler","text":"<p>To define a sampler, use a corresponding method of the <code>Sampler</code> class (<code>io.opentelemetry.sdk.trace.samplers.Sampler</code>)  from the <code>opentelemetry-java</code> SDK that represents the sampling strategy you want to use. </p> <p>The default sampling strategy is as follows:</p> <ul> <li><code>Sampler.alwaysOn()</code>: The default sampling strategy where every span (trace) is sampled.</li> </ul> <p>For more information about available samplers and sampling strategies, see the OpenTelemetry Sampler documentation.</p>"},{"location":"opentelemetry-support/#resource-attributes","title":"Resource attributes","text":"<p>Resource attributes represent additional information about a process producing telemetry data. Koog includes a set of  resource attributes that are set by default:</p> <ul> <li><code>service.name</code></li> <li><code>service.version</code></li> <li><code>service.instance.time</code></li> <li><code>os.type</code></li> <li><code>os.version</code></li> <li><code>os.arch</code></li> </ul> <p>The default value of the <code>service.name</code> attribute is <code>ai.koog</code>, while the default <code>service.version</code> value is the currently used Koog library version.</p> <p>In addition to default resource attributes, you can also add custom attributes. To add a custom attribute to an  OpenTelemetry configuration in Koog, use the <code>addResourceAttributes()</code> method in an OpenTelemetry configuration that  takes a key and a value as its arguments.</p> <pre><code>addResourceAttributes(mapOf(\n    AttributeKey.stringKey(\"custom.attribute\") to \"custom-value\")\n)\n</code></pre>"},{"location":"opentelemetry-support/#span-types-and-attributes","title":"Span types and attributes","text":"<p>The OpenTelemetry feature automatically creates different types of spans to track various operations in your agent:</p> <ul> <li>CreateAgentSpan: created when you run an agent, closed when the agent is closed or the process is terminated.</li> <li>InvokeAgentSpan: the invocation of an agent.</li> <li>StrategySpan: the execution of an agent's strategy (the top-level execution flow).</li> <li>NodeExecuteSpan: the execution of a node in the agent's strategy. This is a custom, Koog-specific span.</li> <li>SubgraphExecuteSpan: the execution of a subgraph within the agent strategy. This is a custom, Koog-specific span.</li> <li>InferenceSpan: an LLM call.</li> <li>ExecuteToolSpan: a tool call.</li> </ul> <p>Spans are organized in a nested, hierarchical structure. Here is an example of a span structure:</p> <pre><code>CreateAgentSpan\n    InvokeAgentSpan\n        StrategySpan\n            NodeExecuteSpan\n                InferenceSpan\n            NodeExecuteSpan\n                ExecuteToolSpan\n            SubgraphExecuteSpan\n                NodeExecuteSpan\n                    InferenceSpan\n</code></pre>"},{"location":"opentelemetry-support/#span-attributes","title":"Span attributes","text":"<p>Span attributes provide metadata related to a span. Each span has its set of attributes, while some spans can also  repeat attributes.</p> <p>Koog supports a list of predefined attributes that follow OpenTelemetry's Semantic conventions for generative AI events. For example, the conventions define an attribute named  <code>gen_ai.conversation.id</code>, which is usually a required attribute for a span. In Koog, the value of this attribute is the  unique identifier for an agent run, that is automatically set when you call the <code>agent.run()</code> method.</p> <p>In addition, Koog also includes custom, Koog-specific attributes. You can recognize most of these attributes by the <code>koog.</code> prefix. Here are the available custom attributes:</p> <ul> <li><code>koog.strategy.name</code>: the name of the agent strategy. A strategy is a Koog-related entity that describes the purpose of the agent. Used in the <code>StrategySpan</code> span.</li> <li><code>koog.node.id</code>: the identifier (name) of the node being executed. Used in the <code>NodeExecuteSpan</code> span.</li> <li><code>koog.node.input</code>: the input passed to the node at the beginning of execution. Present on <code>NodeExecuteSpan</code> when node starts.</li> <li><code>koog.node.output</code>: the output produced by the node upon completion. Present on <code>NodeExecuteSpan</code> when node completes successfully.</li> <li><code>koog.subgraph.id</code>: the identifier (name) of the subgraph being executed. Used in the <code>SubgraphExecuteSpan</code> span.</li> <li><code>koog.subgraph.input</code>: the input passed to the subgraph at the beginning of execution. Present on <code>SubgraphExecuteSpan</code> when subgraph starts.</li> <li><code>koog.subgraph.output</code>: the output produced by the subgraph upon completion. Present on <code>SubgraphExecuteSpan</code> when subgraph completes successfully.</li> </ul>"},{"location":"opentelemetry-support/#events","title":"Events","text":"<p>A span can also have an event attached to the span. Events describe a specific point in time when something relevant  happened. For example, when an LLM call started or finished. Events also have attributes and additionally include event  body fields.</p> <p>The following event types are supported in line with OpenTelemetry's Semantic conventions for generative AI events:</p> <ul> <li>SystemMessageEvent: the system instructions passed to the model.</li> <li>UserMessageEvent: the user message passed to the model.</li> <li>AssistantMessageEvent: the assistant message passed to the model.</li> <li>ToolMessageEvent: the response from a tool or function call passed to the model.</li> <li>ChoiceEvent: the response message from a model.</li> <li>ModerationResponseEvent: the model moderation result or signal.</li> </ul> <p>Note</p> <p>The <code>optentelemetry-java</code> SDK does not support the event body fields parameter when adding an event. Therefore, in  the OpenTelemetry support in Koog, event body fields are a separate attribute whose key is <code>body</code> and value type is  string. The string includes the content or payload for the event body field, which is usually a JSON-like object. For  examples of event body fields, see the OpenTelemetry documentation. For the state of support for event body  fields in <code>opentelemetry-java</code>, see the related GitHub issue.</p>"},{"location":"opentelemetry-support/#exporters","title":"Exporters","text":"<p>Exporters send collected telemetry data to an OpenTelemetry Collector or other types of destinations or backend  implementations. To add an exporter, use the <code>addSpanExporter()</code> method when installing the OpenTelemetry feature. The  method takes the following argument:</p> Name Data type Required Default Description <code>exporter</code> SpanExporter Yes The SpanExporter instance to be added to the list of custom span exporters. <p>The sections below provide information about some of the most commonly used exporters from the <code>opentelemetry-java</code> SDK.</p> <p>Note</p> <p>If you do not configure any custom exporters, Koog will use a console LoggingSpanExporter by default. This helps during local development and debugging.</p>"},{"location":"opentelemetry-support/#logging-exporter","title":"Logging exporter","text":"<p>A logging exporter that outputs trace information to the console. <code>LoggingSpanExporter</code>  (<code>io.opentelemetry.exporter.logging.LoggingSpanExporter</code>) is a part of the <code>opentelemetry-java</code> SDK.</p> <p>This type of export is useful for development and debugging purposes.</p> <pre><code>install(OpenTelemetry) {\n    // Add the logging exporter\n    addSpanExporter(LoggingSpanExporter.create())\n    // Add more exporters as needed\n}\n</code></pre>"},{"location":"opentelemetry-support/#opentelemetry-http-exporter","title":"OpenTelemetry HTTP exporter","text":"<p>OpenTelemetry HTTP exporter (<code>OtlpHttpSpanExporter</code>) is a part of the <code>opentelemetry-java</code> SDK  (<code>io.opentelemetry.exporter.otlp.http.trace.OtlpHttpSpanExporter</code>) and sends span data to a backend through HTTP.</p> <pre><code>install(OpenTelemetry) {\n   // Add OpenTelemetry HTTP exporter \n   addSpanExporter(\n      OtlpHttpSpanExporter.builder()\n         // Set the maximum time to wait for the collector to process an exported batch of spans \n         .setTimeout(30, TimeUnit.SECONDS)\n         // Set the OpenTelemetry endpoint to connect to\n         .setEndpoint(\"http://localhost:3000/api/public/otel/v1/traces\")\n         // Add the authorization header\n         .addHeader(\"Authorization\", \"Basic $AUTH_STRING\")\n         .build()\n   )\n}\n</code></pre>"},{"location":"opentelemetry-support/#opentelemetry-grpc-exporter","title":"OpenTelemetry gRPC exporter","text":"<p>OpenTelemetry gRPC exporter (<code>OtlpGrpcSpanExporter</code>) is a part of the <code>opentelemetry-java</code> SDK  (<code>io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter</code>). It exports telemetry data to a backend through gRPC and  lets you define the host and port of the backend, collector, or endpoint that receives the data. The default port is  <code>4317</code>.</p> <pre><code>install(OpenTelemetry) {\n   // Add OpenTelemetry gRPC exporter \n   addSpanExporter(\n      OtlpGrpcSpanExporter.builder()\n          // Set the host and the port\n         .setEndpoint(\"http://localhost:4317\")\n         .build()\n   )\n}\n</code></pre>"},{"location":"opentelemetry-support/#integration-with-langfuse","title":"Integration with Langfuse","text":"<p>Langfuse provides trace visualization and analytics for LLM/agent workloads.</p> <p>You can configure Koog to export OpenTelemetry traces directly to Langfuse using a helper function:</p> <pre><code>install(OpenTelemetry) {\n    addLangfuseExporter(\n        langfuseUrl = \"https://cloud.langfuse.com\",\n        langfusePublicKey = \"...\",\n        langfuseSecretKey = \"...\"\n    )\n}\n</code></pre> <p>Please read the full documentation about integration with Langfuse.</p>"},{"location":"opentelemetry-support/#integration-with-wb-weave","title":"Integration with W&amp;B Weave","text":"<p>W&amp;B Weave provides trace visualization and analytics for LLM/agent workloads. Integration with W&amp;B Weave can be configured via a predefined exporter:</p> <pre><code>install(OpenTelemetry) {\n    addWeaveExporter(\n        weaveOtelBaseUrl = \"https://trace.wandb.ai\",\n        weaveEntity = \"my-team\",\n        weaveProjectName = \"my-project\",\n        weaveApiKey = \"...\"\n    )\n}\n</code></pre> <p>Please read the full documentation about integration with W&amp;B Weave.</p>"},{"location":"opentelemetry-support/#integration-with-jaeger","title":"Integration with Jaeger","text":"<p>Jaeger is a popular distributed tracing system that works with OpenTelemetry. The <code>opentelemetry</code> directory within  <code>examples</code> in the Koog repository includes an example of using OpenTelemetry with Jaeger and Koog agents.</p>"},{"location":"opentelemetry-support/#prerequisites","title":"Prerequisites","text":"<p>To test OpenTelemetry with Koog and Jaeger, start the Jaeger OpenTelemetry all-in-one process using the provided  <code>docker-compose.yaml</code> file, by running the following command:</p> <pre><code>docker compose up -d\n</code></pre> <p>The provided Docker Compose YAML file includes the following content:</p> <pre><code># docker-compose.yaml\nservices:\n  jaeger-all-in-one:\n    image: jaegertracing/all-in-one:1.39\n    container_name: jaeger-all-in-one\n    environment:\n      - COLLECTOR_OTLP_ENABLED=true\n    ports:\n      - \"4317:4317\"\n      - \"16686:16686\"\n</code></pre> <p>To access the Jaeger UI and view your traces, open <code>http://localhost:16686</code>.</p>"},{"location":"opentelemetry-support/#example","title":"Example","text":"<p>To export telemetry data for use in Jaeger, the example uses <code>LoggingSpanExporter</code>  (<code>io.opentelemetry.exporter.logging.LoggingSpanExporter</code>) and <code>OtlpGrpcSpanExporter</code>  (<code>io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter</code>) from the <code>opentelemetry-java</code> SDK.</p> <p>Here is the full code sample:</p> <pre><code>fun main() {\n    runBlocking {\n        val agent = AIAgent(\n            promptExecutor = simpleOpenAIExecutor(openAIApiKey),\n            llmModel = OpenAIModels.Chat.O4Mini,\n            systemPrompt = \"You are a code assistant. Provide concise code examples.\"\n        ) {\n            install(OpenTelemetry) {\n                // Add a console logger for local debugging\n                addSpanExporter(LoggingSpanExporter.create())\n\n                // Send traces to OpenTelemetry collector\n                addSpanExporter(\n                    OtlpGrpcSpanExporter.builder()\n                        .setEndpoint(\"http://localhost:4317\")\n                        .build()\n                )\n            }\n        }\n\n        agent.use { agent -&gt;\n            println(\"Running the agent with OpenTelemetry tracing...\")\n\n            val result = agent.run(\"Tell me a joke about programming\")\n\n            println(\"Agent run completed with result: '$result'.\" +\n                    \"\\nCheck Jaeger UI at http://localhost:16686 to view traces\")\n        }\n    }\n}\n</code></pre>"},{"location":"opentelemetry-support/#troubleshooting","title":"Troubleshooting","text":""},{"location":"opentelemetry-support/#common-issues","title":"Common issues","text":"<ol> <li> <p>No traces appearing in Jaeger, Langfuse, or W&amp;B Weave</p> <ul> <li>Ensure the service is running and the OpenTelemetry port (4317) is accessible.</li> <li>Check that the OpenTelemetry exporter is configured with the correct endpoint.</li> <li>Make sure to wait a few seconds after agent execution for traces to be exported.</li> </ul> </li> <li> <p>Missing spans or incomplete traces</p> <ul> <li>Verify that the agent execution completes successfully.</li> <li>Ensure that you're not closing the application too quickly after agent execution.</li> <li>Add a delay after agent execution to allow time for spans to be exported.</li> </ul> </li> <li> <p>Excessive number of spans</p> <ul> <li>Consider using a different sampling strategy by configuring the <code>sampler</code> property.</li> <li>For example, use <code>Sampler.traceIdRatioBased(0.1)</code> to sample only 10% of traces.</li> </ul> </li> <li> <p>Span adapters override each other</p> <ul> <li>Currently, the OpenTelemetry agent feature does not support applying multiple span adapters KG-265.</li> </ul> </li> </ol>"},{"location":"opentelemetry-weave-exporter/","title":"W&amp;B Weave exporter","text":"<p>Koog provides built-in support for exporting agent traces to W&amp;B Weave, a developer tool from Weights &amp; Biases for observability and analytics of AI applications. With the Weave integration, you can capture prompts, completions, system context, and execution traces  and visualize them directly in your W&amp;B workspace.</p> <p>For background on Koog\u2019s OpenTelemetry support, see the OpenTelemetry support.</p>"},{"location":"opentelemetry-weave-exporter/#setup-instructions","title":"Setup instructions","text":"<ol> <li>Get up a W&amp;B account at https://wandb.ai</li> <li>Get your API key from https://wandb.ai/authorize.</li> <li>Find your entity name by visiting your W&amp;B dashboard at https://wandb.ai/home.  Your entity is usually your username if it's a personal account or your team/org name.</li> <li>Define a name for your project. You don't have to create a project beforehand, it will be created automatically when the first trace is sent.</li> <li>Pass the Weave entity, project name, and API key to the Weave exporter.    This can be done by providing them as parameters to the <code>addWeaveExporter()</code> function,    or by setting environment variables as shown below:</li> </ol> <pre><code>export WEAVE_API_KEY=\"&lt;your-api-key&gt;\"\nexport WEAVE_ENTITY=\"&lt;your-entity&gt;\"\nexport WEAVE_PROJECT_NAME=\"koog-tracing\"\n</code></pre>"},{"location":"opentelemetry-weave-exporter/#configuration","title":"Configuration","text":"<p>To enable Weave export, install the OpenTelemetry feature and add the <code>WeaveExporter</code>. The exporter uses Weave\u2019s OpenTelemetry endpoint via <code>OtlpHttpSpanExporter</code>.</p>"},{"location":"opentelemetry-weave-exporter/#example-agent-with-weave-tracing","title":"Example: agent with Weave tracing","text":"<pre><code>fun main() = runBlocking {\n    val apiKey = \"api-key\"\n    val entity = System.getenv()[\"WEAVE_ENTITY\"] ?: throw IllegalArgumentException(\"WEAVE_ENTITY is not set\")\n    val projectName = System.getenv()[\"WEAVE_PROJECT_NAME\"] ?: \"koog-tracing\"\n\n    val agent = AIAgent(\n        promptExecutor = simpleOpenAIExecutor(apiKey),\n        llmModel = OpenAIModels.Chat.GPT4oMini,\n        systemPrompt = \"You are a code assistant. Provide concise code examples.\"\n    ) {\n        install(OpenTelemetry) {\n            addWeaveExporter()\n        }\n    }\n\n    println(\"Running agent with Weave tracing\")\n\n    val result = agent.run(\"Tell me a joke about programming\")\n\n    println(\"Result: $result\\nSee traces on https://wandb.ai/$entity/$projectName/weave/traces\")\n}\n</code></pre>"},{"location":"opentelemetry-weave-exporter/#what-gets-traced","title":"What gets traced","text":"<p>When enabled, the Weave exporter captures the same spans as Koog\u2019s general OpenTelemetry integration, including:</p> <ul> <li>Agent lifecycle events: agent start, stop, errors</li> <li>LLM interactions: prompts, completions, latency</li> <li>Tool calls: execution traces for tool invocations</li> <li>System context: metadata such as model name, environment, Koog version</li> </ul> <p>For security reasons, some content of OpenTelemetry spans is masked by default. To make the content available in Weave, use the setVerbose method in the OpenTelemetry configuration and set its <code>verbose</code> argument to <code>true</code> as follows:</p> <pre><code>install(OpenTelemetry) {\n    addWeaveExporter()\n    setVerbose(true)\n}\n</code></pre> <p>When visualized in W&amp;B Weave, the trace appears as follows:  </p> <p>For more details, see the official Weave OpenTelemetry Docs.</p>"},{"location":"opentelemetry-weave-exporter/#troubleshooting","title":"Troubleshooting","text":""},{"location":"opentelemetry-weave-exporter/#no-traces-appear-in-weave","title":"No traces appear in Weave","text":"<ul> <li>Confirm that <code>WEAVE_API_KEY</code>, <code>WEAVE_ENTITY</code>, and <code>WEAVE_PROJECT_NAME</code> are set in your environment.</li> <li>Ensure that your W&amp;B account has access to the specified entity and project.</li> </ul>"},{"location":"opentelemetry-weave-exporter/#authentication-errors","title":"Authentication errors","text":"<ul> <li>Check that your <code>WEAVE_API_KEY</code> is valid.</li> <li>API key must have permission to write traces for the selected entity.</li> </ul>"},{"location":"opentelemetry-weave-exporter/#connection-issues","title":"Connection issues","text":"<ul> <li>Make sure your environment has network access to W&amp;B\u2019s OpenTelemetry ingestion endpoints.</li> </ul>"},{"location":"parallel-node-execution/","title":"Parallel node execution","text":""},{"location":"parallel-node-execution/#overview","title":"Overview","text":"<p>Parallel node execution lets you run multiple AI agent nodes concurrently, improving performance and enabling complex workflows. This feature is particularly useful when you need to:</p> <ul> <li>Process the same input through different models or approaches simultaneously</li> <li>Perform multiple independent operations in parallel</li> <li>Implement competitive evaluation patterns where multiple solutions are generated and then compared</li> </ul>"},{"location":"parallel-node-execution/#key-components","title":"Key components","text":"<p>Parallel node execution in Koog consists of the methods and data structures described below.</p>"},{"location":"parallel-node-execution/#methods","title":"Methods","text":"<ul> <li><code>parallel()</code>: executes multiple nodes in parallel and collects their results.</li> </ul>"},{"location":"parallel-node-execution/#data-structures","title":"Data structures","text":"<ul> <li><code>ParallelResult</code>: represents the completed result of a parallel node execution.</li> <li><code>NodeExecutionResult</code>: contains the output and context of a node execution.</li> </ul>"},{"location":"parallel-node-execution/#basic-usage","title":"Basic usage","text":""},{"location":"parallel-node-execution/#running-nodes-in-parallel","title":"Running nodes in parallel","text":"<p>To initiate parallel execution of nodes, use the <code>parallel</code> method in the following format:</p> <pre><code>val nodeName by parallel&lt;Input, Output&gt;(\n   firstNode, secondNode, thirdNode /* Add more nodes if needed */\n) {\n   // Merge strategy goes here, for example: \n   selectByMax { it.length }\n}\n</code></pre> <p>Here is an actual example of running three nodes in parallel and selecting the result with the maximum length:</p> <pre><code>val calc by parallel&lt;String, Int&gt;(\n   nodeCalcTokens, nodeCalcSymbols, nodeCalcWords,\n) {\n   selectByMax { it }\n}\n</code></pre> <p>The code above runs the <code>nodeCalcTokens</code>, <code>nodeCalcSymbols</code>, and <code>nodeCalcWords</code> nodes in parallel and returns the result with the maximum value.</p>"},{"location":"parallel-node-execution/#merge-strategies","title":"Merge strategies","text":"<p>After executing nodes in parallel, you need to specify how to merge the results. Koog provides the following merge strategies:</p> <ul> <li><code>selectBy()</code>: selects a result based on a predicate function.</li> <li><code>selectByMax()</code>: selects the result with the maximum value based on a comparison function.</li> <li><code>selectByIndex()</code>: selects a result based on an index returned by a selection function.</li> <li><code>fold()</code>: folds the results into a single value using an operation function.</li> </ul>"},{"location":"parallel-node-execution/#selectby","title":"selectBy","text":"<p>Selects a result based on a predicate function:</p> <pre><code>val nodeSelectJoke by parallel&lt;String, String&gt;(\n   nodeOpenAI, nodeAnthropicSonnet, nodeAnthropicOpus,\n) {\n   selectBy { it.contains(\"programmer\") }\n}\n</code></pre> <p>This selects the first joke that contains the word \"programmer\".</p>"},{"location":"parallel-node-execution/#selectbymax","title":"selectByMax","text":"<p>Selects the result with the maximum value based on a comparison function:</p> <pre><code>val nodeLongestJoke by parallel&lt;String, String&gt;(\n   nodeOpenAI, nodeAnthropicSonnet, nodeAnthropicOpus,\n) {\n   selectByMax { it.length }\n}\n</code></pre> <p>This selects the joke with the maximum length.</p>"},{"location":"parallel-node-execution/#selectbyindex","title":"selectByIndex","text":"<p>Selects a result based on an index returned by a selection function:</p> <pre><code>val nodeBestJoke by parallel&lt;String, String&gt;(\n   nodeOpenAI, nodeAnthropicSonnet, nodeAnthropicOpus,\n) {\n   selectByIndex { jokes -&gt;\n      // Use another LLM to determine the best joke\n      llm.writeSession {\n         model = OpenAIModels.Chat.GPT4o\n         appendPrompt {\n            system(\"You are a comedy critic. Select the best joke.\")\n            user(\"Here are three jokes: ${jokes.joinToString(\"\\n\\n\")}\")\n         }\n         val response = requestLLMStructured&lt;JokeRating&gt;()\n         response.getOrNull()!!.data.bestJokeIndex\n      }\n   }\n}\n</code></pre> <p>This uses another LLM call to determine the index of the best joke.</p>"},{"location":"parallel-node-execution/#fold","title":"fold","text":"<p>Folds the results into a single value using an operation function:</p> <pre><code>val nodeAllJokes by parallel&lt;String, String&gt;(\n   nodeOpenAI, nodeAnthropicSonnet, nodeAnthropicOpus,\n) {\n   fold(\"Jokes:\\n\") { result, joke -&gt; \"$result\\n$joke\" }\n}\n</code></pre> <p>This combines all jokes into a single string.</p>"},{"location":"parallel-node-execution/#example-best-joke-agent","title":"Example: Best joke agent","text":"<p>Here is a complete example that uses parallel execution to generate jokes from different LLM models and select the best one:</p> <pre><code>val strategy = strategy(\"best-joke\") {\n   // Define nodes for different LLM models\n   val nodeOpenAI by node&lt;String, String&gt; { topic -&gt;\n      llm.writeSession {\n         model = OpenAIModels.Chat.GPT4o\n         appendPrompt {\n            system(\"You are a comedian. Generate a funny joke about the given topic.\")\n            user(\"Tell me a joke about $topic.\")\n         }\n         val response = requestLLMWithoutTools()\n         response.content\n      }\n   }\n\n   val nodeAnthropicSonnet by node&lt;String, String&gt; { topic -&gt;\n      llm.writeSession {\n         model = AnthropicModels.Sonnet_3_5\n         appendPrompt {\n            system(\"You are a comedian. Generate a funny joke about the given topic.\")\n            user(\"Tell me a joke about $topic.\")\n         }\n         val response = requestLLMWithoutTools()\n         response.content\n      }\n   }\n\n   val nodeAnthropicOpus by node&lt;String, String&gt; { topic -&gt;\n      llm.writeSession {\n         model = AnthropicModels.Opus_3\n         appendPrompt {\n            system(\"You are a comedian. Generate a funny joke about the given topic.\")\n            user(\"Tell me a joke about $topic.\")\n         }\n         val response = requestLLMWithoutTools()\n         response.content\n      }\n   }\n\n   // Execute joke generation in parallel and select the best joke\n   val nodeGenerateBestJoke by parallel(\n      nodeOpenAI, nodeAnthropicSonnet, nodeAnthropicOpus,\n   ) {\n      selectByIndex { jokes -&gt;\n         // Another LLM (e.g., GPT4o) would find the funniest joke:\n         llm.writeSession {\n            model = OpenAIModels.Chat.GPT4o\n            appendPrompt {\n               prompt(\"best-joke-selector\") {\n                  system(\"You are a comedy critic. Give a critique for the given joke.\")\n                  user(\n                     \"\"\"\n                            Here are three jokes about the same topic:\n\n                            ${jokes.mapIndexed { index, joke -&gt; \"Joke $index:\\n$joke\" }.joinToString(\"\\n\\n\")}\n\n                            Select the best joke and explain why it's the best.\n                            \"\"\".trimIndent()\n                  )\n               }\n            }\n\n            val response = requestLLMStructured&lt;JokeRating&gt;()\n            val bestJoke = response.getOrNull()!!.data\n            bestJoke.bestJokeIndex\n         }\n      }\n   }\n\n   // Connect the nodes\n   nodeStart then nodeGenerateBestJoke then nodeFinish\n}\n</code></pre>"},{"location":"parallel-node-execution/#best-practices","title":"Best practices","text":"<ol> <li> <p>Consider resource constraints: Be mindful of resource usage when executing nodes in parallel, especially when making multiple LLM API calls simultaneously.</p> </li> <li> <p>Context management: Each parallel execution creates a forked context. When merging results, choose which context to preserve or how to combine contexts from different executions.</p> </li> <li> <p>Optimize for your use case:</p> <ul> <li>For competitive evaluation (like the joke example), use <code>selectByIndex</code> to select the best result</li> <li>For finding the maximum value, use <code>selectByMax</code></li> <li>For filtering based on a condition, use <code>selectBy</code></li> <li>For aggregation, use <code>fold</code> to combine all results into a composite output</li> </ul> </li> </ol>"},{"location":"parallel-node-execution/#performance-considerations","title":"Performance considerations","text":"<p>Parallel execution can significantly improve throughput, but it comes with some overhead:</p> <ul> <li>Each parallel node creates a new coroutine</li> <li>Context forking and merging add some computational cost</li> <li>Resource contention may occur with many parallel executions</li> </ul> <p>For optimal performance, parallelize operations that:</p> <ul> <li>Are independent of each other</li> <li>Have significant execution time</li> <li>Don't share mutable state</li> </ul>"},{"location":"planner-agents/","title":"Planner agents","text":"<p>Planner agents are AI agents that can plan and execute multistep tasks through iterative planning cycles.  They continuously build or update plans, execute steps, and check completion criteria against the current state.</p> <p>Planner agents are suitable for complex tasks that require breaking down a high-level goal into smaller, actionable steps and adapting the plan based on the results of each step.</p> <p>Planner agents operate through an iterative planning cycle:</p> <ol> <li>The planner creates or updates a plan based on the current state.</li> <li>The planner executes a single step from the plan, updating the state.</li> <li>The planner determines whether the plan is completed according to the current state.<ul> <li>If the plan is completed, the cycle ends.</li> <li>If the plan is not completed, the cycle repeats from the first step.</li> </ul> </li> </ol> <pre><code>graph LR\n  A[Create or update plan] --&gt; B[\"Execute step and update state\"]\n  B --&gt; C[\"Check completion\"]\n  C --&gt;|Completed| D[[Done]]\n  C --&gt;|\"Not completed\"| A</code></pre>"},{"location":"planner-agents/#prerequisites","title":"Prerequisites","text":"<p>Before you start, make sure that you have the following:</p> <ul> <li>A working Kotlin/JVM project.</li> <li>Java 17+ installed.</li> <li>A valid API key from the LLM provider that you use to implement an AI agent. For a list of all available providers,  see LLM providers.</li> </ul> <p>Tip</p> <p>Use environment variables or a secure configuration management system to store your API keys. Avoid hardcoding API keys directly in your source code.</p>"},{"location":"planner-agents/#add-dependencies","title":"Add dependencies","text":"<p>To use planner agents, include the following dependencies in your build configuration:</p> <pre><code>dependencies {\n    implementation(\"ai.koog:koog-agents:VERSION\")\n}\n</code></pre> <p>For all available installation methods, see Install Koog.</p>"},{"location":"planner-agents/#simple-llm-based-planners","title":"Simple LLM-based planners","text":"<p>Simple LLM-based planners use LLMs to generate and evaluate plans.  They operate on a string-based state and execute steps through LLM requests. String-based state means that the agent state is noted as a single string, where the agent accepts an initial state string and returns the final state string as the result.</p> <p>Koog provides two simple planners: </p> <ul> <li>SimpleLLMPlanner     generates a plan only once at the very beginning and then follows the plan until it is completed.      To include replanning, extend <code>SimpleLLMPlanner</code> and override the <code>assessPlan</code> method,     indicating when the agent should replan.</li> <li>SimpleLLMWithCriticPlanner     implements the <code>assessPlan</code> method that uses an LLM.     The method checks the validity of the plan via an LLM request and assesses whether the agent should replan.</li> </ul> <p>The following example shows how to create a simple planner agent using <code>SimpleLLMPlanner</code>:</p> <pre><code>// Create the planner\nval planner = SimpleLLMPlanner()\n\n// Wrap it in a planner strategy\nval strategy = AIAgentPlannerStrategy(\n    name = \"simple-planner\",\n    planner = planner\n)\n\n// Configure the agent\nval agentConfig = AIAgentConfig(\n    prompt = prompt(\"planner\") {\n        system(\"You are a helpful planning assistant.\")\n    },\n    model = OpenAIModels.Chat.GPT4o,\n    maxAgentIterations = 50\n)\n\n// Create the planner agent\nval agent = PlannerAIAgent(\n    promptExecutor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    strategy = strategy,\n    agentConfig = agentConfig\n)\n\nsuspend fun main() {\n    // Run the agent with a task\n    val result = agent.run(\"Create a plan to organize a team meeting\")\n    println(result)\n}\n</code></pre>"},{"location":"planner-agents/#goap-goal-oriented-action-planning","title":"GOAP (Goal-Oriented Action Planning)","text":"<p>GOAP is an algorithmic planning approach that uses A* search to find optimal action sequences. Instead of using an LLM to generate plans, a GOAP agent automatically discovers action sequences based on predefined goals and actions. In Koog, GOAP is implemented through a DSL that lets you define goals and actions declaratively.</p> <p>GOAP planners work with three main concepts:</p> <ul> <li>State: Represents the current state of the world.</li> <li>Actions: Define what can be done, including preconditions, effects (beliefs), costs, and execution logic.</li> <li>Goals: Define target conditions, heuristic costs, and value functions.</li> </ul> <p>A GOAP planner uses A* search to find the sequence of actions that satisfies the goal condition while minimizing total cost.</p> <p>To create a GOAP agent, you need to:</p> <ol> <li>Define the state as a data class with properties representing various aspects specific to your goal.</li> <li>Create a GOAPPlanner instance using the goap() function.<ol> <li>Define actions with preconditions and beliefs using the action() function.</li> <li>Define goals with completion conditions using the goal() function.</li> </ol> </li> <li>Wrap the planner with AIAgentPlannerStrategy and pass it to the PlannerAIAgent constructor.</li> </ol> <p>Note</p> <p>The planner selects individual actions and their sequence. Each action includes a precondition that must hold true for the action to be executed and a belief that defines the predicted outcome. For more information about beliefs, see State beliefs compared to actual execution.</p> <p>In the following example, GOAP handles the high-level planning for creating an article (outline \u2192 draft \u2192 review \u2192 publish), while the LLM performs the actual content generation within each action.</p> <pre><code>// Define a state for content creation\ndata class ContentState(\n    val topic: String,\n    val hasOutline: Boolean = false,\n    val outline: String = \"\",\n    val hasDraft: Boolean = false,\n    val draft: String = \"\",\n    val hasReview: Boolean = false,\n    val isPublished: Boolean = false\n)\n\n// Create GOAP planner with LLM-powered actions\nval planner = goap&lt;ContentState&gt;(typeOf&lt;ContentState&gt;()) {\n    // Define actions with preconditions and beliefs\n    action(\n        name = \"Create outline\",\n        precondition = { state -&gt; !state.hasOutline },\n        belief = { state -&gt; state.copy(hasOutline = true, outline = \"Outline\") },\n        cost = { 1.0 }\n    ) { ctx, state -&gt;\n        // Use LLM to create the outline\n        val response = ctx.llm.writeSession {\n            appendPrompt {\n                user(\"Create a detailed outline for an article about: ${state.topic}\")\n            }\n            requestLLM()\n        }\n        state.copy(hasOutline = true, outline = response.content)\n    }\n\n    action(\n        name = \"Write draft\",\n        precondition = { state -&gt; state.hasOutline &amp;&amp; !state.hasDraft },\n        belief = { state -&gt; state.copy(hasDraft = true, draft = \"Draft\") },\n        cost = { 2.0 }\n    ) { ctx, state -&gt;\n        // Use LLM to write the draft\n        val response = ctx.llm.writeSession {\n            appendPrompt {\n                user(\"Write an article based on this outline:\\n${state.outline}\")\n            }\n            requestLLM()\n        }\n        state.copy(hasDraft = true, draft = response.content)\n    }\n\n    action(\n        name = \"Review content\",\n        precondition = { state -&gt; state.hasDraft &amp;&amp; !state.hasReview },\n        belief = { state -&gt; state.copy(hasReview = true) },\n        cost = { 1.0 }\n    ) { ctx, state -&gt;\n        // Use LLM to review the draft\n        val response = ctx.llm.writeSession {\n            appendPrompt {\n                user(\"Review this article and suggest improvements:\\n${state.draft}\")\n            }\n            requestLLM()\n        }\n        println(\"Review feedback: ${response.content}\")\n        state.copy(hasReview = true)\n    }\n\n    action(\n        name = \"Publish\",\n        precondition = { state -&gt; state.hasReview &amp;&amp; !state.isPublished },\n        belief = { state -&gt; state.copy(isPublished = true) },\n        cost = { 1.0 }\n    ) { ctx, state -&gt;\n        println(\"Publishing article...\")\n        state.copy(isPublished = true)\n    }\n\n    // Define the goal with a completion condition\n    goal(\n        name = \"Published article\",\n        description = \"Complete and publish the article\",\n        condition = { state -&gt; state.isPublished }\n    )\n}\n\n// Create and run the agent\nval agentConfig = AIAgentConfig(\n    prompt = prompt(\"writer\") {\n        system(\"You are a professional content writer.\")\n    },\n    model = OpenAIModels.Chat.GPT4o,\n    maxAgentIterations = 20\n)\n\nval agent = PlannerAIAgent(\n    promptExecutor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    strategy = AIAgentPlannerStrategy(\"content-planner\", planner),\n    agentConfig = agentConfig\n)\n\nsuspend fun main() {\n    val result = agent.run(ContentState(topic = \"The Future of AI in Software Development\"))\n    println(\"Final state: $result\")\n}\n</code></pre>"},{"location":"planner-agents/#advanced-goap-features","title":"Advanced GOAP features","text":""},{"location":"planner-agents/#custom-cost-functions","title":"Custom cost functions","text":"<p>As A* search uses cost as a factor in finding the optimal sequence of actions, you can define custom cost functions for actions and goals to guide the planner:</p> <pre><code>action(\n    name = \"Expensive operation\",\n    precondition = { true },\n    belief = { state -&gt; state.copy(operationDone = true) },\n    cost = { state -&gt;\n        // Dynamic cost based on state\n        if (state.hasOptimization) 1.0 else 10.0\n    }\n) { ctx, state -&gt;\n    // Execute action\n    state.copy(operationDone = true)\n}\n</code></pre>"},{"location":"planner-agents/#state-beliefs-compared-to-actual-execution","title":"State beliefs compared to actual execution","text":"<p>GOAP distinguishes between the concepts of beliefs (optimistic predictions) and actual execution:</p> <ul> <li>Belief: What the planner thinks will happen, used for planning.</li> <li>Execution: What actually happens, used for real state updates.</li> </ul> <p>This allows the planner to make plans based on expected outcomes while handling actual results properly:</p> <pre><code>action(\n    name = \"Attempt complex task\",\n    precondition = { state -&gt; !state.taskComplete },\n    belief = { state -&gt;\n        // Optimistic belief: task will succeed\n        state.copy(taskComplete = true)\n    },\n    cost = { 5.0 }\n) { ctx, state -&gt;\n    // Actual execution might fail or have different results\n    val success = performComplexTask()\n    state.copy(\n        taskComplete = success,\n        attempts = state.attempts + 1\n    )\n}\n</code></pre>"},{"location":"predefined-agent-strategies/","title":"Predefined agent strategies","text":"<p>To make agent implementations easier, Koog provides predefined agent strategies for common agent use cases. The following predefined strategies are available:</p> <ul> <li>Chat agent strategy</li> <li>ReAct strategy</li> </ul>"},{"location":"predefined-agent-strategies/#chat-agent-strategy","title":"Chat agent strategy","text":"<p>The Chat agent strategy is designed for executing a chat interaction process. It orchestrates interactions between different stages, nodes, and tools to handle user input, execute tools, and provide responses in a chat-like manner.</p>"},{"location":"predefined-agent-strategies/#overview","title":"Overview","text":"<p>The Chat agent strategy implements a pattern where the agent:</p> <ol> <li>Receives user input</li> <li>Processes the input using an LLM</li> <li>Either calls a tool or provides a direct response</li> <li>Processes tool results and continues the conversation</li> <li>Provides feedback if the LLM tries to respond with plain text instead of using tools</li> </ol> <p>This approach creates a conversational interface where the agent can use tools to fulfill user requests.</p>"},{"location":"predefined-agent-strategies/#setup-and-dependencies","title":"Setup and dependencies","text":"<p>The implementation of Chat agent strategy in Koog is done through the <code>chatAgentStrategy</code> function. To make the function available in your agent code, add the following dependency import:</p> <pre><code>ai.koog.agents.ext.agent.chatAgentStrategy\n</code></pre> <p>To use the strategy, create an AI agent following the pattern below:</p> <pre><code>val chatAgent = AIAgent(\n    promptExecutor = promptExecutor,\n    toolRegistry = toolRegistry,\n    llmModel = model,\n    // Set chatAgentStrategy as the agent strategy\n    strategy = chatAgentStrategy()\n)\n</code></pre>"},{"location":"predefined-agent-strategies/#when-to-use-the-chat-agent-strategy","title":"When to use the Chat agent strategy","text":"<p>The Chat agent strategy is particularly useful for:</p> <ul> <li>Building conversational agents that need to use tools</li> <li>Creating assistants that can perform actions based on user requests</li> <li>Implementing chatbots that need to access external systems or data</li> <li>Scenarios where you want to enforce tool usage rather than plain text responses</li> </ul>"},{"location":"predefined-agent-strategies/#example","title":"Example","text":"<p>Here is a code sample of an AI agent that implements the predefined Chat agent strategy (<code>chatAgentStrategy</code>) and tools that the agent may use:</p> <pre><code>val chatAgent = AIAgent(\n    promptExecutor = promptExecutor,\n    llmModel = model,\n    // Use chatAgentStrategy as the agent strategy\n    strategy = chatAgentStrategy(),\n    // Add tools the agent can use\n    toolRegistry = ToolRegistry {\n        tool(searchTool)\n        tool(weatherTool)\n    }\n)\n\nsuspend fun main() { \n    // Run the agent with a user query\n    val result = chatAgent.run(\"What's the weather like today and should I bring an umbrella?\")\n}\n</code></pre>"},{"location":"predefined-agent-strategies/#react-strategy","title":"ReAct strategy","text":"<p>The ReAct (Reasoning and Acting) strategy is an AI agent strategy that alternates between reasoning and execution stages to dynamically process tasks and request output from a Large Language Model (LLM).</p>"},{"location":"predefined-agent-strategies/#overview_1","title":"Overview","text":"<p>The ReAct strategy implements a pattern where the agent:</p> <ol> <li>Reasons about the current state and plans the next steps</li> <li>Takes actions based on that reasoning</li> <li>Observes the results of those actions</li> <li>Repeats the cycle</li> </ol> <p>This approach combines the strengths of reasoning (thinking through problems step by step) and acting (executing tools to gather information or perform operations).</p>"},{"location":"predefined-agent-strategies/#flow-diagram","title":"Flow diagram","text":"<p>Here is the flow diagram of the ReAct strategy:</p> <p> </p>"},{"location":"predefined-agent-strategies/#setup-and-dependencies_1","title":"Setup and dependencies","text":"<p>The implementation of ReAct strategy in Koog is done through the <code>reActStrategy</code> function. To make the function available in your agent code, add the following dependency import:</p> <pre><code>ai.koog.agents.ext.agent.reActStrategy\n</code></pre> <p>To use the strategy, create an AI agent following the pattern below:</p> <pre><code>val reActAgent = AIAgent(\n    promptExecutor = promptExecutor,\n    toolRegistry = toolRegistry,\n    llmModel = model,\n    // Set reActStrategy as the agent strategy\n    strategy = reActStrategy(\n        // Set optional parameter values\n        reasoningInterval = 1,\n        name = \"react_agent\"\n    )\n)\n</code></pre>"},{"location":"predefined-agent-strategies/#parameters","title":"Parameters","text":"<p>The <code>reActStrategy</code> function takes the following parameters:</p> Parameter Type Default Description <code>reasoningInterval</code> Int 1 Specifies the interval for reasoning steps. Must be greater than 0. <code>name</code> String <code>re_act</code> The name of the strategy."},{"location":"predefined-agent-strategies/#example-use-case","title":"Example use case","text":"<p>Here is an example of how the ReAct strategy works with a simple banking agent:</p>"},{"location":"predefined-agent-strategies/#1-user-input","title":"1. User input","text":"<p>The user sends the initial prompt. For example, this can be a question such as <code>How much did I spend last month?</code>.</p>"},{"location":"predefined-agent-strategies/#2-reasoning","title":"2. Reasoning","text":"<p>The agent performs the initial reasoning by taking the user input and the reasoning prompt. The reasoning can look as follows:</p> <pre><code>I need to follow these steps:\n1. Get all transactions from last month\n2. Filter out deposits (positive amounts)\n3. Calculate total spending\n</code></pre>"},{"location":"predefined-agent-strategies/#3-action-and-execution-phase-1","title":"3. Action and execution, phase 1","text":"<p>Based on the action items that the agent defined in the previous step, it runs a tool to get all transactions from the previous month.</p> <p>In this case, the tool to run is <code>get_transactions</code>, along with the defined <code>startDate</code> and <code>endDate</code> arguments that match the request to get all transactions during the previous month:</p> <pre><code>{tool: \"get_transactions\", args: {startDate: \"2025-05-19\", endDate: \"2025-06-18\"}}\n</code></pre> <p>The tool returns a result that can look as follows:</p> <pre><code>[\n  {date: \"2025-05-25\", amount: -100.00, description: \"Grocery Store\"},\n  {date: \"2025-05-31\", amount: +1000.00, description: \"Salary Deposit\"},\n  {date: \"2025-06-10\", amount: -500.00, description: \"Rent Payment\"},\n  {date: \"2025-06-13\", amount: -200.00, description: \"Utilities\"}\n]\n</code></pre>"},{"location":"predefined-agent-strategies/#4-reasoning","title":"4. Reasoning","text":"<p>With the result returned by the tool, the agent performs reasoning again to determine the next steps in its flow:</p> <pre><code>I have the transactions. Now I need to:\n1. Remove the salary deposit of +1000.00\n2. Sum up the remaining transactions\n</code></pre>"},{"location":"predefined-agent-strategies/#5-action-and-execution-phase-2","title":"5. Action and execution, phase 2","text":"<p>Based on the previous reasoning step, the agent calls the <code>calculate_sum</code> tool that sums up the amounts provided as tool arguments. As the reasoning also resulted in the action point of removing the positive amount from transactions, the amounts provided as tool arguments are only the negative ones:</p> <pre><code>{tool: \"calculate_sum\", args: {amounts: [-100.00, -500.00, -200.00]}}\n</code></pre> <p>The tool returns the final result:</p> <pre><code>-800.00\n</code></pre>"},{"location":"predefined-agent-strategies/#6-final-response","title":"6. Final response","text":"<p>The agent returns the final response (assistant message) that includes the calculated sum:</p> <pre><code>You spent $800.00 last month on groceries, rent, and utilities.\n</code></pre>"},{"location":"predefined-agent-strategies/#when-to-use-the-react-strategy","title":"When to use the ReAct strategy","text":"<p>The ReAct strategy is particularly useful for:</p> <ul> <li>Complex tasks requiring multistep reasoning</li> <li>Scenarios where the agent needs to gather information before providing a final answer</li> <li>Problems that benefit from breaking down into smaller steps</li> <li>Tasks requiring both analytical thinking and tool usage</li> </ul>"},{"location":"predefined-agent-strategies/#example_1","title":"Example","text":"<p>Here is a code sample of an AI agent that implements the predefined ReAct strategy (<code>reActStrategy</code>) and tools that the agent may use:</p> <pre><code>val bankingAgent = AIAgent(\n    promptExecutor = promptExecutor,\n    llmModel = model,\n    // Use reActStrategy as the agent strategy\n    strategy = reActStrategy(\n        reasoningInterval = 1,\n        name = \"banking_agent\"\n    ),\n    // Add tools the agent can use\n    toolRegistry = ToolRegistry {\n        tool(getTransactions)\n        tool(calculateSum)\n    }\n)\n\nsuspend fun main() { \n    // Run the agent with a user query\n    val result = bankingAgent.run(\"How much did I spend last month?\")\n}\n</code></pre>"},{"location":"ranked-document-storage/","title":"Document storage","text":"<p>To let you provide up-to-date and searchable information sources for use with Large Language Models (LLMs), Koog supports Resource-Augmented Generation (RAG) to store and retrieve information from documents.</p>"},{"location":"ranked-document-storage/#key-rag-features","title":"Key RAG features","text":"<p>The core components of a common RAG system include:</p> <ul> <li>Document storage: a repository of documents, files, or text chunks that contain information.</li> <li>Vector embeddings: numerical representations of a text that capture semantic meaning. For more information on embeddings in Koog, see Embeddings.</li> <li>Retrieval mechanism: a system that finds the most relevant documents based on a query.</li> <li>Generation component: an LLM that uses the retrieved information to generate responses.</li> </ul> <p>RAG addresses several limitations of traditional LLMs:</p> <ul> <li>Knowledge cutoff: RAG can access the most recent information, not limited to training data.</li> <li>Hallucinations: by grounding responses in retrieved documents, RAG reduces fabricated information.</li> <li>Domain specificity: RAG can be tailored to specific domains by curating the knowledge base.</li> <li>Transparency: the sources of information can be cited, making the system more explainable.</li> </ul>"},{"location":"ranked-document-storage/#finding-information-in-a-rag-system","title":"Finding information in a RAG system","text":"<p>Finding relevant information in a RAG system involves storing documents as vector embeddings and ranking them based on their similarity to a user's query. This approach works with various document types, including PDFs, images, text files, or even individual text chunks.</p> <p>The process involves:</p> <ol> <li>Document embedding: converting documents into vector representations that capture their semantic meaning.</li> <li>Vector storage: storing these embeddings efficiently for quick retrieval.</li> <li>Similarity search: finding documents whose embeddings are most similar to the query embedding.</li> <li>Ranking: ordering documents by their relevance score.</li> </ol>"},{"location":"ranked-document-storage/#implementing-a-rag-system-in-koog","title":"Implementing a RAG system in Koog","text":"<p>To implement a RAG system in Koog, follow the steps below:</p> <ol> <li>Create an embedder using Ollama or OpenAI. The embedder is an instance of the <code>LLMEmbedder</code> class that takes an LLM client instance and model as parameters. For more information, see Embeddings.</li> <li>Create a document embedder based on the created general embedder.</li> <li>Create a document storage.</li> <li>Add documents to the storage.</li> <li>Find the most relevant documents using a defined query.</li> </ol> <p>This sequence of steps represents a relevance search flow that returns the most relevant documents for a given user query. Here is a code sample showing how to implement the entire sequence of steps described above:</p> <pre><code>// Create an embedder using Ollama\nval embedder = LLMEmbedder(OllamaClient(), OllamaEmbeddingModels.NOMIC_EMBED_TEXT)\n// You may also use OpenAI embeddings with:\n// val embedder = LLMEmbedder(OpenAILLMClient(\"API_KEY\"), OpenAIModels.Embeddings.TextEmbeddingAda3Large)\n\n// Create a JVM-specific document embedder\nval documentEmbedder = JVMTextDocumentEmbedder(embedder)\n\n// Create a ranked document storage using in-memory vector storage\nval rankedDocumentStorage = EmbeddingBasedDocumentStorage(documentEmbedder, InMemoryVectorStorage())\n\n// Store documents in the storage\nrankedDocumentStorage.store(Path.of(\"./my/documents/doc1.txt\"))\nrankedDocumentStorage.store(Path.of(\"./my/documents/doc2.txt\"))\nrankedDocumentStorage.store(Path.of(\"./my/documents/doc3.txt\"))\n// ... store more documents as needed\nrankedDocumentStorage.store(Path.of(\"./my/documents/doc100.txt\"))\n\n// Find the most relevant documents for a user query\nval query = \"I want to open a bank account but I'm getting a 404 when I open your website. I used to be your client with a different account 5 years ago before you changed your firm name\"\nval relevantFiles = rankedDocumentStorage.mostRelevantDocuments(query, count = 3)\n\n// Process the relevant files\nrelevantFiles.forEach { file -&gt;\n    println(\"Relevant file: ${file.toAbsolutePath()}\")\n    // Process the file content as needed\n}\n</code></pre>"},{"location":"ranked-document-storage/#providing-relevance-search-for-use-by-ai-agents","title":"Providing relevance search for use by AI agents","text":"<p>Once you have a ranked document storage system, you can use it to provide relevant context to an AI agent for answering user queries. This enhances the agent's ability to provide accurate and contextually appropriate responses.</p> <p>Here is an example of how to implement the defined RAG system for an AI agent to be able to answer queries by getting information from the document storage: </p> <pre><code>suspend fun solveUserRequest(query: String) {\n    // Retrieve top-5 documents from the document provider\n    val relevantDocuments = rankedDocumentStorage.mostRelevantDocuments(query, count = 5)\n\n    // Create an AI Agent with the relevant context\n    val agentConfig = AIAgentConfig(\n        prompt = prompt(\"context\") {\n            system(\"You are a helpful assistant. Use the provided context to answer the user's question accurately.\")\n            user {\n                +\"Relevant context:\"\n                relevantDocuments.forEach {\n                    file(it.pathString, \"text/plain\")\n                }\n            }\n        },\n        model = OpenAIModels.Chat.GPT4o, // Or a different model of your choice\n        maxAgentIterations = 100,\n    )\n\n    val agent = AIAgent(\n        promptExecutor = simpleOpenAIExecutor(apiKey),\n        llmModel = OpenAIModels.Chat.GPT4o\n    )\n\n\n    // Run the agent to get a response\n    val response = agent.run(query)\n\n    // Return or process the response\n    println(\"Agent response: $response\")\n}\n</code></pre>"},{"location":"ranked-document-storage/#providing-relevance-search-as-a-tool","title":"Providing relevance search as a tool","text":"<p>Instead of directly providing document content as context, you can also implement a tool that allows the agent to perform relevance searches on demand. This gives the agent more flexibility in deciding when and how to use the document storage.</p> <p>Here is an example of how to implement a relevance search tool:</p> <pre><code>@Tool\n@LLMDescription(\"Search for relevant documents about any topic (if exists). Returns the content of the most relevant documents.\")\nsuspend fun searchDocuments(\n    @LLMDescription(\"Query to search relevant documents about\")\n    query: String,\n    @LLMDescription(\"Maximum number of documents\")\n    count: Int\n): String {\n    val relevantDocuments =\n        rankedDocumentStorage.mostRelevantDocuments(query, count = count, similarityThreshold = 0.9).toList()\n\n    if (!relevantDocuments.isEmpty()) {\n        return \"No relevant documents found for the query: $query\"\n    }\n\n    val result = StringBuilder(\"Found ${relevantDocuments.size} relevant documents:\\n\\n\")\n\n    relevantDocuments.forEachIndexed { index, document -&gt;\n        val content = Files.readString(document)\n        result.append(\"Document ${index + 1}: ${document.fileName}\\n\")\n        result.append(\"Content: $content\\n\\n\")\n    }\n\n    return result.toString()\n}\n\nfun main() {\n    runBlocking {\n        val tools = ToolRegistry {\n            tool(::searchDocuments.asTool())\n        }\n\n        val agent = AIAgent(\n            toolRegistry = tools,\n            promptExecutor = simpleOpenAIExecutor(apiKey),\n            llmModel = OpenAIModels.Chat.GPT4o\n        )\n\n        val response = agent.run(\"How to make a cake?\")\n        println(\"Agent response: $response\")\n\n    }\n}\n</code></pre> <p>With this approach, the agent can decide when to use the search tool based on your query. This is particularly useful for complex queries that may require information from multiple documents or when the agent needs to search for specific details.</p>"},{"location":"ranked-document-storage/#existing-implementations-of-vector-storage-and-document-embedding-providers","title":"Existing implementations of vector storage and document embedding providers","text":"<p>For convenience and easier implementation of a RAG system, Koog provides several out-of-the-box implementations for vector storage, document embedding, and combined embedding and storage components.</p>"},{"location":"ranked-document-storage/#vector-storage","title":"Vector storage","text":""},{"location":"ranked-document-storage/#inmemoryvectorstorage","title":"InMemoryVectorStorage","text":"<p>A simple in-memory implementation that stores documents and their vector embeddings in memory. Suitable for testing or small-scale applications.</p> <pre><code>val inMemoryStorage = InMemoryVectorStorage&lt;Path&gt;()\n</code></pre> <p>For more information, see the InMemoryVectorStorage reference.</p>"},{"location":"ranked-document-storage/#filevectorstorage","title":"FileVectorStorage","text":"<p>A file-based implementation that stores documents and their vector embeddings on disk. Suitable for persistent storage across application restarts.</p> <pre><code>val fileStorage = FileVectorStorage&lt;Document, Path&gt;(\n   documentReader = documentProvider,\n   fs = fileSystemProvider,\n   root = rootPath\n)\n</code></pre> <p>For more information, see the FileVectorStorage reference.</p>"},{"location":"ranked-document-storage/#jvmfilevectorstorage","title":"JVMFileVectorStorage","text":"<p>A JVM-specific implementation of <code>FileVectorStorage</code> that works with <code>java.nio.file.Path</code>.</p> <pre><code>val jvmFileStorage = JVMFileVectorStorage(root = Path.of(\"/path/to/storage\"))\n</code></pre> <p>For more information, see the JVMFileVectorStorage reference.</p>"},{"location":"ranked-document-storage/#document-embedder","title":"Document embedder","text":""},{"location":"ranked-document-storage/#textdocumentembedder","title":"TextDocumentEmbedder","text":"<p>A generic implementation that works with any document type that can be converted to text.</p> <pre><code>val textEmbedder = TextDocumentEmbedder&lt;Document, Path&gt;(\n   documentReader = documentProvider,\n   embedder = embedder\n)\n</code></pre> <p>For more information, see the TextDocumentEmbedder reference.</p>"},{"location":"ranked-document-storage/#jvmtextdocumentembedder","title":"JVMTextDocumentEmbedder","text":"<p>A JVM-specific implementation that works with <code>java.nio.file.Path</code>.</p> <pre><code>val embedder = LLMEmbedder(OllamaClient(), OllamaEmbeddingModels.NOMIC_EMBED_TEXT)\nval jvmTextEmbedder = JVMTextDocumentEmbedder(embedder = embedder)\n</code></pre> <p>For more information, see the JVMTextDocumentEmbedder reference.</p>"},{"location":"ranked-document-storage/#combined-storage-implementations","title":"Combined storage implementations","text":""},{"location":"ranked-document-storage/#embeddingbaseddocumentstorage","title":"EmbeddingBasedDocumentStorage","text":"<p>Combines a document embedder and a vector storage to provide a complete solution for storing and ranking documents.</p> <pre><code>val embeddingStorage = EmbeddingBasedDocumentStorage(\n    embedder = documentEmbedder,\n    storage = vectorStorage\n)\n</code></pre> <p>For more information, see the EmbeddingBasedDocumentStorage reference.</p>"},{"location":"ranked-document-storage/#inmemorydocumentembeddingstorage","title":"InMemoryDocumentEmbeddingStorage","text":"<p>An in-memory implementation of <code>EmbeddingBasedDocumentStorage</code>.</p> <pre><code>val inMemoryEmbeddingStorage = InMemoryDocumentEmbeddingStorage&lt;Document&gt;(\n    embedder = documentEmbedder\n)\n</code></pre> <p>For more information, see the InMemoryDocumentEmbeddingStorage reference.</p>"},{"location":"ranked-document-storage/#filedocumentembeddingstorage","title":"FileDocumentEmbeddingStorage","text":"<p>A file-based implementation of <code>EmbeddingBasedDocumentStorage</code>.</p> <pre><code>val fileEmbeddingStorage = FileDocumentEmbeddingStorage&lt;Document, Path&gt;(\n   embedder = documentEmbedder,\n   documentProvider = documentProvider,\n   fs = fileSystemProvider,\n   root = rootPath\n)\n</code></pre> <p>For more information, see the FileDocumentEmbeddingStorage reference.</p>"},{"location":"ranked-document-storage/#jvmfiledocumentembeddingstorage","title":"JVMFileDocumentEmbeddingStorage","text":"<p>A JVM-specific implementation of <code>FileDocumentEmbeddingStorage</code>.</p> <pre><code>val jvmFileEmbeddingStorage = JVMFileDocumentEmbeddingStorage(\n   embedder = documentEmbedder,\n   root = Path.of(\"/path/to/storage\")\n)\n</code></pre> <p>For more information, see the JVMFileDocumentEmbeddingStorage reference.</p>"},{"location":"ranked-document-storage/#jvmtextfiledocumentembeddingstorage","title":"JVMTextFileDocumentEmbeddingStorage","text":"<p>A JVM-specific implementation that combines <code>JVMTextDocumentEmbedder</code> and <code>JVMFileVectorStorage</code>.</p> <pre><code>val jvmTextFileEmbeddingStorage = JVMTextFileDocumentEmbeddingStorage(\n   embedder = embedder,\n   root = Path.of(\"/path/to/storage\")\n)\n</code></pre> <p>For more information, see the JVMTextFileDocumentEmbeddingStorage reference.</p> <p>These implementations provide a flexible and extensible framework for working with document embeddings and vector storage in various environments.</p>"},{"location":"ranked-document-storage/#implementing-your-own-vector-storage-and-document-embedder","title":"Implementing your own vector storage and document embedder","text":"<p>You can extend Koog's vector storage framework by implementing your own custom document embedders and vector storage solutions. This is particularly useful when working with specialized document types or storage requirements.</p> <p>Here's an example of implementing a custom document embedder for PDF documents:</p> <pre><code>// Define a PDFDocument class\nclass PDFDocument(private val path: Path) {\n    fun readText(): String {\n        // Use a PDF library to extract text from the PDF\n        return \"Text extracted from PDF at $path\"\n    }\n}\n\n// Implement a DocumentProvider for PDFDocument\nclass PDFDocumentProvider : DocumentProvider&lt;Path, PDFDocument&gt; {\n    override suspend fun document(path: Path): PDFDocument? {\n        return if (path.toString().endsWith(\".pdf\")) {\n            PDFDocument(path)\n        } else {\n            null\n        }\n    }\n\n    override suspend fun text(document: PDFDocument): CharSequence {\n        return document.readText()\n    }\n}\n\n// Implement a DocumentEmbedder for PDFDocument\nclass PDFDocumentEmbedder(private val embedder: Embedder) : DocumentEmbedder&lt;PDFDocument&gt; {\n    override suspend fun embed(document: PDFDocument): Vector {\n        val text = document.readText()\n        return embed(text)\n    }\n\n    override suspend fun embed(text: String): Vector {\n        return embedder.embed(text)\n    }\n\n    override fun diff(embedding1: Vector, embedding2: Vector): Double {\n        return embedder.diff(embedding1, embedding2)\n    }\n}\n\n// Create a custom vector storage for PDF documents\nclass PDFVectorStorage(\n    private val pdfProvider: PDFDocumentProvider,\n    private val embedder: PDFDocumentEmbedder,\n    private val storage: VectorStorage&lt;PDFDocument&gt;\n) : RankedDocumentStorage&lt;PDFDocument&gt; {\n    override fun rankDocuments(query: String): Flow&lt;RankedDocument&lt;PDFDocument&gt;&gt; = flow {\n        val queryVector = embedder.embed(query)\n        storage.allDocumentsWithPayload().collect { (document, documentVector) -&gt;\n            emit(\n                RankedDocument(\n                    document = document,\n                    similarity = 1.0 - embedder.diff(queryVector, documentVector)\n                )\n            )\n        }\n    }\n\n    override suspend fun store(document: PDFDocument, data: Unit): String {\n        val vector = embedder.embed(document)\n        return storage.store(document, vector)\n    }\n\n    override suspend fun delete(documentId: String): Boolean {\n        return storage.delete(documentId)\n    }\n\n    override suspend fun read(documentId: String): PDFDocument? {\n        return storage.read(documentId)\n    }\n\n    override fun allDocuments(): Flow&lt;PDFDocument&gt; = flow {\n        storage.allDocumentsWithPayload().collect {\n            emit(it.document)\n        }\n    }\n}\n\n// Usage example\nsuspend fun main() {\n    val pdfProvider = PDFDocumentProvider()\n    val embedder = LLMEmbedder(OllamaClient(), OllamaEmbeddingModels.NOMIC_EMBED_TEXT)\n    val pdfEmbedder = PDFDocumentEmbedder(embedder)\n    val storage = InMemoryVectorStorage&lt;PDFDocument&gt;()\n    val pdfStorage = PDFVectorStorage(pdfProvider, pdfEmbedder, storage)\n\n    // Store PDF documents\n    val pdfDocument = PDFDocument(Path.of(\"./documents/sample.pdf\"))\n    pdfStorage.store(pdfDocument)\n\n    // Query for relevant PDF documents\n    val relevantPDFs = pdfStorage.mostRelevantDocuments(\"information about climate change\", count = 3)\n\n}\n</code></pre>"},{"location":"ranked-document-storage/#implementing-custom-non-embedding-based-rankeddocumentstorage","title":"Implementing custom non-embedding-based RankedDocumentStorage","text":"<p>While embedding-based document ranking is powerful, there are scenarios where you might want to implement a custom ranking mechanism that does not rely on embeddings. For example, you might want to rank documents based on:</p> <ul> <li>PageRank-like algorithms</li> <li>Keyword frequency</li> <li>Recency of documents</li> <li>User interaction history</li> <li>Domain-specific heuristics</li> </ul> <p>Here's an example of implementing a custom <code>RankedDocumentStorage</code> that uses a simple keyword-based ranking approach:</p> <pre><code>class KeywordBasedDocumentStorage&lt;Document&gt;(\n    private val documentProvider: DocumentProvider&lt;Path, Document&gt;,\n    private val storage: DocumentStorage&lt;Document&gt;\n) : RankedDocumentStorage&lt;Document&gt; {\n\n    override fun rankDocuments(query: String): Flow&lt;RankedDocument&lt;Document&gt;&gt; = flow {\n        // Split the query into keywords\n        val keywords = query.lowercase().split(Regex(\"\\\\W+\")).filter { it.length &gt; 2 }\n\n        // Process each document\n        storage.allDocuments().collect { document -&gt;\n            // Get the document text\n            val documentText = documentProvider.text(document).toString().lowercase()\n\n            // Calculate a simple similarity score based on keyword frequency\n            var similarity = 0.0\n            for (keyword in keywords) {\n                val count = countOccurrences(documentText, keyword)\n                if (count &gt; 0) {\n                    similarity += count.toDouble() / documentText.length * 1000\n                }\n            }\n\n            // Emit the document with its similarity score\n            emit(RankedDocument(document, similarity))\n        }\n    }\n\n    private fun countOccurrences(text: String, keyword: String): Int {\n        var count = 0\n        var index = 0\n        while (index != -1) {\n            index = text.indexOf(keyword, index)\n            if (index != -1) {\n                count++\n                index += keyword.length\n            }\n        }\n        return count\n    }\n\n    override suspend fun store(document: Document, data: Unit): String {\n        return storage.store(document)\n    }\n\n    override suspend fun delete(documentId: String): Boolean {\n        return storage.delete(documentId)\n    }\n\n    override suspend fun read(documentId: String): Document? {\n        return storage.read(documentId)\n    }\n\n    override fun allDocuments(): Flow&lt;Document&gt; {\n        return storage.allDocuments()\n    }\n}\n</code></pre> <p>This implementation ranks documents based on the frequency of keywords from the query appearing in the document text. You could extend this approach with more sophisticated algorithms like TF-IDF (Term Frequency-Inverse Document Frequency) or BM25.</p> <p>Another example is a time-based ranking system that prioritizes recent documents:</p> <pre><code>class TimeBasedDocumentStorage&lt;Document&gt;(\n    private val storage: DocumentStorage&lt;Document&gt;,\n    private val getDocumentTimestamp: (Document) -&gt; Long\n) : RankedDocumentStorage&lt;Document&gt; {\n\n    override fun rankDocuments(query: String): Flow&lt;RankedDocument&lt;Document&gt;&gt; = flow {\n        val currentTime = System.currentTimeMillis()\n\n        storage.allDocuments().collect { document -&gt;\n            val timestamp = getDocumentTimestamp(document)\n            val ageInHours = (currentTime - timestamp) / (1000.0 * 60 * 60)\n\n            // Calculate a decay factor based on age (newer documents get higher scores)\n            val decayFactor = Math.exp(-0.01 * ageInHours)\n\n            emit(RankedDocument(document, decayFactor))\n        }\n    }\n\n    // Implement other required methods from RankedDocumentStorage\n    override suspend fun store(document: Document, data: Unit): String {\n        return storage.store(document)\n    }\n\n    override suspend fun delete(documentId: String): Boolean {\n        return storage.delete(documentId)\n    }\n\n    override suspend fun read(documentId: String): Document? {\n        return storage.read(documentId)\n    }\n\n    override fun allDocuments(): Flow&lt;Document&gt; {\n        return storage.allDocuments()\n    }\n}\n</code></pre> <p>By implementing the <code>RankedDocumentStorage</code> interface, you can create custom ranking mechanisms tailored to your specific use case while still leveraging the rest of the RAG infrastructure.</p> <p>The flexibility of Koog's design allows you to mix and match different storage and ranking strategies to build a system that meets your specific requirements.</p>"},{"location":"sessions/","title":"LLM sessions and manual history management","text":"<p>This page provides detailed information about LLM sessions, including how to work with read and write sessions, manage conversation history, and make requests to language models.</p>"},{"location":"sessions/#introduction","title":"Introduction","text":"<p>LLM sessions are a fundamental concept that provides a structured way to interact with language models (LLMs).  They manage the conversation history, handle requests to the LLM, and provide a consistent interface for running tools and processing responses.</p>"},{"location":"sessions/#understanding-llm-sessions","title":"Understanding LLM sessions","text":"<p>An LLM session represents a context for interacting with a language model. It encapsulates:</p> <ul> <li>The conversation history (prompt)</li> <li>Available tools</li> <li>Methods for making requests to the LLM</li> <li>Methods for updating the conversation history</li> <li>Methods for running tools</li> </ul> <p>Sessions are managed by the <code>AIAgentLLMContext</code> class, which provides methods for creating read and write sessions.</p>"},{"location":"sessions/#session-types","title":"Session types","text":"<p>The Koog framework provides two types of sessions:</p> <ol> <li> <p>Write Sessions (<code>AIAgentLLMWriteSession</code>): Allow modifying the prompt and tools, making LLM requests, and    running tools. Changes made in a write session are persisted back to the LLM context.</p> </li> <li> <p>Read Sessions (<code>AIAgentLLMReadSession</code>): Provide read-only access to the prompt and tools. They are useful for    inspecting the current state without making changes.</p> </li> </ol> <p>The key difference is that write sessions can modify the conversation history, while read sessions cannot.</p>"},{"location":"sessions/#session-lifecycle","title":"Session lifecycle","text":"<p>Sessions have a defined lifecycle:</p> <ol> <li>Creation: a session is created using <code>llm.writeSession { ... }</code> or <code>llm.readSession { ... }</code>.</li> <li>Active phase: the session is active while the lambda block is executing.</li> <li>Termination: the session is automatically closed when the lambda block completes.</li> </ol> <p>Sessions implement the <code>AutoCloseable</code> interface, ensuring they are properly cleaned up even if an exception occurs.</p>"},{"location":"sessions/#working-with-llm-sessions","title":"Working with LLM sessions","text":""},{"location":"sessions/#creating-sessions","title":"Creating sessions","text":"<p>Sessions are created using extension functions on the <code>AIAgentLLMContext</code> class:</p> <pre><code>// Creating a write session\nllm.writeSession {\n    // Session code here\n}\n\n// Creating a read session\nllm.readSession {\n    // Session code here\n}\n</code></pre> <p>These functions take a lambda block that runs within the context of the session. The session is automatically closed when the block completes.</p>"},{"location":"sessions/#session-scope-and-thread-safety","title":"Session scope and thread safety","text":"<p>Sessions use a read-write lock to ensure thread safety:</p> <ul> <li>Multiple read sessions can be active simultaneously.</li> <li>Only one write session can be active at a time.</li> <li>A write session blocks all other sessions (both read and write).</li> </ul> <p>This ensures that the conversation history is not corrupted by concurrent modifications.</p>"},{"location":"sessions/#accessing-session-properties","title":"Accessing session properties","text":"<p>Within a session, you can access the prompt and tools:</p> <pre><code>llm.readSession {\n    val messageCount = prompt.messages.size\n    val availableTools = tools.map { it.name }\n}\n</code></pre> <p>In a write session, you can also modify these properties:</p> <pre><code>llm.writeSession {\n    // Modify the prompt\n    appendPrompt {\n        user(\"New user message\")\n    }\n\n    // Modify the tools\n    tools = newTools\n}\n</code></pre> <p>For more information, see the detailed API reference for AIAgentLLMReadSession and AIAgentLLMWriteSession.</p>"},{"location":"sessions/#making-llm-requests","title":"Making LLM requests","text":""},{"location":"sessions/#basic-request-methods","title":"Basic request methods","text":"<p>The most common methods for making LLM requests are:</p> <ol> <li> <p><code>requestLLM()</code>: makes a request to the LLM with the current prompt and tools, returning a single response.</p> </li> <li> <p><code>requestLLMMultiple()</code>: makes a request to the LLM with the current prompt and tools, returning multiple    responses.</p> </li> <li> <p><code>requestLLMWithoutTools()</code>: makes a request to the LLM with the current prompt but without any tools, returning a    single response.</p> </li> <li> <p><code>requestLLMForceOneTool</code>: makes a request to the LLM with the current prompt and tools, forcing the use of one tool.</p> </li> <li> <p><code>requestLLMOnlyCallingTools</code>: makes a request to the LLM that should be processed by only using tools.</p> </li> </ol> <p>Example:</p> <pre><code>llm.writeSession {\n    // Make a request with tools enabled\n    val response = requestLLM()\n\n    // Make a request without tools\n    val responseWithoutTools = requestLLMWithoutTools()\n\n    // Make a request that returns multiple responses\n    val responses = requestLLMMultiple()\n}\n</code></pre>"},{"location":"sessions/#how-requests-work","title":"How requests work","text":"<p>LLM requests are made when you explicitly call one of the request methods. The key points to understand are:</p> <ol> <li>Explicit invocation: requests only happen when you call methods like <code>requestLLM()</code>, <code>requestLLMWithoutTools()</code> and so on.</li> <li>Immediate execution: when you call a request method, the request is made immediately, and the method blocks until a response is received.</li> <li>Automatic history update: in a write session, the response is automatically added to the conversation history.</li> <li>No implicit requests: the system does not make implicit requests; you need to explicitly call a request method.</li> </ol>"},{"location":"sessions/#request-methods-with-tools","title":"Request methods with tools","text":"<p>When making requests with tools enabled, the LLM may respond with a tool call instead of a text response. The request methods handle this transparently:</p> <pre><code>llm.writeSession {\n    val response = requestLLM()\n\n    // The response might be a tool call or a text response\n    if (response is Message.Tool.Call) {\n        // Handle tool call\n    } else {\n        // Handle text response\n    }\n}\n</code></pre> <p>In practice, you typically do not need to check the response type manually, as the agent graph handles this routing automatically.</p>"},{"location":"sessions/#structured-and-streaming-requests","title":"Structured and streaming requests","text":"<p>For more advanced use cases, the platform provides methods for structured and streaming requests:</p> <ol> <li> <p><code>requestLLMStructured()</code>: requests the LLM to provide a response in a specific structured format.</p> </li> <li> <p><code>requestLLMStructuredOneShot()</code>: similar to <code>requestLLMStructured()</code> but without retries or corrections.</p> </li> <li> <p><code>requestLLMStreaming()</code>: makes a streaming request to the LLM, returning a flow of response chunks.</p> </li> </ol> <p>Example:</p> <pre><code>llm.writeSession {\n    // Make a structured request\n    val structuredResponse = requestLLMStructured&lt;JokeRating&gt;()\n\n    // Make a streaming request\n    val responseStream = requestLLMStreaming()\n    responseStream.collect { chunk -&gt;\n        // Process each chunk as it arrives\n    }\n}\n</code></pre>"},{"location":"sessions/#managing-conversation-history","title":"Managing conversation history","text":""},{"location":"sessions/#updating-the-prompt","title":"Updating the prompt","text":"<p>In a write session, you can add messages to the prompt (conversation history) using the <code>appendPrompt</code> method:</p> <pre><code>llm.writeSession {\n    appendPrompt {\n        // Add a system message\n        system(\"You are a helpful assistant.\")\n\n        // Add a user message\n        user(\"Hello, can you help me with a coding question?\")\n\n        // Add an assistant message\n        assistant(\"Of course! What's your question?\")\n\n        // Add a tool result\n        tool {\n            result(myToolResult)\n        }\n    }\n}\n</code></pre> <p>You can also completely rewrite the prompt using the <code>rewritePrompt</code> method:</p> <pre><code>llm.writeSession {\n    rewritePrompt { oldPrompt -&gt;\n        // Create a new prompt based on the old one\n        oldPrompt.copy(messages = filteredMessages)\n    }\n}\n</code></pre>"},{"location":"sessions/#automatic-history-update-on-response","title":"Automatic history update on response","text":"<p>When you make an LLM request in a write session, the response is automatically added to the conversation history:</p> <pre><code>llm.writeSession {\n    // Add a user message\n    appendPrompt {\n        user(\"What's the capital of France?\")\n    }\n\n    // Make a request \u2013 the response is automatically added to the history\n    val response = requestLLM()\n\n    // The prompt now includes both the user message and the model's response\n}\n</code></pre> <p>This automatic history update is the key feature of write sessions, ensuring that the conversation flows naturally.</p>"},{"location":"sessions/#history-compression","title":"History compression","text":"<p>For long-running conversations, the history can grow large and consume a lot of tokens. The platform provides methods for compressing history:</p> <pre><code>llm.writeSession {\n    // Compress the history using a TLDR approach\n    replaceHistoryWithTLDR(HistoryCompressionStrategy.WholeHistory, preserveMemory = true)\n}\n</code></pre> <p>You can also use the <code>nodeLLMCompressHistory</code> node in a strategy graph to compress history at specific points.</p> <p>For more information about history compression and compression strategies, see History compression.</p>"},{"location":"sessions/#running-tools-in-sessions","title":"Running tools in sessions","text":""},{"location":"sessions/#calling-tools","title":"Calling tools","text":"<p>Write sessions provide several methods for calling tools:</p> <ol> <li> <p><code>callTool(tool, args)</code>: calls a tool by reference.</p> </li> <li> <p><code>callTool(toolName, args)</code>: calls a tool by name.</p> </li> <li> <p><code>callTool(toolClass, args)</code>: calls a tool by class.</p> </li> <li> <p><code>callToolRaw(toolName, args)</code>: calls a tool by name and returns the raw string result.</p> </li> </ol> <p>Example:</p> <pre><code>llm.writeSession {\n    // Call a tool by reference\n    val result = callTool(myTool, myArgs)\n\n    // Call a tool by name\n    val result2 = callTool(\"myToolName\", myArgs)\n\n    // Call a tool by class\n    val result3 = callTool(MyTool::class, myArgs)\n\n    // Call a tool and get the raw result\n    val rawResult = callToolRaw(\"myToolName\", myArgs)\n}\n</code></pre>"},{"location":"sessions/#parallel-tool-runs","title":"Parallel tool runs","text":"<p>To run multiple tools in parallel, write sessions provide extension functions on <code>Flow</code>:</p> <pre><code>llm.writeSession {\n    // Run tools in parallel\n    parseDataToArgs(data).toParallelToolCalls(MyTool::class).collect { result -&gt;\n        // Process each result\n    }\n\n    // Run tools in parallel and get raw results\n    parseDataToArgs(data).toParallelToolCallsRaw(MyTool::class).collect { rawResult -&gt;\n        // Process each raw result\n    }\n}\n</code></pre> <p>This is useful for processing large amounts of data efficiently.</p>"},{"location":"sessions/#best-practices","title":"Best practices","text":"<p>When working with LLM sessions, follow these best practices:</p> <ol> <li> <p>Use the right session type: Use write sessions when you need to modify the conversation history and read    sessions when you only need to read it.</p> </li> <li> <p>Keep sessions short: Sessions should be focused on a specific task and closed as soon as possible to release    resources.</p> </li> <li> <p>Handle exceptions: Make sure to handle exceptions within sessions to prevent resource leaks.</p> </li> <li> <p>Manage history size: For long-running conversations, use history compression to reduce token usage.</p> </li> <li> <p>Prefer high-Level abstractions: When possible, use the node-based API. For example, <code>nodeLLMRequest</code> instead of directly working with sessions.</p> </li> <li> <p>Be mindful of thread safety: Remember that write sessions block other sessions, so keep write operations as short    as possible.</p> </li> <li> <p>Use structured requests for complex data: When you need the LLM to return structured data, use    <code>requestLLMStructured</code> instead of parsing free-form text.</p> </li> <li> <p>Use streaming for long responses: For long responses, use <code>requestLLMStreaming</code> to process the response as it    arrives.</p> </li> </ol>"},{"location":"sessions/#troubleshooting","title":"Troubleshooting","text":""},{"location":"sessions/#session-already-closed","title":"Session already closed","text":"<p>If you see an error such as <code>Cannot use session after it was closed</code>, you are trying to use a session after its lambda  block has completed. Make sure all session operations are performed within the session block.</p>"},{"location":"sessions/#history-too-large","title":"History too large","text":"<p>If your history becomes too large and consumes too many tokens, use history compression techniques:</p> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR(HistoryCompressionStrategy.FromLastNMessages(10), preserveMemory = true)\n}\n</code></pre> <p>For more information, see History compression</p>"},{"location":"sessions/#tool-not-found","title":"Tool not found","text":"<p>If you see errors about tools not being found, check that:</p> <ul> <li>The tool is correctly registered in the tool registry.</li> <li>You are using the correct tool name or class.</li> </ul>"},{"location":"sessions/#api-documentation","title":"API documentation","text":"<p>For more information, see the full AIAgentLLMSession and AIAgentLLMContext reference.</p>"},{"location":"spring-boot/","title":"Spring Boot Integration","text":"<p>Koog provides seamless Spring Boot integration through its auto-configuration starter, making it easy to incorporate AI agents into your Spring Boot applications with minimal setup.</p>"},{"location":"spring-boot/#overview","title":"Overview","text":"<p>The <code>koog-spring-boot-starter</code> automatically configures LLM clients based on your application properties and provides ready-to-use beans for dependency injection. It supports all major LLM providers including:</p> <ul> <li>OpenAI</li> <li>Anthropic</li> <li>Google</li> <li>OpenRouter</li> <li>DeepSeek</li> <li>Ollama</li> </ul>"},{"location":"spring-boot/#getting-started","title":"Getting Started","text":""},{"location":"spring-boot/#1-add-dependency","title":"1. Add Dependency","text":"<p>Add the Koog Spring Boot starter and Ktor Client Engine  to your <code>build.gradle.kts</code> or <code>pom.xml</code>:</p> <pre><code>dependencies {\n    implementation(\"ai.koog:koog-spring-boot-starter:$koogVersion\")\n    implementation(\"io.ktor:ktor-client-okhttp-jvm:$ktorVersion\")\n}\n</code></pre>"},{"location":"spring-boot/#2-configure-providers","title":"2. Configure Providers","text":"<p>Configure your preferred LLM providers in <code>application.properties</code>:</p> <pre><code># OpenAI Configuration\nai.koog.openai.enabled=true\nai.koog.openai.api-key=${OPENAI_API_KEY}\nai.koog.openai.base-url=https://api.openai.com\n# Anthropic Configuration  \nai.koog.anthropic.enabled=true\nai.koog.anthropic.api-key=${ANTHROPIC_API_KEY}\nai.koog.anthropic.base-url=https://api.anthropic.com\n# Google Configuration\nai.koog.google.enabled=true\nai.koog.google.api-key=${GOOGLE_API_KEY}\nai.koog.google.base-url=https://generativelanguage.googleapis.com\n# OpenRouter Configuration\nai.koog.openrouter.enabled=true\nai.koog.openrouter.api-key=${OPENROUTER_API_KEY}\nai.koog.openrouter.base-url=https://openrouter.ai\n# DeepSeek Configuration\nai.koog.deepseek.enabled=true\nai.koog.deepseek.api-key=${DEEPSEEK_API_KEY}\nai.koog.deepseek.base-url=https://api.deepseek.com\n# Ollama Configuration (local - no API key required)\nai.koog.ollama.enabled=true\nai.koog.ollama.base-url=http://localhost:11434\n</code></pre> <p>Or using YAML format (<code>application.yml</code>):</p> <pre><code>ai:\n    koog:\n        openai:\n            enabled: true\n            api-key: ${OPENAI_API_KEY}\n            base-url: https://api.openai.com\n        anthropic:\n            enabled: true\n            api-key: ${ANTHROPIC_API_KEY}\n            base-url: https://api.anthropic.com\n        google:\n            enabled: true\n            api-key: ${GOOGLE_API_KEY}\n            base-url: https://generativelanguage.googleapis.com\n        openrouter:\n            enabled: true\n            api-key: ${OPENROUTER_API_KEY}\n            base-url: https://openrouter.ai\n        deepseek:\n            enabled: true\n            api-key: ${DEEPSEEK_API_KEY}\n            base-url: https://api.deepseek.com\n        ollama:\n            enabled: true # Set it to `true` explicitly to activate !!!\n            base-url: http://localhost:11434\n</code></pre> <p>Both <code>ai.koog.PROVIDER.api-key</code> and <code>ai.koog.PROVIDER.enabled</code> properties are used to activate the provider.</p> <p>If the provider supports the API Key (like OpenAI, Anthropic, Google), then <code>ai.koog.PROVIDER.enabled</code> is set to <code>true</code> by default.</p> <p>If the provider does not support the API Key, like Ollama, <code>ai.koog.PROVIDER.enabled</code> is set to <code>false</code> by default, and provider should be enabled explicitly in the application configuration.</p> <p>Provider's base urls are set to their default values in the Spring Boot starter, but you may override it in your application.</p> <p>Environment Variables</p> <p>It's recommended to use environment variables for API keys to keep them secure and out of version control. Spring configuration uses LLM provider's well-known environment variables. For example, setting the environment variable <code>OPENAI_API_KEY</code> is enough for OpenAI spring configuration to activate.</p> LLM Provider Environment Variables Open AI <code>OPENAI_API_KEY</code> Anthropic <code>ANTHROPIC_API_KEY</code> Google <code>GOOGLE_API_KEY</code> OpenRouter <code>OPENROUTER_API_KEY</code> DeepSeek <code>DEEPSEEK_API_KEY</code>"},{"location":"spring-boot/#3-inject-and-use","title":"3. Inject and Use","text":"<p>Inject the auto-configured executors into your services:</p> <pre><code>@Service\nclass AIService(\n    private val openAIExecutor: SingleLLMPromptExecutor?,\n    private val anthropicExecutor: SingleLLMPromptExecutor?\n) {\n\n    suspend fun generateResponse(input: String): String {\n        val prompt = prompt {\n            system(\"You are a helpful AI assistant\")\n            user(input)\n        }\n\n        return when {\n            openAIExecutor != null -&gt; {\n                val result = openAIExecutor.execute(prompt)\n                result.text\n            }\n            anthropicExecutor != null -&gt; {\n                val result = anthropicExecutor.execute(prompt)\n                result.text\n            }\n            else -&gt; throw IllegalStateException(\"No LLM provider configured\")\n        }\n    }\n}\n</code></pre>"},{"location":"spring-boot/#advanced-usage","title":"Advanced Usage","text":""},{"location":"spring-boot/#rest-controller-example","title":"REST Controller Example","text":"<p>Create a chat endpoint using auto-configured executors:</p> <pre><code>@RestController\n@RequestMapping(\"/api/chat\")\nclass ChatController(\n    private val anthropicExecutor: SingleLLMPromptExecutor?\n) {\n\n    @PostMapping\n    suspend fun chat(@RequestBody request: ChatRequest): ResponseEntity&lt;ChatResponse&gt; {\n        return if (anthropicExecutor != null) {\n            try {\n                val prompt = prompt {\n                    system(\"You are a helpful assistant\")\n                    user(request.message)\n                }\n\n                val result = anthropicExecutor.execute(prompt)\n                ResponseEntity.ok(ChatResponse(result.text))\n            } catch (e: Exception) {\n                ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)\n                    .body(ChatResponse(\"Error processing request\"))\n            }\n        } else {\n            ResponseEntity.status(HttpStatus.SERVICE_UNAVAILABLE)\n                .body(ChatResponse(\"AI service not configured\"))\n        }\n    }\n}\n\ndata class ChatRequest(val message: String)\ndata class ChatResponse(val response: String)\n</code></pre>"},{"location":"spring-boot/#multiple-provider-support","title":"Multiple Provider Support","text":"<p>Handle multiple providers with fallback logic:</p> <pre><code>@Service\nclass RobustAIService(\n    private val openAIExecutor: SingleLLMPromptExecutor?,\n    private val anthropicExecutor: SingleLLMPromptExecutor?,\n    private val openRouterExecutor: SingleLLMPromptExecutor?\n) {\n\n    suspend fun generateWithFallback(input: String): String {\n        val prompt = prompt {\n            system(\"You are a helpful AI assistant\")\n            user(input)\n        }\n\n        val executors = listOfNotNull(openAIExecutor, anthropicExecutor, openRouterExecutor)\n\n        for (executor in executors) {\n            try {\n                val result = executor.execute(prompt)\n                return result.text\n            } catch (e: Exception) {\n                logger.warn(\"Executor failed, trying next: ${e.message}\")\n                continue\n            }\n        }\n\n        throw IllegalStateException(\"All AI providers failed\")\n    }\n\n    companion object {\n        private val logger = LoggerFactory.getLogger(RobustAIService::class.java)\n    }\n}\n</code></pre>"},{"location":"spring-boot/#configuration-properties","title":"Configuration Properties","text":"<p>You can also inject configuration properties for custom logic:</p> <pre><code>@Service\nclass ConfigurableAIService(\n    private val openAIExecutor: SingleLLMPromptExecutor?,\n    @Value(\"\\${ai.koog.openai.api-key:}\") private val openAIKey: String\n) {\n\n    fun isOpenAIConfigured(): Boolean = openAIKey.isNotBlank() &amp;&amp; openAIExecutor != null\n\n    suspend fun processIfConfigured(input: String): String? {\n        return if (isOpenAIConfigured()) {\n            val result = openAIExecutor!!.execute(prompt { user(input) })\n            result.text\n        } else {\n            null\n        }\n    }\n}\n</code></pre>"},{"location":"spring-boot/#configuration-reference","title":"Configuration Reference","text":""},{"location":"spring-boot/#available-properties","title":"Available Properties","text":"Property Description Bean Condition Default <code>ai.koog.openai.api-key</code> OpenAI API key Required for <code>openAIExecutor</code> bean - <code>ai.koog.openai.base-url</code> OpenAI base URL Optional <code>https://api.openai.com</code> <code>ai.koog.anthropic.api-key</code> Anthropic API key Required for <code>anthropicExecutor</code> bean - <code>ai.koog.anthropic.base-url</code> Anthropic base URL Optional <code>https://api.anthropic.com</code> <code>ai.koog.google.api-key</code> Google API key Required for <code>googleExecutor</code> bean - <code>ai.koog.google.base-url</code> Google base URL Optional <code>https://generativelanguage.googleapis.com</code> <code>ai.koog.openrouter.api-key</code> OpenRouter API key Required for <code>openRouterExecutor</code> bean - <code>ai.koog.openrouter.base-url</code> OpenRouter base URL Optional <code>https://openrouter.ai</code> <code>ai.koog.deepseek.api-key</code> DeepSeek API key Required for <code>deepSeekExecutor</code> bean - <code>ai.koog.deepseek.base-url</code> DeepSeek base URL Optional <code>https://api.deepseek.com</code> <code>ai.koog.ollama.base-url</code> Ollama base URL Any <code>ai.koog.ollama.*</code> property activates <code>ollamaExecutor</code> bean <code>http://localhost:11434</code>"},{"location":"spring-boot/#bean-names","title":"Bean Names","text":"<p>The auto-configuration creates the following beans (when configured):</p> <ul> <li><code>openAIExecutor</code> - OpenAI executor (requires <code>ai.koog.openai.api-key</code>)</li> <li><code>anthropicExecutor</code> - Anthropic executor (requires <code>ai.koog.anthropic.api-key</code>)</li> <li><code>googleExecutor</code> - Google executor (requires <code>ai.koog.google.api-key</code>)</li> <li><code>openRouterExecutor</code> - OpenRouter executor (requires <code>ai.koog.openrouter.api-key</code>)</li> <li><code>deepSeekExecutor</code> - DeepSeek executor (requires <code>ai.koog.deepseek.api-key</code>)</li> <li><code>ollamaExecutor</code> - Ollama executor (requires any <code>ai.koog.ollama.*</code> property)</li> </ul>"},{"location":"spring-boot/#troubleshooting","title":"Troubleshooting","text":""},{"location":"spring-boot/#common-issues","title":"Common Issues","text":"<p>Bean not found error:</p> <pre><code>No qualifying bean of type 'SingleLLMPromptExecutor' available\n</code></pre> <p>Solution: Ensure you have configured at least one provider in your properties file.</p> <p>Multiple beans error:</p> <pre><code>Multiple qualifying beans of type 'SingleLLMPromptExecutor' available\n</code></pre> <p>Solution: Use <code>@Qualifier</code> to specify which bean you want:</p> <pre><code>@Service\nclass MyService(\n    @Qualifier(\"openAIExecutor\") private val openAIExecutor: SingleLLMPromptExecutor,\n    @Qualifier(\"anthropicExecutor\") private val anthropicExecutor: SingleLLMPromptExecutor\n) {\n    // ...\n}\n</code></pre> <p>API key not loaded:</p> <pre><code>API key is required but not provided\n</code></pre> <p>Solution: Check that your environment variables are properly set and accessible to your Spring Boot application.</p>"},{"location":"spring-boot/#best-practices","title":"Best Practices","text":"<ol> <li>Environment Variables: Always use environment variables for API keys</li> <li>Nullable Injection: Use nullable types (<code>SingleLLMPromptExecutor?</code>) to handle cases where providers aren't    configured</li> <li>Fallback Logic: Implement fallback mechanisms when using multiple providers</li> <li>Error Handling: Always wrap executor calls in try-catch blocks for production code</li> <li>Testing: Use mocks in tests to avoid making actual API calls</li> <li>Configuration Validation: Check if executors are available before using them</li> </ol>"},{"location":"spring-boot/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about the basic agents to build minimal AI workflows</li> <li>Explore complex workflow agents for advanced use cases</li> <li>See the tools overview to extend your agents' capabilities</li> <li>Check out examples for real-world implementations</li> <li>Read the glossary to understand the framework better</li> </ul>"},{"location":"streaming-api/","title":"Streaming API","text":""},{"location":"streaming-api/#introduction","title":"Introduction","text":"<p>Koog\u2019s Streaming API lets you consume LLM output incrementally as a <code>Flow&lt;StreamFrame&gt;</code>. Instead of waiting for a full response, your code can:</p> <ul> <li>render assistant text as it arrives,</li> <li>detect tool calls live and act on them,</li> <li>know when a stream ends and why.</li> </ul> <p>The stream carries typed frames:</p> <ul> <li><code>StreamFrame.Append(text: String)</code> \u2014 incremental assistant text</li> <li><code>StreamFrame.ToolCall(id: String?, name: String, content: String)</code> \u2014 tool invocation (combined safely)</li> <li><code>StreamFrame.End(finishReason: String?)</code> \u2014 end-of-stream marker</li> </ul> <p>Helpers are provided to extract plain text, convert frames to <code>Message.Response</code> objects, and safely combine chunked tool calls.</p>"},{"location":"streaming-api/#streaming-api-overview","title":"Streaming API overview","text":"<p>With streaming you can:</p> <ul> <li>Process data as it arrives (improves UI responsiveness)</li> <li>Parse structured info on the fly (Markdown/JSON/etc.)</li> <li>Emit objects as they complete</li> <li>Trigger tools in real time</li> </ul> <p>You can operate either on the frames themselves or on plain text derived from frames.</p>"},{"location":"streaming-api/#usage","title":"Usage","text":""},{"location":"streaming-api/#working-with-frames-directly","title":"Working with frames directly","text":"<p>This is the most general approach: react to each frame kind.</p> <pre><code>llm.writeSession {\n    appendPrompt { user(\"Tell me a joke, then call a tool with JSON args.\") }\n\n    val stream = requestLLMStreaming() // Flow&lt;StreamFrame&gt;\n\n    stream.collect { frame -&gt;\n        when (frame) {\n            is StreamFrame.Append -&gt; print(frame.text)\n            is StreamFrame.ToolCall -&gt; {\n                println(\"\\n\ud83d\udd27 Tool call: ${frame.name} args=${frame.content}\")\n                // Optionally parse lazily:\n                // val json = frame.contentJson\n            }\n            is StreamFrame.End -&gt; println(\"\\n[END] reason=${frame.finishReason}\")\n        }\n    }\n}\n</code></pre> <p>It is important to note that you can parse the output by working directly with a raw string stream. This approach gives you more flexibility and control over the parsing process.</p> <p>Here is a raw string stream with the Markdown definition of the output structure:</p> <pre><code>fun markdownBookDefinition(): MarkdownStructureDefinition {\n    return MarkdownStructureDefinition(\"name\", schema = { /*...*/ })\n}\n\nval mdDefinition = markdownBookDefinition()\n\nllm.writeSession {\n    val stream = requestLLMStreaming(mdDefinition)\n    // Access the raw string chunks directly\n    stream.collect { chunk -&gt;\n        // Process each chunk of text as it arrives\n        println(\"Received chunk: $chunk\") // The chunks together will be structured as a text following the mdDefinition schema\n    }\n}\n</code></pre>"},{"location":"streaming-api/#working-with-a-raw-text-stream-derived","title":"Working with a raw text stream (derived)","text":"<p>If you have existing streaming parsers that expect <code>Flow&lt;String&gt;</code>,  derive text chunks via <code>filterTextOnly()</code> or collect them with <code>collectText()</code>.</p> <pre><code>llm.writeSession {\n    val frames = requestLLMStreaming()\n\n    // Stream text chunks as they come:\n    frames.filterTextOnly().collect { chunk -&gt; print(chunk) }\n\n    // Or, gather all text into one String after End:\n    val fullText = frames.collectText()\n    println(\"\\n---\\n$fullText\")\n}\n</code></pre>"},{"location":"streaming-api/#listening-to-stream-events-in-event-handlers","title":"Listening to stream events in event handlers","text":"<p>You can listen to stream events in agent event handlers.</p> <pre><code>handleEvents {\n    onToolCallStarting { context -&gt;\n        println(\"\\n\ud83d\udd27 Using ${context.toolName} with ${context.toolArgs}... \")\n    }\n    onLLMStreamingFrameReceived { context -&gt;\n        (context.streamFrame as? StreamFrame.Append)?.let { frame -&gt;\n            print(frame.text)\n        }\n    }\n    onLLMStreamingFailed { context -&gt; \n        println(\"\u274c Error: ${context.error}\")\n    }\n    onLLMStreamingCompleted {\n        println(\"\ud83c\udfc1 Done\")\n    }\n}\n</code></pre>"},{"location":"streaming-api/#converting-frames-to-messageresponse","title":"Converting frames to <code>Message.Response</code>","text":"<p>You can transform a collected list of frames to standard message objects: - <code>toAssistantMessageOrNull()</code> - <code>toToolCallMessages()</code> - <code>toMessageResponses()</code></p>"},{"location":"streaming-api/#examples","title":"Examples","text":""},{"location":"streaming-api/#structured-data-while-streaming-markdown-example","title":"Structured data while streaming (Markdown example)","text":"<p>Although it is possible to work with a raw string stream, it is often more convenient to work with structured data.</p> <p>The structured data approach includes the following key components:</p> <ol> <li>MarkdownStructureDefinition: a class to help you define the schema and examples for structured data in    Markdown format.</li> <li>markdownStreamingParser: a function to create a parser that processes a stream of Markdown chunks and emits    events.</li> </ol> <p>The sections below provide step-by-step instructions and code samples related to processing a stream of structured data. </p>"},{"location":"streaming-api/#1-define-your-data-structure","title":"1. Define your data structure","text":"<p>First, define a data class to represent your structured data:</p> <pre><code>@Serializable\ndata class Book(\n    val title: String,\n    val author: String,\n    val description: String\n)\n</code></pre>"},{"location":"streaming-api/#2-define-the-markdown-structure","title":"2. Define the Markdown structure","text":"<p>Create a definition that specifies how your data should be structured in Markdown with the <code>MarkdownStructureDefinition</code> class:</p> <pre><code>fun markdownBookDefinition(): MarkdownStructureDefinition {\n    return MarkdownStructureDefinition(\"bookList\", schema = {\n        markdown {\n            header(1, \"title\")\n            bulleted {\n                item(\"author\")\n                item(\"description\")\n            }\n        }\n    }, examples = {\n        markdown {\n            header(1, \"The Great Gatsby\")\n            bulleted {\n                item(\"F. Scott Fitzgerald\")\n                item(\"A novel set in the Jazz Age that tells the story of Jay Gatsby's unrequited love for Daisy Buchanan.\")\n            }\n        }\n    })\n}\n</code></pre>"},{"location":"streaming-api/#3-create-a-parser-for-your-data-structure","title":"3. Create a parser for your data structure","text":"<p>The <code>markdownStreamingParser</code> provides several handlers for different Markdown elements:</p> <pre><code>markdownStreamingParser {\n    // Handle level 1 headings (level ranges from 1 to 6)\n    onHeader(1) { headerText -&gt; }\n    // Handle bullet points\n    onBullet { bulletText -&gt; }\n    // Handle code blocks\n    onCodeBlock { codeBlockContent -&gt; }\n    // Handle lines matching a regex pattern\n    onLineMatching(Regex(\"pattern\")) { line -&gt; }\n    // Handle the end of the stream\n    onFinishStream { remainingText -&gt; }\n}\n</code></pre> <p>Using the defined handlers, you can implement a function that parses the Markdown stream and emits your data objects  with the <code>markdownStreamingParser</code> function.</p> <pre><code>fun parseMarkdownStreamToBooks(markdownStream: Flow&lt;StreamFrame&gt;): Flow&lt;Book&gt; {\n   return flow {\n      markdownStreamingParser {\n         var currentBookTitle = \"\"\n         val bulletPoints = mutableListOf&lt;String&gt;()\n\n         // Handle the event of receiving the Markdown header in the response stream\n         onHeader(1) { headerText -&gt;\n            // If there was a previous book, emit it\n            if (currentBookTitle.isNotEmpty() &amp;&amp; bulletPoints.isNotEmpty()) {\n               val author = bulletPoints.getOrNull(0) ?: \"\"\n               val description = bulletPoints.getOrNull(1) ?: \"\"\n               emit(Book(currentBookTitle, author, description))\n            }\n\n            currentBookTitle = headerText\n            bulletPoints.clear()\n         }\n\n         // Handle the event of receiving the Markdown bullets list in the response stream\n         onBullet { bulletText -&gt;\n            bulletPoints.add(bulletText)\n         }\n\n         // Handle the end of the response stream\n         onFinishStream {\n            // Emit the last book, if present\n            if (currentBookTitle.isNotEmpty() &amp;&amp; bulletPoints.isNotEmpty()) {\n               val author = bulletPoints.getOrNull(0) ?: \"\"\n               val description = bulletPoints.getOrNull(1) ?: \"\"\n               emit(Book(currentBookTitle, author, description))\n            }\n         }\n      }.parseStream(markdownStream.filterTextOnly())\n   }\n}\n</code></pre>"},{"location":"streaming-api/#4-use-the-parser-in-your-agent-strategy","title":"4. Use the parser in your agent strategy","text":"<pre><code>val agentStrategy = strategy&lt;String, List&lt;Book&gt;&gt;(\"library-assistant\") {\n   // Describe the node containing the output stream parsing\n   val getMdOutput by node&lt;String, List&lt;Book&gt;&gt; { booksDescription -&gt;\n      val books = mutableListOf&lt;Book&gt;()\n      val mdDefinition = markdownBookDefinition()\n\n      llm.writeSession {\n         appendPrompt { user(booksDescription) }\n         // Initiate the response stream in the form of the definition `mdDefinition`\n         val markdownStream = requestLLMStreaming(mdDefinition)\n         // Call the parser with the result of the response stream and perform actions with the result\n         parseMarkdownStreamToBooks(markdownStream).collect { book -&gt;\n            books.add(book)\n            println(\"Parsed Book: ${book.title} by ${book.author}\")\n         }\n      }\n\n      books\n   }\n   // Describe the agent's graph making sure the node is accessible\n   edge(nodeStart forwardTo getMdOutput)\n   edge(getMdOutput forwardTo nodeFinish)\n}\n</code></pre>"},{"location":"streaming-api/#advanced-usage-streaming-with-tools","title":"Advanced usage: Streaming with tools","text":"<p>You can also use the Streaming API with tools to process data as it arrives.  The following sections provide a brief step-by-step guide on how to define a tool and use it with streaming data.</p>"},{"location":"streaming-api/#1-define-a-tool-for-your-data-structure","title":"1. Define a tool for your data structure","text":"<pre><code>@Serializable\ndata class Book(\n   val title: String,\n   val author: String,\n   val description: String\n)\n\nclass BookTool(): SimpleTool&lt;Book&gt;(\n    argsSerializer = Book.serializer(),\n    name = NAME,\n    description = \"A tool to parse book information from Markdown\"\n) {\n\n    companion object { const val NAME = \"book\" }\n\n    override suspend fun execute(args: Book): String {\n        println(\"${args.title} by ${args.author}:\\n ${args.description}\")\n        return \"Done\"\n    }\n}\n</code></pre>"},{"location":"streaming-api/#2-use-the-tool-with-streaming-data","title":"2. Use the tool with streaming data","text":"<pre><code>val agentStrategy = strategy&lt;String, Unit&gt;(\"library-assistant\") {\n   val getMdOutput by node&lt;String, Unit&gt; { input -&gt;\n      val mdDefinition = markdownBookDefinition()\n\n      llm.writeSession {\n         appendPrompt { user(input) }\n         val markdownStream = requestLLMStreaming(mdDefinition)\n\n         parseMarkdownStreamToBooks(markdownStream).collect { book -&gt;\n            callToolRaw(BookTool.NAME, book)\n            /* Other possible options:\n                callTool(BookTool::class, book)\n                callTool&lt;BookTool&gt;(book)\n                findTool(BookTool::class).execute(book)\n            */\n         }\n\n         // We can make parallel tool calls\n         parseMarkdownStreamToBooks(markdownStream).toParallelToolCallsRaw(toolClass=BookTool::class).collect {\n            println(\"Tool call result: $it\")\n         }\n      }\n   }\n\n   edge(nodeStart forwardTo getMdOutput)\n   edge(getMdOutput forwardTo nodeFinish)\n }\n</code></pre>"},{"location":"streaming-api/#3-register-the-tool-in-your-agent-configuration","title":"3. Register the tool in your agent configuration","text":"<pre><code>val toolRegistry = ToolRegistry {\n   tool(BookTool())\n}\n\nval runner = AIAgent(\n   promptExecutor = simpleOpenAIExecutor(token),\n   toolRegistry = toolRegistry,\n   strategy = agentStrategy,\n   agentConfig = agentConfig\n)\n</code></pre>"},{"location":"streaming-api/#best-practices","title":"Best practices","text":"<ol> <li> <p>Define clear structures: create clear and unambiguous markdown structures for your data.</p> </li> <li> <p>Provide good examples: include comprehensive examples in your <code>MarkdownStructureDefinition</code> to guide the LLM.</p> </li> <li> <p>Handle incomplete data: always check for null or empty values when parsing data from the stream.</p> </li> <li> <p>Clean up resources: use the <code>onFinishStream</code> handler to clean up resources and process any remaining data.</p> </li> <li> <p>Handle errors: implement proper error handling for malformed Markdown or unexpected data.</p> </li> <li> <p>Testing: test your parser with various input scenarios, including partial chunks and malformed input.</p> </li> <li> <p>Parallel processing: for independent data items, consider using parallel tool calls for better performance.</p> </li> </ol>"},{"location":"structured-output/","title":"Structured output","text":""},{"location":"structured-output/#introduction","title":"Introduction","text":"<p>The Structured Output API provides a way to ensure that responses from Large Language Models (LLMs)  conform to specific data structures. This is crucial for building reliable AI applications where you need predictable, well-formatted data rather than free-form text.</p> <p>This page explains how to use this API to define data structures, generate schemas, and  request structured responses from LLMs.</p>"},{"location":"structured-output/#key-components-and-concepts","title":"Key components and concepts","text":"<p>The Structured Output API consists of several key components:</p> <ol> <li>Data structure definition: Kotlin data classes annotated with kotlinx.serialization and LLM-specific annotations.</li> <li>JSON Schema generation: tools to generate JSON schemas from Kotlin data classes.</li> <li>Structured LLM requests: methods to request responses from LLMs that conform to the defined structures.</li> <li>Response handling: processing and validating the structured responses.</li> </ol>"},{"location":"structured-output/#defining-data-structures","title":"Defining data structures","text":"<p>The first step in using the Structured Output API is to define your data structures using Kotlin data classes.</p>"},{"location":"structured-output/#basic-structure","title":"Basic structure","text":"<pre><code>@Serializable\n@SerialName(\"WeatherForecast\")\n@LLMDescription(\"Weather forecast for a given location\")\ndata class WeatherForecast(\n    @property:LLMDescription(\"Temperature in Celsius\")\n    val temperature: Int,\n    @property:LLMDescription(\"Weather conditions (e.g., sunny, cloudy, rainy)\")\n    val conditions: String,\n    @property:LLMDescription(\"Chance of precipitation in percentage\")\n    val precipitation: Int\n)\n</code></pre>"},{"location":"structured-output/#key-annotations","title":"Key annotations","text":"<ul> <li><code>@Serializable</code>: required for kotlinx.serialization to work with the class.</li> <li><code>@SerialName</code>: specifies the name to use during serialization.</li> <li><code>@LLMDescription</code>: provides a description of the class for the LLM. For field annotations, use <code>@property:LLMDescription</code>.</li> </ul>"},{"location":"structured-output/#supported-features","title":"Supported features","text":"<p>The API supports a wide range of data structure features:</p>"},{"location":"structured-output/#nested-classes","title":"Nested classes","text":"<pre><code>@Serializable\n@SerialName(\"WeatherForecast\")\ndata class WeatherForecast(\n    // Other fields\n    @property:LLMDescription(\"Coordinates of the location\")\n    val latLon: LatLon\n) {\n    @Serializable\n    @SerialName(\"LatLon\")\n    data class LatLon(\n        @property:LLMDescription(\"Latitude of the location\")\n        val lat: Double,\n        @property:LLMDescription(\"Longitude of the location\")\n        val lon: Double\n    )\n}\n</code></pre>"},{"location":"structured-output/#collections-lists-and-maps","title":"Collections (lists and maps)","text":"<pre><code>@Serializable\n@SerialName(\"WeatherForecast\")\ndata class WeatherForecast(\n    // Other fields\n    @property:LLMDescription(\"List of news articles\")\n    val news: List&lt;WeatherNews&gt;,\n    @property:LLMDescription(\"Map of weather sources\")\n    val sources: Map&lt;String, WeatherSource&gt;\n)\n</code></pre>"},{"location":"structured-output/#enums","title":"Enums","text":"<pre><code>@Serializable\n@SerialName(\"Pollution\")\nenum class Pollution { Low, Medium, High }\n</code></pre>"},{"location":"structured-output/#polymorphism-with-sealed-classes","title":"Polymorphism with sealed classes","text":"<pre><code>@Serializable\n@SerialName(\"WeatherAlert\")\nsealed class WeatherAlert {\n    abstract val severity: Severity\n    abstract val message: String\n\n    @Serializable\n    @SerialName(\"Severity\")\n    enum class Severity { Low, Moderate, Severe, Extreme }\n\n    @Serializable\n    @SerialName(\"StormAlert\")\n    data class StormAlert(\n        override val severity: Severity,\n        override val message: String,\n        @property:LLMDescription(\"Wind speed in km/h\")\n        val windSpeed: Double\n    ) : WeatherAlert()\n\n    @Serializable\n    @SerialName(\"FloodAlert\")\n    data class FloodAlert(\n        override val severity: Severity,\n        override val message: String,\n        @property:LLMDescription(\"Expected rainfall in mm\")\n        val expectedRainfall: Double\n    ) : WeatherAlert()\n}\n</code></pre>"},{"location":"structured-output/#providing-examples","title":"Providing examples","text":"<p>You can provide examples to help the LLM understand the expected format:</p> <pre><code>val exampleForecasts = listOf(\n  WeatherForecast(\n    news = listOf(WeatherNews(0.0), WeatherNews(5.0)),\n    sources = mutableMapOf(\n      \"openweathermap\" to WeatherSource(Url(\"https://api.openweathermap.org/data/2.5/weather\")),\n      \"googleweather\" to WeatherSource(Url(\"https://weather.google.com\"))\n    )\n    // Other fields\n  ),\n  WeatherForecast(\n    news = listOf(WeatherNews(25.0), WeatherNews(35.0)),\n    sources = mutableMapOf(\n      \"openweathermap\" to WeatherSource(Url(\"https://api.openweathermap.org/data/2.5/weather\")),\n      \"googleweather\" to WeatherSource(Url(\"https://weather.google.com\"))\n    )\n  )\n)\n</code></pre>"},{"location":"structured-output/#requesting-structured-responses","title":"Requesting structured responses","text":"<p>There are three main layers where you can use structured output in Koog:</p> <ol> <li>Prompt executor layer: Make direct LLM calls using a prompt executor</li> <li>Agent LLM context layer: Use within agent sessions for conversational contexts</li> <li>Node layer: Create reusable agent nodes with structured output capabilities</li> </ol>"},{"location":"structured-output/#layer-1-prompt-executor","title":"Layer 1: Prompt executor","text":"<p>The prompt executor layer provides the most direct way to make structured LLM calls. Use the <code>executeStructured</code> method for single, standalone requests:</p> <p>This method executes a prompt and ensures the response is properly structured by:</p> <ul> <li>Automatically selecting the best structured output approach based on model capabilities</li> <li>Injecting structured output instructions into the original prompt when needed</li> <li>Using native structured output support when available</li> <li>Providing automatic error correction through an auxiliary LLM when parsing fails</li> </ul> <p>Here is an example of using the <code>executeStructured</code> method:</p> <pre><code>// Define a simple, single-provider prompt executor\nval promptExecutor = simpleOpenAIExecutor(System.getenv(\"OPENAI_KEY\"))\n\n// Make an LLM call that returns a structured response\nval structuredResponse = promptExecutor.executeStructured&lt;WeatherForecast&gt;(\n        // Define the prompt (both system and user messages)\n        prompt = prompt(\"structured-data\") {\n            system(\n                \"\"\"\n                You are a weather forecasting assistant.\n                When asked for a weather forecast, provide a realistic but fictional forecast.\n                \"\"\".trimIndent()\n            )\n            user(\n              \"What is the weather forecast for Amsterdam?\"\n            )\n        },\n        // Define the main model that will execute the request\n        model = OpenAIModels.Chat.GPT4oMini,\n        // Optional: provide examples to help the model understand the format\n        examples = exampleForecasts,\n        // Optional: provide a fixing parser for error correction\n        fixingParser = StructureFixingParser(\n            model = OpenAIModels.Chat.GPT4o,\n            retries = 3\n        )\n    )\n</code></pre> <p>The <code>executeStructured</code> method takes the following arguments:</p> Name Data type Required Default Description <code>prompt</code> Prompt Yes The prompt to execute. For more information, see Prompts. <code>model</code> LLModel Yes The main model to execute the prompt. <code>examples</code> List No <code>emptyList()</code> Optional list of examples to help the model understand the expected format. <code>fixingParser</code> StructureFixingParser? No <code>null</code> Optional parser that handles malformed responses by using an auxiliary LLM to intelligently fix parsing errors. <p>The method returns a <code>Result&lt;StructuredResponse&lt;T&gt;&gt;</code> containing either the successfully parsed structured data or an error.</p>"},{"location":"structured-output/#layer-2-agent-llm-context","title":"Layer 2: Agent LLM context","text":"<p>The agent LLM context layer allows you to request structured responses within agent sessions. This is useful for building conversational agents that need structured data at specific points in their flow.</p> <p>Use the <code>requestLLMStructured</code> method within a <code>writeSession</code> for agent-based interactions:</p> <pre><code>val structuredResponse = llm.writeSession {\n    requestLLMStructured&lt;WeatherForecast&gt;(\n        examples = exampleForecasts,\n        fixingParser = StructureFixingParser(\n            model = OpenAIModels.Chat.GPT4o,\n            retries = 3\n        )\n    )\n}\n</code></pre> <p>The <code>fixingParser</code> parameter specifies a configuration for handling malformed responses through auxiliary LLM processing during retries. This helps ensure that you always get a valid response.</p>"},{"location":"structured-output/#integrating-with-agent-strategies","title":"Integrating with agent strategies","text":"<p>You can integrate structured data processing into your agent strategies:</p> <pre><code>val agentStrategy = strategy(\"weather-forecast\") {\n    val setup by nodeLLMRequest()\n\n    val getStructuredForecast by node&lt;Message.Response, String&gt; { _ -&gt;\n        val structuredResponse = llm.writeSession {\n            requestLLMStructured&lt;WeatherForecast&gt;(\n                fixingParser = StructureFixingParser(\n                    model = OpenAIModels.Chat.GPT4o,\n                    retries = 3\n                )\n            )\n        }\n\n        \"\"\"\n        Response structure:\n        $structuredResponse\n        \"\"\".trimIndent()\n    }\n\n    edge(nodeStart forwardTo setup)\n    edge(setup forwardTo getStructuredForecast)\n    edge(getStructuredForecast forwardTo nodeFinish)\n}\n</code></pre>"},{"location":"structured-output/#layer-3-node-layer","title":"Layer 3: Node layer","text":"<p>The node layer provides the highest level of abstraction for structured output in agent workflows. Use <code>nodeLLMRequestStructured</code> to create reusable agent nodes that handle structured data.</p> <p>This creates an agent node that: - Accepts a <code>String</code> input (user message) - Appends the message to the LLM prompt - Requests structured output from the LLM - Returns <code>Result&lt;StructuredResponse&lt;MyStruct&gt;&gt;</code></p>"},{"location":"structured-output/#node-layer-example","title":"Node layer example","text":"<pre><code>val agentStrategy = strategy(\"weather-forecast\") {\n    val setup by node&lt;Unit, String&gt; { _ -&gt;\n        \"Please provide a weather forecast for Amsterdam\"\n    }\n\n    // Create a structured output node using delegate syntax\n    val getWeatherForecast by nodeLLMRequestStructured&lt;WeatherForecast&gt;(\n        name = \"forecast-node\",\n        examples = exampleForecasts,\n        fixingParser = StructureFixingParser(\n            model = OpenAIModels.Chat.GPT4o,\n            retries = 3\n        )\n    )\n\n    val processResult by node&lt;Result&lt;StructuredResponse&lt;WeatherForecast&gt;&gt;, String&gt; { result -&gt;\n        when {\n            result.isSuccess -&gt; {\n                val forecast = result.getOrNull()?.data\n                \"Weather forecast: $forecast\"\n            }\n            result.isFailure -&gt; {\n                \"Failed to get structured forecast: ${result.exceptionOrNull()?.message}\"\n            }\n            else -&gt; \"Unknown result state\"\n        }\n    }\n\n    edge(nodeStart forwardTo setup)\n    edge(setup forwardTo getWeatherForecast)\n    edge(getWeatherForecast forwardTo processResult)\n    edge(processResult forwardTo nodeFinish)\n}\n</code></pre>"},{"location":"structured-output/#full-code-sample","title":"Full code sample","text":"<p>Here is a full example of using the Structured Output API:</p> <pre><code>// Note: Import statements are omitted for brevity\n@Serializable\n@SerialName(\"SimpleWeatherForecast\")\n@LLMDescription(\"Simple weather forecast for a location\")\ndata class SimpleWeatherForecast(\n    @property:LLMDescription(\"Location name\")\n    val location: String,\n    @property:LLMDescription(\"Temperature in Celsius\")\n    val temperature: Int,\n    @property:LLMDescription(\"Weather conditions (e.g., sunny, cloudy, rainy)\")\n    val conditions: String\n)\n\nval token = System.getenv(\"OPENAI_KEY\") ?: error(\"Environment variable OPENAI_KEY is not set\")\n\nfun main(): Unit = runBlocking {\n    // Create sample forecasts\n    val exampleForecasts = listOf(\n        SimpleWeatherForecast(\n            location = \"New York\",\n            temperature = 25,\n            conditions = \"Sunny\"\n        ),\n        SimpleWeatherForecast(\n            location = \"London\",\n            temperature = 18,\n            conditions = \"Cloudy\"\n        )\n    )\n\n    // Generate JSON Schema\n    val forecastStructure = JsonStructure.create&lt;SimpleWeatherForecast&gt;(\n        schemaGenerator = BasicJsonSchemaGenerator.Default,\n        examples = exampleForecasts\n    )\n\n    // Define the agent strategy\n    val agentStrategy = strategy(\"weather-forecast\") {\n        val setup by nodeLLMRequest()\n\n        val getStructuredForecast by node&lt;Message.Response, String&gt; { _ -&gt;\n            val structuredResponse = llm.writeSession {\n                requestLLMStructured&lt;SimpleWeatherForecast&gt;()\n            }\n\n            \"\"\"\n            Response structure:\n            $structuredResponse\n            \"\"\".trimIndent()\n        }\n\n        edge(nodeStart forwardTo setup)\n        edge(setup forwardTo getStructuredForecast)\n        edge(getStructuredForecast forwardTo nodeFinish)\n    }\n\n\n    // Configure and run the agent\n    val agentConfig = AIAgentConfig(\n        prompt = prompt(\"weather-forecast-prompt\") {\n            system(\n                \"\"\"\n                You are a weather forecasting assistant.\n                When asked for a weather forecast, provide a realistic but fictional forecast.\n                \"\"\".trimIndent()\n            )\n        },\n        model = OpenAIModels.Chat.GPT4o,\n        maxAgentIterations = 5\n    )\n\n    val runner = AIAgent(\n        promptExecutor = simpleOpenAIExecutor(token),\n        toolRegistry = ToolRegistry.EMPTY,\n        strategy = agentStrategy,\n        agentConfig = agentConfig\n    )\n\n    runner.run(\"Get weather forecast for Paris\")\n}\n</code></pre>"},{"location":"structured-output/#advanced-usage","title":"Advanced usage","text":"<p>The examples above demonstrate the simplified API that automatically selects the best structured output approach based on model capabilities.  For more control over the structured output process, you can use the advanced API with manual schema creation and provider-specific configurations.</p>"},{"location":"structured-output/#manual-schema-creation-and-configuration","title":"Manual schema creation and configuration","text":"<p>Instead of relying on automatic schema generation, you can create schemas explicitly using <code>JsonStructure.create</code> and configure structured output behavior manually via the <code>StructuredOutput</code> class.</p> <p>The key difference is that instead of passing simple parameters like <code>examples</code> and <code>fixingParser</code>, you create a <code>StructuredRequestConfig</code> object that allows fine-grained control over:</p> <ul> <li>Schema generation: Choose specific generators (Standard, Basic, or Provider-specific)</li> <li>Output modes: Native structured output support vs Manual prompting</li> <li>Provider mapping: Different configurations for different LLM providers</li> <li>Fallback strategies: Default behavior when provider-specific config is unavailable</li> </ul> <pre><code>// Create different schema structures with different generators\nval genericStructure = JsonStructure.create&lt;WeatherForecast&gt;(\n    schemaGenerator = StandardJsonSchemaGenerator,\n    examples = exampleForecasts\n)\n\nval openAiStructure = JsonStructure.create&lt;WeatherForecast&gt;(\n    schemaGenerator = OpenAIBasicJsonSchemaGenerator,\n    examples = exampleForecasts\n)\n\nval promptExecutor = simpleOpenAIExecutor(System.getenv(\"OPENAI_KEY\"))\n\n// The advanced API uses StructuredRequestConfig instead of simple parameters\nval structuredResponse = promptExecutor.executeStructured(\n    prompt = prompt(\"structured-data\") {\n        system(\"You are a weather forecasting assistant.\")\n        user(\"What is the weather forecast for Amsterdam?\")\n    },\n    model = OpenAIModels.Chat.GPT4oMini,\n    config = StructuredRequestConfig(\n        byProvider = mapOf(\n            LLMProvider.OpenAI to StructuredRequest.Native(openAiStructure),\n        ),\n        default = StructuredRequest.Manual(genericStructure),\n        fixingParser = StructureFixingParser(\n            model = AnthropicModels.Haiku_3_5,\n            retries = 2\n        )\n    )\n)\n</code></pre>"},{"location":"structured-output/#schema-generators","title":"Schema generators","text":"<p>Different schema generators are available depending on your needs:</p> <ul> <li>StandardJsonSchemaGenerator: Full JSON Schema with support for polymorphism, definitions, and recursive references</li> <li>BasicJsonSchemaGenerator: Simplified schema without polymorphism support, compatible with more models  </li> <li>Provider-specific generators: Optimized schemas for specific LLM providers (OpenAI, Google, etc.)</li> </ul>"},{"location":"structured-output/#usage-across-all-layers","title":"Usage across all layers","text":"<p>The advanced configuration works consistently across all three layers of the API. The method names remain the same, only the parameter changes from simple arguments to the more advanced <code>StructuredOutputConfig</code>:</p> <ul> <li>Prompt executor: <code>executeStructured(prompt, model, config: StructuredRequestConfig&lt;T&gt;)</code></li> <li>Agent LLM context: <code>requestLLMStructured(config: StructuredRequestConfig&lt;T&gt;)</code></li> <li>Node layer: <code>nodeLLMRequestStructured(config: StructuredRequestConfig&lt;T&gt;)</code></li> </ul> <p>The simplified API (using just <code>examples</code> and <code>fixingParser</code> parameters) is recommended for most use cases, while the advanced API provides additional control when needed.</p>"},{"location":"structured-output/#best-practices","title":"Best practices","text":"<ol> <li> <p>Use clear descriptions: provide clear and detailed descriptions using <code>@LLMDescription</code> annotations to help the LLM understand the expected data.</p> </li> <li> <p>Provide examples: include examples of valid data structures to guide the LLM.</p> </li> <li> <p>Handle errors gracefully: implement proper error handling to deal with cases where the LLM might not produce a valid structure.</p> </li> <li> <p>Use appropriate schema types: select the appropriate schema format and type based on your needs and the capabilities of the LLM you are using.</p> </li> <li> <p>Test with different models: different LLMs may have varying abilities to follow structured formats, so test with multiple models if possible.</p> </li> <li> <p>Start simple: begin with simple structures and gradually add complexity as needed.</p> </li> <li> <p>Use polymorphism Carefully: while the API supports polymorphism with sealed classes, be aware that it can be more challenging for LLMs to handle correctly.</p> </li> </ol>"},{"location":"subgraphs-overview/","title":"Overview","text":"<p>This page provides detailed information about subgraphs in the Koog framework. Understanding these concepts is crucial for creating complex agent workflows that maintain context across multiple processing steps.</p>"},{"location":"subgraphs-overview/#introduction","title":"Introduction","text":"<p>Subgraphs are a fundamental concept in the Koog framework that lets you break down complex agent workflows into manageable, sequential steps. Each subgraph represents a phase of processing, with its context, responsibilities, and an optional subset of tools.</p> <p>Subgraphs are integral parts of strategies, which are graphs that represent the overall agent workflow. For more information about strategies, see Custom strategy graphs.</p>"},{"location":"subgraphs-overview/#understanding-subgraphs","title":"Understanding subgraphs","text":"<p>A subgraph is a self-contained unit of processing within an agent strategy. Each subgraph:</p> <ul> <li>Has a unique name</li> <li>Contains a graph of nodes or subgraphs connected by edges</li> <li>Can use any tool or a subset of tools from the tool registry</li> <li>Receives input from the previous subgraph (or the initial user input)</li> <li>Produces output that is passed to the next subgraph (or the output)</li> </ul> <p>To define a sequence of subgraphs in a graph, use edge connections or define sequences using the <code>then</code> keyword. For more information, see Custom strategy graphs.</p>"},{"location":"subgraphs-overview/#subgraph-context","title":"Subgraph context","text":"<p>Each subgraph executes within a context that provides access to:</p> <ul> <li>The environment</li> <li>Agent input</li> <li>The agent configuration</li> <li>The LLM context (including the conversation history)</li> <li>The state manager</li> <li>The storage</li> <li>Session and strategy</li> </ul> <p>The context is passed to each node within the subgraph and provides the necessary resources for the node to perform its operations.</p>"},{"location":"testing/","title":"Testing","text":""},{"location":"testing/#overview","title":"Overview","text":"<p>The Testing feature provides a comprehensive framework for testing AI agent pipelines, subgraphs, and tool interactions in the Koog framework. It enables developers to create controlled test environments with mock LLM (Large Language Model) executors, tool registries, and agent environments.</p>"},{"location":"testing/#purpose","title":"Purpose","text":"<p>The primary purpose of this feature is to facilitate testing of agent-based AI features by:</p> <ul> <li>Mocking LLM responses to specific prompts</li> <li>Simulating tool calls and their results</li> <li>Testing agent pipeline subgraphs and their structures</li> <li>Verifying the correct flow of data through agent nodes</li> <li>Providing assertions for expected behaviors</li> </ul>"},{"location":"testing/#configuration-and-initialization","title":"Configuration and initialization","text":""},{"location":"testing/#setting-up-test-dependencies","title":"Setting up test dependencies","text":"<p>Before setting up a test environment, make sure that you have added the following dependencies:</p> <pre><code>// build.gradle.kts\ndependencies {\n   testImplementation(\"ai.koog:agents-test:LATEST_VERSION\")\n   testImplementation(kotlin(\"test\"))\n}\n</code></pre>"},{"location":"testing/#mocking-llm-responses","title":"Mocking LLM responses","text":"<p>The basic form of testing involves mocking LLM responses to ensure deterministic behavior. You can do this using  <code>MockLLMBuilder</code> and related utilities.</p> <pre><code>// Create a mock LLM executor\nval mockLLMApi = getMockExecutor(toolRegistry) {\n  // Mock a simple text response\n  mockLLMAnswer(\"Hello!\") onRequestContains \"Hello\"\n\n  // Mock a default response\n  mockLLMAnswer(\"I don't know how to answer that.\").asDefaultResponse\n}\n</code></pre>"},{"location":"testing/#mocking-tool-calls","title":"Mocking tool calls","text":"<p>You can mock the LLM to call specific tools based on input patterns:</p> <pre><code>// Mock a tool call response\nmockLLMToolCall(CreateTool, CreateTool.Args(\"solve\")) onRequestEquals \"Solve task\"\n\n// Mock tool behavior - simplest form without lambda\nmockTool(PositiveToneTool) alwaysReturns \"The text has a positive tone.\"\n\n// Using lambda when you need to perform extra actions\nmockTool(NegativeToneTool) alwaysTells {\n  // Perform some extra action\n  println(\"Negative tone tool called\")\n\n  // Return the result\n  \"The text has a negative tone.\"\n}\n\n// Mock tool behavior based on specific arguments\nmockTool(AnalyzeTool) returns \"Detailed analysis\" onArguments AnalyzeTool.Args(\"analyze deeply\")\n\n// Mock tool behavior with conditional argument matching\nmockTool(SearchTool) returns \"Found results\" onArgumentsMatching { args -&gt;\n  args.query.contains(\"important\")\n}\n</code></pre> <p>The examples above demonstrate different ways to mock tools, from simple to more complex ones:</p> <ol> <li><code>alwaysReturns</code>: the simplest form, directly returns a value without a lambda.</li> <li><code>alwaysTells</code>: uses a lambda when you need to perform additional actions.</li> <li><code>returns...onArguments</code>: returns specific results for exact argument matches.</li> <li><code>returns...onArgumentsMatching</code>: returns results based on custom argument conditions.</li> </ol>"},{"location":"testing/#enabling-testing-mode","title":"Enabling testing mode","text":"<p>To enable the testing mode on an agent, use the <code>withTesting()</code> function within the AIAgent constructor block:</p> <pre><code>// Create the agent with testing enabled\nAIAgent(\n    promptExecutor = mockLLMApi,\n    toolRegistry = toolRegistry,\n    llmModel = llmModel\n) {\n    // Enable testing mode\n    withTesting()\n}\n</code></pre>"},{"location":"testing/#advanced-testing","title":"Advanced testing","text":""},{"location":"testing/#testing-the-graph-structure","title":"Testing the graph structure","text":"<p>Before testing the detailed node behavior and edge connections, it is important to verify the overall structure of your agent's graph. This includes checking that all required nodes exist and are properly connected in the expected subgraphs.</p> <p>The Testing feature provides a comprehensive way to test your agent's graph structure. This approach is particularly valuable for complex agents with multiple subgraphs and interconnected nodes.</p>"},{"location":"testing/#basic-structure-testing","title":"Basic structure testing","text":"<p>Start by validating the fundamental structure of your agent's graph:</p> <pre><code>AIAgent(\n    // Constructor arguments\n    promptExecutor = mockLLMApi,\n    toolRegistry = toolRegistry,\n    llmModel = llmModel\n) {\n    testGraph&lt;String, String&gt;(\"test\") {\n        val firstSubgraph = assertSubgraphByName&lt;String, String&gt;(\"first\")\n        val secondSubgraph = assertSubgraphByName&lt;String, String&gt;(\"second\")\n\n        // Assert subgraph connections\n        assertEdges {\n            startNode() alwaysGoesTo firstSubgraph\n            firstSubgraph alwaysGoesTo secondSubgraph\n            secondSubgraph alwaysGoesTo finishNode()\n        }\n\n        // Verify the first subgraph\n        verifySubgraph(firstSubgraph) {\n            val start = startNode()\n            val finish = finishNode()\n\n            // Assert nodes by name\n            val askLLM = assertNodeByName&lt;String, Message.Response&gt;(\"callLLM\")\n            val callTool = assertNodeByName&lt;Message.Tool.Call, ReceivedToolResult&gt;(\"executeTool\")\n\n            // Assert node reachability\n            assertReachable(start, askLLM)\n            assertReachable(askLLM, callTool)\n        }\n    }\n}\n</code></pre>"},{"location":"testing/#testing-node-behavior","title":"Testing node behavior","text":"<p>Node behavior testing lets you verify that nodes in your agent's graph produce the expected outputs for the given inputs.  This is crucial for ensuring that your agent's logic works correctly under different scenarios.</p>"},{"location":"testing/#basic-node-testing","title":"Basic node testing","text":"<p>Start with simple input and output validations for individual nodes:</p> <pre><code>assertNodes {\n\n    // Test basic text responses\n    askLLM withInput \"Hello\" outputs assistantMessage(\"Hello!\")\n\n    // Test tool call responses\n    askLLM withInput \"Solve task\" outputs toolCallMessage(CreateTool, CreateTool.Args(\"solve\"))\n}\n</code></pre> <p>The example above shows how to test the following behavior: 1. When the LLM node receives <code>Hello</code> as the input, it responds with a simple text message. 2. When it receives <code>Solve task</code>, it responds with a tool call.</p>"},{"location":"testing/#testing-tool-run-nodes","title":"Testing tool run nodes","text":"<p>You can also test nodes that run tools:</p> <pre><code>assertNodes {\n    // Test tool runs with specific arguments\n    callTool withInput toolCallMessage(\n        SolveTool,\n        SolveTool.Args(\"solve\")\n    ) outputs toolResult(SolveTool, SolveTool.Args(\"solve\"), \"solved\")\n}\n</code></pre> <p>This verifies that when the tool execution node receives a specific tool call signature, it produces the expected tool result.</p>"},{"location":"testing/#advanced-node-testing","title":"Advanced node testing","text":"<p>For more complex scenarios, you can test nodes with structured inputs and outputs:</p> <pre><code>assertNodes {\n    // Test with different inputs to the same node\n    askLLM withInput \"Simple query\" outputs assistantMessage(\"Simple response\")\n\n    // Test with complex parameters\n    askLLM withInput \"Complex query with parameters\" outputs toolCallMessage(\n        AnalyzeTool,\n        AnalyzeTool.Args(query = \"parameters\", depth = 3)\n    )\n}\n</code></pre> <p>You can also test complex tool call scenarios with detailed result structures:</p> <pre><code>assertNodes {\n    // Test a complex tool call with a structured result\n    callTool withInput toolCallMessage(\n        AnalyzeTool,\n        AnalyzeTool.Args(query = \"complex\", depth = 5)\n    ) outputs toolResult(AnalyzeTool, AnalyzeTool.Args(query = \"complex\", depth = 5), AnalyzeTool.Result(\n        analysis = \"Detailed analysis\",\n        confidence = 0.95,\n        metadata = mapOf(\"source\" to \"database\", \"timestamp\" to \"2023-06-15\")\n    ))\n}\n</code></pre> <p>These advanced tests help ensure that your nodes handle complex data structures correctly, which is essential for sophisticated agent behaviors.</p>"},{"location":"testing/#testing-edge-connections","title":"Testing edge connections","text":"<p>Edge connections testing allows you to verify that your agent's graph correctly routes outputs from one node to the appropriate next node. This ensures that your agent follows the intended workflow paths based on different outputs.</p>"},{"location":"testing/#basic-edge-testing","title":"Basic edge testing","text":"<p>Start with simple edge connection tests:</p> <pre><code>assertEdges {\n    // Test text message routing\n    askLLM withOutput assistantMessage(\"Hello!\") goesTo giveFeedback\n\n    // Test tool call routing\n    askLLM withOutput toolCallMessage(CreateTool, CreateTool.Args(\"solve\")) goesTo callTool\n}\n</code></pre> <p>This example verifies the following behavior: 1. When the LLM node outputs a simple text message, the flow is directed to the <code>giveFeedback</code> node. 2. When it outputs a tool call, the flow is directed to the <code>callTool</code> node.</p>"},{"location":"testing/#testing-conditional-routing","title":"Testing conditional routing","text":"<p>You can test a more complex routing logic based on the content of outputs:</p> <pre><code>assertEdges {\n    // Different text responses can route to different nodes\n    askLLM withOutput assistantMessage(\"Need more information\") goesTo askForInfo\n    askLLM withOutput assistantMessage(\"Ready to proceed\") goesTo processRequest\n}\n</code></pre>"},{"location":"testing/#advanced-edge-testing","title":"Advanced edge testing","text":"<p>For sophisticated agents, you can test conditional routing based on structured data in tool results:</p> <pre><code>assertEdges {\n    // Test routing based on tool result content\n    callTool withOutput toolResult(\n        AnalyzeTool,\n        AnalyzeTool.Args(query = \"parameters\", depth = 3),\n        AnalyzeTool.Result(analysis = \"Needs more processing\", confidence = 0.5)\n    ) goesTo processResult\n}\n</code></pre> <p>You can also test complex decision paths based on different result properties:</p> <pre><code>assertEdges {\n    // Route to different nodes based on confidence level\n    callTool withOutput toolResult(\n        AnalyzeTool,\n        AnalyzeTool.Args(query = \"parameters\", depth = 3),\n        AnalyzeTool.Result(analysis = \"Complete\", confidence = 0.9)\n    ) goesTo finish\n\n    callTool withOutput toolResult(\n        AnalyzeTool,\n        AnalyzeTool.Args(query = \"parameters\", depth = 3),\n        AnalyzeTool.Result(analysis = \"Uncertain\", confidence = 0.3)\n    ) goesTo verifyResult\n}\n</code></pre> <p>These advanced edge tests help ensure that your agent makes the correct decisions based on the content and structure of node outputs, which is essential for creating intelligent, context-aware workflows.</p>"},{"location":"testing/#complete-testing-example","title":"Complete testing example","text":"<p>Here is a user story that demonstrates a complete testing scenario:</p> <p>You are developing a tone analysis agent that analyzes the tone of the text and provides feedback. The agent uses tools for detecting positive, negative, and neutral tones.</p> <p>Here is how you can test this agent:</p> <pre><code>@Test\nfun testToneAgent() = runTest {\n    // Create a list to track tool calls\n    var toolCalls = mutableListOf&lt;String&gt;()\n    var result: String? = null\n\n    // Create a tool registry\n    val toolRegistry = ToolRegistry {\n        // A special tool, required with this type of agent\n        tool(SayToUser)\n\n        with(ToneTools) {\n            tools()\n        }\n    }\n\n    // Create an event handler\n    val eventHandler = EventHandler {\n        onToolCallStarting { tool, args -&gt;\n            println(\"[DEBUG_LOG] Tool called: tool ${tool.name}, args $args\")\n            toolCalls.add(tool.name)\n        }\n\n        handleError {\n            println(\"[DEBUG_LOG] An error occurred: ${it.message}\\n${it.stackTraceToString()}\")\n            true\n        }\n\n        handleResult {\n            println(\"[DEBUG_LOG] Result: $it\")\n            result = it\n        }\n    }\n\n    val positiveText = \"I love this product!\"\n    val negativeText = \"Awful service, hate the app.\"\n    val defaultText = \"I don't know how to answer this question.\"\n\n    val positiveResponse = \"The text has a positive tone.\"\n    val negativeResponse = \"The text has a negative tone.\"\n    val neutralResponse = \"The text has a neutral tone.\"\n\n    val mockLLMApi = getMockExecutor(toolRegistry, eventHandler) {\n        // Set up LLM responses for different input texts\n        mockLLMToolCall(NeutralToneTool, ToneTool.Args(defaultText)) onRequestEquals defaultText\n        mockLLMToolCall(PositiveToneTool, ToneTool.Args(positiveText)) onRequestEquals positiveText\n        mockLLMToolCall(NegativeToneTool, ToneTool.Args(negativeText)) onRequestEquals negativeText\n\n        // Mock the behavior where the LLM responds with just tool responses when the tools return results\n        mockLLMAnswer(positiveResponse) onRequestContains positiveResponse\n        mockLLMAnswer(negativeResponse) onRequestContains negativeResponse\n        mockLLMAnswer(neutralResponse) onRequestContains neutralResponse\n\n        mockLLMAnswer(defaultText).asDefaultResponse\n\n        // Tool mocks\n        mockTool(PositiveToneTool) alwaysTells {\n            toolCalls += \"Positive tone tool called\"\n            positiveResponse\n        }\n        mockTool(NegativeToneTool) alwaysTells {\n            toolCalls += \"Negative tone tool called\"\n            negativeResponse\n        }\n        mockTool(NeutralToneTool) alwaysTells {\n            toolCalls += \"Neutral tone tool called\"\n            neutralResponse\n        }\n    }\n\n    // Create a strategy\n    val strategy = toneStrategy(\"tone_analysis\")\n\n    // Create an agent configuration\n    val agentConfig = AIAgentConfig(\n        prompt = prompt(\"test-agent\") {\n            system(\n                \"\"\"\n                You are an question answering agent with access to the tone analysis tools.\n                You need to answer 1 question with the best of your ability.\n                Be as concise as possible in your answers.\n                DO NOT ANSWER ANY QUESTIONS THAT ARE BESIDES PERFORMING TONE ANALYSIS!\n                DO NOT HALLUCINATE!\n            \"\"\".trimIndent()\n            )\n        },\n        model = mockk&lt;LLModel&gt;(relaxed = true),\n        maxAgentIterations = 10\n    )\n\n    // Create an agent with testing enabled\n    val agent = AIAgent(\n        promptExecutor = mockLLMApi,\n        toolRegistry = toolRegistry,\n        strategy = strategy,\n        eventHandler = eventHandler,\n        agentConfig = agentConfig,\n    ) {\n        withTesting()\n    }\n\n    // Test the positive text\n    agent.run(positiveText)\n    assertEquals(\"The text has a positive tone.\", result, \"Positive tone result should match\")\n    assertEquals(1, toolCalls.size, \"One tool is expected to be called\")\n\n    // Test the negative text\n    agent.run(negativeText)\n    assertEquals(\"The text has a negative tone.\", result, \"Negative tone result should match\")\n    assertEquals(2, toolCalls.size, \"Two tools are expected to be called\")\n\n    //Test the neutral text\n    agent.run(defaultText)\n    assertEquals(\"The text has a neutral tone.\", result, \"Neutral tone result should match\")\n    assertEquals(3, toolCalls.size, \"Three tools are expected to be called\")\n}\n</code></pre> <p>For more complex agents with multiple subgraphs, you can also test the graph structure:</p> <pre><code>@Test\nfun testMultiSubgraphAgentStructure() = runTest {\n    val strategy = strategy(\"test\") {\n        val firstSubgraph by subgraph(\n            \"first\",\n            tools = listOf(DummyTool, CreateTool, SolveTool)\n        ) {\n            val callLLM by nodeLLMRequest(allowToolCalls = false)\n            val executeTool by nodeExecuteTool()\n            val sendToolResult by nodeLLMSendToolResult()\n            val giveFeedback by node&lt;String, String&gt; { input -&gt;\n                llm.writeSession {\n                    appendPrompt {\n                        user(\"Call tools! Don't chat!\")\n                    }\n                }\n                input\n            }\n\n            edge(nodeStart forwardTo callLLM)\n            edge(callLLM forwardTo executeTool onToolCall { true })\n            edge(callLLM forwardTo giveFeedback onAssistantMessage { true })\n            edge(giveFeedback forwardTo giveFeedback onAssistantMessage { true })\n            edge(giveFeedback forwardTo executeTool onToolCall { true })\n            edge(executeTool forwardTo nodeFinish transformed { it.content })\n        }\n\n        val secondSubgraph by subgraph&lt;String, String&gt;(\"second\") {\n            edge(nodeStart forwardTo nodeFinish)\n        }\n\n        edge(nodeStart forwardTo firstSubgraph)\n        edge(firstSubgraph forwardTo secondSubgraph)\n        edge(secondSubgraph forwardTo nodeFinish)\n    }\n\n    val toolRegistry = ToolRegistry {\n        tool(DummyTool)\n        tool(CreateTool)\n        tool(SolveTool)\n    }\n\n    val mockLLMApi = getMockExecutor(toolRegistry) {\n        mockLLMAnswer(\"Hello!\") onRequestContains \"Hello\"\n        mockLLMToolCall(CreateTool, CreateTool.Args(\"solve\")) onRequestEquals \"Solve task\"\n    }\n\n    val basePrompt = prompt(\"test\") {}\n\n    AIAgent(\n        toolRegistry = toolRegistry,\n        strategy = strategy,\n        eventHandler = EventHandler {},\n        agentConfig = AIAgentConfig(prompt = basePrompt, model = OpenAIModels.Chat.GPT4o, maxAgentIterations = 100),\n        promptExecutor = mockLLMApi,\n    ) {\n        testGraph(\"test\") {\n            val firstSubgraph = assertSubgraphByName&lt;String, String&gt;(\"first\")\n            val secondSubgraph = assertSubgraphByName&lt;String, String&gt;(\"second\")\n\n            assertEdges {\n                startNode() alwaysGoesTo firstSubgraph\n                firstSubgraph alwaysGoesTo secondSubgraph\n                secondSubgraph alwaysGoesTo finishNode()\n            }\n\n            verifySubgraph(firstSubgraph) {\n                val start = startNode()\n                val finish = finishNode()\n\n                val askLLM = assertNodeByName&lt;String, Message.Response&gt;(\"callLLM\")\n                val callTool = assertNodeByName&lt;Message.Tool.Call, ReceivedToolResult&gt;(\"executeTool\")\n                val giveFeedback = assertNodeByName&lt;Any?, Any?&gt;(\"giveFeedback\")\n\n                assertReachable(start, askLLM)\n                assertReachable(askLLM, callTool)\n\n                assertNodes {\n                    askLLM withInput \"Hello\" outputs Message.Assistant(\"Hello!\")\n                    askLLM withInput \"Solve task\" outputs toolCallMessage(CreateTool, CreateTool.Args(\"solve\"))\n\n                    callTool withInput toolCallSignature(\n                        SolveTool,\n                        SolveTool.Args(\"solve\")\n                    ) outputs toolResult(SolveTool, \"solved\")\n\n                    callTool withInput toolCallSignature(\n                        CreateTool,\n                        CreateTool.Args(\"solve\")\n                    ) outputs toolResult(CreateTool, \"created\")\n                }\n\n                assertEdges {\n                    askLLM withOutput Message.Assistant(\"Hello!\") goesTo giveFeedback\n                    askLLM withOutput toolCallMessage(CreateTool, CreateTool.Args(\"solve\")) goesTo callTool\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"testing/#api-reference","title":"API reference","text":"<p>For a complete API reference related to the Testing feature, see the reference documentation for the agents-test module.</p>"},{"location":"testing/#faq-and-troubleshooting","title":"FAQ and troubleshooting","text":""},{"location":"testing/#how-do-i-mock-a-specific-tool-response","title":"How do I mock a specific tool response?","text":"<p>Use the <code>mockTool</code> method in <code>MockLLMBuilder</code>:</p> <pre><code>val mockExecutor = getMockExecutor {\n    mockTool(myTool) alwaysReturns myResult\n\n    // Or with conditions\n    mockTool(myTool) returns myResult onArguments myArgs\n}\n</code></pre>"},{"location":"testing/#how-can-i-test-complex-graph-structures","title":"How can I test complex graph structures?","text":"<p>Use the subgraph assertions, <code>verifySubgraph</code>, and node references:</p> <pre><code>testGraph&lt;Unit, String&gt;(\"test\") {\n    val mySubgraph = assertSubgraphByName&lt;Unit, String&gt;(\"mySubgraph\")\n\n    verifySubgraph(mySubgraph) {\n        // Get references to nodes\n        val nodeA = assertNodeByName&lt;Unit, String&gt;(\"nodeA\")\n        val nodeB = assertNodeByName&lt;String, String&gt;(\"nodeB\")\n\n        // Assert reachability\n        assertReachable(nodeA, nodeB)\n\n        // Assert edge connections\n        assertEdges {\n            nodeA.withOutput(\"result\") goesTo nodeB\n        }\n    }\n}\n</code></pre>"},{"location":"testing/#how-do-i-simulate-different-llm-responses-based-on-input","title":"How do I simulate different LLM responses based on input?","text":"<p>Use pattern matching methods:</p> <pre><code>getMockExecutor {\n    mockLLMAnswer(\"Response A\") onRequestContains \"topic A\"\n    mockLLMAnswer(\"Response B\") onRequestContains \"topic B\"\n    mockLLMAnswer(\"Exact response\") onRequestEquals \"exact question\"\n    mockLLMAnswer(\"Conditional response\") onCondition { it.contains(\"keyword\") &amp;&amp; it.length &gt; 10 }\n}\n</code></pre>"},{"location":"testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"testing/#mock-executor-always-returns-the-default-response","title":"Mock executor always returns the default response","text":"<p>Check that your pattern matching is correct. Patterns are case-sensitive and must match exactly as specified.</p>"},{"location":"testing/#tool-calls-are-not-being-intercepted","title":"Tool calls are not being intercepted","text":"<p>Ensure that:</p> <ol> <li>The tool registry is properly set up.</li> <li>The tool names match exactly.</li> <li>The tool actions are configured correctly.</li> </ol>"},{"location":"testing/#graph-assertions-are-failing","title":"Graph assertions are failing","text":"<ol> <li>Verify that node names are correct.</li> <li>Check that the graph structure matches your expectations.</li> <li>Use the <code>startNode()</code> and <code>finishNode()</code> methods to get the correct entry and exit points.</li> </ol>"},{"location":"tools-overview/","title":"Overview","text":"<p>Agents use tools to perform specific tasks or access external systems.</p>"},{"location":"tools-overview/#tool-workflow","title":"Tool workflow","text":"<p>The Koog framework offers the following workflow for working with tools:</p> <ol> <li>Create a custom tool or use one of the built-in tools.</li> <li>Add the tool to a tool registry.</li> <li>Pass the tool registry to an agent.</li> <li>Use the tool with the agent.</li> </ol>"},{"location":"tools-overview/#available-tool-types","title":"Available tool types","text":"<p>There are three types of tools in the Koog framework:</p> <ul> <li>Built-in tools that provide functionality for agent-user interaction and conversation management. For details, see Built-in tools.</li> <li>Annotation-based custom tools that let you expose functions as tools to LLMs. For details, see Annotation-based tools.</li> <li>Custom tools that let you control tool parameters, metadata, execution logic, and how it is registered and invoked. For details, see Class-based   tools.</li> </ul>"},{"location":"tools-overview/#tool-registry","title":"Tool registry","text":"<p>Before you can use a tool in an agent, you must add it to a tool registry. The tool registry manages all tools available to the agent.</p> <p>The key features of the tool registry:</p> <ul> <li>Organizes tools.</li> <li>Supports merging of multiple tool registries.</li> <li>Provides methods to retrieve tools by name or type.</li> </ul> <p>To learn more, see ToolRegistry.</p> <p>Here is an example of how to create the tool registry and add the tool to it:</p> <pre><code>val toolRegistry = ToolRegistry {\n    tool(SayToUser)\n}\n</code></pre> <p>To merge multiple tool registries, do the following:</p> <pre><code>val firstToolRegistry = ToolRegistry {\n    tool(FirstSampleTool)\n}\n\nval secondToolRegistry = ToolRegistry {\n    tool(SecondSampleTool)\n}\n\nval newRegistry = firstToolRegistry + secondToolRegistry\n</code></pre>"},{"location":"tools-overview/#passing-tools-to-an-agent","title":"Passing tools to an agent","text":"<p>To enable an agent to use a tool, you need to provide a tool registry that contains this tool as an argument when creating the agent:</p> <pre><code>// Agent initialization\nval agent = AIAgent(\n    promptExecutor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    systemPrompt = \"You are a helpful assistant with strong mathematical skills.\",\n    llmModel = OpenAIModels.Chat.GPT4o,\n    // Pass your tool registry to the agent\n    toolRegistry = toolRegistry\n)\n</code></pre>"},{"location":"tools-overview/#calling-tools","title":"Calling tools","text":"<p>There are several ways to call tools within your agent code. The recommended approach is to use the provided methods in the agent context rather than calling tools directly, as this ensures proper handling of tool operation within the agent environment.</p> <p>Tip</p> <p>Ensure you have implemented proper error handling in your tools to prevent agent failure.</p> <p>The tools are called within a specific session context represented by <code>AIAgentLLMWriteSession</code>. It provides several methods for calling tools so that you can:</p> <ul> <li>Call a tool with the given arguments.</li> <li>Call a tool by its name and the given arguments.</li> <li>Call a tool by the provided tool class and arguments.</li> <li>Call a tool of the specified type with the given arguments.</li> <li>Call a tool that returns a raw string result.</li> </ul> <p>For more details, see API reference.</p>"},{"location":"tools-overview/#parallel-tool-calls","title":"Parallel tool calls","text":"<p>You can also call tools in parallel using the <code>toParallelToolCallsRaw</code> extension. For example:</p> <pre><code>@Serializable\ndata class Book(\n    val title: String,\n    val author: String,\n    val description: String\n)\n\nclass BookTool() : SimpleTool&lt;Book&gt;(\n    argsSerializer = Book.serializer(),\n    name = NAME,\n    description = \"A tool to parse book information from Markdown\"\n) {\n    companion object {\n        const val NAME = \"book\"\n    }\n\n    override suspend fun execute(args: Book): String {\n        println(\"${args.title} by ${args.author}:\\n ${args.description}\")\n        return \"Done\"\n    }\n}\n\nval strategy = strategy&lt;Unit, Unit&gt;(\"strategy-name\") {\n\n    /*...*/\n\n    val myNode by node&lt;Unit, Unit&gt; { _ -&gt;\n        llm.writeSession {\n            flow {\n                emit(Book(\"Book 1\", \"Author 1\", \"Description 1\"))\n            }.toParallelToolCallsRaw(BookTool::class).collect()\n        }\n    }\n}\n</code></pre>"},{"location":"tools-overview/#calling-tools-from-nodes","title":"Calling tools from nodes","text":"<p>When building agent workflows with nodes, you can use special nodes to call tools:</p> <ul> <li> <p>nodeExecuteTool: calls a single tool call and returns its result. For details, see API reference.</p> </li> <li> <p>nodeExecuteSingleTool that calls a specific tool with the provided arguments. For details, see API reference.</p> </li> <li> <p>nodeExecuteMultipleTools that performs multiple tool calls and returns their results. For details, see API reference.</p> </li> <li> <p>nodeLLMSendToolResult that sends a tool result to the LLM and gets a response. For details, see API reference.</p> </li> <li> <p>nodeLLMSendMultipleToolResults that sends multiple tool results to the LLM. For details, see API reference.</p> </li> </ul>"},{"location":"tools-overview/#using-agents-as-tools","title":"Using agents as tools","text":"<p>The framework provides the capability to convert any AI agent into a tool that can be used by other agents.  This powerful feature enables you to create hierarchical agent architectures where specialized agents can be called as tools by higher-level orchestrating agents.</p>"},{"location":"tools-overview/#converting-agents-to-tools","title":"Converting agents to tools","text":"<p>To convert an agent into a tool, use the <code>AIAgentService</code> and the <code>createAgentTool()</code> extension function:</p> <pre><code>// Create a specialized agent service, responsible for creating financial analysis agents.\nval analysisAgentService = AIAgentService(\n    promptExecutor = simpleOpenAIExecutor(apiKey),\n    llmModel = OpenAIModels.Chat.GPT4o,\n    systemPrompt = \"You are a financial analysis specialist.\",\n    toolRegistry = analysisToolRegistry\n)\n\n// Create a tool that would run financial analysis agent once called.\nval analysisAgentTool = analysisAgentService.createAgentTool(\n    agentName = \"analyzeTransactions\",\n    agentDescription = \"Performs financial transaction analysis\",\n    inputDescription = \"Transaction analysis request\",\n)\n</code></pre>"},{"location":"tools-overview/#using-agent-tools-in-other-agents","title":"Using agent tools in other agents","text":"<p>Once converted to a tool, you can add the agent tool to another agent's tool registry:</p> <pre><code>// Create a coordinator agent that can use specialized agents as tools\nval coordinatorAgent = AIAgent(\n    promptExecutor = simpleOpenAIExecutor(apiKey),\n    llmModel = OpenAIModels.Chat.GPT4o,\n    systemPrompt = \"You coordinate different specialized services.\",\n    toolRegistry = ToolRegistry {\n        tool(analysisAgentTool)\n        // Add other tools as needed\n    }\n)\n</code></pre>"},{"location":"tools-overview/#agent-tool-execution","title":"Agent tool execution","text":"<p>When an agent tool is called:</p> <ol> <li>The arguments are deserialized according to the input descriptor.</li> <li>The wrapped agent is executed with the deserialized input.</li> <li>The agent's output is serialized and returned as the tool result.</li> </ol>"},{"location":"tools-overview/#benefits-of-agents-as-tools","title":"Benefits of agents as tools","text":"<ul> <li>Modularity: Break complex workflows into specialized agents.</li> <li>Reusability: Use the same specialized agent across multiple coordinator agents.</li> <li>Separation of concerns: Each agent can focus on its specific domain.</li> </ul>"},{"location":"tracing/","title":"Tracing","text":"<p>This page includes details about the Tracing feature, which provides comprehensive tracing capabilities for AI agents.</p>"},{"location":"tracing/#feature-overview","title":"Feature overview","text":"<p>The Tracing feature is a powerful monitoring and debugging tool that captures detailed information about agent runs, including:</p> <ul> <li>Strategy execution</li> <li>LLM calls</li> <li>LLM streaming (start, frames, completion, errors)</li> <li>Tool calls</li> <li>Node execution within the agent graph</li> </ul> <p>This feature operates by intercepting key events in the agent pipeline and forwarding them to configurable message processors. These processors can output the trace information to various destinations such as log files or other types of files in the filesystem, enabling developers to gain insights into agent behavior and troubleshoot issues effectively.</p>"},{"location":"tracing/#event-flow","title":"Event flow","text":"<ol> <li>The Tracing feature intercepts events in the agent pipeline.</li> <li>Events are filtered based on the configured message filter.</li> <li>Filtered events are passed to registered message processors.</li> <li>Message processors format and output the events to their respective destinations.</li> </ol>"},{"location":"tracing/#configuration-and-initialization","title":"Configuration and initialization","text":""},{"location":"tracing/#basic-setup","title":"Basic setup","text":"<p>To use the Tracing feature, you need to:</p> <ol> <li>Have one or more message processors (you can use the existing ones or create your own).</li> <li>Install <code>Tracing</code> in your agent.</li> <li>Configure the message filter (optional).</li> <li>Add the message processors to the feature.</li> </ol> <pre><code>// Defining a logger/file that will be used as a destination of trace messages \nval logger = KotlinLogging.logger { }\nval outputPath = Path(\"/path/to/trace.log\")\n\n// Creating an agent\nval agent = AIAgent(\n    promptExecutor = simpleOllamaAIExecutor(),\n    llmModel = OllamaModels.Meta.LLAMA_3_2,\n) {\n    install(Tracing) {\n\n        // Configure message processors to handle trace events\n        addMessageProcessor(TraceFeatureMessageLogWriter(logger))\n        addMessageProcessor(TraceFeatureMessageFileWriter(\n            outputPath,\n            { path: Path -&gt; SystemFileSystem.sink(path).buffered() }\n        ))\n    }\n}\n</code></pre>"},{"location":"tracing/#message-filtering","title":"Message filtering","text":"<p>You can process all existing events or select some of them based on specific criteria. The message filter lets you control which events are processed. This is useful for focusing on specific aspects of agent runs:</p> <pre><code>val fileWriter = TraceFeatureMessageFileWriter(\n    outputPath,\n    { path: Path -&gt; SystemFileSystem.sink(path).buffered() }\n)\n\naddMessageProcessor(fileWriter)\n\n// Filter for LLM-related events only\nfileWriter.setMessageFilter { message -&gt;\n    message is LLMCallStartingEvent || message is LLMCallCompletedEvent\n}\n\n// Filter for tool-related events only\nfileWriter.setMessageFilter { message -&gt; \n    message is ToolCallStartingEvent ||\n           message is ToolCallCompletedEvent ||\n           message is ToolValidationFailedEvent ||\n           message is ToolCallFailedEvent\n}\n\n// Filter for node execution events only\nfileWriter.setMessageFilter { message -&gt; \n    message is NodeExecutionStartingEvent || message is NodeExecutionCompletedEvent\n}\n</code></pre>"},{"location":"tracing/#large-trace-volumes","title":"Large trace volumes","text":"<p>For agents with complex strategies or long-running executions, the volume of trace events can be substantial. Consider using the following methods to manage the volume of events:</p> <ul> <li>Use specific message filters to reduce the number of events.</li> <li>Implement custom message processors with buffering or sampling.</li> <li>Use file rotation for log files to prevent them from growing too large.</li> </ul>"},{"location":"tracing/#dependency-graph","title":"Dependency graph","text":"<p>The Tracing feature has the following dependencies:</p> <pre><code>Tracing\n\u251c\u2500\u2500 AIAgentPipeline (for intercepting events)\n\u251c\u2500\u2500 TraceFeatureConfig\n\u2502   \u2514\u2500\u2500 FeatureConfig\n\u251c\u2500\u2500 Message Processors\n\u2502   \u251c\u2500\u2500 TraceFeatureMessageLogWriter\n\u2502   \u2502   \u2514\u2500\u2500 FeatureMessageLogWriter\n\u2502   \u251c\u2500\u2500 TraceFeatureMessageFileWriter\n\u2502   \u2502   \u2514\u2500\u2500 FeatureMessageFileWriter\n\u2502   \u2514\u2500\u2500 TraceFeatureMessageRemoteWriter\n\u2502       \u2514\u2500\u2500 FeatureMessageRemoteWriter\n\u2514\u2500\u2500 Event Types (from ai.koog.agents.core.feature.model)\n    \u251c\u2500\u2500 AgentStartingEvent\n    \u251c\u2500\u2500 AgentCompletedEvent\n    \u251c\u2500\u2500 AgentExecutionFailedEvent\n    \u251c\u2500\u2500 AgentClosingEvent\n    \u251c\u2500\u2500 GraphStrategyStartingEvent\n    \u251c\u2500\u2500 FunctionalStrategyStartingEvent\n    \u251c\u2500\u2500 StrategyCompletedEvent\n    \u251c\u2500\u2500 NodeExecutionStartingEvent\n    \u251c\u2500\u2500 NodeExecutionCompletedEvent\n    \u251c\u2500\u2500 NodeExecutionFailedEvent\n    \u251c\u2500\u2500 SubgraphExecutionStartingEvent\n    \u251c\u2500\u2500 SubgraphExecutionCompletedEvent\n    \u251c\u2500\u2500 SubgraphExecutionFailedEvent\n    \u251c\u2500\u2500 LLMCallStartingEvent\n    \u251c\u2500\u2500 LLMCallCompletedEvent\n    \u251c\u2500\u2500 LLMStreamingStartingEvent\n    \u251c\u2500\u2500 LLMStreamingFrameReceivedEvent\n    \u251c\u2500\u2500 LLMStreamingFailedEvent\n    \u251c\u2500\u2500 LLMStreamingCompletedEvent\n    \u251c\u2500\u2500 ToolCallStartingEvent\n    \u251c\u2500\u2500 ToolValidationFailedEvent\n    \u251c\u2500\u2500 ToolCallFailedEvent\n    \u2514\u2500\u2500 ToolCallCompletedEvent\n</code></pre>"},{"location":"tracing/#examples-and-quickstarts","title":"Examples and quickstarts","text":""},{"location":"tracing/#basic-tracing-to-logger","title":"Basic tracing to logger","text":"<pre><code>// Create a logger\nval logger = KotlinLogging.logger { }\n\nfun main() {\n    runBlocking {\n       // Create an agent with tracing\n       val agent = AIAgent(\n          promptExecutor = simpleOllamaAIExecutor(),\n          llmModel = OllamaModels.Meta.LLAMA_3_2,\n       ) {\n          install(Tracing) {\n             addMessageProcessor(TraceFeatureMessageLogWriter(logger))\n          }\n       }\n\n       // Run the agent\n       agent.run(\"Hello, agent!\")\n    }\n}\n</code></pre>"},{"location":"tracing/#error-handling-and-edge-cases","title":"Error handling and edge cases","text":""},{"location":"tracing/#no-message-processors","title":"No message processors","text":"<p>If no message processors are added to the Tracing feature, a warning will be logged:</p> <pre><code>Tracing Feature. No feature out stream providers are defined. Trace streaming has no target.\n</code></pre> <p>The feature will still intercept events, but they will not be processed or output anywhere.</p>"},{"location":"tracing/#resource-management","title":"Resource management","text":"<p>Message processors may hold resources (like file handles) that need to be properly released. Use the <code>use</code> extension function to ensure proper cleanup:</p> <pre><code>// Creating an agent\nval agent = AIAgent(\n    promptExecutor = simpleOllamaAIExecutor(),\n    llmModel = OllamaModels.Meta.LLAMA_3_2,\n) {\n    val writer = TraceFeatureMessageFileWriter(\n        outputPath,\n        { path: Path -&gt; SystemFileSystem.sink(path).buffered() }\n    )\n\n    install(Tracing) {\n        addMessageProcessor(writer)\n    }\n}\n// Run the agent\nagent.run(input)\n// Writer will be automatically closed when the block exits\n</code></pre>"},{"location":"tracing/#tracing-specific-events-to-file","title":"Tracing specific events to file","text":"<pre><code>install(Tracing) {\n\n    val fileWriter = TraceFeatureMessageFileWriter(\n        outputPath, \n        { path: Path -&gt; SystemFileSystem.sink(path).buffered() }\n    )\n    addMessageProcessor(fileWriter)\n\n    // Only trace LLM calls\n    fileWriter.setMessageFilter { message -&gt;\n        message is LLMCallStartingEvent || message is LLMCallCompletedEvent\n    }\n}\n</code></pre>"},{"location":"tracing/#tracing-specific-events-to-remote-endpoint","title":"Tracing specific events to remote endpoint","text":"<p>You use tracing to remote endpoints when you need to send event data via the network. Once initiated, tracing to a remote endpoint launches a light server at the specified port number and sends events via Kotlin Server-Sent Events  (SSE).</p> <pre><code>// Creating an agent\nval agent = AIAgent(\n    promptExecutor = simpleOllamaAIExecutor(),\n    llmModel = OllamaModels.Meta.LLAMA_3_2,\n) {\n    val connectionConfig = DefaultServerConnectionConfig(host = host, port = port)\n    val writer = TraceFeatureMessageRemoteWriter(\n        connectionConfig = connectionConfig\n    )\n\n    install(Tracing) {\n        addMessageProcessor(writer)\n    }\n}\n// Run the agent\nagent.run(input)\n// Writer will be automatically closed when the block exits\n</code></pre> <p>On the client side, you can use <code>FeatureMessageRemoteClient</code> to receive events and deserialize them.</p> <pre><code>val clientConfig = DefaultClientConnectionConfig(host = host, port = port, protocol = URLProtocol.HTTP)\nval agentEvents = mutableListOf&lt;DefinedFeatureEvent&gt;()\n\nval clientJob = launch {\n    FeatureMessageRemoteClient(connectionConfig = clientConfig, scope = this).use { client -&gt;\n        val collectEventsJob = launch {\n            client.receivedMessages.consumeAsFlow().collect { event -&gt;\n                // Collect events from server\n                agentEvents.add(event as DefinedFeatureEvent)\n\n                // Stop collecting events on agent finished\n                if (event is AgentCompletedEvent) {\n                    cancel()\n                }\n            }\n        }\n        client.connect()\n        collectEventsJob.join()\n        client.healthCheck()\n    }\n}\n\nlistOf(clientJob).joinAll()\n</code></pre>"},{"location":"tracing/#api-documentation","title":"API documentation","text":"<p>The Tracing feature follows a modular architecture with these key components:</p> <ol> <li>Tracing: the main feature class that intercepts events in the agent pipeline.</li> <li>TraceFeatureConfig: configuration class for customizing feature behavior.</li> <li>Message Processors: components that process and output trace events:<ul> <li>TraceFeatureMessageLogWriter: writes trace events to a logger.</li> <li>TraceFeatureMessageFileWriter: writes trace events to a file.</li> <li>TraceFeatureMessageRemoteWriter: sends trace events to a remote server.</li> </ul> </li> </ol>"},{"location":"why-koog/","title":"Why Koog","text":"<p>Koog is designed to solve real-world problems with JetBrains-level quality. It provides advanced AI algorithms, out-of-the-box proven techniques, a Kotlin DSL, and robust multi-platform support that goes beyond traditional frameworks.</p>"},{"location":"why-koog/#integration-with-jvm-and-kotlin-applications","title":"Integration with JVM and Kotlin applications","text":"<p>Koog provides a Kotlin Domain-Specific Language (DSL) designed specifically for JVM and Kotlin developers. This ensures smooth integration into Kotlin and Java-based applications,  significantly improving productivity and enhancing the overall developer experience.</p>"},{"location":"why-koog/#real-world-validation-with-jetbrains-products","title":"Real-world validation with JetBrains products","text":"<p>Koog powers multiple JetBrains products, including internal AI agents.  This real-world integration ensures that Koog is continuously tested, refined, and validated against practical use cases.  It focuses on what works in practice, incorporating insights from extensive feedback and real-product scenarios.  This integration provides Koog with strengths that distinguish it from other frameworks.</p>"},{"location":"why-koog/#advanced-solutions-available-out-of-the-box","title":"Advanced solutions available out of the box","text":"<p>Koog includes pre-built, composable solutions to simplify and speed up the development of agentic systems, setting it apart from frameworks that only offer basic components:</p> <ul> <li>Multiple history compression strategies. Koog comes with advanced strategies to compress and manage long-running conversations out of the box, eliminating the need for manual experimentation with approaches. With fine-tuned prompts, techniques, and algorithms tested and refined by ML engineers, you can rely on proven methods to improve performance. For more details on compression strategies, refer to History compression. To explore how Koog handles compression and context management in real-world scenarios, check out this article.</li> <li>Seamless LLM switching. You can switch a conversation to a different large language model (LLM) with a new set of available tools at any point without losing the existing conversation history. Koog automatically rewrites the history and handles unavailable tools, enabling smooth transitions and a natural interaction flow.</li> <li>Advanced persistence. Koog lets you restore full agent state machines instead of just chat messages. This enables features like checkpoints, failure recovery, and even the ability to revert to any point in the execution of the state machine.</li> <li>Robust retry component. Koog includes a retry mechanism that lets you wrap any set of operations within your agentic system and retry them until they meet configurable conditions. You can provide feedback and adjust each attempt to ensure reliable results. If LLM calls time out, tools do not work as expected, or there are network issues, Koog ensures that your agent remains resilient and performs effectively, even during temporary failures. For more technical details, see Retry functionality.</li> <li>Structured typed streaming with Markdown DSL. Koog streams LLM output and parses it into structured, typed events using a Markdown DSL. You can register handlers for specific elements like headers, bullet points, or regex patterns and receive only the relevant parts in real-time. This approach provides human-readable feedback using Markdown and machine-parsable data using structured typing, effectively eliminating the lack of transparency and enhancing the user experience. It ensures predictable output and dynamic user interfaces with progressive content rendering.</li> </ul>"},{"location":"why-koog/#broad-integration-multiplatform-support-enhanced-observability","title":"Broad integration, multiplatform support, enhanced observability","text":"<p>Koog supports the development and deployment of agentic applications across a variety of platforms and environments:</p> <ul> <li>Multiplatform support. You can deploy your agentic applications across JVM, JS, WasmJS, Android, and iOS targets.</li> <li>Broad AI integration. Koog integrates with major LLM providers, including OpenAI and Anthropic, as well as enterprise-level AI clouds like Bedrock. It also supports local models such as Ollama. For the full list of available providers, see LLM providers.</li> <li>OpenTelemetry support. Koog provides out-of-the-box integration with popular observability providers like W&amp;B Weave and Langfuse for monitoring and debugging AI applications. With native OpenTelemetry support, you can trace, log, and measure your agents using the same tools you already use in your system. To learn more, refer to OpenTelemetry.</li> <li>Spring Boot and Ktor integrations. Koog integrates with widely used enterprise environments.<ul> <li>If you have a Ktor server, you can install Koog as a plugin, configure providers using configuration files, and call agents directly from any route without manually connecting LLM clients.</li> <li>For Spring Boot, Koog provides ready-to-use beans and auto-configured LLM clients, making it easy to start building AI-powered workflows.</li> </ul> </li> </ul>"},{"location":"why-koog/#collaboration-with-ml-engineers-and-product-teams","title":"Collaboration with ML engineers and product teams","text":"<p>A unique advantage of Koog is its direct collaboration with JetBrains ML engineers and product teams. This ensures that features built with Koog are not just theoretical but tested and refined based on real-world product requirements. This means that Koog incorporates:</p> <ul> <li>Fine-tuned prompts and strategies optimized for real-world performance.</li> <li>Proven engineering approaches discovered and validated through product development, such as its unique history compression strategies. You can learn more in this detailed article.</li> <li>Continuous improvements that help Koog stay efficient and adaptable to evolving needs.</li> </ul>"},{"location":"why-koog/#commitment-to-the-developer-community","title":"Commitment to the developer community","text":"<p>The Koog team is deeply committed to building a strong developer community. By actively gathering and incorporating feedback, Koog evolves to meet the needs of developers effectively. We are actively expanding support for diverse AI architectures, comprehensive benchmarks, detailed use-case guides, and educational resources to empower developers.</p>"},{"location":"why-koog/#where-to-start","title":"Where to start","text":"<ul> <li>Explore Koog capabilities in Overview.</li> <li>Build your first Koog agent with our Getting started guide.</li> <li>See the latest updates in Koog release notes.</li> <li>Learn from Examples.</li> </ul>"},{"location":"examples/Attachments/","title":"Attachments","text":"<p> Open on GitHub  Download .ipynb</p>"},{"location":"examples/Attachments/#setting-up-the-environment","title":"Setting Up the Environment","text":"<p>Before diving into the code, we make sure our Kotlin Notebook is ready. Here we load the latest descriptors and enable the Koog library, which provides a clean API for working with AI model providers.</p> <pre><code>// Loads the latest descriptors and activates Koog integration for Kotlin Notebook.\n// This makes Koog DSL types and executors available in further cells.\n%useLatestDescriptors\n%use koog\n</code></pre>"},{"location":"examples/Attachments/#configuring-api-keys","title":"Configuring API Keys","text":"<p>We read the API key from an environment variable. This keeps secrets out of the notebook file and lets you switch providers. You can set <code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>, or <code>GEMINI_API_KEY</code>.</p> <pre><code>val apiKey = System.getenv(\"OPENAI_API_KEY\") // or ANTHROPIC_API_KEY, or GEMINI_API_KEY\n</code></pre>"},{"location":"examples/Attachments/#creating-a-simple-openai-executor","title":"Creating a Simple OpenAI Executor","text":"<p>The executor encapsulates authentication, base URLs, and correct defaults. Here we use a simple OpenAI executor, but you can swap it for Anthropic or Gemini without changing the rest of the code.</p> <pre><code>// --- Provider selection ---\n// For OpenAI-compatible models. Alternatives include:\n//   val executor = simpleAnthropicExecutor(System.getenv(\"ANTHROPIC_API_KEY\"))\n//   val executor = simpleGeminiExecutor(System.getenv(\"GEMINI_API_KEY\"))\n// All executors expose the same high\u2011level API.\nval executor = simpleOpenAIExecutor(apiKey)\n</code></pre> <p>Koog\u2019s prompt DSL lets you add structured Markdown and attachments. In this cell we build a prompt that asks the model to generate a short, blog\u2011style \"content card\" and we attach two images from the local <code>images/</code> directory.</p> <pre><code>import ai.koog.prompt.markdown.markdown\nimport kotlinx.io.files.Path\n\nval prompt = prompt(\"images-prompt\") {\n    system(\"You are professional assistant that can write cool and funny descriptions for Instagram posts.\")\n\n    user {\n        markdown {\n            +\"I want to create a new post on Instagram.\"\n            br()\n            +\"Can you write something creative under my instagram post with the following photos?\"\n            br()\n            h2(\"Requirements\")\n            bulleted {\n                item(\"It must be very funny and creative\")\n                item(\"It must increase my chance of becoming an ultra-famous blogger!!!!\")\n                item(\"It not contain explicit content, harassment or bullying\")\n                item(\"It must be a short catching phrase\")\n                item(\"You must include relevant hashtags that would increase the visibility of my post\")\n            }\n        }\n\n        attachments {\n            image(Path(\"images/kodee-loving.png\"))\n            image(Path(\"images/kodee-electrified.png\"))\n        }\n    }\n}\n</code></pre>"},{"location":"examples/Attachments/#execute-and-inspect-the-response","title":"Execute and Inspect the Response","text":"<p>We run the prompt against <code>gpt-4.1</code>, collect the first message, and print its content. If you want streaming, swap to a streaming API in Koog; for tool use, pass your tool list instead of <code>emptyList()</code>.</p> <p>Troubleshooting: * 401/403 \u2014 check your API key/environment variable. * File not found \u2014 verify the <code>images/</code> paths. * Rate limits \u2014 add minimal retry/backoff around the call if needed.</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nrunBlocking {\n    val response = executor.execute(prompt = prompt, model = OpenAIModels.Chat.GPT4_1, tools = emptyList()).first()\n    println(response.content)\n}\n</code></pre> <pre><code>Caption:\nRunning on cuteness and extra giggle power! Warning: Side effects may include heart-thief vibes and spontaneous dance parties. \ud83d\udc9c\ud83e\udd16\ud83d\udc83\n\nHashtags:  \n#ViralVibes #UltraFamousBlogger #CutieAlert #QuirkyContent #InstaFun #SpreadTheLove #DancingIntoFame #RobotLife #InstaFamous #FeedGoals\n</code></pre> <pre><code>runBlocking {\n    val response = executor.executeStreaming(prompt = prompt, model = OpenAIModels.Chat.GPT4_1)\n    response.collect { print(it) }\n}\n</code></pre> <pre><code>Caption:  \nRunning on good vibes &amp; wi-fi only! \ud83e\udd16\ud83d\udc9c Drop a like if you feel the circuit-joy! #BlogBotInTheWild #HeartDeliveryService #DancingWithWiFi #UltraFamousBlogger #MoreFunThanYourAICat #ViralVibes #InstaFun #BeepBoopFamous\n</code></pre>"},{"location":"examples/Banking/","title":"Building an AI Banking Assistant with Koog","text":"<p> Open on GitHub  Download .ipynb</p> <p>In this tutorial we\u2019ll build a small banking assistant using Koog agents in Kotlin. You\u2019ll learn how to: - Define domain models and sample data - Expose capability-focused tools for money transfers and transaction analytics - Classify user intent (Transfer vs Analytics) - Orchestrate calls in two styles:   1) a graph/subgraph strategy   2) \u201cagents as tools\u201d</p> <p>By the end, you\u2019ll be able to route free-form user requests to the right tools and produce helpful, auditable responses.</p>"},{"location":"examples/Banking/#setup-dependencies","title":"Setup &amp; Dependencies","text":"<p>We\u2019ll use the Kotlin Notebook kernel. Make sure your Koog artifacts are resolvable from Maven Central and your LLM provider key is available via <code>OPENAI_API_KEY</code>.</p> <pre><code>%useLatestDescriptors\n%use datetime\n\n// uncomment this for using koog from Maven Central\n// %use koog\n</code></pre> <pre><code>import ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor\n\nval apiKey = System.getenv(\"OPENAI_API_KEY\") ?: error(\"Please set OPENAI_API_KEY environment variable\")\nval openAIExecutor = simpleOpenAIExecutor(apiKey)\n</code></pre>"},{"location":"examples/Banking/#defining-the-system-prompt","title":"Defining the System Prompt","text":"<p>A well-crafted system prompt helps the AI understand its role and constraints. This prompt will guide all our agents' behavior.</p> <pre><code>val bankingAssistantSystemPrompt = \"\"\"\n    |You are a banking assistant interacting with a user (userId=123).\n    |Your goal is to understand the user's request and determine whether it can be fulfilled using the available tools.\n    |\n    |If the task can be accomplished with the provided tools, proceed accordingly,\n    |at the end of the conversation respond with: \"Task completed successfully.\"\n    |If the task cannot be performed with the tools available, respond with: \"Can't perform the task.\"\n\"\"\".trimMargin()\n</code></pre>"},{"location":"examples/Banking/#domain-model-sample-data","title":"Domain model &amp; Sample data","text":"<p>First, let's define our domain models and sample data. We'll use Kotlin's data classes with serialization support.</p> <pre><code>import kotlinx.serialization.Serializable\n\n@Serializable\ndata class Contact(\n    val id: Int,\n    val name: String,\n    val surname: String? = null,\n    val phoneNumber: String\n)\n\nval contactList = listOf(\n    Contact(100, \"Alice\", \"Smith\", \"+1 415 555 1234\"),\n    Contact(101, \"Bob\", \"Johnson\", \"+49 151 23456789\"),\n    Contact(102, \"Charlie\", \"Williams\", \"+36 20 123 4567\"),\n    Contact(103, \"Daniel\", \"Anderson\", \"+46 70 123 45 67\"),\n    Contact(104, \"Daniel\", \"Garcia\", \"+34 612 345 678\"),\n)\n\nval contactById = contactList.associateBy(Contact::id)\n</code></pre>"},{"location":"examples/Banking/#tools-money-transfer","title":"Tools: Money Transfer","text":"<p>Tools should be pure and predictable.</p> <p>We model two \u201csoft contracts\u201d: - <code>chooseRecipient</code> returns candidates when ambiguity is detected. - <code>sendMoney</code> supports a <code>confirmed</code> flag. If <code>false</code>, it asks the agent to confirm with the user.</p> <pre><code>import ai.koog.agents.core.tools.annotations.LLMDescription\nimport ai.koog.agents.core.tools.annotations.Tool\nimport ai.koog.agents.core.tools.reflect.ToolSet\n\n@LLMDescription(\"Tools for money transfer operations.\")\nclass MoneyTransferTools : ToolSet {\n\n    @Tool\n    @LLMDescription(\n        \"\"\"\n        Returns the list of contacts for the given user.\n        The user in this demo is always userId=123.\n        \"\"\"\n    )\n    fun getContacts(\n        @LLMDescription(\"The unique identifier of the user whose contact list is requested.\") userId: Int\n    ): String = buildString {\n        contactList.forEach { c -&gt;\n            appendLine(\"${c.id}: ${c.name} ${c.surname ?: \"\"} (${c.phoneNumber})\")\n        }\n    }.trimEnd()\n\n    @Tool\n    @LLMDescription(\"Returns the current balance (demo value).\")\n    fun getBalance(\n        @LLMDescription(\"The unique identifier of the user.\") userId: Int\n    ): String = \"Balance: 200.00 EUR\"\n\n    @Tool\n    @LLMDescription(\"Returns the default user currency (demo value).\")\n    fun getDefaultCurrency(\n        @LLMDescription(\"The unique identifier of the user.\") userId: Int\n    ): String = \"EUR\"\n\n    @Tool\n    @LLMDescription(\"Returns a demo FX rate between two ISO currencies (e.g. EUR\u2192USD).\")\n    fun getExchangeRate(\n        @LLMDescription(\"Base currency (e.g., EUR).\") from: String,\n        @LLMDescription(\"Target currency (e.g., USD).\") to: String\n    ): String = when (from.uppercase() to to.uppercase()) {\n        \"EUR\" to \"USD\" -&gt; \"1.10\"\n        \"EUR\" to \"GBP\" -&gt; \"0.86\"\n        \"GBP\" to \"EUR\" -&gt; \"1.16\"\n        \"USD\" to \"EUR\" -&gt; \"0.90\"\n        else -&gt; \"No information about exchange rate available.\"\n    }\n\n    @Tool\n    @LLMDescription(\n        \"\"\"\n        Returns a ranked list of possible recipients for an ambiguous name.\n        The agent should ask the user to pick one and then use the selected contact id.\n        \"\"\"\n    )\n    fun chooseRecipient(\n        @LLMDescription(\"An ambiguous or partial contact name.\") confusingRecipientName: String\n    ): String {\n        val matches = contactList.filter { c -&gt;\n            c.name.contains(confusingRecipientName, ignoreCase = true) ||\n                (c.surname?.contains(confusingRecipientName, ignoreCase = true) ?: false)\n        }\n        if (matches.isEmpty()) {\n            return \"No candidates found for '$confusingRecipientName'. Use getContacts and ask the user to choose.\"\n        }\n        return matches.mapIndexed { idx, c -&gt;\n            \"${idx + 1}. ${c.id}: ${c.name} ${c.surname ?: \"\"} (${c.phoneNumber})\"\n        }.joinToString(\"\\n\")\n    }\n\n    @Tool\n    @LLMDescription(\n        \"\"\"\n        Sends money from the user to a contact.\n        If confirmed=false, return \"REQUIRES_CONFIRMATION\" with a human-readable summary.\n        The agent should confirm with the user before retrying with confirmed=true.\n        \"\"\"\n    )\n    fun sendMoney(\n        @LLMDescription(\"Sender user id.\") senderId: Int,\n        @LLMDescription(\"Amount in sender's default currency.\") amount: Double,\n        @LLMDescription(\"Recipient contact id.\") recipientId: Int,\n        @LLMDescription(\"Short purpose/description.\") purpose: String,\n        @LLMDescription(\"Whether the user already confirmed this transfer.\") confirmed: Boolean = false\n    ): String {\n        val recipient = contactById[recipientId] ?: return \"Invalid recipient.\"\n        val summary = \"Transfer \u20ac%.2f to %s %s (%s) for \\\"%s\\\".\"\n            .format(amount, recipient.name, recipient.surname ?: \"\", recipient.phoneNumber, purpose)\n\n        if (!confirmed) {\n            return \"REQUIRES_CONFIRMATION: $summary\"\n        }\n\n        // In a real system this is where you'd call a payment API.\n        return \"Money was sent. $summary\"\n    }\n}\n</code></pre>"},{"location":"examples/Banking/#creating-your-first-agent","title":"Creating Your First Agent","text":"<p>Now let's create an agent that uses our money transfer tools. An agent combines an LLM with tools to accomplish tasks.</p> <pre><code>import ai.koog.agents.core.agent.AIAgent\nimport ai.koog.agents.core.agent.AIAgentService\nimport ai.koog.agents.core.tools.ToolRegistry\nimport ai.koog.agents.core.tools.reflect.asTools\nimport ai.koog.agents.ext.tool.AskUser\nimport ai.koog.prompt.executor.clients.openai.OpenAIModels\nimport kotlinx.coroutines.runBlocking\n\nval transferAgentService = AIAgentService(\n    executor = openAIExecutor,\n    llmModel = OpenAIModels.Chat.GPT4oMini,\n    systemPrompt = bankingAssistantSystemPrompt,\n    temperature = 0.0,  // Use deterministic responses for financial operations\n    toolRegistry = ToolRegistry {\n        tool(AskUser)\n        tools(MoneyTransferTools().asTools())\n    }\n)\n\n// Test the agent with various scenarios\nprintln(\"Banking Assistant started\")\nval message = \"Send 25 euros to Daniel for dinner at the restaurant.\"\n\n// Other test messages you can try:\n// - \"Send 50 euros to Alice for the concert tickets\"\n// - \"What's my current balance?\"\n// - \"Transfer 100 euros to Bob for the shared vacation expenses\"\n\nrunBlocking {\n    val result = transferAgentService.createAgentAndRun(message)\n    result\n}\n</code></pre> <pre><code>Banking Assistant started\nThere are two contacts named Daniel. Please confirm which one you would like to send money to:\n1. Daniel Anderson (+46 70 123 45 67)\n2. Daniel Garcia (+34 612 345 678)\nPlease confirm the transfer of \u20ac25.00 to Daniel Garcia (+34 612 345 678) for \"Dinner at the restaurant\".\n\n\n\n\n\nTask completed successfully.\n</code></pre>"},{"location":"examples/Banking/#adding-transaction-analytics","title":"Adding Transaction Analytics","text":"<p>Let's expand our assistant's capabilities with transaction analysis tools. First, we'll define the transaction domain model.</p> <pre><code>@Serializable\nenum class TransactionCategory(val title: String) {\n    FOOD_AND_DINING(\"Food &amp; Dining\"),\n    SHOPPING(\"Shopping\"),\n    TRANSPORTATION(\"Transportation\"),\n    ENTERTAINMENT(\"Entertainment\"),\n    GROCERIES(\"Groceries\"),\n    HEALTH(\"Health\"),\n    UTILITIES(\"Utilities\"),\n    HOME_IMPROVEMENT(\"Home Improvement\");\n\n    companion object {\n        fun fromString(value: String): TransactionCategory? =\n            entries.find { it.title.equals(value, ignoreCase = true) }\n\n        fun availableCategories(): String =\n            entries.joinToString(\", \") { it.title }\n    }\n}\n\n@Serializable\ndata class Transaction(\n    val merchant: String,\n    val amount: Double,\n    val category: TransactionCategory,\n    val date: LocalDateTime\n)\n</code></pre>"},{"location":"examples/Banking/#sample-transaction-data","title":"Sample transaction data","text":"<pre><code>val transactionAnalysisPrompt = \"\"\"\nToday is 2025-05-22.\nAvailable categories for transactions: ${TransactionCategory.availableCategories()}\n\"\"\"\n\nval sampleTransactions = listOf(\n    Transaction(\"Starbucks\", 5.99, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 22, 8, 30, 0, 0)),\n    Transaction(\"Amazon\", 129.99, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 22, 10, 15, 0, 0)),\n    Transaction(\n        \"Shell Gas Station\",\n        45.50,\n        TransactionCategory.TRANSPORTATION,\n        LocalDateTime(2025, 5, 21, 18, 45, 0, 0)\n    ),\n    Transaction(\"Netflix\", 15.99, TransactionCategory.ENTERTAINMENT, LocalDateTime(2025, 5, 21, 12, 0, 0, 0)),\n    Transaction(\"AMC Theaters\", 32.50, TransactionCategory.ENTERTAINMENT, LocalDateTime(2025, 5, 20, 19, 30, 0, 0)),\n    Transaction(\"Whole Foods\", 89.75, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 20, 16, 20, 0, 0)),\n    Transaction(\"Target\", 67.32, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 20, 14, 30, 0, 0)),\n    Transaction(\"CVS Pharmacy\", 23.45, TransactionCategory.HEALTH, LocalDateTime(2025, 5, 19, 11, 25, 0, 0)),\n    Transaction(\"Subway\", 12.49, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 19, 13, 15, 0, 0)),\n    Transaction(\"Spotify Premium\", 9.99, TransactionCategory.ENTERTAINMENT, LocalDateTime(2025, 5, 19, 14, 15, 0, 0)),\n    Transaction(\"AT&amp;T\", 85.00, TransactionCategory.UTILITIES, LocalDateTime(2025, 5, 18, 9, 0, 0, 0)),\n    Transaction(\"Home Depot\", 156.78, TransactionCategory.HOME_IMPROVEMENT, LocalDateTime(2025, 5, 18, 15, 45, 0, 0)),\n    Transaction(\"Amazon\", 129.99, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 17, 10, 15, 0, 0)),\n    Transaction(\"Starbucks\", 5.99, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 17, 8, 30, 0, 0)),\n    Transaction(\"Whole Foods\", 89.75, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 16, 16, 20, 0, 0)),\n    Transaction(\"CVS Pharmacy\", 23.45, TransactionCategory.HEALTH, LocalDateTime(2025, 5, 15, 11, 25, 0, 0)),\n    Transaction(\"AT&amp;T\", 85.00, TransactionCategory.UTILITIES, LocalDateTime(2025, 5, 14, 9, 0, 0, 0)),\n    Transaction(\"Xbox Game Pass\", 14.99, TransactionCategory.ENTERTAINMENT, LocalDateTime(2025, 5, 14, 16, 45, 0, 0)),\n    Transaction(\"Aldi\", 76.45, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 13, 17, 30, 0, 0)),\n    Transaction(\"Chipotle\", 15.75, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 13, 12, 45, 0, 0)),\n    Transaction(\"Best Buy\", 299.99, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 12, 14, 20, 0, 0)),\n    Transaction(\"Olive Garden\", 89.50, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 12, 19, 15, 0, 0)),\n    Transaction(\"Whole Foods\", 112.34, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 11, 10, 30, 0, 0)),\n    Transaction(\"Old Navy\", 45.99, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 11, 13, 45, 0, 0)),\n    Transaction(\"Panera Bread\", 18.25, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 10, 11, 30, 0, 0)),\n    Transaction(\"Costco\", 245.67, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 10, 15, 20, 0, 0)),\n    Transaction(\"Five Guys\", 22.50, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 9, 18, 30, 0, 0)),\n    Transaction(\"Macy's\", 156.78, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 9, 14, 15, 0, 0)),\n    Transaction(\"Hulu Plus\", 12.99, TransactionCategory.ENTERTAINMENT, LocalDateTime(2025, 5, 8, 20, 0, 0, 0)),\n    Transaction(\"Whole Foods\", 94.23, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 8, 16, 45, 0, 0)),\n    Transaction(\"Texas Roadhouse\", 78.90, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 8, 19, 30, 0, 0)),\n    Transaction(\"Walmart\", 167.89, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 7, 11, 20, 0, 0)),\n    Transaction(\"Chick-fil-A\", 14.75, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 7, 12, 30, 0, 0)),\n    Transaction(\"Aldi\", 82.45, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 6, 15, 45, 0, 0)),\n    Transaction(\"TJ Maxx\", 67.90, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 6, 13, 20, 0, 0)),\n    Transaction(\"P.F. Chang's\", 95.40, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 5, 19, 15, 0, 0)),\n    Transaction(\"Whole Foods\", 78.34, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 4, 14, 30, 0, 0)),\n    Transaction(\"H&amp;M\", 89.99, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 3, 16, 20, 0, 0)),\n    Transaction(\"Red Lobster\", 112.45, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 2, 18, 45, 0, 0)),\n    Transaction(\"Whole Foods\", 67.23, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 2, 11, 30, 0, 0)),\n    Transaction(\"Marshalls\", 123.45, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 1, 15, 20, 0, 0)),\n    Transaction(\n        \"Buffalo Wild Wings\",\n        45.67,\n        TransactionCategory.FOOD_AND_DINING,\n        LocalDateTime(2025, 5, 1, 19, 30, 0, 0)\n    ),\n    Transaction(\"Aldi\", 145.78, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 1, 10, 15, 0, 0))\n)\n</code></pre>"},{"location":"examples/Banking/#transaction-analysis-tools","title":"Transaction Analysis Tools","text":"<pre><code>@LLMDescription(\"Tools for analyzing transaction history\")\nclass TransactionAnalysisTools : ToolSet {\n\n    @Tool\n    @LLMDescription(\n        \"\"\"\n        Retrieves transactions filtered by userId, category, start date, and end date.\n        All parameters are optional. If no parameters are provided, all transactions are returned.\n        Dates should be in the format YYYY-MM-DD.\n        \"\"\"\n    )\n    fun getTransactions(\n        @LLMDescription(\"The ID of the user whose transactions to retrieve.\")\n        userId: String? = null,\n        @LLMDescription(\"The category to filter transactions by (e.g., 'Food &amp; Dining').\")\n        category: String? = null,\n        @LLMDescription(\"The start date to filter transactions by, in the format YYYY-MM-DD.\")\n        startDate: String? = null,\n        @LLMDescription(\"The end date to filter transactions by, in the format YYYY-MM-DD.\")\n        endDate: String? = null\n    ): String {\n        var filteredTransactions = sampleTransactions\n\n        // Validate userId (in production, this would query a real database)\n        if (userId != null &amp;&amp; userId != \"123\") {\n            return \"No transactions found for user $userId.\"\n        }\n\n        // Apply category filter\n        category?.let { cat -&gt;\n            val categoryEnum = TransactionCategory.fromString(cat)\n                ?: return \"Invalid category: $cat. Available: ${TransactionCategory.availableCategories()}\"\n            filteredTransactions = filteredTransactions.filter { it.category == categoryEnum }\n        }\n\n        // Apply date range filters\n        startDate?.let { date -&gt;\n            val startDateTime = parseDate(date, startOfDay = true)\n            filteredTransactions = filteredTransactions.filter { it.date &gt;= startDateTime }\n        }\n\n        endDate?.let { date -&gt;\n            val endDateTime = parseDate(date, startOfDay = false)\n            filteredTransactions = filteredTransactions.filter { it.date &lt;= endDateTime }\n        }\n\n        if (filteredTransactions.isEmpty()) {\n            return \"No transactions found matching the specified criteria.\"\n        }\n\n        return filteredTransactions.joinToString(\"\\n\") { transaction -&gt;\n            \"${transaction.date}: ${transaction.merchant} - \" +\n                \"$${transaction.amount} (${transaction.category.title})\"\n        }\n    }\n\n    @Tool\n    @LLMDescription(\"Calculates the sum of an array of double numbers.\")\n    fun sumArray(\n        @LLMDescription(\"Comma-separated list of double numbers to sum (e.g., '1.5,2.3,4.7').\")\n        numbers: String\n    ): String {\n        val numbersList = numbers.split(\",\")\n            .mapNotNull { it.trim().toDoubleOrNull() }\n        val sum = numbersList.sum()\n        return \"Sum: $%.2f\".format(sum)\n    }\n\n    // Helper function to parse dates\n    private fun parseDate(dateStr: String, startOfDay: Boolean): LocalDateTime {\n        val parts = dateStr.split(\"-\").map { it.toInt() }\n        require(parts.size == 3) { \"Invalid date format. Use YYYY-MM-DD\" }\n\n        return if (startOfDay) {\n            LocalDateTime(parts[0], parts[1], parts[2], 0, 0, 0, 0)\n        } else {\n            LocalDateTime(parts[0], parts[1], parts[2], 23, 59, 59, 999999999)\n        }\n    }\n}\n</code></pre> <pre><code>val analysisAgentService = AIAgentService(\n    executor = openAIExecutor,\n    llmModel = OpenAIModels.Chat.GPT4oMini,\n    systemPrompt = \"$bankingAssistantSystemPrompt\\n$transactionAnalysisPrompt\",\n    temperature = 0.0,\n    toolRegistry = ToolRegistry {\n        tools(TransactionAnalysisTools().asTools())\n    }\n)\n\nprintln(\"Transaction Analysis Assistant started\")\nval analysisMessage = \"How much have I spent on restaurants this month?\"\n\n// Other queries to try:\n// - \"What's my maximum check at a restaurant this month?\"\n// - \"How much did I spend on groceries in the first week of May?\"\n// - \"What's my total spending on entertainment in May?\"\n// - \"Show me all transactions from last week\"\n\nrunBlocking {\n    val result = analysisAgentService.createAgentAndRun(analysisMessage)\n    result\n}\n</code></pre> <pre><code>Transaction Analysis Assistant started\n\n\n\n\n\nYou have spent a total of $517.64 on restaurants this month.\n\nTask completed successfully.\n</code></pre>"},{"location":"examples/Banking/#building-an-agent-with-graph","title":"Building an Agent with Graph","text":"<p>Now let's combine our specialized agents into a graph agent that can route requests to the appropriate handler.</p>"},{"location":"examples/Banking/#request-classification","title":"Request Classification","text":"<p>First, we need a way to classify incoming requests:</p> <pre><code>import ai.koog.agents.core.tools.annotations.LLMDescription\nimport kotlinx.serialization.SerialName\nimport kotlinx.serialization.Serializable\n\n@Suppress(\"unused\")\n@SerialName(\"UserRequestType\")\n@Serializable\n@LLMDescription(\"Type of user request: Transfer or Analytics\")\nenum class RequestType { Transfer, Analytics }\n\n@Serializable\n@LLMDescription(\"The bank request that was classified by the agent.\")\ndata class ClassifiedBankRequest(\n    @property:LLMDescription(\"Type of request: Transfer or Analytics\")\n    val requestType: RequestType,\n    @property:LLMDescription(\"Actual request to be performed by the banking application\")\n    val userRequest: String\n)\n</code></pre>"},{"location":"examples/Banking/#shared-tool-registry","title":"Shared tool registry","text":"<pre><code>// Create a comprehensive tool registry for the multi-agent system\nval toolRegistry = ToolRegistry {\n    tool(AskUser)  // Allow agents to ask for clarification\n    tools(MoneyTransferTools().asTools())\n    tools(TransactionAnalysisTools().asTools())\n}\n</code></pre>"},{"location":"examples/Banking/#agent-strategy","title":"Agent Strategy","text":"<p>Now we'll create a strategy that orchestrates multiple nodes:</p> <pre><code>import ai.koog.agents.core.dsl.builder.forwardTo\nimport ai.koog.agents.core.dsl.builder.strategy\nimport ai.koog.agents.core.dsl.extension.*\nimport ai.koog.agents.ext.agent.subgraphWithTask\nimport ai.koog.prompt.structure.StructureFixingParser\n\nval strategy = strategy&lt;String, String&gt;(\"banking assistant\") {\n\n    // Subgraph for classifying user requests\n    val classifyRequest by subgraph&lt;String, ClassifiedBankRequest&gt;(\n        tools = listOf(AskUser)\n    ) {\n        // Use structured output to ensure proper classification\n        val requestClassification by nodeLLMRequestStructured&lt;ClassifiedBankRequest&gt;(\n            examples = listOf(\n                ClassifiedBankRequest(\n                    requestType = RequestType.Transfer,\n                    userRequest = \"Send 25 euros to Daniel for dinner at the restaurant.\"\n                ),\n                ClassifiedBankRequest(\n                    requestType = RequestType.Analytics,\n                    userRequest = \"Provide transaction overview for the last month\"\n                )\n            ),\n            fixingParser = StructureFixingParser(\n                model = OpenAIModels.Chat.GPT4oMini,\n                retries = 2,\n            )\n        )\n\n        val callLLM by nodeLLMRequest()\n        val callAskUserTool by nodeExecuteTool()\n\n        // Define the flow\n        edge(nodeStart forwardTo requestClassification)\n\n        edge(\n            requestClassification forwardTo nodeFinish\n                onCondition { it.isSuccess }\n                transformed { it.getOrThrow().data }\n        )\n\n        edge(\n            requestClassification forwardTo callLLM\n                onCondition { it.isFailure }\n                transformed { \"Failed to understand the user's intent\" }\n        )\n\n        edge(callLLM forwardTo callAskUserTool onToolCall { true })\n\n        edge(\n            callLLM forwardTo callLLM onAssistantMessage { true }\n                transformed { \"Please call `${AskUser.name}` tool instead of chatting\" }\n        )\n\n        edge(callAskUserTool forwardTo requestClassification\n            transformed { it.result.toString() })\n    }\n\n    // Subgraph for handling money transfers\n    val transferMoney by subgraphWithTask&lt;ClassifiedBankRequest, String&gt;(\n        tools = MoneyTransferTools().asTools() + AskUser,\n        llmModel = OpenAIModels.Chat.GPT4o  // Use more capable model for transfers\n    ) { request -&gt;\n        \"\"\"\n        $bankingAssistantSystemPrompt\n        Specifically, you need to help with the following request:\n        ${request.userRequest}\n        \"\"\".trimIndent()\n    }\n\n    // Subgraph for transaction analysis\n    val transactionAnalysis by subgraphWithTask&lt;ClassifiedBankRequest, String&gt;(\n        tools = TransactionAnalysisTools().asTools() + AskUser,\n    ) { request -&gt;\n        \"\"\"\n        $bankingAssistantSystemPrompt\n        $transactionAnalysisPrompt\n        Specifically, you need to help with the following request:\n        ${request.userRequest}\n        \"\"\".trimIndent()\n    }\n\n    // Connect the subgraphs\n    edge(nodeStart forwardTo classifyRequest)\n\n    edge(classifyRequest forwardTo transferMoney\n        onCondition { it.requestType == RequestType.Transfer })\n\n    edge(classifyRequest forwardTo transactionAnalysis\n        onCondition { it.requestType == RequestType.Analytics })\n\n    // Route results to finish node\n    edge(transferMoney forwardTo nodeFinish)\n    edge(transactionAnalysis forwardTo nodeFinish)\n}\n</code></pre> <pre><code>import ai.koog.agents.core.agent.config.AIAgentConfig\nimport ai.koog.prompt.dsl.prompt\n\nval agentConfig = AIAgentConfig(\n    prompt = prompt(id = \"banking assistant\") {\n        system(\"$bankingAssistantSystemPrompt\\n$transactionAnalysisPrompt\")\n    },\n    model = OpenAIModels.Chat.GPT4o,\n    maxAgentIterations = 50  // Allow for complex multi-step operations\n)\n\nval agent = AIAgent&lt;String, String&gt;(\n    promptExecutor = openAIExecutor,\n    strategy = strategy,\n    agentConfig = agentConfig,\n    toolRegistry = toolRegistry,\n)\n</code></pre>"},{"location":"examples/Banking/#run-graph-agent","title":"Run graph agent","text":"<pre><code>println(\"Banking Assistant started\")\nval testMessage = \"Send 25 euros to Daniel for dinner at the restaurant.\"\n\n// Test various scenarios:\n// Transfer requests:\n//   - \"Send 50 euros to Alice for the concert tickets\"\n//   - \"Transfer 100 to Bob for groceries\"\n//   - \"What's my current balance?\"\n//\n// Analytics requests:\n//   - \"How much have I spent on restaurants this month?\"\n//   - \"What's my maximum check at a restaurant this month?\"\n//   - \"How much did I spend on groceries in the first week of May?\"\n//   - \"What's my total spending on entertainment in May?\"\n\nrunBlocking {\n    val result = agent.run(testMessage)\n    \"Result: $result\"\n}\n</code></pre> <pre><code>Banking Assistant started\nI found multiple contacts with the name Daniel. Please choose the correct one:\n1. Daniel Anderson (+46 70 123 45 67)\n2. Daniel Garcia (+34 612 345 678)\nPlease specify the number of the correct recipient.\nPlease confirm if you would like to proceed with sending \u20ac25 to Daniel Garcia for \"dinner at the restaurant.\"\n\n\n\n\n\nResult: Task completed successfully.\n</code></pre>"},{"location":"examples/Banking/#agent-composition-using-agents-as-tools","title":"Agent Composition \u2014 Using Agents as Tools","text":"<p>Koog allows you to use agents as tools within other agents, enabling powerful composition patterns.</p> <pre><code>import ai.koog.agents.core.agent.createAgentTool\nimport ai.koog.agents.core.tools.ToolParameterDescriptor\nimport ai.koog.agents.core.tools.ToolParameterType\n\nval classifierAgent = AIAgent(\n    executor = openAIExecutor,\n    llmModel = OpenAIModels.Chat.GPT4oMini,\n    toolRegistry = ToolRegistry {\n        tool(AskUser)\n\n        // Convert agents into tools\n        tool(\n            transferAgentService.createAgentTool(\n                agentName = \"transferMoney\",\n                agentDescription = \"Transfers money and handles all related operations\",\n                inputDescriptor = ToolParameterDescriptor(\n                    name = \"request\",\n                    description = \"Transfer request from the user\",\n                    type = ToolParameterType.String\n                )\n            )\n        )\n\n        tool(\n            analysisAgentService.createAgentTool(\n                agentName = \"analyzeTransactions\",\n                agentDescription = \"Performs analytics on user transactions\",\n                inputDescriptor = ToolParameterDescriptor(\n                    name = \"request\",\n                    description = \"Transaction analytics request\",\n                    type = ToolParameterType.String\n                )\n            )\n        )\n    },\n    systemPrompt = \"$bankingAssistantSystemPrompt\\n$transactionAnalysisPrompt\"\n)\n</code></pre>"},{"location":"examples/Banking/#run-composed-agent","title":"Run composed agent","text":"<pre><code>println(\"Banking Assistant started\")\nval composedMessage = \"Send 25 euros to Daniel for dinner at the restaurant.\"\n\nrunBlocking {\n    val result = classifierAgent.run(composedMessage)\n    \"Result: $result\"\n}\n</code></pre> <pre><code>Banking Assistant started\nThere are two contacts named Daniel. Please confirm which one you would like to send money to:\n1. Daniel Anderson (+46 70 123 45 67)\n2. Daniel Garcia (+34 612 345 678)\nPlease confirm the transfer of \u20ac25.00 to Daniel Anderson (+46 70 123 45 67) for \"Dinner at the restaurant\".\n\n\n\n\n\nResult: Can't perform the task.\n</code></pre>"},{"location":"examples/Banking/#summary","title":"Summary","text":"<p>In this tutorial, you've learned how to:</p> <ol> <li>Create LLM-powered tools with clear descriptions that help the AI understand when and how to use them</li> <li>Build single-purpose agents that combine LLMs with tools to accomplish specific tasks</li> <li>Implement graph agent using strategies and subgraphs for complex workflows</li> <li>Compose agents by using them as tools within other agents</li> <li>Handle user interactions including confirmations and disambiguation</li> </ol>"},{"location":"examples/Banking/#best-practices","title":"Best Practices","text":"<ol> <li>Clear tool descriptions: Write detailed LLMDescription annotations to help the AI understand tool usage</li> <li>Idiomatic Kotlin: Use Kotlin features like data classes, extension functions, and scope functions</li> <li>Error handling: Always validate inputs and provide meaningful error messages</li> <li>User experience: Include confirmation steps for critical operations like money transfers</li> <li>Modularity: Separate concerns into different tools and agents for better maintainability</li> </ol>"},{"location":"examples/BedrockAgent/","title":"Building AI Agents with AWS Bedrock and Koog Framework","text":"<p> Open on GitHub  Download .ipynb</p> <p>Welcome to this comprehensive guide on creating intelligent AI agents using the Koog framework with AWS Bedrock integration. In this notebook, we'll walk through building a functional agent that can control a simple switch device through natural language commands.</p>"},{"location":"examples/BedrockAgent/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>How to define custom tools for AI agents using Kotlin annotations</li> <li>Setting up AWS Bedrock integration for LLM-powered agents</li> <li>Creating tool registries and connecting them to agents</li> <li>Building interactive agents that can understand and execute commands</li> </ul>"},{"location":"examples/BedrockAgent/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS Bedrock access with appropriate permissions</li> <li>AWS credentials configured (access key and secret key)</li> <li>Basic understanding of Kotlin coroutines</li> </ul> <p>Let's dive into building our first Bedrock-powered AI agent!</p> <pre><code>%useLatestDescriptors\n// %use koog\n</code></pre> <pre><code>import ai.koog.agents.core.tools.annotations.LLMDescription\nimport ai.koog.agents.core.tools.annotations.Tool\nimport ai.koog.agents.core.tools.reflect.ToolSet\n\n// Simple state-holding device that our agent will control\nclass Switch {\n    private var state: Boolean = false\n\n    fun switch(on: Boolean) {\n        state = on\n    }\n\n    fun isOn(): Boolean {\n        return state\n    }\n}\n\n/**\n * ToolSet implementation that exposes switch operations to the AI agent.\n *\n * Key concepts:\n * - @Tool annotation marks methods as callable by the agent\n * - @LLMDescription provides natural language descriptions for the LLM\n * - ToolSet interface allows grouping related tools together\n */\nclass SwitchTools(val switch: Switch) : ToolSet {\n\n    @Tool\n    @LLMDescription(\"Switches the state of the switch to on or off\")\n    fun switchState(state: Boolean): String {\n        switch.switch(state)\n        return \"Switch turned ${if (state) \"on\" else \"off\"} successfully\"\n    }\n\n    @Tool\n    @LLMDescription(\"Returns the current state of the switch (on or off)\")\n    fun getCurrentState(): String {\n        return \"Switch is currently ${if (switch.isOn()) \"on\" else \"off\"}\"\n    }\n}\n</code></pre> <pre><code>import ai.koog.agents.core.tools.ToolRegistry\nimport ai.koog.agents.core.tools.reflect.asTools\n\n// Create our switch instance\nval switch = Switch()\n\n// Build the tool registry with our switch tools\nval toolRegistry = ToolRegistry {\n    // Convert our ToolSet to individual tools and register them\n    tools(SwitchTools(switch).asTools())\n}\n\nprintln(\"\u2705 Tool registry created with ${toolRegistry.tools.size} tools:\")\ntoolRegistry.tools.forEach { tool -&gt;\n    println(\"  - ${tool.name}\")\n}\n</code></pre> <pre><code>\u2705 Tool registry created with 2 tools:\n  - getCurrentState\n  - switchState\n</code></pre> <pre><code>import ai.koog.prompt.executor.clients.bedrock.BedrockClientSettings\nimport ai.koog.prompt.executor.clients.bedrock.BedrockRegions\n\nval region = BedrockRegions.US_WEST_2.regionCode\nval maxRetries = 3\n\n// Configure Bedrock client settings\nval bedrockSettings = BedrockClientSettings(\n    region = region, // Choose your preferred AWS region\n    maxRetries = maxRetries // Number of retry attempts for failed requests\n)\n\nprintln(\"\ud83c\udf10 Bedrock configured for region: $region\")\nprintln(\"\ud83d\udd04 Max retries set to: $maxRetries\")\n</code></pre> <pre><code>\ud83c\udf10 Bedrock configured for region: us-west-2\n\ud83d\udd04 Max retries set to: 3\n</code></pre> <pre><code>import ai.koog.prompt.executor.llms.all.simpleBedrockExecutor\n\n// Create the Bedrock LLM executor with credentials from environment\nval executor = simpleBedrockExecutor(\n    awsAccessKeyId = System.getenv(\"AWS_BEDROCK_ACCESS_KEY\")\n        ?: throw IllegalStateException(\"AWS_BEDROCK_ACCESS_KEY environment variable not set\"),\n    awsSecretAccessKey = System.getenv(\"AWS_BEDROCK_SECRET_ACCESS_KEY\")\n        ?: throw IllegalStateException(\"AWS_BEDROCK_SECRET_ACCESS_KEY environment variable not set\"),\n    settings = bedrockSettings\n)\n\nprintln(\"\ud83d\udd10 Bedrock executor initialized successfully\")\nprintln(\"\ud83d\udca1 Pro tip: Set AWS_BEDROCK_ACCESS_KEY and AWS_BEDROCK_SECRET_ACCESS_KEY environment variables\")\n</code></pre> <pre><code>\ud83d\udd10 Bedrock executor initialized successfully\n\ud83d\udca1 Pro tip: Set AWS_BEDROCK_ACCESS_KEY and AWS_BEDROCK_SECRET_ACCESS_KEY environment variables\n</code></pre> <pre><code>import ai.koog.agents.core.agent.AIAgent\nimport ai.koog.prompt.executor.clients.bedrock.BedrockModels\n\nval agent = AIAgent(\n    executor = executor,\n    llmModel = BedrockModels.AnthropicClaude35SonnetV2, // State-of-the-art reasoning model\n    systemPrompt = \"\"\"\n        You are a helpful assistant that controls a switch device.\n\n        You can:\n        - Turn the switch on or off when requested\n        - Check the current state of the switch\n        - Explain what you're doing\n\n        Always be clear about the switch's current state and confirm actions taken.\n    \"\"\".trimIndent(),\n    temperature = 0.1, // Low temperature for consistent, focused responses\n    toolRegistry = toolRegistry\n)\n\nprintln(\"\ud83e\udd16 AI Agent created successfully!\")\nprintln(\"\ud83d\udccb System prompt configured\")\nprintln(\"\ud83d\udee0\ufe0f  Tools available: ${toolRegistry.tools.size}\")\nprintln(\"\ud83c\udfaf Model: ${BedrockModels.AnthropicClaude35SonnetV2}\")\nprintln(\"\ud83c\udf21\ufe0f  Temperature: 0.1 (focused responses)\")\n</code></pre> <pre><code>\ud83e\udd16 AI Agent created successfully!\n\ud83d\udccb System prompt configured\n\ud83d\udee0\ufe0f  Tools available: 2\n\ud83c\udfaf Model: LLModel(provider=Bedrock, id=us.anthropic.claude-3-5-sonnet-20241022-v2:0, capabilities=[Temperature, Tools, ToolChoice, Image, Document, Completion], contextLength=200000, maxOutputTokens=8192)\n\ud83c\udf21\ufe0f  Temperature: 0.1 (focused responses)\n</code></pre> <pre><code>import kotlinx.coroutines.runBlocking\n\nprintln(\"\ud83c\udf89 Bedrock Agent with Switch Tools - Ready to Go!\")\nprintln(\"\ud83d\udcac You can ask me to:\")\nprintln(\"   \u2022 Turn the switch on/off\")\nprintln(\"   \u2022 Check the current switch state\")\nprintln(\"   \u2022 Ask questions about the switch\")\nprintln()\nprintln(\"\ud83d\udca1 Example: 'Please turn on the switch' or 'What's the current state?'\")\nprintln(\"\ud83d\udcdd Type your request:\")\n\nval input = readln()\nprintln(\"\\n\ud83e\udd16 Processing your request...\")\n\nrunBlocking {\n    val response = agent.run(input)\n    println(\"\\n\u2728 Agent response:\")\n    println(response)\n}\n</code></pre> <pre><code>\ud83c\udf89 Bedrock Agent with Switch Tools - Ready to Go!\n\ud83d\udcac You can ask me to:\n   \u2022 Turn the switch on/off\n   \u2022 Check the current switch state\n   \u2022 Ask questions about the switch\n\n\ud83d\udca1 Example: 'Please turn on the switch' or 'What's the current state?'\n\ud83d\udcdd Type your request:\n\n\n\nThe execution was interrupted\n</code></pre>"},{"location":"examples/BedrockAgent/#what-just-happened","title":"What Just Happened? \ud83c\udfaf","text":"<p>When you run the agent, here's the magic that occurs behind the scenes:</p> <ol> <li>Natural Language Processing: Your input is sent to Claude 3.5 Sonnet via Bedrock</li> <li>Intent Recognition: The model understands what you want to do with the switch</li> <li>Tool Selection: Based on your request, the agent decides which tools to call</li> <li>Action Execution: The appropriate tool methods are invoked on your switch object</li> <li>Response Generation: The agent formulates a natural language response about what happened</li> </ol> <p>This demonstrates the core power of the Koog framework - seamless integration between natural language understanding and programmatic actions.</p>"},{"location":"examples/BedrockAgent/#next-steps-extensions","title":"Next Steps &amp; Extensions","text":"<p>Ready to take this further? Here are some ideas to explore:</p>"},{"location":"examples/BedrockAgent/#enhanced-tools","title":"\ud83d\udd27 Enhanced Tools","text":"<pre><code>@Tool\n@LLMDescription(\"Sets a timer to automatically turn off the switch after specified seconds\")\nfun setAutoOffTimer(seconds: Int): String\n\n@Tool\n@LLMDescription(\"Gets the switch usage statistics and history\")\nfun getUsageStats(): String\n</code></pre>"},{"location":"examples/BedrockAgent/#multiple-devices","title":"\ud83c\udf10 Multiple Devices","text":"<pre><code>class HomeAutomationTools : ToolSet {\n    @Tool fun controlLight(room: String, on: Boolean): String\n    @Tool fun setThermostat(temperature: Double): String\n    @Tool fun lockDoor(doorName: String): String\n}\n</code></pre>"},{"location":"examples/BedrockAgent/#memory-context","title":"\ud83e\udde0 Memory &amp; Context","text":"<pre><code>val agent = AIAgent(\n    executor = executor,\n    // ... other config\n    features = listOf(\n        MemoryFeature(), // Remember past interactions\n        LoggingFeature()  // Track all actions\n    )\n)\n</code></pre>"},{"location":"examples/BedrockAgent/#advanced-workflows","title":"\ud83d\udd04 Advanced Workflows","text":"<pre><code>// Multi-step workflows with conditional logic\n@Tool\n@LLMDescription(\"Executes evening routine: dims lights, locks doors, sets thermostat\")\nfun eveningRoutine(): String\n</code></pre>"},{"location":"examples/BedrockAgent/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Tools are functions: Any Kotlin function can become an agent capability \u2705 Annotations drive behavior: @Tool and @LLMDescription make functions discoverable \u2705 ToolSets organize capabilities: Group related tools together logically \u2705 Registries are toolboxes: ToolRegistry contains all available agent capabilities \u2705 Agents orchestrate everything: AIAgent brings LLM intelligence + tools together</p> <p>The Koog framework makes it incredibly straightforward to build sophisticated AI agents that can understand natural language and take real-world actions. Start simple, then expand your agent's capabilities by adding more tools and features as needed.</p> <p>Happy agent building! \ud83d\ude80</p>"},{"location":"examples/BedrockAgent/#testing-the-agent","title":"Testing the Agent","text":"<p>Time to see our agent in action! The agent can now understand natural language requests and use the tools we've provided to control the switch.</p> <p>Try these commands: - \"Turn on the switch\" - \"What's the current state?\" - \"Switch it off please\" - \"Is the switch on or off?\"</p>"},{"location":"examples/Calculator/","title":"Building a Tool-Calling Calculator Agent with Koog","text":"<p> Open on GitHub  Download .ipynb</p> <p>In this mini-tutorial we\u2019ll build a calculator agent powered by Koog tool-calling. You\u2019ll learn how to: - Design small, pure tools for arithmetic - Orchestrate parallel tool calls with Koog\u2019s multiple-call strategy - Add lightweight event logging for transparency - Run with OpenAI (and optionally Ollama)</p> <p>We\u2019ll keep the API tidy and idiomatic Kotlin, returning predictable results and handling edge cases (like division by zero) gracefully.</p>"},{"location":"examples/Calculator/#setup","title":"Setup","text":"<p>We assume you\u2019re in a Kotlin Notebook environment with Koog available. Provide an LLM executor</p> <pre><code>%useLatestDescriptors\n%use koog\n\n\nval OPENAI_API_KEY = System.getenv(\"OPENAI_API_KEY\")\n    ?: error(\"Please set the OPENAI_API_KEY environment variable\")\n\nval executor = simpleOpenAIExecutor(OPENAI_API_KEY)\n</code></pre>"},{"location":"examples/Calculator/#calculator-tools","title":"Calculator Tools","text":"<p>Tools are small, pure functions with clear contracts. We\u2019ll use <code>Double</code> for better precision and format outputs consistently.</p> <pre><code>import ai.koog.agents.core.tools.annotations.Tool\n\n// Format helper: integers render cleanly, decimals keep reasonable precision.\nprivate fun Double.pretty(): String =\n    if (abs(this % 1.0) &lt; 1e-9) this.toLong().toString() else \"%.10g\".format(this)\n\n@LLMDescription(\"Tools for basic calculator operations\")\nclass CalculatorTools : ToolSet {\n\n    @Tool\n    @LLMDescription(\"Adds two numbers and returns the sum as text.\")\n    fun plus(\n        @LLMDescription(\"First addend.\") a: Double,\n        @LLMDescription(\"Second addend.\") b: Double\n    ): String = (a + b).pretty()\n\n    @Tool\n    @LLMDescription(\"Subtracts the second number from the first and returns the difference as text.\")\n    fun minus(\n        @LLMDescription(\"Minuend.\") a: Double,\n        @LLMDescription(\"Subtrahend.\") b: Double\n    ): String = (a - b).pretty()\n\n    @Tool\n    @LLMDescription(\"Multiplies two numbers and returns the product as text.\")\n    fun multiply(\n        @LLMDescription(\"First factor.\") a: Double,\n        @LLMDescription(\"Second factor.\") b: Double\n    ): String = (a * b).pretty()\n\n    @Tool\n    @LLMDescription(\"Divides the first number by the second and returns the quotient as text. Returns an error message on division by zero.\")\n    fun divide(\n        @LLMDescription(\"Dividend.\") a: Double,\n        @LLMDescription(\"Divisor (must not be zero).\") b: Double\n    ): String = if (abs(b) &lt; 1e-12) {\n        \"ERROR: Division by zero\"\n    } else {\n        (a / b).pretty()\n    }\n}\n</code></pre>"},{"location":"examples/Calculator/#tool-registry","title":"Tool Registry","text":"<p>Expose our tools (plus two built-ins for interaction/logging).</p> <pre><code>val toolRegistry = ToolRegistry {\n    tool(AskUser)   // enables explicit user clarification when needed\n    tool(SayToUser) // allows the agent to present the final message to the user\n    tools(CalculatorTools())\n}\n</code></pre>"},{"location":"examples/Calculator/#strategy-multiple-tool-calls-with-optional-compression","title":"Strategy: Multiple Tool Calls (with Optional Compression)","text":"<p>This strategy lets the LLM propose multiple tool calls at once (e.g., <code>plus</code>, <code>minus</code>, <code>multiply</code>, <code>divide</code>) and then sends the results back. If the token usage grows too large, we compress the history of tool results before continuing.</p> <pre><code>import ai.koog.agents.core.environment.ReceivedToolResult\n\nobject CalculatorStrategy {\n    private const val MAX_TOKENS_THRESHOLD = 1000\n\n    val strategy = strategy&lt;String, String&gt;(\"test\") {\n        val callLLM by nodeLLMRequestMultiple()\n        val executeTools by nodeExecuteMultipleTools(parallelTools = true)\n        val sendToolResults by nodeLLMSendMultipleToolResults()\n        val compressHistory by nodeLLMCompressHistory&lt;List&lt;ReceivedToolResult&gt;&gt;()\n\n        edge(nodeStart forwardTo callLLM)\n\n        // If the assistant produced a final answer, finish.\n        edge((callLLM forwardTo nodeFinish) transformed { it.first() } onAssistantMessage { true })\n\n        // Otherwise, run the tools LLM requested (possibly several in parallel).\n        edge((callLLM forwardTo executeTools) onMultipleToolCalls { true })\n\n        // If we\u2019re getting large, compress past tool results before continuing.\n        edge(\n            (executeTools forwardTo compressHistory)\n                onCondition { llm.readSession { prompt.latestTokenUsage &gt; MAX_TOKENS_THRESHOLD } }\n        )\n        edge(compressHistory forwardTo sendToolResults)\n\n        // Normal path: send tool results back to the LLM.\n        edge(\n            (executeTools forwardTo sendToolResults)\n                onCondition { llm.readSession { prompt.latestTokenUsage &lt;= MAX_TOKENS_THRESHOLD } }\n        )\n\n        // LLM might request more tools after seeing results.\n        edge((sendToolResults forwardTo executeTools) onMultipleToolCalls { true })\n\n        // Or it can produce the final answer.\n        edge((sendToolResults forwardTo nodeFinish) transformed { it.first() } onAssistantMessage { true })\n    }\n}\n</code></pre>"},{"location":"examples/Calculator/#agent-configuration","title":"Agent Configuration","text":"<p>A minimal, tool-forward prompt works well. Keep temperature low for deterministic math.</p> <pre><code>val agentConfig = AIAgentConfig(\n    prompt = prompt(\"calculator\") {\n        system(\"You are a calculator. Always use the provided tools for arithmetic.\")\n    },\n    model = OpenAIModels.Chat.GPT4o,\n    maxAgentIterations = 50\n)\n</code></pre> <pre><code>import ai.koog.agents.features.eventHandler.feature.handleEvents\n\nval agent = AIAgent(\n    promptExecutor = executor,\n    strategy = CalculatorStrategy.strategy,\n    agentConfig = agentConfig,\n    toolRegistry = toolRegistry\n) {\n    handleEvents {\n        onToolCallStarting { e -&gt;\n            println(\"Tool called: ${e.tool.name}, args=${e.toolArgs}\")\n        }\n        onAgentExecutionFailed { e -&gt;\n            println(\"Agent error: ${e.throwable.message}\")\n        }\n        onAgentCompleted { e -&gt;\n            println(\"Final result: ${e.result}\")\n        }\n    }\n}\n</code></pre>"},{"location":"examples/Calculator/#try-it","title":"Try It","text":"<p>The agent should decompose the expression into parallel tool calls and return a neatly formatted result.</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nrunBlocking {\n    agent.run(\"(10 + 20) * (5 + 5) / (2 - 11)\")\n}\n// Expected final value \u2248 -33.333...\n</code></pre> <pre><code>Tool called: plus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=10.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=20.0})\nTool called: plus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=5.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=5.0})\nTool called: minus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.minus(kotlin.Double, kotlin.Double): kotlin.String=2.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.minus(kotlin.Double, kotlin.Double): kotlin.String=11.0})\nTool called: multiply, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.multiply(kotlin.Double, kotlin.Double): kotlin.String=30.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.multiply(kotlin.Double, kotlin.Double): kotlin.String=10.0})\nTool called: divide, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=1.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=-9.0})\nTool called: divide, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=300.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=-9.0})\nFinal result: The result of the expression \\((10 + 20) * (5 + 5) / (2 - 11)\\) is approximately \\(-33.33\\).\n\n\n\n\n\nThe result of the expression \\((10 + 20) * (5 + 5) / (2 - 11)\\) is approximately \\(-33.33\\).\n</code></pre>"},{"location":"examples/Calculator/#try-forcing-parallel-calls","title":"Try Forcing Parallel Calls","text":"<p>Ask the model to call all needed tools at once. You should still see a correct plan and stable execution.</p> <pre><code>runBlocking {\n    agent.run(\"Use tools to calculate (10 + 20) * (5 + 5) / (2 - 11). Please call all the tools at once.\")\n}\n</code></pre> <pre><code>Tool called: plus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=10.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=20.0})\nTool called: plus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=5.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=5.0})\nTool called: minus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.minus(kotlin.Double, kotlin.Double): kotlin.String=2.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.minus(kotlin.Double, kotlin.Double): kotlin.String=11.0})\nTool called: multiply, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.multiply(kotlin.Double, kotlin.Double): kotlin.String=30.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.multiply(kotlin.Double, kotlin.Double): kotlin.String=10.0})\nTool called: divide, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=30.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=-9.0})\nFinal result: The result of \\((10 + 20) * (5 + 5) / (2 - 11)\\) is approximately \\(-3.33\\).\n\n\n\n\n\nThe result of \\((10 + 20) * (5 + 5) / (2 - 11)\\) is approximately \\(-3.33\\).\n</code></pre>"},{"location":"examples/Calculator/#running-with-ollama","title":"Running with Ollama","text":"<p>Swap the executor and model if you prefer local inference.</p> <pre><code>val ollamaExecutor: PromptExecutor = simpleOllamaAIExecutor()\n\nval ollamaAgentConfig = AIAgentConfig(\n    prompt = prompt(\"calculator\", LLMParams(temperature = 0.0)) {\n        system(\"You are a calculator. Always use the provided tools for arithmetic.\")\n    },\n    model = OllamaModels.Meta.LLAMA_3_2,\n    maxAgentIterations = 50\n)\n\n\nval ollamaAgent = AIAgent(\n    promptExecutor = ollamaExecutor,\n    strategy = CalculatorStrategy.strategy,\n    agentConfig = ollamaAgentConfig,\n    toolRegistry = toolRegistry\n)\n\nrunBlocking {\n    ollamaAgent.run(\"(10 + 20) * (5 + 5) / (2 - 11)\")\n}\n</code></pre> <pre><code>Agent says: The result of the expression (10 + 20) * (5 + 5) / (2 - 11) is approximately -33.33.\n\n\n\n\n\nIf you have any more questions or need further assistance, feel free to ask!\n</code></pre>"},{"location":"examples/Chess/","title":"Building an AI Chess Player with Koog Framework","text":"<p> Open on GitHub  Download .ipynb</p> <p>This tutorial demonstrates how to build an intelligent chess-playing agent using the Koog framework. We'll explore key concepts including tool integration, agent strategies, memory optimization, and interactive AI decision-making.</p>"},{"location":"examples/Chess/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>How to model domain-specific data structures for complex games</li> <li>Creating custom tools that agents can use to interact with the environment</li> <li>Implementing efficient agent strategies with memory management</li> <li>Building interactive AI systems with choice selection capabilities</li> <li>Optimizing agent performance for turn-based games</li> </ul>"},{"location":"examples/Chess/#setup","title":"Setup","text":"<p>First, let's import the Koog framework and set up our development environment:</p> <pre><code>%useLatestDescriptors\n%use koog\n</code></pre>"},{"location":"examples/Chess/#modeling-the-chess-domain","title":"Modeling the Chess Domain","text":"<p>Creating a robust domain model is essential for any game AI. In chess, we need to represent players, pieces, and their relationships. Let's start by defining our core data structures:</p>"},{"location":"examples/Chess/#core-enums-and-types","title":"Core Enums and Types","text":"<pre><code>enum class Player {\n    White, Black, None;\n\n    fun opponent(): Player = when (this) {\n        White -&gt; Black\n        Black -&gt; White\n        None -&gt; throw IllegalArgumentException(\"No opponent for None player\")\n    }\n}\n\nenum class PieceType(val id: Char) {\n    King('K'), Queen('Q'), Rook('R'),\n    Bishop('B'), Knight('N'), Pawn('P'), None('*');\n\n    companion object {\n        fun fromId(id: String): PieceType {\n            require(id.length == 1) { \"Invalid piece id: $id\" }\n\n            return entries.first { it.id == id.single() }\n        }\n    }\n}\n\nenum class Side {\n    King, Queen\n}\n</code></pre> <p>The <code>Player</code> enum represents the two sides in chess, with an <code>opponent()</code> method for easy switching between players. The <code>PieceType</code> enum maps each chess piece to its standard notation character, enabling easy parsing of chess moves.</p> <p>The <code>Side</code> enum helps distinguish between kingside and queenside castling moves.</p>"},{"location":"examples/Chess/#piece-and-position-modeling","title":"Piece and Position Modeling","text":"<pre><code>data class Piece(val pieceType: PieceType, val player: Player) {\n    init {\n        require((pieceType == PieceType.None) == (player == Player.None)) {\n            \"Invalid piece: $pieceType $player\"\n        }\n    }\n\n    fun toChar(): Char = when (player) {\n        Player.White -&gt; pieceType.id.uppercaseChar()\n        Player.Black -&gt; pieceType.id.lowercaseChar()\n        Player.None -&gt; pieceType.id\n    }\n\n    fun isNone(): Boolean = pieceType == PieceType.None\n\n    companion object {\n        val None = Piece(PieceType.None, Player.None)\n    }\n}\n\ndata class Position(val row: Int, val col: Char) {\n    init {\n        require(row in 1..8 &amp;&amp; col in 'a'..'h') { \"Invalid position: $col$row\" }\n    }\n\n    constructor(position: String) : this(\n        position[1].digitToIntOrNull() ?: throw IllegalArgumentException(\"Incorrect position: $position\"),\n        position[0],\n    ) {\n        require(position.length == 2) { \"Invalid position: $position\" }\n    }\n}\n\nclass ChessBoard {\n    private val backRow = listOf(\n        PieceType.Rook, PieceType.Knight, PieceType.Bishop,\n        PieceType.Queen, PieceType.King,\n        PieceType.Bishop, PieceType.Knight, PieceType.Rook\n    )\n\n    private val board: List&lt;MutableList&lt;Piece&gt;&gt; = listOf(\n        backRow.map { Piece(it, Player.Black) }.toMutableList(),\n        List(8) { Piece(PieceType.Pawn, Player.Black) }.toMutableList(),\n        List(8) { Piece.None }.toMutableList(),\n        List(8) { Piece.None }.toMutableList(),\n        List(8) { Piece.None }.toMutableList(),\n        List(8) { Piece.None }.toMutableList(),\n        List(8) { Piece(PieceType.Pawn, Player.White) }.toMutableList(),\n        backRow.map { Piece(it, Player.White) }.toMutableList()\n    )\n\n    override fun toString(): String = board\n        .withIndex().joinToString(\"\\n\") { (index, row) -&gt;\n            \"${8 - index} ${row.map { it.toChar() }.joinToString(\" \")}\"\n        } + \"\\n  a b c d e f g h\"\n\n    fun getPiece(position: Position): Piece = board[8 - position.row][position.col - 'a']\n    fun setPiece(position: Position, piece: Piece) {\n        board[8 - position.row][position.col - 'a'] = piece\n    }\n}\n</code></pre> <p>The <code>Piece</code> data class combines a piece type with its owner, using uppercase letters for white pieces and lowercase for black pieces in the visual representation. The <code>Position</code> class encapsulates chess coordinates (e.g., \"e4\") with built-in validation.</p>"},{"location":"examples/Chess/#game-state-management","title":"Game State Management","text":""},{"location":"examples/Chess/#chessboard-implementation","title":"ChessBoard Implementation","text":"<p>The <code>ChessBoard</code> class manages the 8\u00d78 grid and piece positions. Key design decisions include:</p> <ul> <li>Internal Representation: Uses a list of mutable lists for efficient access and modification</li> <li>Visual Display: The <code>toString()</code> method provides a clear ASCII representation with rank numbers and file letters</li> <li>Position Mapping: Converts between chess notation (a1-h8) and internal array indices</li> </ul>"},{"location":"examples/Chess/#chessgame-logic","title":"ChessGame Logic","text":"<pre><code>/**\n * Simple chess game without checks for valid moves.\n * Stores a correct state of the board if the entered moves are valid\n */\nclass ChessGame {\n    private val board: ChessBoard = ChessBoard()\n    private var currentPlayer: Player = Player.White\n    val moveNotation: String = \"\"\"\n        0-0 - short castle\n        0-0-0 - long castle\n        &lt;piece&gt;-&lt;from&gt;-&lt;to&gt; - usual move. e.g. p-e2-e4\n        &lt;piece&gt;-&lt;from&gt;-&lt;to&gt;-&lt;promotion&gt; - promotion move. e.g. p-e7-e8-q.\n        Piece names:\n            p - pawn\n            n - knight\n            b - bishop\n            r - rook\n            q - queen\n            k - king\n    \"\"\".trimIndent()\n\n    fun move(move: String) {\n        when {\n            move == \"0-0\" -&gt; castleMove(Side.King)\n            move == \"0-0-0\" -&gt; castleMove(Side.Queen)\n            move.split(\"-\").size == 3 -&gt; {\n                val (_, from, to) = move.split(\"-\")\n                usualMove(Position(from), Position(to))\n            }\n\n            move.split(\"-\").size == 4 -&gt; {\n                val (piece, from, to, promotion) = move.split(\"-\")\n\n                require(PieceType.fromId(piece) == PieceType.Pawn) { \"Only pawn can be promoted\" }\n\n                usualMove(Position(from), Position(to))\n                board.setPiece(Position(to), Piece(PieceType.fromId(promotion), currentPlayer))\n            }\n\n            else -&gt; throw IllegalArgumentException(\"Invalid move: $move\")\n        }\n\n        updateCurrentPlayer()\n    }\n\n    fun getBoard(): String = board.toString()\n    fun currentPlayer(): String = currentPlayer.name.lowercase()\n\n    private fun updateCurrentPlayer() {\n        currentPlayer = currentPlayer.opponent()\n    }\n\n    private fun usualMove(from: Position, to: Position) {\n        if (board.getPiece(from).pieceType == PieceType.Pawn &amp;&amp; from.col != to.col &amp;&amp; board.getPiece(to).isNone()) {\n            // the move is en passant\n            board.setPiece(Position(from.row, to.col), Piece.None)\n        }\n\n        movePiece(from, to)\n    }\n\n    private fun castleMove(side: Side) {\n        val row = if (currentPlayer == Player.White) 1 else 8\n        val kingFrom = Position(row, 'e')\n        val (rookFrom, kingTo, rookTo) = if (side == Side.King) {\n            Triple(Position(row, 'h'), Position(row, 'g'), Position(row, 'f'))\n        } else {\n            Triple(Position(row, 'a'), Position(row, 'c'), Position(row, 'd'))\n        }\n\n        movePiece(kingFrom, kingTo)\n        movePiece(rookFrom, rookTo)\n    }\n\n    private fun movePiece(from: Position, to: Position) {\n        board.setPiece(to, board.getPiece(from))\n        board.setPiece(from, Piece.None)\n    }\n}\n</code></pre> <p>The <code>ChessGame</code> class orchestrates the game logic and maintains state. Notable features include:</p> <ul> <li>Move Notation Support: Accepts standard chess notation for regular moves, castling (0-0, 0-0-0), and pawn promotion</li> <li>Special Move Handling: Implements en passant capture and castling logic</li> <li>Turn Management: Automatically alternates between players after each move</li> <li>Validation: While it doesn't validate move legality (trusting the AI to make valid moves), it handles move parsing and state updates correctly</li> </ul> <p>The <code>moveNotation</code> string provides clear documentation for the AI agent on acceptable move formats.</p>"},{"location":"examples/Chess/#integrating-with-koog-framework","title":"Integrating with Koog Framework","text":""},{"location":"examples/Chess/#creating-custom-tools","title":"Creating Custom Tools","text":"<pre><code>import kotlinx.serialization.Serializable\n\nclass Move(val game: ChessGame) : SimpleTool&lt;Move.Args&gt;(\n    argsSerializer = Args.serializer(),\n    descriptor = ToolDescriptor(\n        name = \"move\",\n        description = \"Moves a piece according to the notation:\\n${game.moveNotation}\",\n        requiredParameters = listOf(\n            ToolParameterDescriptor(\n                name = \"notation\",\n                description = \"The notation of the piece to move\",\n                type = ToolParameterType.String,\n            )\n        )\n    )\n) {\n    @Serializable\n    data class Args(val notation: String) : ToolArgs\n\n    override suspend fun execute(args: Args): String {\n        game.move(args.notation)\n        println(game.getBoard())\n        println(\"-----------------\")\n        return \"Current state of the game:\\n${game.getBoard()}\\n${game.currentPlayer()} to move! Make the move!\"\n    }\n}\n</code></pre> <p>The <code>Move</code> tool demonstrates the Koog framework's tool integration pattern:</p> <ol> <li>Extends SimpleTool: Inherits the basic tool functionality with type-safe argument handling</li> <li>Serializable Arguments: Uses Kotlin serialization to define the tool's input parameters</li> <li>Rich Documentation: The <code>ToolDescriptor</code> provides the LLM with detailed information about the tool's purpose and parameters</li> <li>Constructor Parameters: Passes <code>argsSerializer</code> and <code>descriptor</code> to the constructor</li> <li>Execution Logic: The <code>execute</code> method handles the actual move execution and provides formatted feedback</li> </ol> <p>Key design aspects: - Context Injection: The tool receives the <code>ChessGame</code> instance, allowing it to modify game state - Feedback Loop: Returns the current board state and prompts the next player, maintaining conversational flow - Error Handling: Relies on the game class for move validation and error reporting</p>"},{"location":"examples/Chess/#agent-strategy-design","title":"Agent Strategy Design","text":""},{"location":"examples/Chess/#memory-optimization-technique","title":"Memory Optimization Technique","text":"<pre><code>import ai.koog.agents.core.environment.ReceivedToolResult\n\n/**\n * Chess position is (almost) completely defined by the board state,\n * So we can trim the history of the LLM to only contain the system prompt and the last move.\n */\ninline fun &lt;reified T&gt; AIAgentSubgraphBuilderBase&lt;*, *&gt;.nodeTrimHistory(\n    name: String? = null\n): AIAgentNodeDelegate&lt;T, T&gt; = node(name) { result -&gt;\n    llm.writeSession {\n        rewritePrompt { prompt -&gt;\n            val messages = prompt.messages\n\n            prompt.copy(messages = listOf(messages.first(), messages.last()))\n        }\n    }\n\n    result\n}\n\nval strategy = strategy&lt;String, String&gt;(\"chess_strategy\") {\n    val nodeCallLLM by nodeLLMRequest(\"sendInput\")\n    val nodeExecuteTool by nodeExecuteTool(\"nodeExecuteTool\")\n    val nodeSendToolResult by nodeLLMSendToolResult(\"nodeSendToolResult\")\n    val nodeTrimHistory by nodeTrimHistory&lt;ReceivedToolResult&gt;()\n\n    edge(nodeStart forwardTo nodeCallLLM)\n    edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })\n    edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeExecuteTool forwardTo nodeTrimHistory)\n    edge(nodeTrimHistory forwardTo nodeSendToolResult)\n    edge(nodeSendToolResult forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeSendToolResult forwardTo nodeExecuteTool onToolCall { true })\n}\n</code></pre> <p>The <code>nodeTrimHistory</code> function implements a crucial optimization for chess games. Since chess positions are largely determined by the current board state rather than the full move history, we can significantly reduce token usage by keeping only:</p> <ol> <li>System Prompt: Contains the agent's core instructions and behavior guidelines</li> <li>Latest Message: The most recent board state and game context</li> </ol> <p>This approach: - Reduces Token Consumption: Prevents exponential growth of conversation history - Maintains Context: Preserves essential game state information - Improves Performance: Faster processing with shorter prompts - Enables Long Games: Allows for extended gameplay without hitting token limits</p> <p>The chess strategy demonstrates Koog's graph-based agent architecture:</p> <p>Node Types: - <code>nodeCallLLM</code>: Processes input and generates responses/tool calls - <code>nodeExecuteTool</code>: Executes the Move tool with the provided parameters - <code>nodeTrimHistory</code>: Optimizes conversation memory as described above - <code>nodeSendToolResult</code>: Sends tool execution results back to the LLM</p> <p>Control Flow: - Linear Path: Start \u2192 LLM Request \u2192 Tool Execution \u2192 History Trim \u2192 Send Result - Decision Points: LLM responses can either finish the conversation or trigger another tool call - Memory Management: History trimming occurs after each tool execution</p> <p>This strategy ensures efficient, stateful gameplay while maintaining conversational coherence.</p>"},{"location":"examples/Chess/#setting-up-the-ai-agent","title":"Setting up the AI Agent","text":"<pre><code>val baseExecutor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\"))\n</code></pre> <p>This section initializes our OpenAI executor. The <code>simpleOpenAIExecutor</code> creates a connection to OpenAI's API using your API key from environment variables.</p> <p>Configuration Notes: - Store your OpenAI API key in the <code>OPENAI_API_KEY</code> environment variable - The executor handles authentication and API communication automatically - Different executor types are available for various LLM providers</p>"},{"location":"examples/Chess/#agent-assembly","title":"Agent Assembly","text":"<pre><code>val game = ChessGame()\nval toolRegistry = ToolRegistry { tools(listOf(Move(game))) }\n\n// Create a chat agent with a system prompt and the tool registry\nval agent = AIAgent(\n    executor = baseExecutor,\n    strategy = strategy,\n    llmModel = OpenAIModels.Chat.O3Mini,\n    systemPrompt = \"\"\"\n            You are an agent who plays chess.\n            You should always propose a move in response to the \"Your move!\" message.\n\n            DO NOT HALLUCINATE!!!\n            DO NOT PLAY ILLEGAL MOVES!!!\n            YOU CAN SEND A MESSAGE ONLY IF IT IS A RESIGNATION OR A CHECKMATE!!!\n        \"\"\".trimMargin(),\n    temperature = 0.0,\n    toolRegistry = toolRegistry,\n    maxIterations = 200,\n)\n</code></pre> <p>Here we assemble all components into a functional chess-playing agent:</p> <p>Key Configuration:</p> <ul> <li>Model Choice: Using <code>OpenAIModels.Chat.O3Mini</code> for high-quality chess play</li> <li>Temperature: Set to 0.0 for deterministic, strategic moves</li> <li>System Prompt: Carefully crafted instructions emphasizing legal moves and proper behavior</li> <li>Tool Registry: Provides the agent access to the Move tool</li> <li>Max Iterations: Set to 200 to allow for complete games</li> </ul> <p>System Prompt Design: - Emphasizes move proposal responsibility - Prohibits hallucination and illegal moves - Restricts messaging to only resignations or checkmate declarations - Creates focused, game-oriented behavior</p>"},{"location":"examples/Chess/#running-the-basic-agent","title":"Running the Basic Agent","text":"<pre><code>import kotlinx.coroutines.runBlocking\n\nprintln(\"Chess Game started!\")\n\nval initialMessage = \"Starting position is ${game.getBoard()}. White to move!\"\n\nrunBlocking {\n    agent.run(initialMessage)\n}\n</code></pre> <pre><code>Chess Game started!\n8 r n b q k b n r\n7 p p p p p p p p\n6 * * * * * * * *\n5 * * * * * * * *\n4 * * * * P * * *\n3 * * * * * * * *\n2 P P P P * P P P\n1 R N B Q K B N R\n  a b c d e f g h\n-----------------\n8 r n b q k b n r\n7 p p p p * p p p\n6 * * * * * * * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * * * *\n2 P P P P * P P P\n1 R N B Q K B N R\n  a b c d e f g h\n-----------------\n8 r n b q k b n r\n7 p p p p * p p p\n6 * * * * * * * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * N * *\n2 P P P P * P P P\n1 R N B Q K B * R\n  a b c d e f g h\n-----------------\n8 r n b q k b * r\n7 p p p p * p p p\n6 * * * * * n * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * N * *\n2 P P P P * P P P\n1 R N B Q K B * R\n  a b c d e f g h\n-----------------\n8 r n b q k b * r\n7 p p p p * p p p\n6 * * * * * n * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * N * * N * *\n2 P P P P * P P P\n1 R * B Q K B * R\n  a b c d e f g h\n-----------------\n\n\n\nThe execution was interrupted\n</code></pre> <p>This basic agent plays autonomously, making moves automatically. The game output shows the sequence of moves and board states as the AI plays against itself.</p>"},{"location":"examples/Chess/#advanced-feature-interactive-choice-selection","title":"Advanced Feature: Interactive Choice Selection","text":"<p>The next sections demonstrate a more sophisticated approach where users can participate in the AI's decision-making process by choosing from multiple AI-generated moves.</p>"},{"location":"examples/Chess/#custom-choice-selection-strategy","title":"Custom Choice Selection Strategy","text":"<pre><code>import ai.koog.agents.core.feature.choice.ChoiceSelectionStrategy\n\n/**\n * `AskUserChoiceStrategy` allows users to interactively select a choice from a list of options\n * presented by a language model. The strategy uses customizable methods to display the prompt\n * and choices and read user input to determine the selected choice.\n *\n * @property promptShowToUser A function that formats and displays a given `Prompt` to the user.\n * @property choiceShowToUser A function that formats and represents a given `LLMChoice` to the user.\n * @property print A function responsible for displaying messages to the user, e.g., for showing prompts or feedback.\n * @property read A function to capture user input.\n */\nclass AskUserChoiceSelectionStrategy(\n    private val promptShowToUser: (Prompt) -&gt; String = { \"Current prompt: $it\" },\n    private val choiceShowToUser: (LLMChoice) -&gt; String = { \"$it\" },\n    private val print: (String) -&gt; Unit = ::println,\n    private val read: () -&gt; String? = ::readlnOrNull\n) : ChoiceSelectionStrategy {\n    override suspend fun choose(prompt: Prompt, choices: List&lt;LLMChoice&gt;): LLMChoice {\n        print(promptShowToUser(prompt))\n\n        print(\"Available LLM choices\")\n\n        choices.withIndex().forEach { (index, choice) -&gt;\n            print(\"Choice number ${index + 1}: ${choiceShowToUser(choice)}\")\n        }\n\n        var choiceNumber = ask(choices.size)\n        while (choiceNumber == null) {\n            print(\"Invalid response.\")\n            choiceNumber = ask(choices.size)\n        }\n\n        return choices[choiceNumber - 1]\n    }\n\n    private fun ask(numChoices: Int): Int? {\n        print(\"Please choose a choice. Enter a number between 1 and $numChoices: \")\n\n        return read()?.toIntOrNull()?.takeIf { it in 1..numChoices }\n    }\n}\n</code></pre> <p>The <code>AskUserChoiceSelectionStrategy</code> implements Koog's <code>ChoiceSelectionStrategy</code> interface to enable human participation in AI decision-making:</p> <p>Key Features: - Customizable Display: Functions for formatting prompts and choices - Interactive Input: Uses standard input/output for user interaction - Validation: Ensures user input is within valid range - Flexible I/O: Configurable print and read functions for different environments</p> <p>Use Cases: - Human-AI collaboration in gameplay - AI decision transparency and explainability - Training and debugging scenarios - Educational demonstrations</p>"},{"location":"examples/Chess/#enhanced-strategy-with-choice-selection","title":"Enhanced Strategy with Choice Selection","text":"<pre><code>inline fun &lt;reified T&gt; AIAgentSubgraphBuilderBase&lt;*, *&gt;.nodeTrimHistory(\n    name: String? = null\n): AIAgentNodeDelegate&lt;T, T&gt; = node(name) { result -&gt;\n    llm.writeSession {\n        rewritePrompt { prompt -&gt;\n            val messages = prompt.messages\n\n            prompt.copy(messages = listOf(messages.first(), messages.last()))\n        }\n    }\n\n    result\n}\n\nval strategy = strategy&lt;String, String&gt;(\"chess_strategy\") {\n    val nodeCallLLM by nodeLLMRequest(\"sendInput\")\n    val nodeExecuteTool by nodeExecuteTool(\"nodeExecuteTool\")\n    val nodeSendToolResult by nodeLLMSendToolResult(\"nodeSendToolResult\")\n    val nodeTrimHistory by nodeTrimHistory&lt;ReceivedToolResult&gt;()\n\n    edge(nodeStart forwardTo nodeCallLLM)\n    edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })\n    edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeExecuteTool forwardTo nodeTrimHistory)\n    edge(nodeTrimHistory forwardTo nodeSendToolResult)\n    edge(nodeSendToolResult forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeSendToolResult forwardTo nodeExecuteTool onToolCall { true })\n}\n\nval askChoiceStrategy = AskUserChoiceSelectionStrategy(promptShowToUser = { prompt -&gt;\n    val lastMessage = prompt.messages.last()\n    if (lastMessage is Message.Tool.Call) {\n        lastMessage.content\n    } else {\n        \"\"\n    }\n})\n</code></pre> <pre><code>val promptExecutor = PromptExecutorWithChoiceSelection(baseExecutor, askChoiceStrategy)\n</code></pre> <p>The first interactive approach uses <code>PromptExecutorWithChoiceSelection</code>, which wraps the base executor with choice selection capability. The custom display function extracts move information from tool calls to show users what the AI wants to do.</p> <p>Architecture Changes: - Wrapped Executor: <code>PromptExecutorWithChoiceSelection</code> adds choice functionality to any base executor - Context-Aware Display: Shows the last tool call content instead of the full prompt - Higher Temperature: Increased to 1.0 for more diverse move options</p>"},{"location":"examples/Chess/#advanced-strategy-manual-choice-selection","title":"Advanced Strategy: Manual Choice Selection","text":"<pre><code>val game = ChessGame()\nval toolRegistry = ToolRegistry { tools(listOf(Move(game))) }\n\nval agent = AIAgent(\n    executor = promptExecutor,\n    strategy = strategy,\n    llmModel = OpenAIModels.Chat.O3Mini,\n    systemPrompt = \"\"\"\n            You are an agent who plays chess.\n            You should always propose a move in response to the \"Your move!\" message.\n\n            DO NOT HALLUCINATE!!!\n            DO NOT PLAY ILLEGAL MOVES!!!\n            YOU CAN SEND A MESSAGE ONLY IF IT IS A RESIGNATION OR A CHECKMATE!!!\n        \"\"\".trimMargin(),\n    temperature = 1.0,\n    toolRegistry = toolRegistry,\n    maxIterations = 200,\n    numberOfChoices = 3,\n)\n</code></pre> <p>The advanced strategy integrates choice selection directly into the agent's execution graph:</p> <p>New Nodes: - <code>nodeLLMSendResultsMultipleChoices</code>: Handles multiple LLM choices simultaneously - <code>nodeSelectLLMChoice</code>: Integrates the choice selection strategy into the workflow</p> <p>Enhanced Control Flow: - Tool results are wrapped in lists to support multiple choices - User selection occurs before continuing with the chosen path - The selected choice is unwrapped and continues through the normal flow</p> <p>Benefits: - Greater Control: Fine-grained integration with agent workflow - Flexibility: Can be combined with other agent features - Transparency: Users see exactly what the AI is considering</p>"},{"location":"examples/Chess/#running-interactive-agents","title":"Running Interactive Agents","text":"<pre><code>println(\"Chess Game started!\")\n\nval initialMessage = \"Starting position is ${game.getBoard()}. White to move!\"\n\nrunBlocking {\n    agent.run(initialMessage)\n}\n</code></pre> <pre><code>Chess Game started!\n\nAvailable LLM choices\nChoice number 1: [Call(id=call_K46Upz7XoBIG5RchDh7bZE8F, tool=move, content={\"notation\": \"p-e2-e4\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:40.368252Z, totalTokensCount=773, inputTokensCount=315, outputTokensCount=458, additionalInfo={}))]\nChoice number 2: [Call(id=call_zJ6OhoCHrVHUNnKaxZkOhwoU, tool=move, content={\"notation\": \"p-e2-e4\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:40.368252Z, totalTokensCount=773, inputTokensCount=315, outputTokensCount=458, additionalInfo={}))]\nChoice number 3: [Call(id=call_nwX6ZMJ3F5AxiNUypYlI4BH4, tool=move, content={\"notation\": \"p-e2-e4\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:40.368252Z, totalTokensCount=773, inputTokensCount=315, outputTokensCount=458, additionalInfo={}))]\nPlease choose a choice. Enter a number between 1 and 3: \n8 r n b q k b n r\n7 p p p p p p p p\n6 * * * * * * * *\n5 * * * * * * * *\n4 * * * * P * * *\n3 * * * * * * * *\n2 P P P P * P P P\n1 R N B Q K B N R\n  a b c d e f g h\n-----------------\n\nAvailable LLM choices\nChoice number 1: [Call(id=call_2V93GXOcIe0fAjUAIFEk9h5S, tool=move, content={\"notation\": \"p-e7-e5\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:47.949303Z, totalTokensCount=1301, inputTokensCount=341, outputTokensCount=960, additionalInfo={}))]\nChoice number 2: [Call(id=call_INM59xRzKMFC1w8UAV74l9e1, tool=move, content={\"notation\": \"p-e7-e5\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:47.949303Z, totalTokensCount=1301, inputTokensCount=341, outputTokensCount=960, additionalInfo={}))]\nChoice number 3: [Call(id=call_r4QoiTwn0F3jizepHH5ia8BU, tool=move, content={\"notation\": \"p-e7-e5\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:47.949303Z, totalTokensCount=1301, inputTokensCount=341, outputTokensCount=960, additionalInfo={}))]\nPlease choose a choice. Enter a number between 1 and 3: \n8 r n b q k b n r\n7 p p p p * p p p\n6 * * * * * * * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * * * *\n2 P P P P * P P P\n1 R N B Q K B N R\n  a b c d e f g h\n-----------------\n\nAvailable LLM choices\nChoice number 1: [Call(id=call_f9XTizn41svcrtvnmkCfpSUQ, tool=move, content={\"notation\": \"n-g1-f3\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:55.467712Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]\nChoice number 2: [Call(id=call_c0Dfce5RcSbN3cOOm5ESYriK, tool=move, content={\"notation\": \"n-g1-f3\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:55.467712Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]\nChoice number 3: [Call(id=call_Lr4Mdro1iolh0fDyAwZsutrW, tool=move, content={\"notation\": \"n-g1-f3\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:55.467712Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]\nPlease choose a choice. Enter a number between 1 and 3: \n8 r n b q k b n r\n7 p p p p * p p p\n6 * * * * * * * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * N * *\n2 P P P P * P P P\n1 R N B Q K B * R\n  a b c d e f g h\n-----------------\n\n\n\nThe execution was interrupted\n</code></pre> <pre><code>import ai.koog.agents.core.feature.choice.nodeLLMSendResultsMultipleChoices\nimport ai.koog.agents.core.feature.choice.nodeSelectLLMChoice\n\ninline fun &lt;reified T&gt; AIAgentSubgraphBuilderBase&lt;*, *&gt;.nodeTrimHistory(\n    name: String? = null\n): AIAgentNodeDelegate&lt;T, T&gt; = node(name) { result -&gt;\n    llm.writeSession {\n        rewritePrompt { prompt -&gt;\n            val messages = prompt.messages\n\n            prompt.copy(messages = listOf(messages.first(), messages.last()))\n        }\n    }\n\n    result\n}\n\nval strategy = strategy&lt;String, String&gt;(\"chess_strategy\") {\n    val nodeCallLLM by nodeLLMRequest(\"sendInput\")\n    val nodeExecuteTool by nodeExecuteTool(\"nodeExecuteTool\")\n    val nodeSendToolResult by nodeLLMSendResultsMultipleChoices(\"nodeSendToolResult\")\n    val nodeSelectLLMChoice by nodeSelectLLMChoice(askChoiceStrategy, \"chooseLLMChoice\")\n    val nodeTrimHistory by nodeTrimHistory&lt;ReceivedToolResult&gt;()\n\n    edge(nodeStart forwardTo nodeCallLLM)\n    edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })\n    edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeExecuteTool forwardTo nodeTrimHistory)\n    edge(nodeTrimHistory forwardTo nodeSendToolResult transformed { listOf(it) })\n    edge(nodeSendToolResult forwardTo nodeSelectLLMChoice)\n    edge(nodeSelectLLMChoice forwardTo nodeFinish transformed { it.first() } onAssistantMessage { true })\n    edge(nodeSelectLLMChoice forwardTo nodeExecuteTool transformed { it.first() } onToolCall { true })\n}\n</code></pre> <pre><code>val game = ChessGame()\nval toolRegistry = ToolRegistry { tools(listOf(Move(game))) }\n\nval agent = AIAgent(\n    executor = baseExecutor,\n    strategy = strategy,\n    llmModel = OpenAIModels.Chat.O3Mini,\n    systemPrompt = \"\"\"\n            You are an agent who plays chess.\n            You should always propose a move in response to the \"Your move!\" message.\n\n            DO NOT HALLUCINATE!!!\n            DO NOT PLAY ILLEGAL MOVES!!!\n            YOU CAN SEND A MESSAGE ONLY IF IT IS A RESIGNATION OR A CHECKMATE!!!\n        \"\"\".trimMargin(),\n    temperature = 1.0,\n    toolRegistry = toolRegistry,\n    maxIterations = 200,\n    numberOfChoices = 3,\n)\n</code></pre> <pre><code>println(\"Chess Game started!\")\n\nval initialMessage = \"Starting position is ${game.getBoard()}. White to move!\"\n\nrunBlocking {\n    agent.run(initialMessage)\n}\n</code></pre> <pre><code>Chess Game started!\n8 r n b q k b n r\n7 p p p p p p p p\n6 * * * * * * * *\n5 * * * * * * * *\n4 * * * * P * * *\n3 * * * * * * * *\n2 P P P P * P P P\n1 R N B Q K B N R\n  a b c d e f g h\n-----------------\n\nAvailable LLM choices\nChoice number 1: [Call(id=call_gqMIar0z11CyUl5nup3zbutj, tool=move, content={\"notation\": \"p-e7-e5\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:17.313548Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]\nChoice number 2: [Call(id=call_6niUGnZPPJILRFODIlJsCKax, tool=move, content={\"notation\": \"p-e7-e5\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:17.313548Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]\nChoice number 3: [Call(id=call_q1b8ZmIBph0EoVaU3Ic9A09j, tool=move, content={\"notation\": \"p-e7-e5\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:17.313548Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]\nPlease choose a choice. Enter a number between 1 and 3: \n8 r n b q k b n r\n7 p p p p * p p p\n6 * * * * * * * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * * * *\n2 P P P P * P P P\n1 R N B Q K B N R\n  a b c d e f g h\n-----------------\n\nAvailable LLM choices\nChoice number 1: [Call(id=call_pdBIX7MVi82MyWwawTm1Q2ef, tool=move, content={\"notation\": \"n-g1-f3\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:24.505344Z, totalTokensCount=1237, inputTokensCount=341, outputTokensCount=896, additionalInfo={}))]\nChoice number 2: [Call(id=call_oygsPHaiAW5OM6pxhXhtazgp, tool=move, content={\"notation\": \"n-g1-f3\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:24.505344Z, totalTokensCount=1237, inputTokensCount=341, outputTokensCount=896, additionalInfo={}))]\nChoice number 3: [Call(id=call_GJTEsZ8J8cqOKZW4Tx54RqCh, tool=move, content={\"notation\": \"n-g1-f3\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:24.505344Z, totalTokensCount=1237, inputTokensCount=341, outputTokensCount=896, additionalInfo={}))]\nPlease choose a choice. Enter a number between 1 and 3: \n8 r n b q k b n r\n7 p p p p * p p p\n6 * * * * * * * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * N * *\n2 P P P P * P P P\n1 R N B Q K B * R\n  a b c d e f g h\n-----------------\n\nAvailable LLM choices\nChoice number 1: [Call(id=call_5C7HdlTU4n3KdXcyNogE4rGb, tool=move, content={\"notation\": \"n-g8-f6\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:34.646667Z, totalTokensCount=1621, inputTokensCount=341, outputTokensCount=1280, additionalInfo={}))]\nChoice number 2: [Call(id=call_EjCcyeMLQ88wMa5yh3vmeJ2w, tool=move, content={\"notation\": \"n-g8-f6\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:34.646667Z, totalTokensCount=1621, inputTokensCount=341, outputTokensCount=1280, additionalInfo={}))]\nChoice number 3: [Call(id=call_NBMMSwmFIa8M6zvfbPw85NKh, tool=move, content={\"notation\": \"n-g8-f6\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:34.646667Z, totalTokensCount=1621, inputTokensCount=341, outputTokensCount=1280, additionalInfo={}))]\nPlease choose a choice. Enter a number between 1 and 3: \n8 r n b q k b * r\n7 p p p p * p p p\n6 * * * * * n * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * N * *\n2 P P P P * P P P\n1 R N B Q K B * R\n  a b c d e f g h\n-----------------\n\n\n\nThe execution was interrupted\n</code></pre> <p>The interactive examples show how users can guide the AI's decision-making process. In the output, you can see:</p> <ol> <li>Multiple Choices: The AI generates 3 different move options</li> <li>User Selection: Users input numbers 1-3 to choose their preferred move</li> <li>Game Continuation: The selected move is executed and the game continues</li> </ol>"},{"location":"examples/Chess/#conclusion","title":"Conclusion","text":"<p>This tutorial demonstrates several key aspects of building intelligent agents with the Koog framework:</p>"},{"location":"examples/Chess/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Domain Modeling: Well-structured data models are crucial for complex applications</li> <li>Tool Integration: Custom tools enable agents to interact with external systems effectively</li> <li>Memory Management: Strategic history trimming optimizes performance for long interactions</li> <li>Strategy Graphs: Koog's graph-based approach provides flexible control flow</li> <li>Interactive AI: Choice selection enables human-AI collaboration and transparency</li> </ol>"},{"location":"examples/Chess/#framework-features-explored","title":"Framework Features Explored","text":"<ul> <li>\u2705 Custom tool creation and integration</li> <li>\u2705 Agent strategy design and graph-based control flow</li> <li>\u2705 Memory optimization techniques</li> <li>\u2705 Interactive choice selection</li> <li>\u2705 Multiple LLM response handling</li> <li>\u2705 Stateful game management</li> </ul> <p>The Koog framework provides the foundation for building sophisticated AI agents that can handle complex, multi-turn interactions while maintaining efficiency and transparency.</p>"},{"location":"examples/GoogleMapsMcp/","title":"Google Maps MCP with Koog: From Zero to Elevation in a Kotlin Notebook","text":"<p> Open on GitHub  Download .ipynb</p> <p>In this short, blog-style walkthrough, we\u2019ll connect Koog to a Model Context Protocol (MCP) server for Google Maps. We\u2019ll spin up the server with Docker, discover the available tools, and let an AI agent geocode an address and fetch its elevation \u2014 all from a Kotlin Notebook.</p> <p>By the end, you\u2019ll have a reproducible, end\u2011to\u2011end example you can drop into your workflow or documentation.</p> <pre><code>%useLatestDescriptors\n%use koog\n</code></pre>"},{"location":"examples/GoogleMapsMcp/#prerequisites","title":"Prerequisites","text":"<p>Before you run the cells below, make sure you have:</p> <ul> <li>Docker installed and running</li> <li>A valid Google Maps API key exported as an environment variable: <code>GOOGLE_MAPS_API_KEY</code></li> <li>An OpenAI API key exported as <code>OPENAI_API_KEY</code></li> </ul> <p>You can set them in your shell like this (macOS/Linux example):</p> <pre><code>export GOOGLE_MAPS_API_KEY=\"&lt;your-key&gt;\"\nexport OPENAI_API_KEY=\"&lt;your-openai-key&gt;\"\n</code></pre> <pre><code>// Get the API key from environment variables\nval googleMapsApiKey = System.getenv(\"GOOGLE_MAPS_API_KEY\") ?: error(\"GOOGLE_MAPS_API_KEY environment variable not set\")\nval openAIApiToken = System.getenv(\"OPENAI_API_KEY\") ?: error(\"OPENAI_API_KEY environment variable not set\")\n</code></pre>"},{"location":"examples/GoogleMapsMcp/#start-the-google-maps-mcp-server-docker","title":"Start the Google Maps MCP server (Docker)","text":"<p>We\u2019ll use the official <code>mcp/google-maps</code> image. The container will expose tools such as <code>maps_geocode</code> and <code>maps_elevation</code> over MCP. We pass the API key via environment variables and launch it attached so the notebook can talk to it over stdio.</p> <pre><code>// Start the Docker container with the Google Maps MCP server\nval process = ProcessBuilder(\n    \"docker\",\n    \"run\",\n    \"-i\",\n    \"-e\",\n    \"GOOGLE_MAPS_API_KEY=$googleMapsApiKey\",\n    \"mcp/google-maps\"\n).start()\n</code></pre>"},{"location":"examples/GoogleMapsMcp/#discover-tools-via-mcptoolregistry","title":"Discover tools via McpToolRegistry","text":"<p>Koog can connect to an MCP server over stdio. Here, we create a tool registry from the running process and print out the discovered tools and their descriptors.</p> <pre><code>val toolRegistry = McpToolRegistryProvider.fromTransport(\n    transport = McpToolRegistryProvider.defaultStdioTransport(process)\n)\ntoolRegistry.tools.forEach {\n    println(it.name)\n    println(it.descriptor)\n}\n</code></pre>"},{"location":"examples/GoogleMapsMcp/#build-an-ai-agent-with-openai","title":"Build an AI Agent with OpenAI","text":"<p>Next we assemble a simple agent backed by the OpenAI executor and model. The agent will be able to call tools exposed by the MCP server through the registry we just created.</p> <pre><code>val agent = AIAgent(\n    executor = simpleOpenAIExecutor(openAIApiToken),\n    llmModel = OpenAIModels.Chat.GPT4o,\n    toolRegistry = toolRegistry,\n)\n</code></pre>"},{"location":"examples/GoogleMapsMcp/#ask-for-elevation-geocode-first-then-elevation","title":"Ask for elevation: geocode first, then elevation","text":"<p>We prompt the agent to find the elevation of the JetBrains office in Munich. The instruction explicitly tells the agent to use only the available tools and which ones to prefer for the task.</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nval request = \"Get elevation of the Jetbrains Office in Munich, Germany?\"\nrunBlocking {\n    agent.run(\n        request +\n            \"You can only call tools. Get it by calling maps_geocode and maps_elevation tools.\"\n    )\n}\n</code></pre>"},{"location":"examples/GoogleMapsMcp/#clean-up","title":"Clean up","text":"<p>When you\u2019re done, stop the Docker process so you don\u2019t leave anything running in the background.</p> <pre><code>process.destroy()\n</code></pre>"},{"location":"examples/GoogleMapsMcp/#troubleshooting-and-next-steps","title":"Troubleshooting and next steps","text":"<ul> <li>If the container fails to start, check that Docker is running and your <code>GOOGLE_MAPS_API_KEY</code> is valid.</li> <li>If the agent can\u2019t call tools, re-run the discovery cell to ensure the tool registry is populated.</li> <li>Try other prompts like route planning or place searches using the available Google Maps tools.</li> </ul> <p>Next, consider composing multiple MCP servers (e.g., Playwright for web automation + Google Maps) and let Koog orchestrate tool usage for richer tasks.</p>"},{"location":"examples/Guesser/","title":"Building a Number\u2011Guessing Agent with Koog","text":"<p> Open on GitHub  Download .ipynb</p> <p>Let\u2019s build a small but fun agent that guesses a number you\u2019re thinking of. We\u2019ll lean on Koog\u2019s tool-calling to ask targeted questions and converge using a classic binary search strategy. The result is an idiomatic Kotlin Notebook that you can drop straight into docs.</p> <p>We\u2019ll keep the code minimal and the flow transparent: a few tiny tools, a compact prompt, and an interactive CLI loop.</p>"},{"location":"examples/Guesser/#setup","title":"Setup","text":"<p>This notebook assumes: - You\u2019re running in a Kotlin Notebook with Koog available. - The environment variable <code>OPENAI_API_KEY</code> is set. The agent uses it via <code>simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\"))</code>.</p> <p>Load the Koog kernel:</p> <pre><code>%useLatestDescriptors\n%use koog\n</code></pre>"},{"location":"examples/Guesser/#tools-asking-targeted-questions","title":"Tools: asking targeted questions","text":"<p>Tools are small, well-described functions the LLM can call. We\u2019ll provide three: - <code>lessThan(value)</code>: \u201cIs your number less than value?\u201d - <code>greaterThan(value)</code>: \u201cIs your number greater than value?\u201d - <code>proposeNumber(value)</code>: \u201cIs your number equal to value?\u201d (used once the range is tight)</p> <p>Each tool returns a simple \"YES\"/\"NO\" string. The helper <code>ask</code> implements a minimal Y/n loop and validates input. Descriptions via <code>@LLMDescription</code> help the model select tools correctly.</p> <pre><code>import ai.koog.agents.core.tools.annotations.Tool\n\nclass GuesserTool : ToolSet {\n\n    @Tool\n    @LLMDescription(\"Asks the user if his number is STRICTLY less than a given value.\")\n    fun lessThan(\n        @LLMDescription(\"A value to compare the guessed number with.\") value: Int\n    ): String = ask(\"Is your number less than $value?\", value)\n\n    @Tool\n    @LLMDescription(\"Asks the user if his number is STRICTLY greater than a given value.\")\n    fun greaterThan(\n        @LLMDescription(\"A value to compare the guessed number with.\") value: Int\n    ): String = ask(\"Is your number greater than $value?\", value)\n\n    @Tool\n    @LLMDescription(\"Asks the user if his number is EXACTLY equal to the given number. Only use this tool once you've narrowed down your answer.\")\n    fun proposeNumber(\n        @LLMDescription(\"A value to compare the guessed number with.\") value: Int\n    ): String = ask(\"Is your number equal to $value?\", value)\n\n    fun ask(question: String, value: Int): String {\n        print(\"$question [Y/n]: \")\n        val input = readln()\n        println(input)\n\n        return when (input.lowercase()) {\n            \"\", \"y\", \"yes\" -&gt; \"YES\"\n            \"n\", \"no\" -&gt; \"NO\"\n            else -&gt; {\n                println(\"Invalid input! Please, try again.\")\n                ask(question, value)\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"examples/Guesser/#tool-registry","title":"Tool Registry","text":"<p>Expose your tools to the agent. We also add a built\u2011in <code>SayToUser</code> tool so the agent can surface messages directly to the user.</p> <pre><code>val toolRegistry = ToolRegistry {\n    tool(SayToUser)\n    tools(GuesserTool())\n}\n</code></pre>"},{"location":"examples/Guesser/#agent-configuration","title":"Agent configuration","text":"<p>A short, tool\u2011forward system prompt is all we need. We\u2019ll suggest a binary search strategy and keep <code>temperature = 0.0</code> for stable, deterministic behavior. Here we use OpenAI\u2019s reasoning model <code>GPT4oMini</code> for crisp planning.</p> <pre><code>val agent = AIAgent(\n    executor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    llmModel = OpenAIModels.Chat.GPT4oMini,\n    systemPrompt = \"\"\"\n            You are a number guessing agent. Your goal is to guess a number that the user is thinking of.\n\n            Follow these steps:\n            1. Start by asking the user to think of a number between 1 and 100.\n            2. Use the less_than and greater_than tools to narrow down the range.\n                a. If it's neither greater nor smaller, use the propose_number tool.\n            3. Once you're confident about the number, use the propose_number tool to check if your guess is correct.\n            4. If your guess is correct, congratulate the user. If not, continue guessing.\n\n            Be efficient with your guessing strategy. A binary search approach works well.\n        \"\"\".trimIndent(),\n    temperature = 0.0,\n    toolRegistry = toolRegistry\n)\n</code></pre>"},{"location":"examples/Guesser/#run-it","title":"Run it","text":"<ul> <li>Think of a number between 1 and 100.</li> <li>Type <code>start</code> to begin.</li> <li>Answer the agent\u2019s questions with <code>Y</code>/<code>Enter</code> for yes or <code>n</code> for no. The agent should zero in on your number in ~7 steps.</li> </ul> <pre><code>import kotlinx.coroutines.runBlocking\n\nprintln(\"Number Guessing Game started!\")\nprintln(\"Think of a number between 1 and 100, and I'll try to guess it.\")\nprintln(\"Type 'start' to begin the game.\")\n\nval initialMessage = readln()\nrunBlocking {\n    agent.run(initialMessage)\n}\n</code></pre>"},{"location":"examples/Guesser/#how-it-works","title":"How it works","text":"<ul> <li>The agent reads the system prompt and plans a binary search.</li> <li>On each iteration it calls one of your tools: <code>lessThan</code>, <code>greaterThan</code>, or (when certain) <code>proposeNumber</code>.</li> <li>The helper <code>ask</code> collects your Y/n input and returns a clean \"YES\"/\"NO\" signal back to the model.</li> <li>When it gets confirmation, it congratulates you via <code>SayToUser</code>.</li> </ul>"},{"location":"examples/Guesser/#extend-it","title":"Extend it","text":"<ul> <li>Change the range (e.g., 1..1000) by tweaking the system prompt.</li> <li>Add a <code>between(low, high)</code> tool to reduce calls further.</li> <li>Swap models or executors (e.g., use an Ollama executor and a local model) while keeping the same tools.</li> <li>Persist guesses or outcomes to a store for analytics.</li> </ul>"},{"location":"examples/Guesser/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Missing key: ensure <code>OPENAI_API_KEY</code> is set in your environment.</li> <li>Kernel not found: make sure <code>%useLatestDescriptors</code> and <code>%use koog</code> executed successfully.</li> <li>Tool not called: confirm the <code>ToolRegistry</code> includes <code>GuesserTool()</code> and the names in the prompt match your tool functions.</li> </ul>"},{"location":"examples/Langfuse/","title":"Tracing Koog Agents to Langfuse with OpenTelemetry","text":"<p> Open on GitHub  Download .ipynb</p> <p>This notebook shows how to export Koog agent traces to your Langfuse instance using OpenTelemetry. You'll set up environment variables, run a simple agent, and then inspect spans and traces in Langfuse.</p>"},{"location":"examples/Langfuse/#what-youll-learn","title":"What you'll learn","text":"<ul> <li>How Koog integrates with OpenTelemetry to emit traces</li> <li>How to configure the Langfuse exporter via environment variables</li> <li>How to run an agent and view its trace in Langfuse</li> </ul>"},{"location":"examples/Langfuse/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Langfuse project (host URL, public key, secret key)</li> <li>An OpenAI API key for the LLM executor</li> <li>Environment variables set in your shell:</li> </ul> <pre><code>export OPENAI_API_KEY=sk-...\nexport LANGFUSE_HOST=https://cloud.langfuse.com # or your self-hosted URL\nexport LANGFUSE_PUBLIC_KEY=pk_...\nexport LANGFUSE_SECRET_KEY=sk_...\n</code></pre> <pre><code>%useLatestDescriptors\n//%use koog\n</code></pre> <pre><code>import ai.koog.agents.core.agent.AIAgent\nimport ai.koog.agents.features.opentelemetry.feature.OpenTelemetry\nimport ai.koog.agents.features.opentelemetry.integration.langfuse.addLangfuseExporter\nimport ai.koog.prompt.executor.clients.openai.OpenAIModels\nimport ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor\n\n/**\n * Example of Koog agents tracing to [Langfuse](https://langfuse.com/)\n *\n * Agent traces are exported to:\n * - Langfuse OTLP endpoint instance using [OtlpHttpSpanExporter]\n *\n * To run this example:\n *  1. Set up a Langfuse project and credentials as described [here](https://langfuse.com/docs/get-started#create-new-project-in-langfuse)\n *  2. Get Langfuse credentials as described [here](https://langfuse.com/faq/all/where-are-langfuse-api-keys)\n *  3. Set `LANGFUSE_HOST`, `LANGFUSE_PUBLIC_KEY`, and `LANGFUSE_SECRET_KEY` environment variables\n *\n * @see &lt;a href=\"https://langfuse.com/docs/opentelemetry/get-started#opentelemetry-endpoint\"&gt;Langfuse OpenTelemetry Docs&lt;/a&gt;\n */\nval agent = AIAgent(\n    executor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    llmModel = OpenAIModels.Chat.GPT4oMini,\n    systemPrompt = \"You are a code assistant. Provide concise code examples.\"\n) {\n    install(OpenTelemetry) {\n        addLangfuseExporter()\n    }\n}\n</code></pre>"},{"location":"examples/Langfuse/#configure-the-agent-and-langfuse-exporter","title":"Configure the agent and Langfuse exporter","text":"<p>In the next cell, we:</p> <ul> <li>Create an AIAgent that uses OpenAI as the LLM executor</li> <li>Install the OpenTelemetry feature and add the Langfuse exporter</li> <li>Rely on environment variables for Langfuse configuration</li> </ul> <p>Under the hood, Koog emits spans for agent lifecycle, LLM calls, and tool execution (if any). The Langfuse exporter ships those spans to your Langfuse instance via the OpenTelemetry endpoint.</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nprintln(\"Running agent with Langfuse tracing\")\n\nrunBlocking {\n    val result = agent.run(\"Tell me a joke about programming\")\n    \"Result: $result\\nSee traces on the Langfuse instance\"\n}\n</code></pre>"},{"location":"examples/Langfuse/#run-the-agent-and-view-traces","title":"Run the agent and view traces","text":"<p>Execute the next cell to trigger a simple prompt. This will generate spans that are exported to your Langfuse project.</p>"},{"location":"examples/Langfuse/#where-to-look-in-langfuse","title":"Where to look in Langfuse","text":"<ol> <li>Open your Langfuse dashboard and select your project</li> <li>Navigate to the Traces/Spans view</li> <li>Look for recent entries around the time you ran this cell</li> <li>Drill down into spans to see:</li> <li>Agent lifecycle events</li> <li>LLM request/response metadata</li> <li>Errors (if any)</li> </ol>"},{"location":"examples/Langfuse/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>No traces showing up?</li> <li>Double-check LANGFUSE_HOST, LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY</li> <li>Ensure your network allows outbound HTTPS to the Langfuse endpoint</li> <li>Verify your Langfuse project is active and keys belong to the correct project</li> <li>Authentication errors</li> <li>Regenerate keys in Langfuse and update env vars</li> <li>OpenAI issues</li> <li>Confirm OPENAI_API_KEY is set and valid</li> </ul>"},{"location":"examples/OpenTelemetry/","title":"OpenTelemetry with Koog: Tracing your AI agent","text":"<p> Open on GitHub  Download .ipynb</p> <p>This notebook demonstrates how to add OpenTelemetry-based tracing to a Koog AI agent. We will: - Emit spans to the console for quick local debugging. - Export spans to an OpenTelemetry Collector and view them in Jaeger.</p> <p>Prerequisites: - Docker/Docker Compose installed - An OpenAI API key available in environment variable <code>OPENAI_API_KEY</code></p> <p>Start the local OpenTelemetry stack (Collector + Jaeger) before running the notebook: <pre><code>./docker-compose up -d\n</code></pre> After the agent runs, open Jaeger UI: - http://localhost:16686</p> <p>To stop the services later: <pre><code>docker-compose down\n</code></pre></p> <pre><code>%useLatestDescriptors\n// %use koog\n</code></pre> <pre><code>import ai.koog.agents.core.agent.AIAgent\nimport ai.koog.agents.features.opentelemetry.feature.OpenTelemetry\nimport ai.koog.prompt.executor.clients.openai.OpenAIModels\nimport ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor\nimport io.opentelemetry.exporter.logging.LoggingSpanExporter\nimport io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter\n</code></pre>"},{"location":"examples/OpenTelemetry/#configure-opentelemetry-exporters","title":"Configure OpenTelemetry exporters","text":"<p>In the next cell, we: - Create a Koog AIAgent - Install the OpenTelemetry feature - Add two span exporters:   - LoggingSpanExporter for console logs   - OTLP gRPC exporter to http://localhost:4317 (Collector)</p> <p>This mirrors the example description: console logs for local debugging and OTLP for viewing traces in Jaeger.</p> <pre><code>val agent = AIAgent(\n    executor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    llmModel = OpenAIModels.Chat.GPT4oMini,\n    systemPrompt = \"You are a code assistant. Provide concise code examples.\"\n) {\n    install(OpenTelemetry) {\n        // Add a console logger for local debugging\n        addSpanExporter(LoggingSpanExporter.create())\n\n        // Send traces to OpenTelemetry collector\n        addSpanExporter(\n            OtlpGrpcSpanExporter.builder()\n                .setEndpoint(\"http://localhost:4317\")\n                .build()\n        )\n    }\n}\n</code></pre>"},{"location":"examples/OpenTelemetry/#run-the-agent-and-view-traces-in-jaeger","title":"Run the agent and view traces in Jaeger","text":"<p>Execute the next cell to trigger a simple prompt. You should see: - Console span logs from the LoggingSpanExporter - Traces exported to your local OpenTelemetry Collector and visible in Jaeger at http://localhost:16686</p> <p>Tip: Use the Jaeger search to find recent traces after you run the cell.</p> <pre><code>import ai.koog.agents.utils.use\nimport kotlinx.coroutines.runBlocking\n\nrunBlocking {\n    agent.use { agent -&gt;\n        println(\"Running agent with OpenTelemetry tracing...\")\n\n        val result = agent.run(\"Tell me a joke about programming\")\n\n        \"Agent run completed with result: '$result'.\\nCheck Jaeger UI at http://localhost:16686 to view traces\"\n    }\n}\n</code></pre>"},{"location":"examples/OpenTelemetry/#cleanup-and-troubleshooting","title":"Cleanup and troubleshooting","text":"<p>When you're done:</p> <ul> <li> <p>Stop services:   <pre><code>docker-compose down\n</code></pre></p> </li> <li> <p>If you don't see traces in Jaeger:</p> </li> <li>Ensure the stack is running: <code>./docker-compose up -d</code> and give it a few seconds to start.</li> <li>Verify ports:<ul> <li>Collector (OTLP gRPC): http://localhost:4317</li> <li>Jaeger UI: http://localhost:16686</li> </ul> </li> <li>Check container logs: <code>docker-compose logs --tail=200</code></li> <li>Confirm your <code>OPENAI_API_KEY</code> is set in the environment where the notebook runs.</li> <li> <p>Make sure the endpoint in the exporter matches the collector: <code>http://localhost:4317</code>.</p> </li> <li> <p>What spans to expect:</p> </li> <li>Koog agent lifecycle</li> <li>LLM request/response metadata</li> <li>Any tool execution spans (if you add tools)</li> </ul> <p>You can now iterate on your agent and observe changes in your tracing pipeline.</p>"},{"location":"examples/PlaywrightMcp/","title":"Drive the browser with Playwright MCP and Koog","text":"<p> Open on GitHub  Download .ipynb</p> <p>In this notebook, you'll connect a Koog agent to Playwright's Model Context Protocol (MCP) server and let it drive a real browser to complete a task: open jetbrains.com, accept cookies, and click the AI section in the toolbar.</p> <p>We'll keep things simple and reproducible, focusing on a minimal but realistic agent + tools setup you can publish and reuse.</p> <pre><code>%useLatestDescriptors\n%use koog\n</code></pre>"},{"location":"examples/PlaywrightMcp/#prerequisites","title":"Prerequisites","text":"<ul> <li>An OpenAI API key exported as an environment variable: <code>OPENAI_API_KEY</code></li> <li>Node.js and npx available on your PATH</li> <li>Kotlin Jupyter notebook environment with Koog available via <code>%use koog</code></li> </ul> <p>Tip: Run the Playwright MCP server in headful mode to watch the browser automate the steps.</p>"},{"location":"examples/PlaywrightMcp/#1-provide-your-openai-api-key","title":"1) Provide your OpenAI API key","text":"<p>We read the API key from the <code>OPENAI_API_KEY</code> environment variable. This keeps secrets out of the notebook.</p> <pre><code>// Get the API key from environment variables\nval openAIApiToken = System.getenv(\"OPENAI_API_KEY\") ?: error(\"OPENAI_API_KEY environment variable not set\")\n</code></pre>"},{"location":"examples/PlaywrightMcp/#2-start-the-playwright-mcp-server","title":"2) Start the Playwright MCP server","text":"<p>We'll launch Playwright's MCP server locally using <code>npx</code>. By default, it will expose an SSE endpoint we can connect to from Koog.</p> <pre><code>// Start the Playwright MCP server via npx\nval process = ProcessBuilder(\n    \"npx\",\n    \"@playwright/mcp@latest\",\n    \"--port\",\n    \"8931\"\n).start()\n</code></pre>"},{"location":"examples/PlaywrightMcp/#3-connect-from-koog-and-run-the-agent","title":"3) Connect from Koog and run the agent","text":"<p>We build a minimal Koog <code>AIAgent</code> with an OpenAI executor and point its tool registry to the MCP server over SSE. Then we ask it to complete the browser task strictly via tools.</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nrunBlocking {\n    println(\"Connecting to Playwright MCP server...\")\n    val toolRegistry = McpToolRegistryProvider.fromTransport(\n        transport = McpToolRegistryProvider.defaultSseTransport(\"http://localhost:8931\")\n    )\n    println(\"Successfully connected to Playwright MCP server\")\n\n    // Create the agent\n    val agent = AIAgent(\n        executor = simpleOpenAIExecutor(openAIApiToken),\n        llmModel = OpenAIModels.Chat.GPT4o,\n        toolRegistry = toolRegistry,\n    )\n\n    val request = \"Open a browser, navigate to jetbrains.com, accept all cookies, click AI in toolbar\"\n    println(\"Sending request: $request\")\n\n    agent.run(\n        request + \". \" +\n            \"You can only call tools. Use the Playwright tools to complete this task.\"\n    )\n}\n</code></pre>"},{"location":"examples/PlaywrightMcp/#4-shut-down-the-mcp-process","title":"4) Shut down the MCP process","text":"<p>Always clean up the external process at the end of your run.</p> <pre><code>// Shutdown the Playwright MCP process\nprintln(\"Closing connection to Playwright MCP server\")\nprocess.destroy()\n</code></pre>"},{"location":"examples/PlaywrightMcp/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If the agent can't connect, make sure the MCP server is running on <code>http://localhost:8931</code>.</li> <li>If you don't see the browser, ensure Playwright is installed and able to launch a browser on your system.</li> <li>If you get authentication errors from OpenAI, double-check the <code>OPENAI_API_KEY</code> environment variable.</li> </ul>"},{"location":"examples/PlaywrightMcp/#next-steps","title":"Next steps","text":"<ul> <li>Try different websites or flows. The MCP server exposes a rich set of Playwright tools.</li> <li>Swap the LLM model, or add more tools to the Koog agent.</li> <li>Integrate this flow into your app, or publish the notebook as documentation.</li> </ul>"},{"location":"examples/UnityMcp/","title":"Unity + Koog: Drive your game from a Kotlin Agent","text":"<p> Open on GitHub  Download .ipynb</p> <p>This notebook walks you through building a Unity-savvy AI agent with Koog using the Model Context Protocol (MCP). We'll connect to a Unity MCP server, discover tools, plan with an LLM, and execute actions against your open scene.</p> <p>Prerequisites - A Unity project with the Unity-MCP server plugin installed - JDK 17+ - An OpenAI API key in the OPENAI_API_KEY environment variable</p> <pre><code>%useLatestDescriptors\n%use koog\n</code></pre> <pre><code>lateinit var process: Process\n</code></pre>"},{"location":"examples/UnityMcp/#1-provide-your-openai-api-key","title":"1) Provide your OpenAI API key","text":"<p>We read the API key from the <code>OPENAI_API_KEY</code> environment variable so you can keep secrets out of the notebook.</p> <pre><code>val token = System.getenv(\"OPENAI_API_KEY\") ?: error(\"OPENAI_API_KEY environment variable not set\")\nval executor = simpleOpenAIExecutor(token)\n</code></pre>"},{"location":"examples/UnityMcp/#2-configure-the-unity-agent","title":"2) Configure the Unity agent","text":"<p>We define a compact system prompt and agent settings for Unity.</p> <pre><code>val agentConfig = AIAgentConfig(\n    prompt = prompt(\"cook_agent_system_prompt\") {\n        system {\n            \"You are a Unity assistant. You can execute different tasks by interacting with tools from the Unity engine.\"\n        }\n    },\n    model = OpenAIModels.Chat.GPT4o,\n    maxAgentIterations = 1000\n)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"examples/UnityMcp/#3-start-the-unity-mcp-server","title":"3) Start the Unity MCP server","text":"<p>We'll launch the Unity MCP server from your Unity project directory and connect over stdio.</p> <pre><code>// https://github.com/IvanMurzak/Unity-MCP\nval pathToUnityProject = \"path/to/unity/project\"\nval process = ProcessBuilder(\n    \"$pathToUnityProject/com.ivanmurzak.unity.mcp.server/bin~/Release/net9.0/com.IvanMurzak.Unity.MCP.Server\",\n    \"60606\"\n).start()\n</code></pre>"},{"location":"examples/UnityMcp/#4-connect-from-koog-and-run-the-agent","title":"4) Connect from Koog and run the agent","text":"<p>We discover tools from the Unity MCP server, build a small plan-first strategy, and run an agent that uses only tools to modify your open scene.</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nrunBlocking {\n    // Create the ToolRegistry with tools from the MCP server\n    val toolRegistry = McpToolRegistryProvider.fromTransport(\n        transport = McpToolRegistryProvider.defaultStdioTransport(process)\n    )\n\n    toolRegistry.tools.forEach {\n        println(it.name)\n        println(it.descriptor)\n    }\n\n    val strategy = strategy&lt;String, String&gt;(\"unity_interaction\") {\n        val nodePlanIngredients by nodeLLMRequest(allowToolCalls = false)\n        val interactionWithUnity by subgraphWithTask&lt;String, String&gt;(\n            // work with plan\n            tools = toolRegistry.tools,\n        ) { input -&gt;\n            \"Start interacting with Unity according to the plan: $input\"\n        }\n\n        edge(\n            nodeStart forwardTo nodePlanIngredients transformed {\n                \"Create detailed plan for \" + agentInput + \"\" +\n                    \"using the following tools: ${toolRegistry.tools.joinToString(\"\\n\") {\n                        it.name + \"\\ndescription:\" + it.descriptor\n                    }}\"\n            }\n        )\n        edge(nodePlanIngredients forwardTo interactionWithUnity onAssistantMessage { true })\n        edge(interactionWithUnity forwardTo nodeFinish)\n    }\n\n    val agent = AIAgent(\n        promptExecutor = executor,\n        strategy = strategy,\n        agentConfig = agentConfig,\n        toolRegistry = toolRegistry,\n        installFeatures = {\n            install(Tracing)\n\n            install(EventHandler) {\n                onAgentStarting { eventContext -&gt;\n                    println(\"OnAgentStarting first (strategy: ${strategy.name})\")\n                }\n\n                onAgentStarting { eventContext -&gt;\n                    println(\"OnAgentStarting second (strategy: ${strategy.name})\")\n                }\n\n                onAgentCompleted { eventContext -&gt;\n                    println(\n                        \"OnAgentCompleted (agent id: ${eventContext.agentId}, result: ${eventContext.result})\"\n                    )\n                }\n            }\n        }\n    )\n\n    val result = agent.run(\n        \" extend current opened scene for the towerdefence game. \" +\n            \"Add more placements for the towers, change the path for the enemies\"\n    )\n\n    result\n}\n</code></pre>"},{"location":"examples/UnityMcp/#5-shut-down-the-mcp-process","title":"5) Shut down the MCP process","text":"<p>Always clean up the external Unity MCP server process at the end of your run.</p> <pre><code>// Shutdown the Unity MCP process\nprocess.destroy()\n</code></pre>"},{"location":"examples/VaccumAgent/","title":"Build a Simple Vacuum Cleaner Agent","text":"<p> Open on GitHub  Download .ipynb</p> <p>In this notebook, we'll explore how to implement a basic reflex agent using the new Kotlin agents framework. Our example will be the classic \"vacuum world\" problem \u2014 a simple environment with two locations that can be clean or dirty, and an agent that needs to clean them.</p> <p>First, let's understand our environment model:</p> <pre><code>import kotlin.random.Random\n\n/**\n * Represents a simple vacuum world with two locations (A and B).\n *\n * The environment tracks:\n * - The current location of the vacuum agent ('A' or 'B')\n * - The cleanliness status of each location (true = dirty, false = clean)\n */\nclass VacuumEnv {\n    var location: Char = 'A'\n        private set\n\n    private val status = mutableMapOf(\n        'A' to Random.nextBoolean(),\n        'B' to Random.nextBoolean()\n    )\n\n    fun percept(): Pair&lt;Char, Boolean&gt; = location to status.getValue(location)\n\n    fun clean(): String {\n        status[location] = false\n        return \"cleaned\"\n    }\n\n    fun moveLeft(): String {\n        location = 'A'\n        return \"move to A\"\n    }\n\n    fun moveRight(): String {\n        location = 'B'\n        return \"move to B\"\n    }\n\n    fun isClean(): Boolean = status.values.all { it }\n\n    fun worldLayout(): String = \"${status.keys}\"\n\n    override fun toString(): String = \"location=$location, dirtyA=${status['A']}, dirtyB=${status['B']}\"\n}\n</code></pre> <p>The VacuumEnv class models our simple world: - Two locations are represented by characters 'A' and 'B' - Each location can be either clean or dirty (randomly initialized) - The agent can be at either location at any given time - The agent can perceive its current location and whether it's dirty - The agent can take actions: move to a specific location or clean the current location</p>"},{"location":"examples/VaccumAgent/#creating-tools-for-vacuum-agent","title":"Creating Tools for Vacuum Agent","text":"<p>Now, let's define the tools our AI agent will use to interact with the environment:</p> <pre><code>import ai.koog.agents.core.tools.annotations.LLMDescription\nimport ai.koog.agents.core.tools.annotations.Tool\nimport ai.koog.agents.core.tools.reflect.ToolSet\n\n\n/**\n * Provides tools for the LLM agent to control the vacuum robot.\n * All methods either mutate or read from the VacuumEnv passed to the constructor.\n */\n@LLMDescription(\"Tools for controlling a two-cell vacuum world\")\nclass VacuumTools(private val env: VacuumEnv) : ToolSet {\n\n    @Tool\n    @LLMDescription(\"Returns current location and whether it is dirty\")\n    fun sense(): String {\n        val (loc, dirty) = env.percept()\n        return \"location=$loc, dirty=$dirty, locations=${env.worldLayout()}\"\n    }\n\n    @Tool\n    @LLMDescription(\"Cleans the current cell\")\n    fun clean(): String = env.clean()\n\n    @Tool\n    @LLMDescription(\"Moves the agent to cell A\")\n    fun moveLeft(): String = env.moveLeft()\n\n    @Tool\n    @LLMDescription(\"Moves the agent to cell B\")\n    fun moveRight(): String = env.moveRight()\n}\n</code></pre> <p>The <code>VacuumTools</code> class creates an interface between our LLM agent and the environment:</p> <ul> <li>It implements <code>ToolSet</code> from the Kotlin AI Agents framework</li> <li>Each tool is annotated with <code>@Tool</code> and has a description for the LLM</li> <li>The tools allow the agent to sense its environment and take actions</li> <li>Each method returns a string that describes the outcome of the action</li> </ul>"},{"location":"examples/VaccumAgent/#setting-up-the-agent","title":"Setting Up the Agent","text":"<p>Next, we'll configure and create our AI agent:</p> <pre><code>import ai.koog.agents.core.agent.AIAgent\nimport ai.koog.agents.core.agent.config.AIAgentConfig\nimport ai.koog.agents.core.tools.ToolRegistry\nimport ai.koog.agents.core.tools.reflect.asTools\nimport ai.koog.agents.ext.agent.chatAgentStrategy\nimport ai.koog.agents.ext.tool.AskUser\nimport ai.koog.agents.ext.tool.SayToUser\nimport ai.koog.prompt.dsl.prompt\nimport ai.koog.prompt.executor.clients.openai.OpenAIModels\nimport ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor\nimport ai.koog.prompt.params.LLMParams\n\n\nval env = VacuumEnv()\nval apiToken = System.getenv(\"OPENAI_API_KEY\") ?: error(\"OPENAI_API_KEY environment variable not set\")\nval executor = simpleOpenAIExecutor(apiToken = apiToken)\n\nval toolRegistry = ToolRegistry {\n    tool(SayToUser)\n    tool(AskUser)\n    tools(VacuumTools(env).asTools())\n}\n\nval systemVacuumPrompt = \"\"\"\n    You are a reflex vacuum-cleaner agent living in a two-cell world labelled A and B.\n    Your goal: make both cells clean, using the provided tools.\n    First, call sense() to inspect where you are. Then decide: if dirty \u2192 clean(); else moveLeft()/moveRight().\n    Continue until both cells are clean, then tell the user \"done\".\n    Use sayToUser to inform the user about each step.\n\"\"\".trimIndent()\n\nval agentConfig = AIAgentConfig(\n    prompt = prompt(\"chat\", params = LLMParams(temperature = 1.0)) {\n        system(systemVacuumPrompt)\n    },\n    model = OpenAIModels.Chat.GPT4o,\n    maxAgentIterations = 50,\n)\n\nval agent = AIAgent(\n    promptExecutor = executor,\n    strategy = chatAgentStrategy(),\n    agentConfig = agentConfig,\n    toolRegistry = toolRegistry\n)\n</code></pre> <p>In this setup:</p> <ol> <li>We create an instance of our environment</li> <li>We set up a connection to OpenAI's GPT-4o model</li> <li>We register the tools our agent can use</li> <li>We define a system prompt that gives the agent its goal and behavior rules</li> <li>We create the agent using the <code>AIAgent</code> constructor with a chat strategy</li> </ol>"},{"location":"examples/VaccumAgent/#running-the-agent","title":"Running the Agent","text":"<p>Finally, let's run our agent:</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nrunBlocking {\n    agent.run(\"Start cleaning, please\")\n}\n</code></pre> <pre><code>Agent says: Currently in cell A. It's already clean.\nAgent says: Moved to cell B. It's already clean.\n</code></pre> <p>When we run this code:</p> <ol> <li>The agent receives the initial prompt to start cleaning</li> <li>It uses its tools to sense the environment and make decisions</li> <li>It continues cleaning until both cells are clean</li> <li>Throughout the process, it keeps the user informed about what it's doing</li> </ol> <pre><code>// Finally we can validate that the work is finished by printing the env state\n\nenv\n</code></pre> <pre><code>location=B, dirtyA=false, dirtyB=false\n</code></pre>"},{"location":"examples/Weave/","title":"Weave tracing for Koog agents","text":"<p> Open on GitHub  Download .ipynb</p> <p>This notebook demonstrates how to trace Koog agents to W&amp;B Weave using OpenTelemetry (OTLP). You will create a simple Koog <code>AIAgent</code>, enable the Weave exporter, run a prompt, and view rich traces in the Weave UI.</p> <p>For background, see Weave OpenTelemetry docs: https://weave-docs.wandb.ai/guides/tracking/otel/</p>"},{"location":"examples/Weave/#prerequisites","title":"Prerequisites","text":"<p>Before running the example, make sure you have:</p> <ul> <li>A Weave/W&amp;B account: https://wandb.ai</li> <li>Your API key from https://wandb.ai/authorize exposed as an environment variable: <code>WEAVE_API_KEY</code></li> <li>Your Weave entity (team or user) name exposed as <code>WEAVE_ENTITY</code></li> <li>Find it on your W&amp;B dashboard: https://wandb.ai/home (left sidebar \"Teams\")</li> <li>A project name exposed as <code>WEAVE_PROJECT_NAME</code> (if not set, this example uses <code>koog-tracing</code>)</li> <li>An OpenAI API key exposed as <code>OPENAI_API_KEY</code> to run the Koog agent</li> </ul> <p>Example (macOS/Linux): <pre><code>export WEAVE_API_KEY=...  # required by Weave\nexport WEAVE_ENTITY=your-team-or-username\nexport WEAVE_PROJECT_NAME=koog-tracing\nexport OPENAI_API_KEY=...\n</code></pre></p>"},{"location":"examples/Weave/#notebook-setup","title":"Notebook setup","text":"<p>We use the latest Kotlin Jupyter descriptors. If you have Koog preconfigured as a <code>%use</code> plugin, you can uncomment the line below.</p> <pre><code>%useLatestDescriptors\n//%use koog\n</code></pre>"},{"location":"examples/Weave/#create-an-agent-and-enable-weave-tracing","title":"Create an agent and enable Weave tracing","text":"<p>We construct a minimal <code>AIAgent</code> and install the <code>OpenTelemetry</code> feature with the Weave exporter. The exporter sends OTLP spans to Weave using your environment configuration: - <code>WEAVE_API_KEY</code> \u2014 authentication to Weave - <code>WEAVE_ENTITY</code> \u2014 which team/user owns the traces - <code>WEAVE_PROJECT_NAME</code> \u2014 the Weave project to store traces in</p> <pre><code>import ai.koog.agents.core.agent.AIAgent\nimport ai.koog.agents.features.opentelemetry.feature.OpenTelemetry\nimport ai.koog.agents.features.opentelemetry.integration.weave.addWeaveExporter\nimport ai.koog.prompt.executor.clients.openai.OpenAIModels\nimport ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor\n\nval entity = System.getenv()[\"WEAVE_ENTITY\"] ?: throw IllegalArgumentException(\"WEAVE_ENTITY is not set\")\nval projectName = System.getenv()[\"WEAVE_PROJECT_NAME\"] ?: \"koog-tracing\"\n\nval agent = AIAgent(\n    executor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    llmModel = OpenAIModels.Chat.GPT4oMini,\n    systemPrompt = \"You are a code assistant. Provide concise code examples.\"\n) {\n    install(OpenTelemetry) {\n        addWeaveExporter(\n            weaveEntity = entity,\n            weaveProjectName = projectName\n        )\n    }\n}\n</code></pre>"},{"location":"examples/Weave/#run-the-agent-and-view-traces-in-weave","title":"Run the agent and view traces in Weave","text":"<p>Execute a simple prompt. After completion, open the printed link to view the trace in Weave. You should see spans for the agent\u2019s run, model calls, and other instrumented operations.</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nprintln(\"Running agent with Weave tracing\")\n\nrunBlocking {\n    val result = agent.run(\"Tell me a joke about programming\")\n    \"Result: $result\\nSee traces on https://wandb.ai/$entity/$projectName/weave/traces\"\n}\n</code></pre>"},{"location":"examples/Weave/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If you don't see traces, verify <code>WEAVE_API_KEY</code>, <code>WEAVE_ENTITY</code>, and <code>WEAVE_PROJECT_NAME</code> are set in your environment.</li> <li>Ensure your network allows outbound HTTPS to Weave's OTLP endpoint.</li> <li>Confirm your OpenAI key is valid and the selected model is accessible from your account.</li> </ul>"},{"location":"examples/WebMcpClient/","title":"Web Scraping with The Web MCP by Bright Data and Koog","text":"<p> Open on GitHub  Download .kt</p> <p>In this tutorial, you'll connect a Koog agent to Bright Data's Web MCP server and let it perform web scraping and data collection tasks. We'll demonstrate how to search for information about Koog.ai using Bright Data's powerful web scraping infrastructure through the Model Context Protocol.</p> <p>We'll keep things simple and reproducible, focusing on a minimal but realistic agent + tools setup you can adapt for your own web scraping needs.</p>"},{"location":"examples/WebMcpClient/#prerequisites","title":"Prerequisites","text":"<ul> <li>An OpenAI API key exported as an environment variable: <code>OPENAI_API_KEY</code></li> <li>A Bright Data API token exported as an environment variable: <code>BRIGHT_DATA_API_TOKEN</code></li> <li>Node.js and npx available on your PATH</li> <li>Kotlin development environment with Koog dependencies</li> </ul> <p>Tip: The Bright Data MCP server provides access to enterprise-grade web scraping tools that can handle complex websites, CAPTCHAs, and anti-bot measures.</p>"},{"location":"examples/WebMcpClient/#1-set-up-your-api-credentials","title":"1) Set up your API credentials","text":"<p>We read both API keys from environment variables to keep secrets secure and out of your code.</p> <pre><code>// Get API keys from environment variables\nval openAIApiKey = System.getenv(\"OPENAI_API_KEY\")\n    ?: error(\"OPENAI_API_KEY environment variable is not set\")\nval brightDataToken = System.getenv(\"BRIGHT_DATA_API_TOKEN\")\n    ?: error(\"BRIGHT_DATA_API_TOKEN environment variable is not set\")\n</code></pre>"},{"location":"examples/WebMcpClient/#2-start-the-web-mcp-server-by-bright-data","title":"2) Start The Web MCP server by Bright Data","text":"<p>We'll launch Bright Data's MCP server using <code>npx</code> and configure it with your API token. The server will expose web scraping capabilities through the Model Context Protocol.</p> <pre><code>println(\"Starting Bright Data MCP server...\")\n\n// Start the Bright Data MCP server as a separate process\nval processBuilder = ProcessBuilder(\"npx\", \"@brightdata/mcp\")\n\n// Set the API_TOKEN environment variable for the MCP server process\nval environment = processBuilder.environment()\nenvironment[\"API_TOKEN\"] = brightDataToken\n\n// Start the process\nval process = processBuilder.start()\n\n// Give the process a moment to start\nThread.sleep(2000)\n</code></pre>"},{"location":"examples/WebMcpClient/#3-connect-from-koog-and-create-the-agent","title":"3) Connect from Koog and create the agent","text":"<p>We build a Koog <code>AIAgent</code> with an OpenAI executor and connect its tool registry to the Bright Data MCP server via STDIO transport. Then we'll explore the available tools and run a web scraping task.</p> <pre><code>println(\"Creating STDIO transport...\")\ntry {\n    // Create the STDIO transport\n    val transport = McpToolRegistryProvider.defaultStdioTransport(process)\n\n    println(\"Creating tool registry...\")\n\n    // Create a tool registry with tools from the Bright Data MCP server\n    val toolRegistry = McpToolRegistryProvider.fromTransport(\n        transport = transport,\n        name = \"bright-data-client\",\n        version = \"1.0.0\"\n    )\n\n    // Print available tools (optional - for debugging)\n    println(\"Available tools from Bright Data MCP server:\")\n    toolRegistry.tools.forEach { tool -&gt;\n        println(\"- ${tool.name}\")\n    }\n\n    // Create the agent with MCP tools\n    val agent = AIAgent(\n        executor = simpleOpenAIExecutor(openAIApiKey),\n        systemPrompt = \"You are a helpful assistant with access to web scraping and data collection tools from Bright Data. You can help users gather information from websites, analyze web data, and provide insights.\",\n        llmModel = OpenAIModels.Chat.GPT4o,\n        temperature = 0.7,\n        toolRegistry = toolRegistry,\n        maxIterations = 100\n    )\n\n    val result = agent.run(\"Please search for Koog.ai and tell me what is it and who invented it\")\n\n    println(\"\\nAgent response:\")\n    println(result)\n\n} catch (e: Exception) {\n    println(\"Error: ${e.message}\")\n    e.printStackTrace()\n} finally {\n    println(\"Shutting down MCP server...\")\n    process.destroyForcibly()\n}\n</code></pre>"},{"location":"examples/WebMcpClient/#4-complete-code-example","title":"4) Complete code example","text":"<p>Here's the complete working example that demonstrates web scraping with The Web MCP by Bright Data:</p> <pre><code>package koog\n\nimport ai.koog.agents.core.agent.AIAgent\nimport ai.koog.agents.mcp.McpToolRegistryProvider\nimport ai.koog.prompt.executor.clients.openai.OpenAIModels\nimport ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor\nimport kotlinx.coroutines.runBlocking\n\n/**\n * The entry point of the program demonstrating AI-driven web scraping and data collection.\n *\n * This function initializes a Bright Data MCP server, sets up tool integration,\n * and defines an AI agent for interacting with web scraping tools. It demonstrates the\n * following key operations:\n *\n * 1. Starts the Bright Data MCP server using a subprocess with proper API token configuration.\n * 2. Configures a registry of tools from the MCP server via STDIO transport communication.\n * 3. Creates an AI agent leveraging OpenAI's GPT-4o model with web scraping capabilities.\n * 4. Runs the agent to perform a specified task (e.g., searching for and analyzing web content\n *    about Koog.ai).\n * 5. Cleans up by shutting down the MCP server process after execution.\n *\n * This function is intended for tutorial purposes, demonstrating how to integrate\n * MCP (Model Context Protocol) servers with AI agents for web data collection and analysis.\n * It requires OPENAI_API_KEY and BRIGHT_DATA_API_TOKEN environment variables to be set.\n */\nfun main() = runBlocking {\n    // Get API keys from environment variables\n    val openAIApiKey = System.getenv(\"OPENAI_API_KEY\")\n        ?: error(\"OPENAI_API_KEY environment variable is not set\")\n    val brightDataToken = System.getenv(\"BRIGHT_DATA_API_TOKEN\")\n        ?: error(\"BRIGHT_DATA_API_TOKEN environment variable is not set\")\n\n    println(\"Starting Bright Data MCP server...\")\n\n    // Start the Bright Data MCP server as a separate process\n    val processBuilder = ProcessBuilder(\"npx\", \"@brightdata/mcp\")\n\n    // Set the API_TOKEN environment variable for the MCP server process\n    val environment = processBuilder.environment()\n    environment[\"API_TOKEN\"] = brightDataToken\n\n    // Start the process\n    val process = processBuilder.start()\n\n    // Give the process a moment to start\n    Thread.sleep(2000)\n\n    println(\"Creating STDIO transport...\")\n\n    try {\n        // Create the STDIO transport\n        val transport = McpToolRegistryProvider.defaultStdioTransport(process)\n\n        println(\"Creating tool registry...\")\n\n        // Create a tool registry with tools from the Bright Data MCP server\n        val toolRegistry = McpToolRegistryProvider.fromTransport(\n            transport = transport,\n            name = \"bright-data-client\",\n            version = \"1.0.0\"\n        )\n\n        // Print available tools (optional - for debugging)\n        println(\"Available tools from Bright Data MCP server:\")\n        toolRegistry.tools.forEach { tool -&gt;\n            println(\"- ${tool.name}\")\n        }\n\n        // Create the agent with MCP tools\n        val agent = AIAgent(\n            executor = simpleOpenAIExecutor(openAIApiKey),\n            systemPrompt = \"You are a helpful assistant with access to web scraping and data collection tools from Bright Data. You can help users gather information from websites, analyze web data, and provide insights.\",\n            llmModel = OpenAIModels.Chat.GPT4o,\n            temperature = 0.7,\n            toolRegistry = toolRegistry,\n            maxIterations = 100\n        )\n\n        val result = agent.run(\"Please search for Koog.ai and tell me what is it and who invented it\")\n\n        println(\"\\nAgent response:\")\n        println(result)\n\n    } catch (e: Exception) {\n        println(\"Error: ${e.message}\")\n        e.printStackTrace()\n    } finally {\n        println(\"Shutting down MCP server...\")\n        process.destroyForcibly()\n    }\n}\n</code></pre>"},{"location":"examples/WebMcpClient/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Connection issues: If the agent can't connect to the MCP server, ensure the Bright Data MCP package is properly installed via <code>npx @brightdata/mcp</code>.</li> <li>API token errors: Double-check that your <code>BRIGHT_DATA_API_TOKEN</code> is valid and has the necessary permissions for web scraping.</li> <li>OpenAI authentication: Verify that your <code>OPENAI_API_KEY</code> environment variable is correctly set and the API key is valid.</li> <li>Process timeout: If the server takes longer to start, increase the <code>Thread.sleep(2000)</code> duration.</li> </ul>"},{"location":"examples/WebMcpClient/#next-steps","title":"Next steps","text":"<ul> <li>Explore different queries: Try scraping different websites or searching for various topics.</li> <li>Custom tool integration: Add your own tools alongside Bright Data's web scraping capabilities.</li> <li>Advanced scraping: Leverage Bright Data's advanced features like residential proxies, CAPTCHA solving, and JavaScript rendering.</li> <li>Data processing: Combine the scraped data with other Koog agents for analysis and insights.</li> <li>Production deployment: Integrate this pattern into your applications for automated web data collection.</li> </ul>"},{"location":"examples/WebMcpClient/#what-youve-learned","title":"What you've learned","text":"<p>This tutorial demonstrated how to: - Set up and configure The Web MCP by Bright Data - Connect a Koog AI agent to external MCP servers via STDIO transport - Perform AI-driven web scraping tasks using natural language instructions - Handle proper resource cleanup and error management - Structure code for production-ready web scraping applications</p> <p>The combination of Koog's AI agent capabilities with Bright Data's enterprise web scraping infrastructure provides a powerful foundation for automated data collection and analysis workflows.</p>"},{"location":"prompts/","title":"Prompts","text":"<p>Prompts are instructions for Large Language Models (LLMs) that guide them in generating responses. They define the content and structure of your interactions with LLMs. This section describes how to create and run prompts with Koog.</p>"},{"location":"prompts/#creating-prompts","title":"Creating prompts","text":"<p>In Koog, prompts are instances of the Prompt  data class with the following properties:</p> <ul> <li><code>id</code>: A unique identifier for the prompt.</li> <li><code>messages</code>: A list of messages that represent the conversation with the LLM.</li> <li><code>params</code>: Optional LLM configuration parameters (such as temperature, tool choice, and others).</li> </ul> <p>Although you can instantiate the <code>Prompt</code> class directly, the recommended way to create prompts is by using the Kotlin DSL,  which provides a structured way to define the conversation.</p> <pre><code>val myPrompt = prompt(\"hello-koog\") {\n    system(\"You are a helpful assistant.\")\n    user(\"What is Koog?\")\n}\n</code></pre> <p>Note</p> <p>AI agents can take a simple text prompt as input. They automatically convert the text prompt to the Prompt object and send it to the LLM for execution. This is useful for a basic agent that only needs to run a single request and does not require complex conversation logic.</p>"},{"location":"prompts/#running-prompts","title":"Running prompts","text":"<p>Koog provides two levels of abstraction for running prompts against LLMs: LLM clients and prompt executors. Both accept Prompt objects and can be used for direct prompt execution, without an AI agent. The execution flow is the same for both clients and executors:</p> <pre><code>flowchart TB\n    A([Prompt built with Kotlin DSL])\n    B{LLM client or prompt executor}\n    C[LLM provider]\n    D([Response to your application])\n\n    A --&gt;|\"passed to\"| B\n    B --&gt;|\"sends request\"| C\n    C --&gt;|\"returns response\"| B\n    B --&gt;|\"returns result\"| D</code></pre> <ul> <li> <p> LLM clients</p> <p>Low\u2011level interfaces for direct interaction with specific LLM providers. Use them when you work with a single provider and do not need advanced lifecycle management.</p> </li> <li> <p> Prompt executors</p> <p>High-level abstractions that manage the lifecycles of one or multiple LLM clients. Use them when you need a unified API for running prompts across multiple providers, with dynamic switching between them and fallbacks.</p> </li> </ul>"},{"location":"prompts/#optimizing-performance-and-handling-failures","title":"Optimizing performance and handling failures","text":"<p>Koog allows you to optimize performance and handle failures when running prompts.</p> <ul> <li> <p> LLM response caching</p> <p>Cache LLM responses to optimize performance and reduce costs for repeated requests.</p> </li> <li> <p> Handling failures</p> <p>Use built-in retries, timeouts, and other error handling mechanisms in your application.</p> </li> </ul>"},{"location":"prompts/#prompts-in-ai-agents","title":"Prompts in AI agents","text":"<p>In Koog, AI agents maintain and manage prompts during their lifecycle. While LLM clients or executors are used to run prompts, agents handle the flow of prompt updates, ensuring the  conversation history remains relevant and consistent.</p> <p>The prompt lifecycle in an agent usually includes several stages:</p> <ol> <li>Initial prompt setup.</li> <li>Automatic prompt updates.</li> <li>Context window management.</li> <li>Manual prompt management.</li> </ol>"},{"location":"prompts/#initial-prompt-setup","title":"Initial prompt setup","text":"<p>When you initialize an agent, you define  a system message that sets the agent's behavior. Then, when you call the agent's <code>run()</code> method, you typically provide an initial user message as input. Together, these messages form the agent's initial prompt. For example: </p> <pre><code>// Create an agent\nval agent = AIAgent(\n    promptExecutor = simpleOpenAIExecutor(apiKey),\n    systemPrompt = \"You are a helpful assistant.\",\n    llmModel = OpenAIModels.Chat.GPT4o\n)\n\n// Run the agent\nval result = agent.run(\"What is Koog?\")\n</code></pre> <p>In the example, the agent automatically converts the text prompt to the Prompt object and sends it to the prompt executor:</p> <pre><code>flowchart TB\n    A([Your application])\n    B{{Configured AI agent}}\n    C[\"Text prompt\"]\n    D[\"Prompt object\"]\n    E{{Prompt executor}}\n    F[LLM provider]\n\n    A --&gt;|\"run() with text\"| B\n    B --&gt;|\"takes\"| C\n    C --&gt;|\"converted to\"| D\n    D --&gt;|\"sent via\"| E\n    E --&gt;|\"calls\"| F\n    F --&gt;|\"responds to\"| E\n    E --&gt;|\"result to\"| B\n    B --&gt;|\"result to\"| A</code></pre> <p>For more advanced configurations, you can also use  AIAgentConfig to define the agent's initial prompt.</p>"},{"location":"prompts/#automatic-prompt-updates","title":"Automatic prompt updates","text":"<p>As the agent runs its strategy, predefined nodes automatically update the prompt. For example:</p> <ul> <li><code>nodeLLMRequest</code>: Appends a user message to the prompt and captures the LLM response.</li> <li><code>nodeLLMSendToolResult</code>: Appends tool execution results to the conversation.</li> <li><code>nodeAppendPrompt</code>: Inserts specific messages into the prompt at any point in the workflow.</li> </ul>"},{"location":"prompts/#context-window-management","title":"Context window management","text":"<p>To avoid exceeding the LLM context window in long-running interactions, agents can use the history compression feature.</p>"},{"location":"prompts/#manual-prompt-management","title":"Manual prompt management","text":"<p>For complex workflows, you can manage the prompt manually using LLM sessions. In an agent strategy or custom node, you can use <code>llm.writeSession</code> to access and change the <code>Prompt</code> object. This lets you add, remove, or reorder messages as needed.</p>"},{"location":"prompts/handling-failures/","title":"Handling failures","text":"<p>This page describes how to handle failures for LLM clients and prompt executors using the built-in retry and timeout mechanisms.</p>"},{"location":"prompts/handling-failures/#retry-functionality","title":"Retry functionality","text":"<p>When working with LLM providers, transient errors like rate limits or temporary service unavailability may occur. The <code>RetryingLLMClient</code> decorator adds automatic retry logic to any LLM client.</p>"},{"location":"prompts/handling-failures/#basic-usage","title":"Basic usage","text":"<p>Wrap any existing client with the retry capability:</p> <pre><code>// Wrap any client with the retry capability\nval client = OpenAILLMClient(apiKey)\nval resilientClient = RetryingLLMClient(client)\n\n// Now all operations will automatically retry on transient errors\nval response = resilientClient.execute(prompt, OpenAIModels.Chat.GPT4o)\n</code></pre>"},{"location":"prompts/handling-failures/#configuring-retry-behavior","title":"Configuring retry behavior","text":"<p>By default, <code>RetryingLLMClient</code> configures an LLM client with the maximum of 3 retry attempts, a 1-second initial delay, and a 30-second maximum delay. You can specify a different retry configuration using a <code>RetryConfig</code> passed to <code>RetryingLLMClient</code>. For example:</p> <pre><code>// Use the predefined configuration\nval conservativeClient = RetryingLLMClient(\n    delegate = client,\n    config = RetryConfig.CONSERVATIVE\n)\n</code></pre> <p>Koog provides several predefined retry configurations:</p> Configuration Max attempts Initial delay Max delay Use case <code>RetryConfig.DISABLED</code> 1 (no retry) - - Development, testing, and debugging. <code>RetryConfig.CONSERVATIVE</code> 3 2s 30s Background or scheduled tasks where reliability is more important than speed. <code>RetryConfig.AGGRESSIVE</code> 5 500ms 20s Critical operations where fast recovery from transient errors is more important than reducing API calls. <code>RetryConfig.PRODUCTION</code> 3 1s 20s General production use. <p>You can use them directly or create custom configurations:</p> <pre><code>// Or create a custom configuration\nval customClient = RetryingLLMClient(\n    delegate = client,\n    config = RetryConfig(\n        maxAttempts = 5,\n        initialDelay = 1.seconds,\n        maxDelay = 30.seconds,\n        backoffMultiplier = 2.0,\n        jitterFactor = 0.2\n    )\n)\n</code></pre>"},{"location":"prompts/handling-failures/#retry-error-patterns","title":"Retry error patterns","text":"<p>By default, the <code>RetryingLLMClient</code> recognizes common transient errors. This behavior is controlled by the <code>RetryConfig.retryablePatterns</code> patterns. Each pattern is represented by <code>RetryablePattern</code> that checks the error message from a failed request and determines whether it should be retried.</p> <p>Koog provides the predefined retry configurations and patterns that work across all the supported LLM providers. You can keep the defaults or customize them for your specific needs.</p>"},{"location":"prompts/handling-failures/#pattern-types","title":"Pattern types","text":"<p>You can use the following pattern types and combine any number of them:</p> <ul> <li><code>RetryablePattern.Status</code>: Matches a specific HTTP status code in the error message (such as <code>429</code>, <code>500</code>,<code>502</code>, etc.).</li> <li><code>RetryablePattern.Keyword</code>: Matches a keyword in the error message (such as <code>rate limit</code> or <code>request timeout</code>).</li> <li><code>RetryablePattern.Regex</code>: Matches a regular expression in the error message.</li> <li><code>RetryablePattern.Custom</code>: Matches a custom logic using a lambda function.</li> </ul> <p>If any pattern returns <code>true</code>, the error is considered retryable, and the LLM client retries the request.</p>"},{"location":"prompts/handling-failures/#default-patterns","title":"Default patterns","text":"<p>Unless you customize the retry configuration, the following patterns are used by default:</p> <ul> <li> <p>HTTP status codes:</p> <ul> <li><code>429</code>: Rate limit</li> <li><code>500</code>: Internal server error</li> <li><code>502</code>: Bad gateway</li> <li><code>503</code>: Service unavailable</li> <li><code>504</code>: Gateway timeout</li> <li><code>529</code>: Anthropic overloaded</li> </ul> </li> <li> <p>Error keywords:</p> <ul> <li>rate limit</li> <li>too many requests</li> <li>request timeout</li> <li>connection timeout</li> <li>read timeout</li> <li>write timeout</li> <li>connection reset by peer</li> <li>connection refused</li> <li>temporarily unavailable</li> <li>service unavailable</li> </ul> </li> </ul> <p>These default patterns are defined in Koog as <code>RetryConfig.DEFAULT_PATTERNS</code>.</p>"},{"location":"prompts/handling-failures/#custom-patterns","title":"Custom patterns","text":"<p>You can define custom patterns for your specific needs:</p> <pre><code>val config = RetryConfig(\n    retryablePatterns = listOf(\n        RetryablePattern.Status(429),   // Specific status code\n        RetryablePattern.Keyword(\"quota\"),  // Keyword in error message\n        RetryablePattern.Regex(Regex(\"ERR_\\\\d+\")),  // Custom regex pattern\n        RetryablePattern.Custom { error -&gt;  // Custom logic\n            error.contains(\"temporary\") &amp;&amp; error.length &gt; 20\n        }\n    )\n)\n</code></pre> <p>You can also append custom patterns to the default <code>RetryConfig.DEFAULT_PATTERNS</code>:</p> <pre><code>val config = RetryConfig(\n    retryablePatterns = RetryConfig.DEFAULT_PATTERNS + listOf(\n        RetryablePattern.Keyword(\"custom_error\")\n    )\n)\n</code></pre>"},{"location":"prompts/handling-failures/#streaming-with-retry","title":"Streaming with retry","text":"<p>Streaming operations can optionally be retried. This feature is disabled by default.</p> <pre><code>val config = RetryConfig(\n    maxAttempts = 3\n)\n\nval client = RetryingLLMClient(baseClient, config)\nval stream = client.executeStreaming(prompt, OpenAIModels.Chat.GPT4o)\n</code></pre> <p>Note</p> <p>Streaming retries only apply to connection failures that occur before the first token is received. Once streaming has started, the retry logic is disabled. If an error occurs during streaming, the operation is terminated.</p>"},{"location":"prompts/handling-failures/#retry-with-prompt-executors","title":"Retry with prompt executors","text":"<p>When working with prompt executors, you can wrap the underlying LLM client with a retry mechanism before creating the executor. To learn more about prompt executors, see Prompt executors.</p> <pre><code>// Single provider executor with retry\nval resilientClient = RetryingLLMClient(\n    OpenAILLMClient(System.getenv(\"OPENAI_API_KEY\")),\n    RetryConfig.PRODUCTION\n)\nval executor = SingleLLMPromptExecutor(resilientClient)\n\n// Multi-provider executor with flexible client configuration\nval multiExecutor = MultiLLMPromptExecutor(\n    LLMProvider.OpenAI to RetryingLLMClient(\n        OpenAILLMClient(System.getenv(\"OPENAI_API_KEY\")),\n        RetryConfig.CONSERVATIVE\n    ),\n    LLMProvider.Anthropic to RetryingLLMClient(\n        AnthropicLLMClient(System.getenv(\"ANTHROPIC_API_KEY\")),\n        RetryConfig.AGGRESSIVE  \n    ),\n    // The Bedrock client already has a built-in AWS SDK retry \n    LLMProvider.Bedrock to BedrockLLMClient(\n        identityProvider = StaticCredentialsProvider {\n            accessKeyId = System.getenv(\"AWS_ACCESS_KEY_ID\")\n            secretAccessKey = System.getenv(\"AWS_SECRET_ACCESS_KEY\")\n            sessionToken = System.getenv(\"AWS_SESSION_TOKEN\")\n        },\n    ),\n)\n</code></pre>"},{"location":"prompts/handling-failures/#timeout-configuration","title":"Timeout configuration","text":"<p>All LLM clients support timeout configuration to prevent hanging requests. You can specify timeout values for network connections when creating the client using the <code>ConnectionTimeoutConfig</code> class.</p> <p><code>ConnectionTimeoutConfig</code> has the following properties:</p> Property Default Value Description <code>connectTimeoutMillis</code> 60 seconds (60,000) Maximum time to establish a connection to the server. <code>requestTimeoutMillis</code> 15 minutes (900,000) Maximum time for the entire request to complete. <code>socketTimeoutMillis</code> 15 minutes (900,000) Maximum time to wait for data over an established connection. <p>You can customize these values for your specific needs. For example:</p> <pre><code>val client = OpenAILLMClient(\n    apiKey = apiKey,\n    settings = OpenAIClientSettings(\n        timeoutConfig = ConnectionTimeoutConfig(\n            connectTimeoutMillis = 5000,    // 5 seconds to establish connection\n            requestTimeoutMillis = 60000,    // 60 seconds for the entire request\n            socketTimeoutMillis = 120000   // 120 seconds for data on the socket\n        )\n    )\n)\n</code></pre> <p>Tip</p> <p>For long-running or streaming calls, set higher values for <code>requestTimeoutMillis</code> and <code>socketTimeoutMillis</code>.</p>"},{"location":"prompts/handling-failures/#error-handling","title":"Error handling","text":"<p>When working with LLMs in production, you need to implement error handling, including:</p> <ul> <li>Try-catch blocks to handle unexpected errors.</li> <li>Logging errors with context for debugging.</li> <li>Fallbacks for critical operations.</li> <li>Monitoring retry patterns to identify recurring issues.</li> </ul> <p>Here is an example of error handling:</p> <pre><code>fun main() {\n    runBlocking {\n        val logger = LoggerFactory.getLogger(\"Example\")\n        val resilientClient = RetryingLLMClient(\n            OpenAILLMClient(System.getenv(\"OPENAI_API_KEY\")),\n            RetryConfig.PRODUCTION\n        )\n        val prompt = prompt(\"test\") { user(\"Hello\") }\n        val model = OpenAIModels.Chat.GPT4o\n\n        fun processResponse(response: Any) { /* implmenentation */ }\n        fun scheduleRetryLater() { /* implmenentation */ }\n        fun notifyAdministrator() { /* implmenentation */ }\n        fun useDefaultResponse() { /* implmenentation */ }\n\n        try {\n            val response = resilientClient.execute(prompt, model)\n            processResponse(response)\n        } catch (e: Exception) {\n            logger.error(\"LLM operation failed\", e)\n\n            when {\n                e.message?.contains(\"rate limit\") == true -&gt; {\n                    // Handle rate limiting specifically\n                    scheduleRetryLater()\n                }\n                e.message?.contains(\"invalid api key\") == true -&gt; {\n                    // Handle authentication errors\n                    notifyAdministrator()\n                }\n                else -&gt; {\n                    // Fall back to an alternative solution\n                    useDefaultResponse()\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"prompts/llm-clients/","title":"LLM clients","text":"<p>LLM clients are designed for direct interaction with LLM providers. Each client implements the <code>LLMClient</code> interface, which provides methods for executing prompts and streaming responses.</p> <p>You can use an LLM client when you work with a single LLM provider and don't need advanced lifecycle management. If you need to manage multiple LLM providers, use a prompt executor.</p> <p>The table below shows the available LLM clients and their capabilities.</p> LLM provider LLMClient Toolcalling Streaming Multiplechoices Embeddings Moderation Modellisting Notes OpenAI OpenAILLMClient \u2713 \u2713 \u2713 \u2713 \u2713<sup>1</sup> \u2713 Anthropic AnthropicLLMClient \u2713 \u2713 - - - - - Google GoogleLLMClient \u2713 \u2713 \u2713 \u2713 - \u2713 - DeepSeek DeepSeekLLMClient \u2713 \u2713 \u2713 - - \u2713 OpenAI-compatible chat client. OpenRouter OpenRouterLLMClient \u2713 \u2713 \u2713 - - \u2713 OpenAI-compatible router client. Amazon Bedrock BedrockLLMClient \u2713 \u2713 - \u2713 \u2713<sup>2</sup> - JVM-only AWS SDK client that supports multiple model families. Mistral MistralAILLMClient \u2713 \u2713 \u2713 \u2713 \u2713<sup>3</sup> \u2713 OpenAI-compatible client. Alibaba DashScopeLLMClient \u2713 \u2713 \u2713 - - \u2713 OpenAI-compatible client that exposes provider-specific parameters (<code>enableSearch</code>, <code>parallelToolCalls</code>, <code>enableThinking</code>). Ollama OllamaClient \u2713 \u2713 - \u2713 \u2713 - Local server client with model management APIs."},{"location":"prompts/llm-clients/#running-a-prompt","title":"Running a prompt","text":"<p>To run a prompt using an LLM client, perform the following:</p> <ol> <li>Create an LLM client that handles the connection between your application and LLM providers.</li> <li>Call the <code>execute()</code> method with the prompt and LLM as arguments.</li> </ol> <p>Here is an example that uses <code>OpenAILLMClient</code> to run prompts:</p> <pre><code>fun main() = runBlocking {\n    // Create an OpenAI client\n    val token = System.getenv(\"OPENAI_API_KEY\")\n    val client = OpenAILLMClient(token)\n\n    // Create a prompt\n    val prompt = prompt(\"prompt_name\", LLMParams()) {\n        // Add a system message to set the context\n        system(\"You are a helpful assistant.\")\n\n        // Add a user message\n        user(\"Tell me about Kotlin\")\n\n        // You can also add assistant messages for few-shot examples\n        assistant(\"Kotlin is a modern programming language...\")\n\n        // Add another user message\n        user(\"What are its key features?\")\n    }\n\n    // Run the prompt\n    val response = client.execute(prompt, OpenAIModels.Chat.GPT4o)\n    // Print the response\n    println(response)\n}\n</code></pre>"},{"location":"prompts/llm-clients/#streaming-responses","title":"Streaming responses","text":"<p>Note</p> <p>Available for all LLM clients.</p> <p>When you need to process responses as they are generated, you can use the <code>executeStreaming()</code> method to stream the model output:</p> <pre><code>// Set up the OpenAI client with your API key\nval token = System.getenv(\"OPENAI_API_KEY\")\nval client = OpenAILLMClient(token)\n\nval response = client.executeStreaming(\n    prompt = prompt(\"stream_demo\") { user(\"Stream this response in short chunks.\") },\n    model = OpenAIModels.Chat.GPT4_1\n)\n\nresponse.collect { event -&gt;\n    when (event) {\n        is StreamFrame.Append -&gt; println(event.text)\n        is StreamFrame.ToolCall -&gt; println(\"\\nTool call: ${event.name}\")\n        is StreamFrame.End -&gt; println(\"\\n[done] Reason: ${event.finishReason}\")\n    }\n}\n</code></pre>"},{"location":"prompts/llm-clients/#multiple-choices","title":"Multiple choices","text":"<p>Note</p> <p>Available for all LLM clients except <code>GoogleLLMClient</code>, <code>BedrockLLMClient</code>, and <code>OllamaClient</code></p> <p>You can request multiple alternative responses from the model in a single call by using the <code>executeMultipleChoices()</code> method. It requires additionally specifying the <code>numberOfChoices</code> LLM parameter in the prompt being executed.</p> <pre><code>fun main() = runBlocking {\n    val apiKey = System.getenv(\"OPENAI_API_KEY\")\n    val client = OpenAILLMClient(apiKey)\n\n    val choices = client.executeMultipleChoices(\n        prompt = prompt(\"n_best\", params = LLMParams(numberOfChoices = 3)) {\n            system(\"You are a creative assistant.\")\n            user(\"Give me three different opening lines for a story.\")\n        },\n        model = OpenAIModels.Chat.GPT4o\n    )\n\n    choices.forEachIndexed { i, choice -&gt;\n        val text = choice.joinToString(\" \") { it.content }\n        println(\"Line #${i + 1}: $text\")\n    }\n}\n</code></pre>"},{"location":"prompts/llm-clients/#listing-available-models","title":"Listing available models","text":"<p>Note</p> <p>Available for all LLM clients except <code>GoogleLLMClient</code>, <code>BedrockLLMClient</code>, and <code>OllamaClient</code>.</p> <p>To get a list of available model IDs supported by the LLM client, use the <code>models()</code> method:    </p> <pre><code>fun main() = runBlocking {\n    val apiKey = System.getenv(\"OPENAI_API_KEY\")\n    val client = OpenAILLMClient(apiKey)\n\n    val ids: List&lt;String&gt; = client.models()\n    ids.forEach { println(it) }\n}\n</code></pre>"},{"location":"prompts/llm-clients/#embeddings","title":"Embeddings","text":"<p>Note</p> <p>Available for <code>OpenAILLMClient</code>, <code>GoogleLLMClient</code>, <code>BedrockLLMClient</code>, <code>MistralAILLMClient</code>, and <code>OllamaClient</code>.</p> <p>You convert text into embedding vectors using the <code>embed()</code> method. Choose an embedding model and pass your text to this method:</p> <pre><code>fun main() = runBlocking {\n    val apiKey = System.getenv(\"OPENAI_API_KEY\")\n    val client = OpenAILLMClient(apiKey)\n\n    val embedding = client.embed(\n        text = \"This is a sample text for embedding\",\n        model = OpenAIModels.Embeddings.TextEmbedding3Large\n    )\n\n    println(\"Embedding size: ${embedding.size}\")\n}\n</code></pre>"},{"location":"prompts/llm-clients/#moderation","title":"Moderation","text":"<p>Note</p> <p>Available for the following LLM clients: <code>OpenAILLMClient</code>, <code>BedrockLLMClient</code>, <code>MistralAILLMClient</code>, <code>OllamaClient</code>.</p> <p>You can use the <code>moderate()</code> method with a moderation model to check whether a prompt contains inappropriate content:</p> <pre><code>fun main() = runBlocking {\n    val apiKey = System.getenv(\"OPENAI_API_KEY\")\n    val client = OpenAILLMClient(apiKey)\n\n    val result = client.moderate(\n        prompt = prompt(\"moderation\") {\n            user(\"This is a test message that may contain offensive content.\")\n        },\n        model = OpenAIModels.Moderation.Omni\n    )\n\n    println(result)\n}\n</code></pre>"},{"location":"prompts/llm-clients/#integration-with-prompt-executors","title":"Integration with prompt executors","text":"<p>Prompt executors wrap LLM clients and provide additional functionality, such as routing, fallbacks, and unified usage across providers. They are recommended for production use, as they offer flexibility when working with multiple providers.</p> <ol> <li> <p>Supports moderation via the OpenAI Moderation API.\u00a0\u21a9</p> </li> <li> <p>Moderation requires Guardrails configuration.\u00a0\u21a9</p> </li> <li> <p>Supports moderation via the Mistral <code>v1/moderations</code> endpoint.\u00a0\u21a9</p> </li> </ol>"},{"location":"prompts/llm-response-caching/","title":"LLM response caching","text":"<p>For repeated requests that you run with a prompt executor, you can cache LLM responses to optimize performance and reduce costs. In Koog, caching is available for all prompt executors through <code>CachedPromptExecutor</code>,  which is a wrapper around <code>PromptExecutor</code> that adds caching functionality. It lets you store responses from previously executed prompts and retrieve them when the same prompts are run again.</p> <p>To create a cached prompt executor, perform the following:</p> <ol> <li>Create a prompt executor for which you want to cache responses.</li> <li>Create a <code>CachedPromptExecutor</code> instance by providing the desired cache and the prompt executor you created.</li> <li>Run the created <code>CachedPromptExecutor</code> with the desired prompt and model.</li> </ol> <p>Here is an example:</p> <pre><code>// Create a prompt executor\nval client = OpenAILLMClient(System.getenv(\"OPENAI_API_KEY\"))\nval promptExecutor = SingleLLMPromptExecutor(client)\n\n// Create a cached prompt executor\nval cachedExecutor = CachedPromptExecutor(\n    cache = FilePromptCache(Path(\"path/to/your/cache/directory\")),\n    nested = promptExecutor\n)\n\n// Run cached prompt executor for the first time\n// This will perform an actual LLM request\nval firstTime = measureTimeMillis {\n    val firstResponse = cachedExecutor.execute(prompt, OpenAIModels.Chat.GPT4o)\n    println(\"First response: ${firstResponse.first().content}\")\n}\nprintln(\"First execution took: ${firstTime}ms\")\n\n// Run cached prompt executor for the second time\n// This will return the result immediately from the cache\nval secondTime = measureTimeMillis {\n    val secondResponse = cachedExecutor.execute(prompt, OpenAIModels.Chat.GPT4o)\n    println(\"Second response: ${secondResponse.first().content}\")\n}\nprintln(\"Second execution took: ${secondTime}ms\")\n</code></pre> <p>The example produces the following output:</p> <p><pre><code>First response: Hello! It seems like we're starting a new conversation. What can I help you with today?\nFirst execution took: 48ms\nSecond response: Hello! It seems like we're starting a new conversation. What can I help you with today?\nSecond execution took: 1ms\n</code></pre> The second response is retrieved from the cache, which took only 1ms.</p> <p>Note</p> <ul> <li>If you call <code>executeStreaming()</code> with the cached prompt executor, it produces a response as a single chunk.</li> <li>If you call <code>moderate()</code> with the cached prompt executor, it forwards the request to the nested prompt executor and does not use the cache.</li> <li>Caching of multiple choice responses (<code>executeMultipleChoice()</code>) is not supported.</li> </ul>"},{"location":"prompts/prompt-executors/","title":"Prompt executors","text":"<p>Prompt executors provide a higher-level abstraction that lets you manage the lifecycle of one or multiple LLM clients. You can work with multiple LLM providers through a unified interface, abstracting from provider-specific details, with dynamic switching between them and fallbacks.</p>"},{"location":"prompts/prompt-executors/#executor-types","title":"Executor types","text":"<p>Koog provides two main types of prompt executors that implement the <code>PromptExecutor</code> interface:</p> Type Class Description Single-provider <code>SingleLLMPromptExecutor</code> Wraps a single LLM client for one provider. Use this executor if your agent only requires switching between models within a single LLM provider. Multi-provider <code>MultiLLMPromptExecutor</code> Wraps multiple LLM clients and routes calls based on the LLM provider. It can optionally use a configured fallback provider and LLM when the requested client is unavailable. Use this executor if your agent needs to switch between LLMs from different providers."},{"location":"prompts/prompt-executors/#creating-a-single-provider-executor","title":"Creating a single-provider executor","text":"<p>To create a prompt executor for a specific LLM provider, perform the following:</p> <ol> <li>Configure an LLM client for a specific provider with the corresponding API key.</li> <li>Create a prompt executor using <code>SingleLLMPromptExecutor</code>.</li> </ol> <p>Here is an example:</p> <pre><code>val openAIClient = OpenAILLMClient(System.getenv(\"OPENAI_KEY\"))\nval promptExecutor = SingleLLMPromptExecutor(openAIClient)\n</code></pre>"},{"location":"prompts/prompt-executors/#creating-a-multi-provider-executor","title":"Creating a multi-provider executor","text":"<p>To create a prompt executor that works with multiple LLM providers, do the following:</p> <ol> <li>Configure clients for the required LLM providers with the corresponding API keys.</li> <li>Pass the configured clients to the <code>MultiLLMPromptExecutor</code> class constructor to create a prompt executor    with multiple LLM providers.</li> </ol> <pre><code>val openAIClient = OpenAILLMClient(System.getenv(\"OPENAI_KEY\"))\nval ollamaClient = OllamaClient()\n\nval multiExecutor = MultiLLMPromptExecutor(\n    LLMProvider.OpenAI to openAIClient,\n    LLMProvider.Ollama to ollamaClient\n)\n</code></pre>"},{"location":"prompts/prompt-executors/#pre-defined-prompt-executors","title":"Pre-defined prompt executors","text":"<p>For faster setup, Koog provides the ready-to-use executor implementations for common providers.</p> <p>The following table includes the pre-defined single-provider executors that return <code>SingleLLMPromptExecutor</code> configured with a specific LLM client.</p> LLM provider Prompt executor Description OpenAI simpleOpenAIExecutor Wraps <code>OpenAILLMClient</code> that runs prompts with OpenAI models. OpenAI simpleAzureOpenAIExecutor Wraps <code>OpenAILLMClient</code> configured for using Azure OpenAI Service. Anthropic simpleAnthropicExecutor Wraps <code>AnthropicLLMClient</code> that runs prompts with Anthropic models. Google simpleGoogleAIExecutor Wraps <code>GoogleLLMClient</code> that runs prompts with Google models. OpenRouter simpleOpenRouterExecutor Wraps <code>OpenRouterLLMClient</code> that runs prompts with OpenRouter. Amazon Bedrock simpleBedrockExecutor Wraps <code>BedrockLLMClient</code> that runs prompts with AWS Bedrock. Amazon Bedrock simpleBedrockExecutorWithBearerToken Wraps <code>BedrockLLMClient</code> and uses the provided Bedrock API key to send requests. Mistral simpleMistralAIExecutor Wraps <code>MistralAILLMClient</code> that runs prompts with Mistral models. Ollama simpleOllamaAIExecutor Wraps <code>OllamaClient</code> that runs prompts with Ollama. <p>Koog also provides the pre-defined multi-provider executor <code>DefaultMultiLLMPromptExecutor</code>. This is an implementation of <code>MultiLLMPromptExecutor</code> that wraps <code>OpenAILLMClient</code>, <code>AnthropicLLMClient</code>, and <code>GoogleLLMClient</code> models.</p> <p>Here is an example of creating pre-defined single and multi-provider executors:</p> <pre><code>// Create an OpenAI executor\nval promptExecutor = simpleOpenAIExecutor(\"OPENAI_KEY\")\n\n// Create a DefaultMultiLLMPromptExecutor with OpenAI, Anthropic, and Google LLM clients\nval openAIClient = OpenAILLMClient(\"OPENAI_KEY\")\nval anthropicClient = AnthropicLLMClient(\"ANTHROPIC_KEY\")\nval googleClient = GoogleLLMClient(\"GOOGLE_KEY\")\nval multiExecutor = DefaultMultiLLMPromptExecutor(openAIClient, anthropicClient, googleClient)\n</code></pre>"},{"location":"prompts/prompt-executors/#running-a-prompt","title":"Running a prompt","text":"<p>To run a prompt using a prompt executor, do the following:</p> <ol> <li>Create a prompt executor.</li> <li>Run the prompt with the specific LLM using the <code>execute()</code> method.</li> </ol> <p>Here is an example:</p> <pre><code>// Create an OpenAI executor\nval promptExecutor = simpleOpenAIExecutor(\"OPENAI_KEY\")\n\n// Execute a prompt\nval response = promptExecutor.execute(\n    prompt = prompt(\"demo\") { user(\"Summarize this.\") },\n    model = OpenAIModels.Chat.GPT4o\n)\n</code></pre> <p>This will run the prompt with the <code>GPT4o</code> model and return the response.</p> <p>Note</p> <p>The prompt executors provide methods to run prompts using various capabilities,  such as streaming, multiple choice generation, and content moderation. Since prompt executors wrap LLM clients, each executor supports the capabilities of the corresponding client. For details, refer to LLM clients.</p>"},{"location":"prompts/prompt-executors/#switching-between-providers","title":"Switching between providers","text":"<p>When you work with multiple LLM providers using <code>MultiLLMPromptExecutor</code>, you can switch between them. The process is as follows:</p> <ol> <li>Create an LLM client instance for each provider you want to use.</li> <li>Create a <code>MultiLLMPromptExecutor</code> that maps LLM providers to LLM clients.</li> <li>Run a prompt with a model from the corresponding client passed as an argument to the <code>execute()</code> method.    The prompt executor will use the corresponding client based on the model provider to run the prompt.</li> </ol> <p>Here is an example of switching between providers:</p> <pre><code>// Create LLM clients for OpenAI, Anthropic, and Google providers\nval openAIClient = OpenAILLMClient(\"OPENAI_API_KEY\")\nval anthropicClient = AnthropicLLMClient(\"ANTHROPIC_API_KEY\")\nval googleClient = GoogleLLMClient(\"GOOGLE_API_KEY\")\n\n// Create a MultiLLMPromptExecutor that maps LLM providers to LLM clients\nval executor = MultiLLMPromptExecutor(\n    LLMProvider.OpenAI to openAIClient,\n    LLMProvider.Anthropic to anthropicClient,\n    LLMProvider.Google to googleClient\n)\n\n// Create a prompt\nval p = prompt(\"demo\") { user(\"Summarize this.\") }\n\n// Run the prompt with an OpenAI model; the prompt executor automatically switches to the OpenAI client\nval openAIResult = executor.execute(p, OpenAIModels.Chat.GPT4o)\n\n// Run the prompt with an Anthropic model; the prompt executor automatically switches to the Anthropic client\nval anthropicResult = executor.execute(p, AnthropicModels.Sonnet_3_5)\n</code></pre> <p>You can optionally configure a fallback LLM provider and model to use when the requested client is unavailable. For details, refer to Configuring fallbacks.</p>"},{"location":"prompts/prompt-executors/#configuring-fallbacks","title":"Configuring fallbacks","text":"<p>Multi-provider prompt executors can be configured to use a fallback LLM provider and model when the requested LLM client is unavailable. To configure the fallback mechanism, provide the <code>fallback</code> parameter to the <code>MultiLLMPromptExecutor</code> constructor:</p> <pre><code>val openAIClient = OpenAILLMClient(System.getenv(\"OPENAI_KEY\"))\nval ollamaClient = OllamaClient()\n\nval multiExecutor = MultiLLMPromptExecutor(\n    LLMProvider.OpenAI to openAIClient,\n    LLMProvider.Ollama to ollamaClient,\n    fallback = MultiLLMPromptExecutor.FallbackPromptExecutorSettings(\n        fallbackProvider = LLMProvider.Ollama,\n        fallbackModel = OllamaModels.Meta.LLAMA_3_2\n    )\n)\n</code></pre> <p>If you pass a model from an LLM provider that is not included in the <code>MultiLLMPromptExecutor</code>, the prompt executor will use the fallback model:</p> <pre><code>// Create a prompt\nval p = prompt(\"demo\") { user(\"Summarize this\") }\n// If you pass a Google model, the prompt executor will use the fallback model, as the Google client is not included\nval response = multiExecutor.execute(p, GoogleModels.Gemini2_5Pro)\n</code></pre> <p>Note</p> <p>Fallbacks are available for the <code>execute()</code> and <code>executeMultipleChoices()</code> methods only.</p>"},{"location":"prompts/prompt-creation/","title":"Creating prompts","text":"<p>Koog uses the type-safe Kotlin DSL to create prompts with control over message types, their order, and content.</p> <p>These prompts let you pre-configure conversation history with multiple messages, provide multimodal content, examples, tool calls, and their results.</p>"},{"location":"prompts/prompt-creation/#basic-structure","title":"Basic structure","text":"<p>The <code>prompt()</code> function creates a Prompt object with a unique ID and a list of messages:</p> <pre><code>val prompt = prompt(\"unique_prompt_id\") {\n    // List of messages\n}\n</code></pre>"},{"location":"prompts/prompt-creation/#message-types","title":"Message types","text":"<p>The Kotlin DSL supports the following types of messages, each of which corresponds to a specific role in a conversation:</p> <ul> <li>System message: Provides the context, instructions, and constraints to the LLM, defining its behavior.</li> <li>User message: Represents the user input.</li> <li>Assistant message: Represents LLM responses that are used for few-shot learning or to continue the conversation.</li> <li>Tool message: Represents tool calls and their results.</li> </ul> <pre><code>val prompt = prompt(\"unique_prompt_id\") {\n    // Add a system message to set the context\n    system(\"You are a helpful assistant with access to tools.\")\n    // Add a user message\n    user(\"What is 5 + 3 ?\")\n    // Add an assistant message\n    assistant(\"The result is 8.\")\n}\n</code></pre>"},{"location":"prompts/prompt-creation/#system-message","title":"System message","text":"<p>A system message defines the LLM behavior and sets the context for the entire conversation. It can specify the model's role, tone, provide guidelines and constraints on responses, and provide response examples.</p> <p>To create the system message, provide a string to the <code>system()</code> function as an argument:</p> <pre><code>val prompt = prompt(\"assistant\") {\n    system(\"You are a helpful assistant that explains technical concepts.\")\n}\n</code></pre>"},{"location":"prompts/prompt-creation/#user-messages","title":"User messages","text":"<p>A user message represents input from the user. To create the user message, provide a string to the <code>user()</code> function as an argument:</p> <pre><code>val prompt = prompt(\"question\") {\n    system(\"You are a helpful assistant.\")\n    user(\"What is Koog?\")\n}\n</code></pre> <p>Most user messages contain plain text, but they can also include multimodal content, such as images, audio, video, and documents. For details and examples, see Multimodal content.</p>"},{"location":"prompts/prompt-creation/#assistant-messages","title":"Assistant messages","text":"<p>An assistant message represents an LLM response, which can be used for few-shot learning in future similar interactions, to continue a conversation, or to demonstrate the expected output structure.</p> <p>To create the assistant message, provide a string to the <code>assistant()</code> function as an argument:</p> <pre><code>val prompt = prompt(\"article_review\") {\n    system(\"Evaluate the article.\")\n\n    // Example 1\n    user(\"The article is clear and easy to understand.\")\n    assistant(\"positive\")\n\n    // Example 2\n    user(\"The article is hard to read but it's clear and useful.\")\n    assistant(\"neutral\")\n\n    // Example 3\n    user(\"The article is confusing and misleading.\")\n    assistant(\"negative\")\n\n    // New input to classify\n    user(\"The article is interesting and helpful.\")\n}\n</code></pre>"},{"location":"prompts/prompt-creation/#tool-messages","title":"Tool messages","text":"<p>A tool message represents a tool call and its result, which can be used to pre-fill the history of tool calls.</p> <p>Tip</p> <p>An LLM generates tool calls during execution. Pre-filling them is helpful for few-shot learning or demonstrating how the tools are expected to be used.</p> <p>To create the tool message, call the <code>tool()</code> function:</p> <pre><code>val prompt = prompt(\"calculator_example\") {\n    system(\"You are a helpful assistant with access to tools.\")\n    user(\"What is 5 + 3?\")\n    tool {\n        // Tool call\n        call(\n            id = \"calculator_tool_id\",\n            tool = \"calculator\",\n            content = \"\"\"{\"operation\": \"add\", \"a\": 5, \"b\": 3}\"\"\"\n        )\n\n        // Tool result\n        result(\n            id = \"calculator_tool_id\",\n            tool = \"calculator\",\n            content = \"8\"\n        )\n    }\n\n    // LLM response based on tool result\n    assistant(\"The result of 5 + 3 is 8.\")\n    user(\"What is 4 + 5?\")\n}\n</code></pre>"},{"location":"prompts/prompt-creation/#text-message-builders","title":"Text message builders","text":"<p>When building a <code>system()</code>, <code>user()</code>, or <code>assistant()</code> message, you can use  helper text-building functions for rich text formatting.</p> <pre><code>val prompt = prompt(\"text_example\") {\n    user {\n        +\"Review the following code snippet:\"\n        +\"fun greet(name: String) = println(\\\"Hello, \\$name!\\\")\"\n\n        // Paragraph break\n        br()\n        text(\"Please include in your explanation:\")\n\n        // Indent content\n        padding(\"  \") {\n            +\"1. What the function does.\"\n            +\"2. How string interpolation works.\"\n        }\n    }\n}\n</code></pre> <p>You can also use the Markdown  and XML builders to add the content in  the corresponding format.</p> <pre><code>val prompt = prompt(\"markdown_xml_example\") {\n    // A user message in Markdown format\n    user {\n        markdown {\n            h2(\"Evaluate the article using the following criteria:\")\n            bulleted {\n                item { +\"Clarity and readability\" }\n                item { +\"Accuracy of information\" }\n                item { +\"Usefulness to the reader\" }\n            }\n        }\n    }\n    // An assistant message in XML format\n    assistant {\n        xml {\n            xmlDeclaration()\n            tag(\"review\") {\n                tag(\"clarity\") { text(\"positive\") }\n                tag(\"accuracy\") { text(\"neutral\") }\n                tag(\"usefulness\") { text(\"positive\") }\n            }\n        }\n    }\n}\n</code></pre> <p>Tip</p> <p>You can mix the text building functions with the XML and Markdown builders.</p>"},{"location":"prompts/prompt-creation/#prompt-parameters","title":"Prompt parameters","text":"<p>Prompts can be customized by configuring parameters that control the LLM's behavior.</p> <pre><code>val prompt = prompt(\n    id = \"custom_params\",\n    params = LLMParams(\n        temperature = 0.7,\n        numberOfChoices = 1,\n        toolChoice = LLMParams.ToolChoice.Auto\n    )\n) {\n    system(\"You are a creative writing assistant.\")\n    user(\"Write a song about winter.\")\n}\n</code></pre> <p>The following parameters are supported:</p> <ul> <li><code>temperature</code>: Controls randomness in the model's responses.</li> <li><code>toolChoice</code>: Controls tool calling behavior of the model.</li> <li><code>numberOfChoices</code>: Requests multiple alternative responses.</li> <li><code>schema</code>: Defines the structure for the model's response format.</li> <li><code>maxTokens</code>: Limits the number of tokens in the response.</li> <li><code>speculation</code>: Provides a hint about the expected response format (only supported by specific models).</li> </ul> <p>For more information, see LLM parameters.</p>"},{"location":"prompts/prompt-creation/#extending-existing-prompts","title":"Extending existing prompts","text":"<p>You can extend an existing prompt by calling the <code>prompt()</code> function with the existing prompt as an argument:</p> <pre><code>val basePrompt = prompt(\"base\") {\n    system(\"You are a helpful assistant.\")\n    user(\"Hello!\")\n    assistant(\"Hi! How can I help you?\")\n}\n\nval extendedPrompt = prompt(basePrompt) {\n    user(\"What's the weather like?\")\n}\n</code></pre> <p>This creates a new prompt that includes all messages from <code>basePrompt</code> and the new user message.</p>"},{"location":"prompts/prompt-creation/#next-steps","title":"Next steps","text":"<ul> <li>Learn how to work with multimodal content.</li> <li>Run prompts with LLM clients if you work with a single LLM provider.</li> <li>Run prompts with prompt executors if you work with multiple LLM providers.</li> </ul>"},{"location":"prompts/prompt-creation/multimodal-content/","title":"Multimodal content","text":"<p>Multimodal content refers to content of different types, such as text, images, audio, video, and files. Koog lets you send images, audio, video, and files to LLMs within the <code>user</code> message along with text.  You can add them to the <code>user</code> message by using the corresponding functions:</p> <ul> <li><code>image()</code>: Attaches images (JPG, PNG, WebP, GIF).</li> <li><code>audio()</code>: Attaches audio files (MP3, WAV, FLAC).</li> <li><code>video()</code>: Attaches video files (MP4, AVI, MOV).</li> <li><code>file()</code> / <code>binaryFile()</code> / <code>textFile()</code>: Attaches documents (PDF, TXT, MD, etc.).</li> </ul> <p>Each function supports two ways of configuring attachment parameters, so you can:</p> <ul> <li>Pass a URL or a file path to the function, and it automatically handles attachment parameters. For <code>file()</code>, <code>binaryFile()</code>, and <code>textFile()</code>, you must also provide the MIME type.</li> <li>Create and pass a <code>ContentPart</code> object to the function for custom control over attachment parameters.</li> </ul> <p>Note</p> <p>Multimodal content support varies by LLM provider. Check the provider documentation for supported content types.</p>"},{"location":"prompts/prompt-creation/multimodal-content/#auto-configured-attachments","title":"Auto-configured attachments","text":"<p>If you pass a URL or a file path to the attachment functions, Koog automatically constructs the corresponding attachment parameters based on the file extension.</p> <p>The general format of the <code>user</code> message that includes a text message and a list of auto-configured attachments is as follows:</p> <pre><code>user {\n    +\"Describe these images:\"\n\n    image(\"https://example.com/test.png\")\n    image(Path(\"/path/to/image.png\"))\n\n    +\"Focus on the main subjects.\"\n}\n</code></pre> <p>The <code>+</code> operator adds text content to the user message along with the attachments.</p>"},{"location":"prompts/prompt-creation/multimodal-content/#custom-configured-attachments","title":"Custom-configured attachments","text":"<p>The <code>ContentPart</code> interface lets you configure parameters for each attachment individually.</p> <p>All attachments implement the <code>ContentPart.Attachment</code> interface. You can create an instance of a specific implementation for each attachment, configure its parameters, and pass it to the corresponding <code>image()</code>, <code>audio()</code>, <code>video()</code>, or <code>file()</code> functions.</p> <p>The general format of the <code>user</code> message that includes a text message and a list of custom-configured attachments is as follows:</p> <pre><code>user {\n    +\"Describe this image\"\n    image(\n        ContentPart.Image(\n            content = AttachmentContent.URL(\"https://example.com/capture.png\"),\n            format = \"png\",\n            mimeType = \"image/png\",\n            fileName = \"capture.png\"\n        )\n    )\n}\n</code></pre> <p>Koog provides the following specialized classes for each media type that implement the <code>ContentPart.Attachment</code> interface:</p> <ul> <li><code>ContentPart.Image</code>: image attachments, such as JPG or PNG files.</li> <li><code>ContentPart.Audio</code>: audio attachments, such as MP3 or WAV files.</li> <li><code>ContentPart.Video</code>: video attachments, such as MP4 or AVI files.</li> <li><code>ContentPart.File</code>: file attachments, such as PDF or TXT files.</li> </ul> <p>All <code>ContentPart.Attachment</code> types accept the following parameters:</p> Name Data type Required Description <code>content</code> AttachmentContent Yes The source of the provided file content. <code>format</code> String Yes The format of the provided file. For example, <code>png</code>. <code>mimeType</code> String Only for <code>ContentPart.File</code> The MIME Type of the provided file.For <code>ContentPart.Image</code>, <code>ContentPart.Audio</code>, and <code>ContentPart.Video</code>, it defaults to <code>&lt;type&gt;/&lt;format&gt;</code> (for example, <code>image/png</code>).For <code>ContentPart.File</code>, it must be explicitly provided. <code>fileName</code> String? No The name of the provided file including the extension. For example, <code>screenshot.png</code>."},{"location":"prompts/prompt-creation/multimodal-content/#attachment-content","title":"Attachment content","text":"<p>Implementations of the AttachmentContent interface define the type and source of content that is provided as input to the LLM:</p> <ul> <li><code>AttachmentContent.URL</code> defines the URL of the provided content:     <pre><code>AttachmentContent.URL(\"https://example.com/image.png\")\n</code></pre></li> <li> <p><code>AttachmentContent.Binary.Bytes</code> defines the file content as a byte array:     <pre><code>AttachmentContent.Binary.Bytes(byteArrayOf(/* ... */))\n</code></pre></p> </li> <li> <p><code>AttachmentContent.Binary.Base64</code> defines the file content as a Base64-encoded string containing file data:     <pre><code>AttachmentContent.Binary.Base64(\"iVBORw0KGgoAAAANS...\")\n</code></pre></p> </li> <li> <p><code>AttachmentContent.PlainText</code> defines the file content as plain text (for <code>ContentPart.File</code> only):     <pre><code>AttachmentContent.PlainText(\"This is the file content.\")\n</code></pre></p> </li> </ul>"},{"location":"prompts/prompt-creation/multimodal-content/#mixed-attachments","title":"Mixed attachments","text":"<p>In addition to providing different types of attachments in separate prompts or messages, you can also provide multiple and mixed types of attachments in a single <code>user()</code> message:</p> <pre><code>val prompt = prompt(\"mixed_content\") {\n    system(\"You are a helpful assistant.\")\n\n    user {\n        +\"Compare the image with the document content.\"\n        image(Path(\"/path/to/image.png\"))\n        binaryFile(Path(\"/path/to/page.pdf\"), \"application/pdf\")\n        +\"Structure the result as a table\"\n    }\n}\n</code></pre>"},{"location":"prompts/prompt-creation/multimodal-content/#next-steps","title":"Next steps","text":"<ul> <li>Run prompts with LLM clients if you work with a single LLM provider.</li> <li>Run prompts with prompt executors if you work with multiple LLM providers.</li> </ul>"},{"location":"tools/tool-descriptor-schemer/","title":"ToolDescriptorSchemer","text":""},{"location":"tools/tool-descriptor-schemer/#what-is-it","title":"What is it","text":"<p><code>ToolDescriptorSchemer</code> is a extension point that converts a <code>ToolDescriptor</code> into a JSON Schema object compatible with specific LLM providers.</p> <p>Key points:</p> <ul> <li>Location: <code>ai.koog.agents.core.tools.serialization.ToolDescriptorSchemer</code></li> <li>Contract: a single function <code>scheme(toolDescriptor: ToolDescriptor): JsonObject</code></li> <li>Implementations provided:</li> <li><code>OpenAICompatibleToolDescriptorSchemer</code> \u2014 generates schemas compatible with OpenAI\u2011style function/tool definitions.</li> <li><code>OllamaToolDescriptorSchemer</code> \u2014 generates schemas compatible with Ollama tool JSON.</li> </ul> <pre><code>// Interface\ninterface ToolDescriptorSchemaGenerator {\n  fun generate(toolDescriptor: ToolDescriptor): JsonObject\n}\n</code></pre>"},{"location":"tools/tool-descriptor-schemer/#why-to-use-it","title":"Why to use it?","text":"<p>If you want to provide custom scheme for existing or new LLM providers, implement this interface to convert Koog\u2019s <code>ToolDescriptor</code> into the expected JSON Schema format.</p>"},{"location":"tools/tool-descriptor-schemer/#implementation-example","title":"Implementation example","text":"<p>Below is a minimal custom implementation that renders only a subset of parameter types to illustrate how to plug into the SPI. Real implementations should cover all <code>ToolParameterType</code>s (String, Integer, Float, Boolean, Null, Enum, List, Object, AnyOf).</p> <pre><code>class MinimalSchemer : ToolDescriptorSchemaGenerator {\n    override fun generate(toolDescriptor: ToolDescriptor): JsonObject = buildJsonObject {\n        put(\"type\", \"object\")\n        putJsonObject(\"properties\") {\n            (toolDescriptor.requiredParameters + toolDescriptor.optionalParameters).forEach { p -&gt;\n                put(p.name, buildJsonObject {\n                    put(\"description\", p.description)\n                    when (val t = p.type) {\n                        ToolParameterType.String -&gt; put(\"type\", \"string\")\n                        ToolParameterType.Integer -&gt; put(\"type\", \"integer\")\n                        is ToolParameterType.Enum -&gt; {\n                            put(\"type\", \"string\")\n                            putJsonArray(\"enum\") { t.entries.forEach { add(JsonPrimitive(it)) } }\n                        }\n                        else -&gt; put(\"type\", \"string\") // fallback for brevity\n                    }\n                })\n            }\n        }\n        putJsonArray(\"required\") { toolDescriptor.requiredParameters.forEach { add(JsonPrimitive(it.name)) } }\n    }\n}\n</code></pre>"},{"location":"tools/tool-descriptor-schemer/#example-of-usage-with-client","title":"Example of usage with client","text":"<p>Typically you do not need to call a schemer directly. Koog clients accept a list of <code>ToolDescriptor</code> objects and apply the correct schemer internally when serializing requests for the provider.</p> <p>The example below defines a simple tool and passes it to the OpenAI client. The client will use <code>OpenAICompatibleToolDescriptorSchemer</code> under the hood to build the JSON schema.</p> <pre><code>val client = OpenAILLMClient(apiKey = System.getenv(\"OPENAI_API_KEY\"), toolsConverter = MinimalSchemer())\n\nval getUserTool = ToolDescriptor(\n    name = \"get_user\",\n    description = \"Returns user profile by id\",\n    requiredParameters = listOf(\n        ToolParameterDescriptor(\n            name = \"id\",\n            description = \"User id\",\n            type = ToolParameterType.String\n        )\n    )\n)\n\nval prompt = Prompt.build(id = \"p1\") { user(\"Hello\") }\nval responses = runBlocking {\n    client.execute(\n        prompt = prompt,\n        model = OpenAIModels.Chat.GPT4o,\n        tools = listOf(getUserTool)\n    )\n}\n</code></pre> <p>If you need direct access to the produced schema (for debugging or for a custom transport), you can instantiate the provider\u2011specific schemer and serialize the JSON yourself:</p> <pre><code>val json = Json { prettyPrint = true }\nval schema = OpenAICompatibleToolDescriptorSchemaGenerator().generate(getUserTool())\n</code></pre>"}]}