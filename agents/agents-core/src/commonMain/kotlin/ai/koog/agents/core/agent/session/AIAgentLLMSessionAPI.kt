@file:Suppress("EXPECT_ACTUAL_CLASSIFIERS_ARE_IN_BETA_WARNING")

package ai.koog.agents.core.agent.session

import ai.koog.agents.core.agent.config.AIAgentConfig
import ai.koog.agents.core.annotation.InternalAgentsApi
import ai.koog.agents.core.tools.Tool
import ai.koog.agents.core.tools.ToolDescriptor
import ai.koog.prompt.dsl.ModerationResult
import ai.koog.prompt.dsl.Prompt
import ai.koog.prompt.llm.LLModel
import ai.koog.prompt.message.LLMChoice
import ai.koog.prompt.message.Message
import ai.koog.prompt.processor.ResponseProcessor
import ai.koog.prompt.streaming.StreamFrame
import ai.koog.prompt.structure.StructureFixingParser
import ai.koog.prompt.structure.StructuredRequestConfig
import ai.koog.prompt.structure.StructuredResponse
import ai.koog.prompt.structure.executeStructured
import kotlinx.coroutines.flow.Flow
import kotlinx.serialization.KSerializer
import kotlinx.serialization.serializer

/**
 * API for [AIAgentLLMSession]
 */
public interface AIAgentLLMSessionAPI : AutoCloseable {
    /**
     * Represents the configuration settings for an AI agent.
     *
     * This variable holds an instance of [AIAgentConfig], which encapsulates
     * various parameters and options used to customize the behavior and
     * functionality of the AI agent.
     */

    public val config: AIAgentConfig

    /**
     * Represents the current prompt associated with the LLM session.
     * The prompt contains the input messages, model configuration, and parameters.
     *
     * Typical usage includes providing input to LLM requests, such as:
     * - [requestLLMWithoutTools]
     * - [requestLLM]
     * - etc.
     */
    public val prompt: Prompt

    /**
     * Provides a list of tools based on the current active state.
     * This property holds a collection of [ToolDescriptor] instances, which describe the tools available for use.
     */
    public val tools: List<ToolDescriptor>

    /**
     * Represents the active language model used within the session.
     */
    public val model: LLModel

    /**
     * Represents the active response processor within the session.
     * The processor defines the post-processing of messages returned from the LLM.
     */
    public val responseProcessor: ResponseProcessor?

    /**
     * A flag indicating whether the session is currently active.
     *
     * This variable is used to ensure that the session operations are only performed when the session is active.
     * Once the session is closed, this flag is set to `false` to prevent further usage.
     */
    @InternalAgentsApi
    public var isActive: Boolean

    /**
     * Ensures that the session is active before allowing further operations.
     *
     * @throws IllegalStateException if the session is not active.
     */
    @InternalAgentsApi
    public fun validateSession()

    /**
     * Prepares a prompt by incorporating the provided tools into it.
     *
     * @param prompt The initial prompt that needs to be updated or modified.
     * @param tools A list of tool descriptors that may be used to enhance or adapt the prompt.
     * @return The updated prompt after incorporating the provided tools.
     */
    @InternalAgentsApi
    public fun preparePrompt(prompt: Prompt, tools: List<ToolDescriptor>): Prompt

    /**
     * Executes a streaming process based on the provided prompt and tools.
     *
     * @param prompt the input prompt containing the initial data or request for the streaming process.
     * @param tools a list of tool descriptors that provide functionality to assist during streaming execution.
     * @return a flow of StreamFrame objects representing the streamed output.
     */
    @InternalAgentsApi
    public fun executeStreaming(prompt: Prompt, tools: List<ToolDescriptor>): Flow<StreamFrame>

    /**
     * Executes multiple tools using the given prompt and returns their responses.
     *
     * @param prompt The input prompt that guides the execution of the tools.
     * @param tools The list of tool descriptors to be executed.
     * @return A list of responses generated by executing the tools.
     */
    @InternalAgentsApi
    public suspend fun executeMultiple(prompt: Prompt, tools: List<ToolDescriptor>): List<Message.Response>

    /**
     * Executes a single operation based on the provided prompt and tools.
     *
     * @param prompt The input prompt containing the necessary information for the operation.
     * @param tools A list of tool descriptors that can be utilized during the execution.
     * @return The response message resulting from the execution.
     */
    @InternalAgentsApi
    public suspend fun executeSingle(prompt: Prompt, tools: List<ToolDescriptor>): Message.Response

    /**
     * Sends a request to the language model without utilizing any tools and returns multiple responses.
     *
     * @return A list of response messages from the language model.
     */
    public suspend fun requestLLMMultipleWithoutTools(): List<Message.Response>

    /**
     * Sends a request to the language model without utilizing any tools and returns the response.
     *
     * This method validates the session state before proceeding with the operation. If tool usage
     * is disabled (i.e., the tools list is empty), the tool choice parameter will be set to null
     * to ensure compatibility with the underlying LLM client's behavior. It then executes the request
     * and retrieves the response from the LLM.
     *
     * @return The response message from the language model after executing the request, represented
     *         as a [Message.Response] instance.
     */
    public suspend fun requestLLMWithoutTools(): Message.Response

    /**
     * Sends a request to the language model that enforces the usage of tools and retrieves the response.
     *
     * This method:
     * 1. Validates that the session is active.
     * 2. Updates the prompt configuration to mark tool usage as required (`ToolChoice.Required`).
     * 3. Retrieves all generated messages (including potential Chain of Thought/Reasoning blocks).
     * 4. Filters the result to return the first [Message.Tool.Call].
     *
     * If no tool call is found (e.g., the model refused or asked a question), this method throws an exception.
     *
     * @return The tool call response from the language model.
     */
    public suspend fun requestLLMOnlyCallingTools(): Message.Response

    /**
     * Sends a request to the language model that enforces the usage of tools and retrieves all responses.
     *
     * This is useful when the LLM returns multiple messages, such as a "thinking" block followed by tool calls,
     * or multiple parallel tool calls.
     *
     * This method:
     * 1. Validates that the session is active.
     * 2. Updates the prompt configuration to mark tool usage as required (`ToolChoice.Required`).
     *
     * @return A list of responses from the language model.
     */
    public open suspend fun requestLLMMultipleOnlyCallingTools(): List<Message.Response>

    /**
     * Sends a request to the language model while enforcing the use of a specific tool,
     * and returns the response.
     *
     * This method validates that the session is active and checks if the specified tool
     * exists within the session's set of available tools. It updates the prompt configuration
     * to enforce the selection of the specified tool before executing the request.
     *
     * @param tool The tool to be used for the request, represented by a [ToolDescriptor] instance.
     *             This parameter ensures that the language model utilizes the specified tool
     *             during the interaction.
     * @return The response from the language model as a [Message.Response] instance after
     *         processing the request with the enforced tool.
     */
    public suspend fun requestLLMForceOneTool(tool: ToolDescriptor): Message.Response

    /**
     * Sends a request to the language model while enforcing the use of a specific tool, and returns the response.
     *
     * This method ensures the session is active and updates the prompt configuration to enforce the selection of the
     * specified tool before executing the request. It uses the provided tool as a focus for the language model to process
     * the interaction.
     *
     * @param tool The tool to be used for the request, represented as an instance of [Tool]. This parameter ensures
     *             the specified tool is utilized during the LLM interaction.
     * @return The response from the language model as a [Message.Response] instance after processing the request with the
     *         enforced tool.
     */
    public suspend fun requestLLMForceOneTool(tool: Tool<*, *>): Message.Response

    /**
     * Sends a request to the underlying LLM and returns the first response.
     * This method ensures the session is active before executing the request.
     *
     * @return The first response message from the LLM after executing the request.
     */
    public suspend fun requestLLM(): Message.Response

    /**
     * Sends a streaming request to the underlying LLM and returns the streamed response.
     * This method ensures the session is active before executing the request.
     *
     * @return A flow emitting `StreamFrame` objects that represent the streaming output of the language model.
     */
    public suspend fun requestLLMStreaming(): Flow<StreamFrame>

    /**
     * Sends a moderation request to the specified or default large language model (LLM) for content moderation.
     *
     * This method validates the session state before processing the request. It prepares the prompt
     * and uses the executor to perform the moderation check. A specific moderating model can be provided;
     * if not, the default session model will be used.
     *
     * @param moderatingModel An optional [LLModel] instance representing the model to be used for moderation.
     *                        If null, the default model configured for the session will be used.
     * @return A [ModerationResult] instance containing the details of the moderation analysis, including
     *         content classification and flagged categories.
     */
    public suspend fun requestModeration(moderatingModel: LLModel? = null): ModerationResult

    /**
     * Sends a request to the language model, potentially utilizing multiple tools,
     * and returns a list of responses from the model.
     *
     * Before executing the request, the session state is validated to ensure
     * it is active and usable.
     *
     * @return a list of responses from the language model
     */
    public suspend fun requestLLMMultiple(): List<Message.Response>

    /**
     * Sends a request to LLM and gets a structured response.
     *
     * @param config A configuration defining structures and behavior.
     *
     * @see [executeStructured]
     */
    public suspend fun <T> requestLLMStructured(
        config: StructuredRequestConfig<T>,
    ): Result<StructuredResponse<T>>

    /**
     * Sends a request to LLM and gets a structured response.
     *
     * This is a simple version of the full `requestLLMStructured`. Unlike the full version, it does not require specifying
     * struct definitions and structured output modes manually. It attempts to find the best approach to provide a structured
     * output based on the defined [model] capabilities.
     *
     * @param serializer Serializer for the requested structure type.
     * @param examples Optional list of examples in case manual mode will be used. These examples might help the model to
     * understand the format better.
     * @param fixingParser Optional parser that handles malformed responses by using an auxiliary LLM to
     * intelligently fix parsing errors. When specified, parsing errors trigger additional
     * LLM calls with error context to attempt correction of the structure format.
     */
    public suspend fun <T> requestLLMStructured(
        serializer: KSerializer<T>,
        examples: List<T> = emptyList(),
        fixingParser: StructureFixingParser? = null
    ): Result<StructuredResponse<T>>

    /**
     * Parses a structured response from the language model using the specified configuration.
     *
     * This function takes a response message and a structured output configuration,
     * parses the response content based on the defined structure, and returns
     * a structured response containing the parsed data and the original message.
     *
     * @param response The response message from the language model that contains the content to be parsed.
     * The message is expected to match the defined structured output.
     * @param config The configuration defining the expected structure and additional parsing behavior.
     * It includes options such as structure definitions and optional parsers for error handling.
     * @return A structured response containing the parsed data of type `T` along with the original message.
     */
    public suspend fun <T> parseResponseToStructuredResponse(
        response: Message.Assistant,
        config: StructuredRequestConfig<T>
    ): StructuredResponse<T>

    /**
     * Sends a request to the language model, potentially receiving multiple choices,
     * and returns a list of choices from the model.
     *
     * Before executing the request, the session state is validated to ensure
     * it is active and usable.
     *
     * @return a list of choices from the model
     */
    public suspend fun requestLLMMultipleChoices(): List<LLMChoice>

    override fun close()
}
