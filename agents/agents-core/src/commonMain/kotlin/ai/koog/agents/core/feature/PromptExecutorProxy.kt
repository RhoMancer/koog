package ai.koog.agents.core.feature

import ai.koog.agents.core.agent.context.AIAgentContext
import ai.koog.agents.core.agent.context.with
import ai.koog.agents.core.agent.execution.AgentExecutionInfo
import ai.koog.agents.core.tools.ToolDescriptor
import ai.koog.prompt.dsl.ModerationResult
import ai.koog.prompt.dsl.Prompt
import ai.koog.prompt.executor.model.PromptExecutor
import ai.koog.prompt.llm.LLModel
import ai.koog.prompt.message.LLMChoice
import ai.koog.prompt.message.Message
import ai.koog.prompt.streaming.StreamFrame
import io.github.oshai.kotlinlogging.KotlinLogging
import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.catch
import kotlinx.coroutines.flow.onCompletion
import kotlinx.coroutines.flow.onEach
import kotlinx.coroutines.flow.onStart
import kotlin.uuid.ExperimentalUuidApi
import kotlin.uuid.Uuid

/**
 * A wrapper around [ai.koog.prompt.executor.model.PromptExecutor] that allows for adding internal functionality to the executor
 * to catch and log events related to LLM calls.
 *
 * @property executor The [ai.koog.prompt.executor.model.PromptExecutor] to wrap;
 * @property context The [AIAgentContext] associated with the agent that is executing the prompt.
 */
@OptIn(ExperimentalUuidApi::class)
public class PromptExecutorProxy(
    private val executor: PromptExecutor,
    private val context: AIAgentContext,
) : PromptExecutor {

    private companion object {
        private val logger = KotlinLogging.logger { }
    }

    override suspend fun execute(prompt: Prompt, model: LLModel, tools: List<ToolDescriptor>): List<Message.Response> =
        context.with(prompt, ExecutionPathPartPrefix.LLM) { executionInfo ->
            val callId = Uuid.random().toString()

            logger.debug { "Executing LLM call (prompt: $prompt, tools: [${tools.joinToString { it.name }}])" }
            context.pipeline.onLLMCallStarting(context.executionInfo, context.runId, callId, prompt, model, tools)

            val responses = executor.execute(prompt, model, tools)

            logger.trace { "Finished LLM call with responses: [${responses.joinToString { "${it.role}: ${it.content}" }}]" }
            context.pipeline.onLLMCallCompleted(executionInfo, context.runId, callId, prompt, model, tools, responses)

            responses
        }

    /**
     * Executes a streaming call to the language model with tool support.
     *
     * This method wraps the underlying executor's streaming functionality with pipeline hooks
     * to enable monitoring and processing of stream events. It triggers before-stream handlers
     * before starting, stream-frame handlers for each frame received, and after-stream handlers
     * upon completion.
     *
     * @param prompt The prompt to send to the language model
     * @param model The language model to use for streaming
     * @param tools The list of available tool descriptors for the streaming call
     * @return A Flow of StreamFrame objects representing the streaming response
     */
    override fun executeStreaming(
        prompt: Prompt,
        model: LLModel,
        tools: List<ToolDescriptor>
    ): Flow<StreamFrame> {
        logger.debug { "Executing LLM streaming call (prompt: $prompt, tools: [${tools.joinToString { it.name }}])" }
        val callId: String = Uuid.random().toString()
        return executor.executeStreaming(prompt, model, tools)
            .onStart {
                context.with(prompt, ExecutionPathPartPrefix.STREAMING) { executionInfo ->
                    logger.debug { "Starting LLM streaming call" }
                    context.pipeline.onLLMStreamingStarting(executionInfo, context.runId, callId, prompt, model, tools)
                }
            }

            .onEach { frame ->
                context.with(prompt, ExecutionPathPartPrefix.STREAMING) { executionInfo ->
                    logger.debug { "Received frame from LLM streaming call: $frame" }
                    context.pipeline.onLLMStreamingFrameReceived(executionInfo, context.runId, callId, frame)
                }
            }
            .catch { error ->
                context.with(prompt, ExecutionPathPartPrefix.STREAMING) { executionInfo ->
                    logger.debug(error) { "Error in LLM streaming call" }
                    context.pipeline.onLLMStreamingFailed(executionInfo, context.runId, callId, error)
                    throw error
                }
            }
            .onCompletion { error ->
                context.with(prompt, ExecutionPathPartPrefix.STREAMING) { executionInfo ->
                    logger.debug(error) { "Finished LLM streaming call" }
                    context.pipeline.onLLMStreamingCompleted(executionInfo, context.runId, callId, prompt, model, tools)
                }
            }
    }

    // TODO: Add Pipeline interceptors for this method
    override suspend fun executeMultipleChoices(
        prompt: Prompt,
        model: LLModel,
        tools: List<ToolDescriptor>
    ): List<LLMChoice> {
        logger.debug { "Executing LLM call prompt: $prompt with tools: [${tools.joinToString { it.name }}]" }

        val responses = executor.executeMultipleChoices(prompt, model, tools)

        logger.debug {
            val messageBuilder = StringBuilder().appendLine("Finished LLM call with LLM Choice response:")

            responses.forEachIndexed { index, response ->
                messageBuilder.appendLine("- Response #$index")
                response.forEach { message ->
                    messageBuilder.appendLine("  -- [${message.role}] ${message.content}")
                }
            }

            "Finished LLM call with responses: $messageBuilder"
        }

        return responses
    }

    override suspend fun moderate(
        prompt: Prompt,
        model: LLModel
    ): ModerationResult = context.with(prompt, prefix = ExecutionPathPartPrefix.MODERATION) { executionInfo ->
        logger.debug { "Executing moderation LLM request (prompt: $prompt)" }
        val callId = Uuid.random().toString()

        context.pipeline.onLLMCallStarting(executionInfo, context.runId, callId, prompt, model, tools = emptyList())

        val result = executor.moderate(prompt, model)
        logger.trace { "Finished moderation LLM request with response: $result" }

        context.pipeline.onLLMCallCompleted(
            executionInfo, context.runId, callId, prompt, model, tools = emptyList(), responses = emptyList(), moderationResponse = result
        )

        result
    }

    override suspend fun models(): List<String> {
        return executor.models()
    }

    override fun close() {
        executor.close()
    }

    //region Private Methods

    private enum class ExecutionPathPartPrefix(val id: String) {
        LLM("llm"),
        STREAMING("stream"),
        MODERATION("moderation"),
        TOOL("tool")
    }

    private inline fun <T> AIAgentContext.with(
        prompt: Prompt,
        prefix: ExecutionPathPartPrefix,
        block: (executionInfo: AgentExecutionInfo) -> T
    ): T = with(partName = "${prefix.id}:${prompt.messages.lastOrNull()?.content ?: prompt.id}") { executionInfo ->
        block(executionInfo)
    }

    //endregion Private Methods
}
