# Koog

Koog is a Kotlin-based framework designed to build and run AI agents entirely in idiomatic Kotlin.

# Documentation

# Overview

Koog is an open-source JetBrains framework for building AI agents with an idiomatic, type-safe Kotlin DSL designed specifically for JVM and Kotlin developers. It lets you create agents that interact with tools, handle complex workflows, and communicate with users.

You can customize agent capabilities with a modular feature system and deploy your agents across JVM, JS, WasmJS, Android, and iOS targets using Kotlin Multiplatform.

- [**Getting started**](getting-started/)

  ______________________________________________________________________

  Build and run your first AI agent

- [**Glossary**](glossary/)

  ______________________________________________________________________

  Learn the essential terms

## Agent types

- [**Basic agents**](basic-agents/)

  ______________________________________________________________________

  Create and run agents that process a single input and provide a response

- [**Functional agents**](functional-agents/)

  ______________________________________________________________________

  Create and run lightweight agents with custom logic in plain Kotlin

- [**Complex workflow agents**](complex-workflow-agents/)

  ______________________________________________________________________

  Create and run agents that handle complex workflows with custom strategies

- [**Planner agents**](planner-agents/)

  ______________________________________________________________________

  Create and run agents that iteratively build and execute plans

## Core functionality

- [**Prompts**](prompts/)

  ______________________________________________________________________

  Create prompts, run them using LLM clients or prompt executors, switch between LLMs and providers, and handle failures with built-in retries

- [**Tools**](tools-overview/)

  ______________________________________________________________________

  Enhance your agents with builtâ€‘in, annotationâ€‘based, or classâ€‘based tools that can access external systems and APIs

- [**Strategies**](predefined-agent-strategies/)

  ______________________________________________________________________

  Design complex agent behaviors using intuitive graph-based workflows

- [**Events**](agent-events/)

  ______________________________________________________________________

  Monitor and process agent lifecycle, strategy, node, LLM call, and tool call events with predefined handlers

## Advanced usage

- [**History compression**](history-compression/)

  ______________________________________________________________________

  Optimize token usage while maintaining context in long-running conversations using advanced techniques

- [**Agent persistence**](agent-persistence/)

  ______________________________________________________________________

  Restore the agent state at specific points during execution

- [**Structured output**](structured-output/)

  ______________________________________________________________________

  Generate responses in structured formats

- [**Streaming API**](streaming-api/)

  ______________________________________________________________________

  Process responses in real-time with streaming support and parallel tool calls

- [**Knowledge retrieval**](embeddings/)

  ______________________________________________________________________

  Retain and retrieve knowledge across conversations using [vector embeddings](embeddings/), [ranked document storage](ranked-document-storage/), and [shared agent memory](agent-memory/)

- [**Tracing**](tracing/)

  ______________________________________________________________________

  Debug and monitor agent execution with detailed, configurable tracing

## Integrations

- [**Model Context Protocol (MCP)**](model-context-protocol/)

  ______________________________________________________________________

  Use MCP tools directly in AI agents

- [**Spring Boot**](spring-boot/)

  ______________________________________________________________________

  Add Koog to your Spring applications

- [**Ktor**](ktor-plugin/)

  ______________________________________________________________________

  Integrate Koog with Ktor servers

- [**OpenTelemetry**](opentelemetry-support/)

  ______________________________________________________________________

  Trace, log, and measure your agent with popular observability tools

- [**A2A Protocol**](a2a-protocol-overview/)

  ______________________________________________________________________

  Connect agents and services over a shared protocol

# Key features

Key features of Koog include:

- **Multiplatform development**: Deploy agents across JVM, JS, WasmJS, Android, and iOS targets using Kotlin Multiplatform.
- **Reliability and fault-tolerance**: Handle failures with built-in retries and restore the agent state at specific points during execution with the agent persistence feature.
- **Intelligent history compression**: Optimize token usage while maintaining context in long-running conversations using advanced built-in history compression techniques.
- **Enterprise-ready integrations**: Utilize integration with popular JVM frameworks such as Spring Boot and Ktor to embed Koog into your applications.
- **Observability with OpenTelemetry exporters**: Monitor and debug applications with built-in support for popular observability providers (W&B Weave, Langfuse).
- **LLM switching and seamless history adaptation**: Switch to a different LLM at any point without losing the existing conversation history, or reroute between multiple LLM providers.
- **Integration with JVM and Kotlin applications**: Build AI agents with an idiomatic, type-safe Kotlin DSL designed specifically for JVM and Kotlin developers.
- **Model Context Protocol integration**: Use Model Context Protocol (MCP) tools in AI agents.
- **Knowledge retrieval and memory**: Retain and retrieve knowledge across conversations using vector embeddings, ranked document storage, and shared agent memory.
- **Powerful Streaming API**: Process responses in real-time with streaming support and parallel tool calls.
- **Modular feature system**: Customize agent capabilities through a composable architecture.
- **Flexible graph workflows**: Design complex agent behaviors using intuitive graph-based workflows.
- **Custom tool creation**: Enhance your agents with tools that access external systems and APIs.
- **Comprehensive tracing**: Debug and monitor agent execution with detailed, configurable tracing.

# LLM providers

Koog works with major LLM providers and also supports local models using [Ollama](https://ollama.com/). The following providers are currently supported:

| LLM provider                                                                                                                                                | Choose for                                                                                                                       |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| [OpenAI](https://platform.openai.com/docs/overview) (including [Azure OpenAI Service](https://azure.microsoft.com/en-us/products/ai-foundry/models/openai)) | Advanced models with a wide range of capabilities.                                                                               |
| [Anthropic](https://www.anthropic.com/)                                                                                                                     | Long contexts and prompt caching.                                                                                                |
| [Google](https://ai.google.dev/)                                                                                                                            | Multimodal processing (audio, video), large contexts.                                                                            |
| [DeepSeek](https://www.deepseek.com/)                                                                                                                       | Cost-effective reasoning and coding.                                                                                             |
| [OpenRouter](https://openrouter.ai/)                                                                                                                        | One integration with an access to multiple models from multiple providers for flexibility, provider comparison, and unified API. |
| [Amazon Bedrock](https://aws.amazon.com/bedrock/)                                                                                                           | AWS-native environment, enterprise security and compliance, multi-provider access.                                               |
| [Mistral](https://mistral.ai/)                                                                                                                              | European data hosting, GDPR compliance.                                                                                          |
| [Alibaba](https://www.alibabacloud.com/en?_p_lc=1) ([DashScope](https://dashscope.aliyun.com/) OpenAI-compatible client)                                    | Large contexts and cost-efficient Qwen models.                                                                                   |
| [Ollama](https://ollama.com/)                                                                                                                               | Privacy, local development, offline operation, and no API costs.                                                                 |

The table below shows the LLM capabilities that Koog supports and which providers offer these capabilities in their models.

| LLM capability                  | OpenAI                       | Anthropic                       | Google                                        | DeepSeek | OpenRouter       | Amazon Bedrock   | Mistral                         | Alibaba (DashScope OpenAI-compatible client) | Ollama (local models) |
| ------------------------------- | ---------------------------- | ------------------------------- | --------------------------------------------- | -------- | ---------------- | ---------------- | ------------------------------- | -------------------------------------------- | --------------------- |
| Supported input                 | Text, image, audio, document | Text, image, document[1](#fn:1) | Text, image, audio, video, document[1](#fn:1) | Text     | Differs by model | Differs by model | Text, image, document[1](#fn:1) | Text, image, audio, video[1](#fn:1)          | Text, image[1](#fn:1) |
| Response streaming              | âœ“                            | âœ“                               | âœ“                                             | âœ“        | âœ“                | âœ“                | âœ“                               | âœ“                                            | âœ“                     |
| Tools                           | âœ“                            | âœ“                               | âœ“                                             | âœ“        | âœ“                | âœ“[1](#fn:1)      | âœ“                               | âœ“                                            | âœ“                     |
| Tool choice                     | âœ“                            | âœ“                               | âœ“                                             | âœ“        | âœ“                | âœ“[1](#fn:1)      | âœ“                               | âœ“                                            | â€“                     |
| Structured output (JSON Schema) | âœ“                            | â€“                               | âœ“                                             | âœ“        | âœ“[1](#fn:1)      | â€“                | âœ“                               | âœ“[1](#fn:1)                                  | âœ“                     |
| Multiple choices                | âœ“                            | â€“                               | âœ“                                             | â€“        | âœ“[1](#fn:1)      | âœ“[1](#fn:1)      | âœ“                               | âœ“[1](#fn:1)                                  | â€“                     |
| Temperature                     | âœ“                            | âœ“                               | âœ“                                             | âœ“        | âœ“                | âœ“                | âœ“                               | âœ“                                            | âœ“                     |
| Speculation                     | âœ“[1](#fn:1)                  | â€“                               | â€“                                             | â€“        | âœ“[1](#fn:1)      | â€“                | âœ“[1](#fn:1)                     | âœ“[1](#fn:1)                                  | â€“                     |
| Content moderation              | âœ“                            | â€“                               | â€“                                             | â€“        | â€“                | âœ“                | âœ“                               | â€“                                            | âœ“                     |
| Embeddings                      | âœ“                            | â€“                               | â€“                                             | â€“        | â€“                | âœ“                | âœ“                               | â€“                                            | âœ“                     |
| Prompt caching                  | âœ“[1](#fn:1)                  | âœ“                               | â€“                                             | â€“        | â€“                | â€“                | â€“                               | â€“                                            | â€“                     |
| Completion                      | âœ“                            | âœ“                               | âœ“                                             | âœ“        | âœ“                | âœ“                | âœ“                               | âœ“                                            | âœ“                     |
| Local execution                 | â€“                            | â€“                               | â€“                                             | â€“        | â€“                | â€“                | â€“                               | â€“                                            | âœ“                     |

Note

Koog supports the most commonly used capabilities for creating AI agents. LLMs from each provider may have additional features that Koog does not currently support. To learn more, refer to [Model capabilities](../model-capabilities/).

## Working with providers

Koog lets you work with LLM providers on two levels:

- Using an **LLM client** for direct interaction with a specific provider. Each client implements the `LLMClient` interface, handling authentication, request formatting, and response parsing for the provider. For details, see [LLM clients](../prompts/llm-clients/).
- Using a **prompt executor** for a higher-level abstraction that wraps one or multiple LLM clients, manages their lifecycles, and unifies an interface across providers. It can switch between providers and optionally fall back to a configured provider and LLM using the corresponding client. You can either create your own executor or use a pre-defined prompt executor for a specific provider. For details, see [Prompt executors](../prompts/prompt-executors/).

Using a prompt executor offers a higherâ€‘level layer over one or more LLMClients. It manages client lifecycles and exposes a unified interface across providers. In multiâ€‘provider setups, it can route requests between providers and optionally fall back to a designated client when needed for core requests. You can create your own executor or use preâ€‘defined onesâ€”both singleâ€‘provider and multiâ€‘provider options are available.

## Next steps

- [Create and run an agent](../getting-started/) with a specific LLM provider.
- Learn more about [prompts](../prompts/).

______________________________________________________________________

1. Capability is supported only by some models of the provider.Â [â†©](#fnref:1 "Jump back to footnote 1 in the text")[â†©](#fnref2:1 "Jump back to footnote 1 in the text")[â†©](#fnref3:1 "Jump back to footnote 1 in the text")[â†©](#fnref4:1 "Jump back to footnote 1 in the text")[â†©](#fnref5:1 "Jump back to footnote 1 in the text")[â†©](#fnref6:1 "Jump back to footnote 1 in the text")[â†©](#fnref7:1 "Jump back to footnote 1 in the text")[â†©](#fnref8:1 "Jump back to footnote 1 in the text")[â†©](#fnref9:1 "Jump back to footnote 1 in the text")[â†©](#fnref10:1 "Jump back to footnote 1 in the text")[â†©](#fnref11:1 "Jump back to footnote 1 in the text")[â†©](#fnref12:1 "Jump back to footnote 1 in the text")[â†©](#fnref13:1 "Jump back to footnote 1 in the text")[â†©](#fnref14:1 "Jump back to footnote 1 in the text")[â†©](#fnref15:1 "Jump back to footnote 1 in the text")[â†©](#fnref16:1 "Jump back to footnote 1 in the text")[â†©](#fnref17:1 "Jump back to footnote 1 in the text")

# Glossary

## Agent

- **Agent**: an AI entity that can interact with tools, handle complex workflows, and communicate with users.
- **LLM (Large Language Model)**: the underlying AI model that powers agent capabilities.
- **Message**: a unit of communication in the agent system that represents data passed from a user, assistant, or system.
- **Prompt**: the conversation history provided to an LLM that consists of messages from a user, assistant, and system.
- **System prompt**: instructions provided to an agent to guide its behavior, define its role, and supply key information necessary for its tasks.
- **Context**: the environment in which LLM interactions occur, with access to the conversation history and tools.
- **LLM session**: a structured way to interact with LLMs that includes the conversation history, available tools, and methods to make requests.

## Agent workflow

- **Strategy**: a defined workflow for an agent that consists of sequential subgraphs. The strategy defines how the agent processes input, interacts with tools, and generates output. A strategy graph consists of nodes connected by edges that represent transitions between nodes.

### Strategy graphs

- **Graph**: a structure of nodes connected by edges that defines an agent strategy workflow.
- **Node**: a fundamental building block of an agent strategy workflow that represents a specific operation or transformation.
- **Edge**: a connection between nodes in an agent graph that defines the flow of operations, often with conditions that specify when to follow each edge.
- **Conditions**: rules that determine when to follow a particular edge.
- **Subgraph**: a self-contained unit of processing within an agent strategy, with its own set of tools, context, and responsibilities. Information about subgraph operations can be either encapsulated within the subgraph or transferred between subgraphs using the AgentMemory feature.

## Tools

- **Tool**: a function that an agent can use to perform specific tasks or access external systems. The agent is aware of the available tools and their arguments but lacks knowledge of their implementation details.
- **Tool call**: a request from an LLM to run a specific tool using the provided arguments. It functions similarly to a function call.
- **Tool descriptor**: tool metadata that includes its name, description, and parameters.
- **Tool registry**: a list of tools available to an agent. The registry informs the agent about the available tools.
- **Tool result**: an output produced by running a tool. For example, if the tool is a method, the result would be its return value.

## History compression

- **History compression**: the process of reducing the size of the conversation history to manage token usage by applying various compression strategies. To learn more, see [History compression](../history-compression/).

## Features

- **Feature**: a component that extends and enhances the functionality of AI agents.

### EventHandler feature

- **EventHandler**: a feature that enables monitoring and responding to various agent events, providing hooks for tracking agent lifecycle, handling errors, and processing tool invocations throughout the workflow.

### AgentMemory feature

- **AgentMemory**: a feature that enables AI agents to store, retrieve, and use information across conversations. To learn more, see [AgentMemory](../agent-memory/).
- **Concept**: a category of information with associated metadata in the AgentMemory feature, including a keyword, description, and fact type. Concepts are fundamental building blocks of the AgentMemory system that the agent can remember and recall. To learn more, see [AgentMemory](../agent-memory/).
- **Fact**: an individual piece of information stored in the AgentMemory system. Facts are associated with concepts and can either have a single value or multiple values. To learn more, see [AgentMemory](../agent-memory/).
- **Memory scope**: the context in which facts are relevant. To learn more, see [AgentMemory](../agent-memory/).
# Getting started

# Getting started

This guide will help you install Koog and create your first AI agent.

## Prerequisites

Before you start, make sure you have the following:

- A working Kotlin/JVM project with Gradle or Maven.
- Java 17+ installed.
- A valid API key for your preferred [LLM provider](../llm-providers/) (not required for Ollama, which runs locally).

## Install Koog

To use Koog, you need to include all necessary dependencies in your build configuration.

Note

Replace `LATEST_VERSION` with the latest version of Koog published on Maven Central.

1. Add the dependency to the `build.gradle.kts` file.

   ```kotlin
   dependencies {
       implementation("ai.koog:koog-agents:LATEST_VERSION")
   }
   ```

1. Make sure that you have `mavenCentral()` in the list of repositories.

   ```kotlin
   repositories {
       mavenCentral()
   }
   ```

1. Add the dependency to the `build.gradle` file.

   ```groovy
   dependencies {
       implementation 'ai.koog:koog-agents:LATEST_VERSION'
   }
   ```

1. Make sure that you have `mavenCentral()` in the list of repositories.

   ```groovy
   repositories {
       mavenCentral()
   }
   ```

1. Add the dependency to the `pom.xml` file.

   ```xml
   <dependency>
       <groupId>ai.koog</groupId>
       <artifactId>koog-agents-jvm</artifactId>
       <version>LATEST_VERSION</version>
   </dependency>
   ```

1. Make sure that you have `mavenCentral()` in the list of repositories.

   ```xml
    <repositories>
       <repository>
           <id>mavenCentral</id>
           <url>https://repo1.maven.org/maven2/</url>
       </repository>
   </repositories>
   ```

Note

When integrating Koog with [Ktor servers](../ktor-plugin/), [Spring applications](../spring-boot/), or [MCP tools](../model-context-protocol/), you need to include the additional dependencies in your build configuration. For the exact dependencies, refer to the relevant pages in the Koog documentation.

## Set an API key

Tip

Use environment variables or a secure configuration management system to store your API keys. Avoid hardcoding API keys directly in your source code.

Get your [API key](https://platform.openai.com/api-keys) and assign it as an environment variable.

```bash
export OPENAI_API_KEY=your-api-key
```

```shell
setx OPENAI_API_KEY "your-api-key"
```

Restart your terminal to apply the changes. You can now retrieve and use the API key to create an agent.

Get your [API key](https://console.anthropic.com/settings/keys) and assign it as an environment variable.

```bash
export ANTHROPIC_API_KEY=your-api-key
```

```shell
setx ANTHROPIC_API_KEY "your-api-key"
```

Restart your terminal to apply the changes. You can now retrieve and use the API key to create an agent.

Get your [API key](https://aistudio.google.com/app/api-keys) and assign it as an environment variable.

```bash
export GOOGLE_API_KEY=your-api-key
```

```shell
setx GOOGLE_API_KEY "your-api-key"
```

Restart your terminal to apply the changes. You can now retrieve and use the API key to create an agent.

Get your [API key](https://platform.deepseek.com/api_keys) and assign it as an environment variable.

```bash
export DEEPSEEK_API_KEY=your-api-key
```

```shell
setx DEEPSEEK_API_KEY "your-api-key"
```

Restart your terminal to apply the changes. You can now retrieve and use the API key to create an agent.

Get your [API key](https://openrouter.ai/keys) and assign it as an environment variable.

```bash
export OPENROUTER_API_KEY=your-api-key
```

```shell
setx OPENROUTER_API_KEY "your-api-key"
```

Restart your terminal to apply the changes. You can now retrieve and use the API key to create an agent.

Get valid [AWS credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_bedrock.html) (an access key and a secret key) and assign them as environment variables.

```bash
export AWS_BEDROCK_ACCESS_KEY=your-access-key
export AWS_BEDROCK_SECRET_ACCESS_KEY=your-secret-access-key
```

```shell
setx AWS_BEDROCK_ACCESS_KEY "your-access-key"
setx AWS_BEDROCK_SECRET_ACCESS_KEY "your-secret-access-key"
```

Restart your terminal to apply the changes. You can now retrieve and use the API key to create an agent.

Install Ollama and run a model locally without an API key.

For more information, see [Ollama documentation](https://docs.ollama.com/quickstart).

## Create and run an agent

The example below creates and runs a simple AI agent using the [`GPT-4o`](https://platform.openai.com/docs/models/gpt-4o) model.

```kotlin
fun main() = runBlocking {
    // Get an API key from the OPENAI_API_KEY environment variable
    val apiKey = System.getenv("OPENAI_API_KEY")
        ?: error("The API key is not set.")

    // Create an agent
    val agent = AIAgent(
        promptExecutor = simpleOpenAIExecutor(apiKey),
        llmModel = OpenAIModels.Chat.GPT4o
    )

    // Run the agent
    val result = agent.run("Hello! How can you help me?")
    println(result)
}
```

The example can produce the following output:

```text
Hello! I'm here to help you with whatever you need. Here are just a few things I can do:

- Answer questions.
- Explain concepts or topics you're curious about.
- Provide step-by-step instructions for tasks.
- Offer advice, notes, or ideas.
- Help with research or summarize complex material.
- Write or edit text, emails, or other documents.
- Brainstorm creative projects or solutions.
- Solve problems or calculations.

Let me know what you need help withâ€”Iâ€™m here for you!
```

The example below creates and runs a simple AI agent using the [`Claude Opus 4.1`](https://www.anthropic.com/news/claude-opus-4-1) model.

```kotlin
fun main() = runBlocking {
    // Get an API key from the ANTHROPIC_API_KEY environment variable
    val apiKey = System.getenv("ANTHROPIC_API_KEY")
        ?: error("The API key is not set.")

    // Create an agent
    val agent = AIAgent(
        promptExecutor = simpleAnthropicExecutor(apiKey),
        llmModel = AnthropicModels.Opus_4_1
    )

    // Run the agent
    val result = agent.run("Hello! How can you help me?")
    println(result)
}
```

The example can produce the following output:

```text
Hello! I can help you with:

- **Answering questions** and explaining topics
- **Writing** - drafting, editing, proofreading
- **Learning** - homework, math, study help
- **Problem-solving** and brainstorming
- **Research** and information finding
- **General tasks** - instructions, planning, recommendations

What do you need help with today?
```

The example below creates and runs a simple AI agent using the [`Gemini 2.5 Pro`](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro) model.

```kotlin
fun main() = runBlocking {
    // Get an API key from the GOOGLE_API_KEY environment variable
    val apiKey = System.getenv("GOOGLE_API_KEY")
        ?: error("The API key is not set.")

    // Create an agent
    val agent = AIAgent(
        promptExecutor = simpleGoogleAIExecutor(apiKey),
        llmModel = GoogleModels.Gemini2_5Pro
    )

    // Run the agent
    val result = agent.run("Hello! How can you help me?")
    println(result)
}
```

The example can produce the following output:

```text
I'm an AI that can help you with tasks involving language and information. You can ask me to:

*   **Answer questions**
*   **Write or edit text** (emails, stories, code, etc.)
*   **Brainstorm ideas**
*   **Summarize long documents**
*   **Plan things** (like trips or projects)
*   **Be a creative partner**

Just tell me what you need
```

The example below creates and runs a simple AI agent using the `deepseek-chat` model.

```kotlin
fun main() = runBlocking {
    // Get an API key from the DEEPSEEK_API_KEY environment variable
    val apiKey = System.getenv("DEEPSEEK_API_KEY")
        ?: error("The API key is not set.")

    // Create an LLM client
    val deepSeekClient = DeepSeekLLMClient(apiKey)

    // Create an agent
    val agent = AIAgent(
        // Create a prompt executor using the LLM client
        promptExecutor = SingleLLMPromptExecutor(deepSeekClient),
        // Provide a model
        llmModel = DeepSeekModels.DeepSeekChat
    )

    // Run the agent
    val result = agent.run("Hello! How can you help me?")
    println(result)
}
```

The example can produce the following output:

```text
Hello! I'm here to assist you with a wide range of tasks, including answering questions, providing information, helping with problem-solving, offering creative ideas, and even just chatting. Whether you need help with research, writing, learning something new, or simply want to discuss a topic, feel free to askâ€”Iâ€™m happy to help! ðŸ˜Š
```

The example below creates and runs a simple AI agent using the [`GPT-4o`](https://openrouter.ai/openai/gpt-4o) model.

```kotlin
fun main() = runBlocking {
    // Get an API key from the OPENROUTER_API_KEY environment variable
    val apiKey = System.getenv("OPENROUTER_API_KEY")
        ?: error("The API key is not set.")

    // Create an agent
    val agent = AIAgent(
        promptExecutor = simpleOpenRouterExecutor(apiKey),
        llmModel = OpenRouterModels.GPT4o
    )

    // Run the agent
    val result = agent.run("Hello! How can you help me?")
    println(result)
}
```

The example can produce the following output:

```text
I can answer questions, help with writing, solve problems, organize tasks, and moreâ€”just let me know what you need!
```

The example below creates and runs a simple AI agent using the [`Claude Sonnet 4.5`](https://www.anthropic.com/news/claude-sonnet-4-5) model.

```kotlin
fun main() = runBlocking {
    // Get access keys from the AWS_BEDROCK_ACCESS_KEY and AWS_BEDROCK_SECRET_ACCESS_KEY environment variables
    val awsAccessKeyId = System.getenv("AWS_BEDROCK_ACCESS_KEY")
        ?: error("The access key is not set.")

    val awsSecretAccessKey = System.getenv("AWS_BEDROCK_SECRET_ACCESS_KEY")
        ?: error("The secret access key is not set.")

    // Create an agent
    val agent = AIAgent(
        promptExecutor = simpleBedrockExecutor(awsAccessKeyId, awsSecretAccessKey),
        llmModel = BedrockModels.AnthropicClaude4_5Sonnet
    )

    // Run the agent
    val result = agent.run("Hello! How can you help me?")
    println(result)
}
```

The example can produce the following output:

```text
Hello! I'm a helpful assistant and I can assist you in many ways, including:

- **Answering questions** on a wide range of topics (science, history, technology, etc.)
- **Writing help** - drafting emails, essays, creative content, or editing text
- **Problem-solving** - working through math problems, logic puzzles, or troubleshooting issues
- **Learning support** - explaining concepts, providing study notes, or tutoring
- **Planning & organizing** - helping with projects, schedules, or breaking down tasks
- **Coding assistance** - explaining programming concepts or helping debug code
- **Creative brainstorming** - generating ideas for projects, stories, or solutions
- **General conversation** - discussing topics or just chatting

 What would you like help with today?
```

The example below creates and runs a simple AI agent using the [`llama3.2`](https://ollama.com/library/llama3.2) model.

```kotlin
fun main() = runBlocking {
    // Create an agent
    val agent = AIAgent(
        promptExecutor = simpleOllamaAIExecutor(),
        llmModel = OllamaModels.Meta.LLAMA_3_2
    )

    // Run the agent
    val result = agent.run("Hello! How can you help me?")
    println(result)
}
```

The example can produce the following output:

```text
I can assist with various tasks such as answering questions, providing information, and even helping with language-related tasks like proofreading or writing suggestions. What's on your mind today?
```

## What's next

- Explore [key features](../key-features/) of Koog.
- Learn more about available [agent types](../basic-agents/).
# Agent types

# Basic agents

The `AIAgent` class is the core component that lets you create AI agents in your Kotlin applications.

You can build simple agents with minimal configuration or create sophisticated agents with advanced capabilities by defining custom strategies, tools, configurations, and custom input/output types.

This page guides you through the steps necessary to create a basic agent with customizable tools and configurations.

A basic agent processes a single input and provides a response. It operates within a single cycle of tool-calling to complete its task and provide a response. This agent can return either a message or a tool result. The tool result is returned if the tool registry is provided to the agent.

If your goal is to build a simple agent to experiment with, you can provide only a prompt executor and LLM when creating it. But if you want more flexibility and customization, you can pass optional parameters to configure the agent. To learn more about configuration options, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.agent/-a-i-agent/-a-i-agent.html).

## Prerequisites

- You have a valid API key from the LLM provider used to implement an AI agent. For a list of all available providers, see [LLM providers](../llm-providers/).

Tip

Use environment variables or a secure configuration management system to store your API keys. Avoid hardcoding API keys directly in your source code.

## Creating a basic agent

### 1. Add dependencies

To use the `AIAgent` functionality, include all necessary dependencies in your build configuration:

```text
dependencies {
    implementation("ai.koog:koog-agents:VERSION")
}
```

For all available installation methods, see [Install Koog](../getting-started/#install-koog).

### 2. Create an agent

To create an agent, create an instance of the `AIAgent` class and provide the `promptExecutor` and `llmModel` parameters:

```kotlin
val agent = AIAgent(
    promptExecutor = simpleOpenAIExecutor(System.getenv("OPENAI_API_KEY")),
    llmModel = OpenAIModels.Chat.GPT4o
)
```

### 3. Add a system prompt

A system prompt is used to define agent behavior. To provide the prompt, use the `systemPrompt` parameter:

```kotlin
val agent = AIAgent(
    promptExecutor = simpleOpenAIExecutor(System.getenv("YOUR_API_KEY")),
    systemPrompt = "You are a helpful assistant. Answer user questions concisely.",
    llmModel = OpenAIModels.Chat.GPT4o
)
```

### 4. Configure LLM output

Provide a temperature of LLM output generation using the `temperature` parameter:

```kotlin
val agent = AIAgent(
    promptExecutor = simpleOpenAIExecutor(System.getenv("YOUR_API_KEY")),
    systemPrompt = "You are a helpful assistant. Answer user questions concisely.",
    llmModel = OpenAIModels.Chat.GPT4o,
    temperature = 0.7
)
```

### 5. Add tools

Agents use tools to complete specific tasks. You can use the built-in tools or implement your own custom tools if needed.

To configure tools, use the `toolRegistry` parameter that defines the tools available to the agent:

```kotlin
val agent = AIAgent(
    promptExecutor = simpleOpenAIExecutor(System.getenv("YOUR_API_KEY")),
    systemPrompt = "You are a helpful assistant. Answer user questions concisely.",
    llmModel = OpenAIModels.Chat.GPT4o,
    temperature = 0.7,
    toolRegistry = ToolRegistry {
        tool(SayToUser)
    }
)
```

In the example, `SayToUser` is the built-in tool. To learn how to create a custom tool, see [Tools](../tools-overview/).

### 6. Adjust agent iterations

Provide the maximum number of steps the agent can take before it is forced to stop using the `maxIterations` parameter:

```kotlin
val agent = AIAgent(
    promptExecutor = simpleOpenAIExecutor(System.getenv("YOUR_API_KEY")),
    systemPrompt = "You are a helpful assistant. Answer user questions concisely.",
    llmModel = OpenAIModels.Chat.GPT4o,
    temperature = 0.7,
    toolRegistry = ToolRegistry {
        tool(SayToUser)
    },
    maxIterations = 30
)
```

### 7. Handle events during agent runtime

Basic agents support custom event handlers. While having an event handler is not required for creating an agent, it might be helpful for testing, debugging, or making hooks for chained agent interactions.

For more information on how to use the `EventHandler` feature for monitoring your agent interactions, see [Event Handlers](../agent-event-handlers/).

### 8. Run the agent

To run the agent, use the `run()` function:

```kotlin
val agent = AIAgent(
    promptExecutor = simpleOpenAIExecutor(System.getenv("OPENAI_API_KEY")),
    systemPrompt = "You are a helpful assistant. Answer user questions concisely.",
    llmModel = OpenAIModels.Chat.GPT4o,
    temperature = 0.7,
    toolRegistry = ToolRegistry {
        tool(SayToUser)
    },
    maxIterations = 100
)

fun main() = runBlocking {
    val result = agent.run("Hello! How can you help me?")
}
```

The agent produces the following output:

```text
Agent says: Hello! I'm here to assist you with a variety of tasks. Whether you have questions, need information, or require help with specific tasks, feel free to ask. How can I assist you today?
```

# Functional agents

Functional agents are lightweight AI agents that operate without building complex strategy graphs. Instead, the agent logic is implemented as a lambda function that handles user input, interacts with an LLM, optionally calls tools, and produces a final output. It can perform a single LLM call, process multiple LLM calls in sequence, or loop based on user input, as well as LLM and tool outputs.

Tip

- If you already have a [basic agent](../basic-agents/) as your first MVP, but run into task-specific limitations, use a functional agent to prototype custom logic. You can implement custom control flows in plain Kotlin while still using most Koog features, including history compression and automatic state management.
- For production-grade needs, refactor your functional agent into a [complex workflow agent](../complex-workflow-agents/) with strategy graphs. This provides persistence with controllable rollbacks for fault-tolerance and advanced OpenTelemetry tracing with nested graph events.

This page guides you through the steps necessary to create a minimal functional agent and extend it with tools.

## Prerequisites

Before you start, make sure that you have the following:

- A working Kotlin/JVM project.
- Java 17+ installed.
- A valid API key from the LLM provider used to implement an AI agent. For a list of all available providers, refer to [LLM providers](../llm-providers/).
- (Optional) Ollama installed and running locally if you use this provider.

Tip

Use environment variables or a secure configuration management system to store your API keys. Avoid hardcoding API keys directly in your source code.

## Add dependencies

The `AIAgent` class is the main class for creating agents in Koog. Include the following dependency in your build configuration to use the class functionality:

```text
dependencies {
    implementation("ai.koog:koog-agents:VERSION")
}
```

For all available installation methods, see [Install Koog](../getting-started/#install-koog).

## Create a minimal functional agent

To create a minimal functional agent, do the following:

1. Choose the input and output types that the agent handles and create a corresponding `AIAgent<Input, Output>` instance. In this guide, we use `AIAgent<String, String>`, which means the agent receives and returns `String`.
1. Provide the required parameters, including a system prompt, prompt executor, and LLM.
1. Define the agent logic with a lambda function wrapped into the `functionalStrategy {...}` DSL method.

Here is an example of a minimal functional agent that sends user text to a specified LLM and returns a single assistant message.

```kotlin
// Create an AIAgent instance and provide a system prompt, prompt executor, and LLM
val mathAgent = AIAgent<String, String>(
    systemPrompt = "You are a precise math assistant.",
    promptExecutor = simpleOllamaAIExecutor(),
    llmModel = OllamaModels.Meta.LLAMA_3_2,
    strategy = functionalStrategy { input -> // Define the agent logic
        // Make one LLM call
        val response = requestLLM(input)
        // Extract and return the assistant message content from the response
        response.asAssistantMessage().content
    }
)

// Run the agent with a user input and print the result
val result = mathAgent.run("What is 12 Ã— 9?")
println(result)
```

The agent can produce the following output:

```text
The answer to 12 Ã— 9 is 108.
```

This agent makes a single LLM call and returns the assistant message content. You can extend the agent logic to handle multiple sequential LLM calls. For example:

```kotlin
// Create an AIAgent instance and provide a system prompt, prompt executor, and LLM
val mathAgent = AIAgent<String, String>(
    systemPrompt = "You are a precise math assistant.",
    promptExecutor = simpleOllamaAIExecutor(),
    llmModel = OllamaModels.Meta.LLAMA_3_2,
    strategy = functionalStrategy { input -> // Define the agent logic
        // The first LLM call to produce an initial draft based on the user input
        val draft = requestLLM("Draft: $input").asAssistantMessage().content
        // The second LLM call to improve the draft by prompting the LLM again with the draft content
        val improved = requestLLM("Improve and clarify.").asAssistantMessage().content
        // The final LLM call to format the improved text and return the final formatted result
        requestLLM("Format the result as bold.").asAssistantMessage().content
    }
)

// Run the agent with a user input and print the result
val result = mathAgent.run("What is 12 Ã— 9?")
println(result)
```

The agent can produce the following output:

```text
When multiplying 12 by 9, we can break it down as follows:

**12 (tens) Ã— 9 = 108**

Alternatively, we can also use the distributive property to calculate this:

**(10 + 2) Ã— 9**
= **10 Ã— 9 + 2 Ã— 9**
= **90 + 18**
= **108**
```

## Add tools

In many cases, a functional agent needs to complete specific tasks, such as reading and writing data or calling APIs. In Koog, you expose such capabilities as tools and let the LLM call them in the agent logic.

This chapter takes the minimal functional agent created above and demonstrates how to extend the agent logic using tools.

1. Create an annotation-based tool. For more details, see [Annotation-based tools](../annotation-based-tools/).

```kotlin
@LLMDescription("Simple multiplier")
class MathTools : ToolSet {
    @Tool
    @LLMDescription("Multiplies two numbers and returns the result")
    fun multiply(a: Int, b: Int): Int {
        val result = a * b
        return result
    }
}
```

To learn more about available tools, refer to the [Tool overview](../tools-overview/).

2. Register the tool to make it available to the agent.

```kotlin
val toolRegistry = ToolRegistry {
    tools(MathTools())
}
```

3. Pass the tool registry to the agent to enable the LLM to request and use the available tools.

1. Extend the agent logic to identify tool calls, execute the requested tools, send their results back to the LLM, and repeat the process until no tool calls remain.

Note

Use a loop only if the LLM continues to issue tool calls.

```kotlin
val mathWithTools = AIAgent<String, String>(
    systemPrompt = "You are a precise math assistant. When multiplication is needed, use the multiplication tool.",
    promptExecutor = simpleOllamaAIExecutor(),
    llmModel = OllamaModels.Meta.LLAMA_3_2,
    toolRegistry = toolRegistry,
    strategy = functionalStrategy { input -> // Define the agent logic extended with tool calls
        // Send the user input to the LLM
        var responses = requestLLMMultiple(input)

        // Only loop while the LLM requests tools
        while (responses.containsToolCalls()) {
            // Extract tool calls from the response
            val pendingCalls = extractToolCalls(responses)
            // Execute the tools and return the results
            val results = executeMultipleTools(pendingCalls)
            // Send the tool results back to the LLM. The LLM may call more tools or return a final output
            responses = sendMultipleToolResults(results)
        }

        // When no tool calls remain, extract and return the assistant message content from the response
        responses.single().asAssistantMessage().content
    }
)

// Run the agent with a user input and print the result
val reply = mathWithTools.run("Please multiply 12.5 and 4, then add 10 to the result.")
println(reply)
```

The agent can produce the following output:

```text
Here is the step-by-step solution:

1. Multiply 12.5 and 4:
   12.5 Ã— 4 = 50

2. Add 10 to the result:
   50 + 10 = 60
```

## What's next

- Learn how to return structured data using the [Structured output API](../structured-output/).
- Experiment with adding more [tools](../tools-overview/) to the agent.
- Improve observability with the [EventHandler](../agent-events/) feature.
- Learn how to handle long-running conversations with [History compression](../history-compression/).

# Complex workflow agents

In addition to basic agents, the `AIAgent` class lets you build agents that handle complex workflows by defining custom strategies, tools, configurations, and custom input/output types.

Tip

If you are new to Koog and want to create the simplest agent, start with [Basic agents](../basic-agents/).

The process of creating and configuring such an agent typically includes the following steps:

1. Provide a prompt executor to communicate with the LLM.
1. Define a strategy that controls the agent workflow.
1. Configure agent behavior.
1. Implement tools for the agent to use.
1. Add optional features like event handling, memory, or tracing.
1. Run the agent with user input.

## Prerequisites

- You have a valid API key from the LLM provider used to implement an AI agent. For a list of all available providers, see [LLM providers](../llm-providers/).

Tip

Use environment variables or a secure configuration management system to store your API keys. Avoid hardcoding API keys directly in your source code.

## Creating a complex workflow agent

### 1. Add dependencies

To use the `AIAgent` functionality, include all necessary dependencies in your build configuration:

```text
dependencies {
    implementation("ai.koog:koog-agents:VERSION")
}
```

For all available installation methods, see [Install Koog](../getting-started/#install-koog).

### 2. Provide a prompt executor

Prompt executors manage and run prompts. You can choose a prompt executor based on the LLM provider you plan to use. Also, you can create a custom prompt executor using one of the available LLM clients. To learn more, see [Prompt executors](../prompts/prompt-executors/).

For example, to provide the OpenAI prompt executor, you need to call the `simpleOpenAIExecutor` function and provide it with the API key required for authentication with the OpenAI service:

```kotlin
val promptExecutor = simpleOpenAIExecutor(token)
```

To create a prompt executor that works with multiple LLM providers, do the following:

1. Configure clients for the required LLM providers with the corresponding API keys. For example:

```kotlin
val openAIClient = OpenAILLMClient(System.getenv("OPENAI_KEY"))
val anthropicClient = AnthropicLLMClient(System.getenv("ANTHROPIC_KEY"))
val googleClient = GoogleLLMClient(System.getenv("GOOGLE_KEY"))
```

2. Pass the configured clients to the `DefaultMultiLLMPromptExecutor` class constructor to create a prompt executor with multiple LLM providers:

```kotlin
val multiExecutor = DefaultMultiLLMPromptExecutor(openAIClient, anthropicClient, googleClient)
```

### 3. Define a strategy

A strategy defines the workflow of your agent by using nodes and edges. It can have arbitrary input and output types, which can be specified in `strategy` function generic parameters. These will be input/output types of the `AIAgent` as well. Default type for both input and output is `String`.

Tip

To learn more about strategies, see [Custom strategy graphs](../custom-strategy-graphs/)

#### 3.1. Understand nodes and edges

Nodes and edges are the building blocks of the strategy.

Nodes represent processing steps in your agent strategy.

```kotlin
val processNode by node<InputType, OutputType> { input ->
    // Process the input and return an output
    // You can use llm.writeSession to interact with the LLM
    // You can call tools using callTool, callToolRaw, etc.
    transformedOutput
}
```

Tip

There are also pre-defined nodes that you can use in your agent strategy. To learn more, see [Predefined nodes and components](../nodes-and-components/).

Edges define the connections between nodes.

```kotlin
// Basic edge
edge(sourceNode forwardTo targetNode)

// Edge with condition
edge(sourceNode forwardTo targetNode onCondition { output ->
    // Return true to follow this edge, false to skip it
    output.contains("specific text")
})

// Edge with transformation
edge(sourceNode forwardTo targetNode transformed { output ->
    // Transform the output before passing it to the target node
    "Modified: $output"
})

// Combined condition and transformation
edge(sourceNode forwardTo targetNode onCondition { it.isNotEmpty() } transformed { it.uppercase() })
```

#### 3.2. Implement the strategy

To implement the agent strategy, call the `strategy` function and define nodes and edges. For example:

```kotlin
val agentStrategy = strategy("Simple calculator") {
    // Define nodes for the strategy
    val nodeSendInput by nodeLLMRequest()
    val nodeExecuteTool by nodeExecuteTool()
    val nodeSendToolResult by nodeLLMSendToolResult()

    // Define edges between nodes
    // Start -> Send input
    edge(nodeStart forwardTo nodeSendInput)

    // Send input -> Finish
    edge(
        (nodeSendInput forwardTo nodeFinish)
                transformed { it }
                onAssistantMessage { true }
    )

    // Send input -> Execute tool
    edge(
        (nodeSendInput forwardTo nodeExecuteTool)
                onToolCall { true }
    )

    // Execute tool -> Send the tool result
    edge(nodeExecuteTool forwardTo nodeSendToolResult)

    // Send the tool result -> finish
    edge(
        (nodeSendToolResult forwardTo nodeFinish)
                transformed { it }
                onAssistantMessage { true }
    )
}
```

Tip

The `strategy` function lets you define multiple subgraphs, each containing its own set of nodes and edges. This approach offers more flexibility and functionality compared to using simplified strategy builders. To learn more about subgraphs, see [Subgraphs](../subgraphs-overview/).

### 4. Configure the agent

Define agent behavior with a configuration:

```kotlin
val agentConfig = AIAgentConfig.withSystemPrompt(
    prompt = """
        You are a simple calculator assistant.
        You can add two numbers together using the calculator tool.
        When the user provides input, extract the numbers they want to add.
        The input might be in various formats like "add 5 and 7", "5 + 7", or just "5 7".
        Extract the two numbers and use the calculator tool to add them.
        Always respond with a clear, friendly message showing the calculation and result.
        """.trimIndent()
)
```

For more advanced configuration, you can specify which LLM the agent will use and set the maximum number of iterations the agent can perform to respond:

```kotlin
val agentConfig = AIAgentConfig(
    prompt = Prompt.build("simple-calculator") {
        system(
            """
                You are a simple calculator assistant.
                You can add two numbers together using the calculator tool.
                When the user provides input, extract the numbers they want to add.
                The input might be in various formats like "add 5 and 7", "5 + 7", or just "5 7".
                Extract the two numbers and use the calculator tool to add them.
                Always respond with a clear, friendly message showing the calculation and result.
                """.trimIndent()
        )
    },
    model = OpenAIModels.Chat.GPT4o,
    maxAgentIterations = 10
)
```

### 5. Implement tools and set up a tool registry

Tools let your agent perform specific tasks. To make a tool available for the agent, add it to a tool registry. For example:

```kotlin
// Implement a simple calculator tool that can add two numbers
@LLMDescription("Tools for performing basic arithmetic operations")
class CalculatorTools : ToolSet {
    @Tool
    @LLMDescription("Add two numbers together and return their sum")
    fun add(
        @LLMDescription("First number to add (integer value)")
        num1: Int,

        @LLMDescription("Second number to add (integer value)")
        num2: Int
    ): String {
        val sum = num1 + num2
        return "The sum of $num1 and $num2 is: $sum"
    }
}

// Add the tool to the tool registry
val toolRegistry = ToolRegistry {
    tools(CalculatorTools())
}
```

To learn more about tools, see [Tools](../tools-overview/).

### 6. Install features

Features let you add new capabilities to the agent, modify its behavior, provide access to external systems and resources, and log and monitor events while the agent is running. The following features are available:

- EventHandler
- AgentMemory
- Tracing

To install the feature, call the `install` function and provide the feature as an argument. For example, to install the event handler feature, you need to do the following:

```kotlin
// install the EventHandler feature
installFeatures = {
    install(EventHandler) {
        onAgentStarting { eventContext: AgentStartingContext ->
            println("Starting agent: ${eventContext.agent.id}")
        }
        onAgentCompleted { eventContext: AgentCompletedContext ->
            println("Result: ${eventContext.result}")
        }
    }
}
```

To learn more about feature configuration, see the dedicated page.

### 7. Run the agent

Create the agent with the configuration option created in the previous stages and run it with the provided input:

```kotlin
val agent = AIAgent(
    promptExecutor = promptExecutor,
    toolRegistry = toolRegistry,
    strategy = agentStrategy,
    agentConfig = agentConfig,
    installFeatures = {
        install(EventHandler) {
            onAgentStarting { eventContext: AgentStartingContext ->
                println("Starting agent: ${eventContext.agent.id}")
            }
            onAgentCompleted { eventContext: AgentCompletedContext ->
                println("Result: ${eventContext.result}")
            }
        }
    }
)

fun main() {
    runBlocking {
        println("Enter two numbers to add (e.g., 'add 5 and 7' or '5 + 7'):")

        // Read the user input and send it to the agent
        val userInput = readlnOrNull() ?: ""
        val agentResult = agent.run(userInput)
        println("The agent returned: $agentResult")
    }
}
```

## Working with structured data

The `AIAgent` can process structured data from LLM outputs. For more details, see [Structured data processing](../structured-output/).

## Using parallel tool calls

The `AIAgent` supports parallel tool calls. This feature lets you process multiple tools concurrently, improving performance for independent operations.

For more details, see [Parallel tool calls](../tools-overview/#parallel-tool-calls).

## Full code sample

Here is the complete implementation of the agent:

```kotlin
// Use the OpenAI executor with an API key from an environment variable
val promptExecutor = simpleOpenAIExecutor(System.getenv("OPENAI_API_KEY"))

// Create a simple strategy
val agentStrategy = strategy("Simple calculator") {
    // Define nodes for the strategy
    val nodeSendInput by nodeLLMRequest()
    val nodeExecuteTool by nodeExecuteTool()
    val nodeSendToolResult by nodeLLMSendToolResult()

    // Define edges between nodes
    // Start -> Send input
    edge(nodeStart forwardTo nodeSendInput)

    // Send input -> Finish
    edge(
        (nodeSendInput forwardTo nodeFinish)
                transformed { it }
                onAssistantMessage { true }
    )

    // Send input -> Execute tool
    edge(
        (nodeSendInput forwardTo nodeExecuteTool)
                onToolCall { true }
    )

    // Execute tool -> Send the tool result
    edge(nodeExecuteTool forwardTo nodeSendToolResult)

    // Send the tool result -> finish
    edge(
        (nodeSendToolResult forwardTo nodeFinish)
                transformed { it }
                onAssistantMessage { true }
    )
}

// Configure the agent
val agentConfig = AIAgentConfig(
    prompt = Prompt.build("simple-calculator") {
        system(
            """
                You are a simple calculator assistant.
                You can add two numbers together using the calculator tool.
                When the user provides input, extract the numbers they want to add.
                The input might be in various formats like "add 5 and 7", "5 + 7", or just "5 7".
                Extract the two numbers and use the calculator tool to add them.
                Always respond with a clear, friendly message showing the calculation and result.
                """.trimIndent()
        )
    },
    model = OpenAIModels.Chat.GPT4o,
    maxAgentIterations = 10
)

// Implement a simple calculator tool that can add two numbers
@LLMDescription("Tools for performing basic arithmetic operations")
class CalculatorTools : ToolSet {
    @Tool
    @LLMDescription("Add two numbers together and return their sum")
    fun add(
        @LLMDescription("First number to add (integer value)")
        num1: Int,

        @LLMDescription("Second number to add (integer value)")
        num2: Int
    ): String {
        val sum = num1 + num2
        return "The sum of $num1 and $num2 is: $sum"
    }
}

// Add the tool to the tool registry
val toolRegistry = ToolRegistry {
    tools(CalculatorTools())
}

// Create the agent
val agent = AIAgent(
    promptExecutor = promptExecutor,
    toolRegistry = toolRegistry,
    strategy = agentStrategy,
    agentConfig = agentConfig,
    installFeatures = {
        install(EventHandler) {
            onAgentStarting { eventContext: AgentStartingContext ->
                println("Starting agent: ${eventContext.agent.id}")
            }
            onAgentCompleted { eventContext: AgentCompletedContext ->
                println("Result: ${eventContext.result}")
            }
        }
    }
)

fun main() {
    runBlocking {
        println("Enter two numbers to add (e.g., 'add 5 and 7' or '5 + 7'):")

        // Read the user input and send it to the agent
        val userInput = readlnOrNull() ?: ""
        val agentResult = agent.run(userInput)
        println("The agent returned: $agentResult")
    }
}
```
# Prompts

# Prompts

Prompts are instructions for Large Language Models (LLMs) that guide them in generating responses. They define the content and structure of your interactions with LLMs. This section describes how to create and run prompts with Koog.

## Creating prompts

In Koog, prompts are instances of the [**Prompt**](https://api.koog.ai/prompt/prompt-model/ai.koog.prompt.dsl/-prompt/index.html) data class with the following properties:

- `id`: A unique identifier for the prompt.
- `messages`: A list of messages that represent the conversation with the LLM.
- `params`: Optional [LLM configuration parameters](prompt-creation/#prompt-parameters) (such as temperature, tool choice, and others).

Although you can instantiate the `Prompt` class directly, the recommended way to create prompts is by using the [Kotlin DSL](prompt-creation/), which provides a structured way to define the conversation.

```kotlin
val myPrompt = prompt("hello-koog") {
    system("You are a helpful assistant.")
    user("What is Koog?")
}
```

Note

AI agents can take a simple text prompt as input. They automatically convert the text prompt to the Prompt object and send it to the LLM for execution. This is useful for a [basic agent](../basic-agents/) that only needs to run a single request and does not require complex conversation logic.

## Running prompts

Koog provides two levels of abstraction for running prompts against LLMs: LLM clients and prompt executors. Both accept Prompt objects and can be used for direct prompt execution, without an AI agent. The execution flow is the same for both clients and executors:

```
flowchart TB
    A([Prompt built with Kotlin DSL])
    B{LLM client or prompt executor}
    C[LLM provider]
    D([Response to your application])

    A -->|"passed to"| B
    B -->|"sends request"| C
    C -->|"returns response"| B
    B -->|"returns result"| D
```

- [**LLM clients**](llm-clients/)

  ______________________________________________________________________

  Lowâ€‘level interfaces for direct interaction with specific LLM providers. Use them when you work with a single provider and do not need advanced lifecycle management.

- [**Prompt executors**](prompt-executors/)

  ______________________________________________________________________

  High-level abstractions that manage the lifecycles of one or multiple LLM clients. Use them when you need a unified API for running prompts across multiple providers, with dynamic switching between them and fallbacks.

## Optimizing performance and handling failures

Koog allows you to optimize performance and handle failures when running prompts.

- [**LLM response caching**](llm-response-caching/)

  ______________________________________________________________________

  Cache LLM responses to optimize performance and reduce costs for repeated requests.

- [**Handling failures**](handling-failures/)

  ______________________________________________________________________

  Use built-in retries, timeouts, and other error handling mechanisms in your application.

## Prompts in AI agents

In Koog, AI agents maintain and manage prompts during their lifecycle. While LLM clients or executors are used to run prompts, agents handle the flow of prompt updates, ensuring the conversation history remains relevant and consistent.

The prompt lifecycle in an agent usually includes several stages:

1. Initial prompt setup.
1. Automatic prompt updates.
1. Context window management.
1. Manual prompt management.

### Initial prompt setup

When you [initialize an agent](../getting-started/#create-and-run-an-agent), you define a [system message](prompt-creation/#system-message) that sets the agent's behavior. Then, when you call the agent's `run()` method, you typically provide an initial [user message](prompt-creation/#user-messages) as input. Together, these messages form the agent's initial prompt. For example:

```kotlin
// Create an agent
val agent = AIAgent(
    promptExecutor = simpleOpenAIExecutor(apiKey),
    systemPrompt = "You are a helpful assistant.",
    llmModel = OpenAIModels.Chat.GPT4o
)

// Run the agent
val result = agent.run("What is Koog?")
```

In the example, the agent automatically converts the text prompt to the Prompt object and sends it to the prompt executor:

```
flowchart TB
    A([Your application])
    B{{Configured AI agent}}
    C["Text prompt"]
    D["Prompt object"]
    E{{Prompt executor}}
    F[LLM provider]

    A -->|"run() with text"| B
    B -->|"takes"| C
    C -->|"converted to"| D
    D -->|"sent via"| E
    E -->|"calls"| F
    F -->|"responds to"| E
    E -->|"result to"| B
    B -->|"result to"| A
```

For more [advanced configurations](../complex-workflow-agents/#4-configure-the-agent), you can also use [AIAgentConfig](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.agent.config/-a-i-agent-config/index.html) to define the agent's initial prompt.

### Automatic prompt updates

As the agent runs its strategy, [predefined nodes](../nodes-and-components/) automatically update the prompt. For example:

- [`nodeLLMRequest`](../nodes-and-components/#nodellmrequest): Appends a user message to the prompt and captures the LLM response.
- [`nodeLLMSendToolResult`](../nodes-and-components/#nodellmsendtoolresult): Appends tool execution results to the conversation.
- [`nodeAppendPrompt`](../nodes-and-components/#nodeappendprompt): Inserts specific messages into the prompt at any point in the workflow.

### Context window management

To avoid exceeding the LLM context window in long-running interactions, agents can use the [history compression](../history-compression/) feature.

### Manual prompt management

For complex workflows, you can manage the prompt manually using [LLM sessions](../sessions/). In an agent strategy or custom node, you can use `llm.writeSession` to access and change the `Prompt` object. This lets you add, remove, or reorder messages as needed.

# Creating prompts

Koog uses the type-safe Kotlin DSL to create prompts with control over message types, their order, and content.

These prompts let you pre-configure conversation history with multiple messages, provide multimodal content, examples, tool calls, and their results.

## Basic structure

The `prompt()` function creates a Prompt object with a unique ID and a list of messages:

```kotlin
val prompt = prompt("unique_prompt_id") {
    // List of messages
}
```

## Message types

The Kotlin DSL supports the following types of messages, each of which corresponds to a specific role in a conversation:

- **System message**: Provides the context, instructions, and constraints to the LLM, defining its behavior.
- **User message**: Represents the user input.
- **Assistant message**: Represents LLM responses that are used for few-shot learning or to continue the conversation.
- **Tool message**: Represents tool calls and their results.

```kotlin
val prompt = prompt("unique_prompt_id") {
    // Add a system message to set the context
    system("You are a helpful assistant with access to tools.")
    // Add a user message
    user("What is 5 + 3 ?")
    // Add an assistant message
    assistant("The result is 8.")
}
```

### System message

A system message defines the LLM behavior and sets the context for the entire conversation. It can specify the model's role, tone, provide guidelines and constraints on responses, and provide response examples.

To create the system message, provide a string to the `system()` function as an argument:

```kotlin
val prompt = prompt("assistant") {
    system("You are a helpful assistant that explains technical concepts.")
}
```

### User messages

A user message represents input from the user. To create the user message, provide a string to the `user()` function as an argument:

```kotlin
val prompt = prompt("question") {
    system("You are a helpful assistant.")
    user("What is Koog?")
}
```

Most user messages contain plain text, but they can also include multimodal content, such as images, audio, video, and documents. For details and examples, see [Multimodal content](multimodal-content/).

### Assistant messages

An assistant message represents an LLM response, which can be used for few-shot learning in future similar interactions, to continue a conversation, or to demonstrate the expected output structure.

To create the assistant message, provide a string to the `assistant()` function as an argument:

```kotlin
val prompt = prompt("article_review") {
    system("Evaluate the article.")

    // Example 1
    user("The article is clear and easy to understand.")
    assistant("positive")

    // Example 2
    user("The article is hard to read but it's clear and useful.")
    assistant("neutral")

    // Example 3
    user("The article is confusing and misleading.")
    assistant("negative")

    // New input to classify
    user("The article is interesting and helpful.")
}
```

### Tool messages

A tool message represents a tool call and its result, which can be used to pre-fill the history of tool calls.

Tip

An LLM generates tool calls during execution. Pre-filling them is helpful for few-shot learning or demonstrating how the tools are expected to be used.

To create the tool message, call the `tool()` function:

```kotlin
val prompt = prompt("calculator_example") {
    system("You are a helpful assistant with access to tools.")
    user("What is 5 + 3?")
    tool {
        // Tool call
        call(
            id = "calculator_tool_id",
            tool = "calculator",
            content = """{"operation": "add", "a": 5, "b": 3}"""
        )

        // Tool result
        result(
            id = "calculator_tool_id",
            tool = "calculator",
            content = "8"
        )
    }

    // LLM response based on tool result
    assistant("The result of 5 + 3 is 8.")
    user("What is 4 + 5?")
}
```

## Text message builders

When building a `system()`, `user()`, or `assistant()` message, you can use helper [text-building functions](https://api.koog.ai/prompt/prompt-model/ai.koog.prompt.text/-text-content-builder/index.html) for rich text formatting.

```kotlin
val prompt = prompt("text_example") {
    user {
        +"Review the following code snippet:"
        +"fun greet(name: String) = println(\"Hello, \$name!\")"

        // Paragraph break
        br()
        text("Please include in your explanation:")

        // Indent content
        padding("  ") {
            +"1. What the function does."
            +"2. How string interpolation works."
        }
    }
}
```

You can also use the [Markdown](https://api.koog.ai/prompt/prompt-markdown/ai.koog.prompt.markdown/markdown.html) and [XML](https://api.koog.ai/prompt/prompt-xml/ai.koog.prompt.xml/xml.html) builders to add the content in the corresponding format.

```kotlin
val prompt = prompt("markdown_xml_example") {
    // A user message in Markdown format
    user {
        markdown {
            h2("Evaluate the article using the following criteria:")
            bulleted {
                item { +"Clarity and readability" }
                item { +"Accuracy of information" }
                item { +"Usefulness to the reader" }
            }
        }
    }
    // An assistant message in XML format
    assistant {
        xml {
            xmlDeclaration()
            tag("review") {
                tag("clarity") { text("positive") }
                tag("accuracy") { text("neutral") }
                tag("usefulness") { text("positive") }
            }
        }
    }
}
```

Tip

You can mix the text building functions with the XML and Markdown builders.

## Prompt parameters

Prompts can be customized by configuring parameters that control the LLM's behavior.

```kotlin
val prompt = prompt(
    id = "custom_params",
    params = LLMParams(
        temperature = 0.7,
        numberOfChoices = 1,
        toolChoice = LLMParams.ToolChoice.Auto
    )
) {
    system("You are a creative writing assistant.")
    user("Write a song about winter.")
}
```

The following parameters are supported:

- `temperature`: Controls randomness in the model's responses.
- `toolChoice`: Controls tool calling behavior of the model.
- `numberOfChoices`: Requests multiple alternative responses.
- `schema`: Defines the structure for the model's response format.
- `maxTokens`: Limits the number of tokens in the response.
- `speculation`: Provides a hint about the expected response format (only supported by specific models).

For more information, see [LLM parameters](../../llm-parameters/).

## Extending existing prompts

You can extend an existing prompt by calling the `prompt()` function with the existing prompt as an argument:

```kotlin
val basePrompt = prompt("base") {
    system("You are a helpful assistant.")
    user("Hello!")
    assistant("Hi! How can I help you?")
}

val extendedPrompt = prompt(basePrompt) {
    user("What's the weather like?")
}
```

This creates a new prompt that includes all messages from `basePrompt` and the new user message.

## Next steps

- Learn how to work with [multimodal content](multimodal-content/).
- Run prompts with [LLM clients](../llm-clients/) if you work with a single LLM provider.
- Run prompts with [prompt executors](../prompt-executors/) if you work with multiple LLM providers.

# Multimodal content

Multimodal content refers to content of different types, such as text, images, audio, video, and files. Koog lets you send images, audio, video, and files to LLMs within the `user` message along with text. You can add them to the `user` message by using the corresponding functions:

- `image()`: Attaches images (JPG, PNG, WebP, GIF).
- `audio()`: Attaches audio files (MP3, WAV, FLAC).
- `video()`: Attaches video files (MP4, AVI, MOV).
- `file()` / `binaryFile()` / `textFile()`: Attaches documents (PDF, TXT, MD, etc.).

Each function supports two ways of configuring attachment parameters, so you can:

- Pass a URL or a file path to the function, and it automatically handles attachment parameters. For `file()`, `binaryFile()`, and `textFile()`, you must also provide the MIME type.
- Create and pass a `ContentPart` object to the function for custom control over attachment parameters.

Note

Multimodal content support varies by [LLM provider](../llm-providers.md). Check the provider documentation for supported content types.

### Auto-configured attachments

If you pass a URL or a file path to the attachment functions, Koog automatically constructs the corresponding attachment parameters based on the file extension.

The general format of the `user` message that includes a text message and a list of auto-configured attachments is as follows:

```kotlin
user {
    +"Describe these images:"

    image("https://example.com/test.png")
    image(Path("/path/to/image.png"))

    +"Focus on the main subjects."
}
```

The `+` operator adds text content to the user message along with the attachments.

### Custom-configured attachments

The [`ContentPart`](https://api.koog.ai/prompt/prompt-model/ai.koog.prompt.message/-content-part/index.html) interface lets you configure parameters for each attachment individually.

All attachments implement the `ContentPart.Attachment` interface. You can create an instance of a specific implementation for each attachment, configure its parameters, and pass it to the corresponding `image()`, `audio()`, `video()`, or `file()` functions.

The general format of the `user` message that includes a text message and a list of custom-configured attachments is as follows:

```kotlin
user {
    +"Describe this image"
    image(
        ContentPart.Image(
            content = AttachmentContent.URL("https://example.com/capture.png"),
            format = "png",
            mimeType = "image/png",
            fileName = "capture.png"
        )
    )
}
```

Koog provides the following specialized classes for each media type that implement the `ContentPart.Attachment` interface:

- [`ContentPart.Image`](https://api.koog.ai/prompt/prompt-model/ai.koog.prompt.message/-content-part/-image/index.html): image attachments, such as JPG or PNG files.
- [`ContentPart.Audio`](https://api.koog.ai/prompt/prompt-model/ai.koog.prompt.message/-content-part/-audio/index.html): audio attachments, such as MP3 or WAV files.
- [`ContentPart.Video`](https://api.koog.ai/prompt/prompt-model/ai.koog.prompt.message/-content-part/-video/index.html): video attachments, such as MP4 or AVI files.
- [`ContentPart.File`](https://api.koog.ai/prompt/prompt-model/ai.koog.prompt.message/-content-part/-file/index.html): file attachments, such as PDF or TXT files.

All `ContentPart.Attachment` types accept the following parameters:

| Name       | Data type                                                                                                          | Required                    | Description                                                                                                                                                                                                                     |
| ---------- | ------------------------------------------------------------------------------------------------------------------ | --------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `content`  | [AttachmentContent](https://api.koog.ai/prompt/prompt-model/ai.koog.prompt.message/-attachment-content/index.html) | Yes                         | The source of the provided file content.                                                                                                                                                                                        |
| `format`   | String                                                                                                             | Yes                         | The format of the provided file. For example, `png`.                                                                                                                                                                            |
| `mimeType` | String                                                                                                             | Only for `ContentPart.File` | The MIME Type of the provided file. For `ContentPart.Image`, `ContentPart.Audio`, and `ContentPart.Video`, it defaults to `<type>/<format>` (for example, `image/png`). For `ContentPart.File`, it must be explicitly provided. |
| `fileName` | String?                                                                                                            | No                          | The name of the provided file including the extension. For example, `screenshot.png`.                                                                                                                                           |

#### Attachment content

Implementations of the AttachmentContent interface define the type and source of content that is provided as input to the LLM:

- [`AttachmentContent.URL`](https://api.koog.ai/prompt/prompt-model/ai.koog.prompt.message/-attachment-content/-u-r-l/index.html) defines the URL of the provided content:

  ```kotlin
  AttachmentContent.URL("https://example.com/image.png")
  ```

- [`AttachmentContent.Binary.Bytes`](https://api.koog.ai/prompt/prompt-model/ai.koog.prompt.message/-attachment-content/-binary/index.html) defines the file content as a byte array:

  ```kotlin
  AttachmentContent.Binary.Bytes(byteArrayOf(/* ... */))
  ```

- [`AttachmentContent.Binary.Base64`](https://api.koog.ai/prompt/prompt-model/ai.koog.prompt.message/-attachment-content/-binary/index.html) defines the file content as a Base64-encoded string containing file data:

  ```kotlin
  AttachmentContent.Binary.Base64("iVBORw0KGgoAAAANS...")
  ```

- [`AttachmentContent.PlainText`](https://api.koog.ai/prompt/prompt-model/ai.koog.prompt.message/-attachment-content/-plain-text/index.html) defines the file content as plain text (for [`ContentPart.File`](https://api.koog.ai/prompt/prompt-model/ai.koog.prompt.message/-content-part/-file/index.html) only):

  ```kotlin
  AttachmentContent.PlainText("This is the file content.")
  ```

### Mixed attachments

In addition to providing different types of attachments in separate prompts or messages, you can also provide multiple and mixed types of attachments in a single `user()` message:

```kotlin
val prompt = prompt("mixed_content") {
    system("You are a helpful assistant.")

    user {
        +"Compare the image with the document content."
        image(Path("/path/to/image.png"))
        binaryFile(Path("/path/to/page.pdf"), "application/pdf")
        +"Structure the result as a table"
    }
}
```

## Next steps

- Run prompts with [LLM clients](../../llm-clients/) if you work with a single LLM provider.
- Run prompts with [prompt executors](../../prompt-executors/) if you work with multiple LLM providers.

# Handling failures

This page describes how to handle failures for LLM clients and prompt executors using the built-in retry and timeout mechanisms.

## Retry functionality

When working with LLM providers, transient errors like rate limits or temporary service unavailability may occur. The `RetryingLLMClient` decorator adds automatic retry logic to any LLM client.

### Basic usage

Wrap any existing client with the retry capability:

```kotlin
// Wrap any client with the retry capability
val client = OpenAILLMClient(apiKey)
val resilientClient = RetryingLLMClient(client)

// Now all operations will automatically retry on transient errors
val response = resilientClient.execute(prompt, OpenAIModels.Chat.GPT4o)
```

### Configuring retry behavior

By default, `RetryingLLMClient` configures an LLM client with the maximum of 3 retry attempts, a 1-second initial delay, and a 30-second maximum delay. You can specify a different retry configuration using a `RetryConfig` passed to `RetryingLLMClient`. For example:

```kotlin
// Use the predefined configuration
val conservativeClient = RetryingLLMClient(
    delegate = client,
    config = RetryConfig.CONSERVATIVE
)
```

Koog provides several predefined retry configurations:

| Configuration              | Max attempts | Initial delay | Max delay | Use case                                                                                                 |
| -------------------------- | ------------ | ------------- | --------- | -------------------------------------------------------------------------------------------------------- |
| `RetryConfig.DISABLED`     | 1 (no retry) | -             | -         | Development, testing, and debugging.                                                                     |
| `RetryConfig.CONSERVATIVE` | 3            | 2s            | 30s       | Background or scheduled tasks where reliability is more important than speed.                            |
| `RetryConfig.AGGRESSIVE`   | 5            | 500ms         | 20s       | Critical operations where fast recovery from transient errors is more important than reducing API calls. |
| `RetryConfig.PRODUCTION`   | 3            | 1s            | 20s       | General production use.                                                                                  |

You can use them directly or create custom configurations:

```kotlin
// Or create a custom configuration
val customClient = RetryingLLMClient(
    delegate = client,
    config = RetryConfig(
        maxAttempts = 5,
        initialDelay = 1.seconds,
        maxDelay = 30.seconds,
        backoffMultiplier = 2.0,
        jitterFactor = 0.2
    )
)
```

### Retry error patterns

By default, the `RetryingLLMClient` recognizes common transient errors. This behavior is controlled by the [`RetryConfig.retryablePatterns`](https://api.koog.ai/prompt/prompt-executor/prompt-executor-clients/ai.koog.prompt.executor.clients.retry/-retry-config/retryable-patterns.html) patterns. Each pattern is represented by [`RetryablePattern`](https://api.koog.ai/prompt/prompt-executor/prompt-executor-clients/ai.koog.prompt.executor.clients.retry/-retryable-pattern/index.html) that checks the error message from a failed request and determines whether it should be retried.

Koog provides the predefined retry configurations and patterns that work across all the supported LLM providers. You can keep the defaults or customize them for your specific needs.

#### Pattern types

You can use the following pattern types and combine any number of them:

- `RetryablePattern.Status`: Matches a specific HTTP status code in the error message (such as `429`, `500`,`502`, etc.).
- `RetryablePattern.Keyword`: Matches a keyword in the error message (such as `rate limit` or `request timeout`).
- `RetryablePattern.Regex`: Matches a regular expression in the error message.
- `RetryablePattern.Custom`: Matches a custom logic using a lambda function.

If any pattern returns `true`, the error is considered retryable, and the LLM client retries the request.

#### Default patterns

Unless you customize the retry configuration, the following patterns are used by default:

- **HTTP status codes**:

  - `429`: Rate limit
  - `500`: Internal server error
  - `502`: Bad gateway
  - `503`: Service unavailable
  - `504`: Gateway timeout
  - `529`: Anthropic overloaded

- **Error keywords**:

  - rate limit
  - too many requests
  - request timeout
  - connection timeout
  - read timeout
  - write timeout
  - connection reset by peer
  - connection refused
  - temporarily unavailable
  - service unavailable

These default patterns are defined in Koog as [`RetryConfig.DEFAULT_PATTERNS`](https://api.koog.ai/prompt/prompt-executor/prompt-executor-clients/ai.koog.prompt.executor.clients.retry/-retry-config/-companion/-d-e-f-a-u-l-t_-p-a-t-t-e-r-n-s.html).

#### Custom patterns

You can define custom patterns for your specific needs:

```kotlin
val config = RetryConfig(
    retryablePatterns = listOf(
        RetryablePattern.Status(429),   // Specific status code
        RetryablePattern.Keyword("quota"),  // Keyword in error message
        RetryablePattern.Regex(Regex("ERR_\\d+")),  // Custom regex pattern
        RetryablePattern.Custom { error ->  // Custom logic
            error.contains("temporary") && error.length > 20
        }
    )
)
```

You can also append custom patterns to the default `RetryConfig.DEFAULT_PATTERNS`:

```kotlin
val config = RetryConfig(
    retryablePatterns = RetryConfig.DEFAULT_PATTERNS + listOf(
        RetryablePattern.Keyword("custom_error")
    )
)
```

### Streaming with retry

Streaming operations can optionally be retried. This feature is disabled by default.

```kotlin
val config = RetryConfig(
    maxAttempts = 3
)

val client = RetryingLLMClient(baseClient, config)
val stream = client.executeStreaming(prompt, OpenAIModels.Chat.GPT4o)
```

Note

Streaming retries only apply to connection failures that occur before the first token is received. Once streaming has started, the retry logic is disabled. If an error occurs during streaming, the operation is terminated.

### Retry with prompt executors

When working with prompt executors, you can wrap the underlying LLM client with a retry mechanism before creating the executor. To learn more about prompt executors, see [Prompt executors](../prompt-executors/).

```kotlin
// Single provider executor with retry
val resilientClient = RetryingLLMClient(
    OpenAILLMClient(System.getenv("OPENAI_API_KEY")),
    RetryConfig.PRODUCTION
)
val executor = SingleLLMPromptExecutor(resilientClient)

// Multi-provider executor with flexible client configuration
val multiExecutor = MultiLLMPromptExecutor(
    LLMProvider.OpenAI to RetryingLLMClient(
        OpenAILLMClient(System.getenv("OPENAI_API_KEY")),
        RetryConfig.CONSERVATIVE
    ),
    LLMProvider.Anthropic to RetryingLLMClient(
        AnthropicLLMClient(System.getenv("ANTHROPIC_API_KEY")),
        RetryConfig.AGGRESSIVE  
    ),
    // The Bedrock client already has a built-in AWS SDK retry 
    LLMProvider.Bedrock to BedrockLLMClient(
        identityProvider = StaticCredentialsProvider {
            accessKeyId = System.getenv("AWS_ACCESS_KEY_ID")
            secretAccessKey = System.getenv("AWS_SECRET_ACCESS_KEY")
            sessionToken = System.getenv("AWS_SESSION_TOKEN")
        },
    ),
)
```

## Timeout configuration

All LLM clients support timeout configuration to prevent hanging requests. You can specify timeout values for network connections when creating the client using the [`ConnectionTimeoutConfig`](https://api.koog.ai/prompt/prompt-executor/prompt-executor-clients/ai.koog.prompt.executor.clients/-connection-timeout-config/index.html) class.

`ConnectionTimeoutConfig` has the following properties:

| Property               | Default Value        | Description                                                   |
| ---------------------- | -------------------- | ------------------------------------------------------------- |
| `connectTimeoutMillis` | 60 seconds (60,000)  | Maximum time to establish a connection to the server.         |
| `requestTimeoutMillis` | 15 minutes (900,000) | Maximum time for the entire request to complete.              |
| `socketTimeoutMillis`  | 15 minutes (900,000) | Maximum time to wait for data over an established connection. |

You can customize these values for your specific needs. For example:

```kotlin
val client = OpenAILLMClient(
    apiKey = apiKey,
    settings = OpenAIClientSettings(
        timeoutConfig = ConnectionTimeoutConfig(
            connectTimeoutMillis = 5000,    // 5 seconds to establish connection
            requestTimeoutMillis = 60000,    // 60 seconds for the entire request
            socketTimeoutMillis = 120000   // 120 seconds for data on the socket
        )
    )
)
```

Tip

For long-running or streaming calls, set higher values for `requestTimeoutMillis` and `socketTimeoutMillis`.

## Error handling

When working with LLMs in production, you need to implement error handling, including:

- **Try-catch blocks** to handle unexpected errors.
- **Logging errors with context** for debugging.
- **Fallbacks** for critical operations.
- **Monitoring retry patterns** to identify recurring issues.

Here is an example of error handling:

```kotlin
fun main() {
    runBlocking {
        val logger = LoggerFactory.getLogger("Example")
        val resilientClient = RetryingLLMClient(
            OpenAILLMClient(System.getenv("OPENAI_API_KEY")),
            RetryConfig.PRODUCTION
        )
        val prompt = prompt("test") { user("Hello") }
        val model = OpenAIModels.Chat.GPT4o

        fun processResponse(response: Any) { /* implmenentation */ }
        fun scheduleRetryLater() { /* implmenentation */ }
        fun notifyAdministrator() { /* implmenentation */ }
        fun useDefaultResponse() { /* implmenentation */ }

        try {
            val response = resilientClient.execute(prompt, model)
            processResponse(response)
        } catch (e: Exception) {
            logger.error("LLM operation failed", e)

            when {
                e.message?.contains("rate limit") == true -> {
                    // Handle rate limiting specifically
                    scheduleRetryLater()
                }
                e.message?.contains("invalid api key") == true -> {
                    // Handle authentication errors
                    notifyAdministrator()
                }
                else -> {
                    // Fall back to an alternative solution
                    useDefaultResponse()
                }
            }
        }
    }
}
```

# LLM response caching

For repeated requests that you run with a prompt executor, you can cache LLM responses to optimize performance and reduce costs. In Koog, caching is available for all prompt executors through `CachedPromptExecutor`, which is a wrapper around `PromptExecutor` that adds caching functionality. It lets you store responses from previously executed prompts and retrieve them when the same prompts are run again.

To create a cached prompt executor, perform the following:

1. Create a prompt executor for which you want to cache responses.
1. Create a `CachedPromptExecutor` instance by providing the desired cache and the prompt executor you created.
1. Run the created `CachedPromptExecutor` with the desired prompt and model.

Here is an example:

```kotlin
// Create a prompt executor
val client = OpenAILLMClient(System.getenv("OPENAI_API_KEY"))
val promptExecutor = SingleLLMPromptExecutor(client)

// Create a cached prompt executor
val cachedExecutor = CachedPromptExecutor(
    cache = FilePromptCache(Path("path/to/your/cache/directory")),
    nested = promptExecutor
)

// Run cached prompt executor for the first time
// This will perform an actual LLM request
val firstTime = measureTimeMillis {
    val firstResponse = cachedExecutor.execute(prompt, OpenAIModels.Chat.GPT4o)
    println("First response: ${firstResponse.first().content}")
}
println("First execution took: ${firstTime}ms")

// Run cached prompt executor for the second time
// This will return the result immediately from the cache
val secondTime = measureTimeMillis {
    val secondResponse = cachedExecutor.execute(prompt, OpenAIModels.Chat.GPT4o)
    println("Second response: ${secondResponse.first().content}")
}
println("Second execution took: ${secondTime}ms")
```

The example produces the following output:

```text
First response: Hello! It seems like we're starting a new conversation. What can I help you with today?
First execution took: 48ms
Second response: Hello! It seems like we're starting a new conversation. What can I help you with today?
Second execution took: 1ms
```

The second response is retrieved from the cache, which took only 1ms.

Note

- If you call `executeStreaming()` with the cached prompt executor, it produces a response as a single chunk.
- If you call `moderate()` with the cached prompt executor, it forwards the request to the nested prompt executor and does not use the cache.
- Caching of multiple choice responses (`executeMultipleChoice()`) is not supported.
# Running prompts

# LLM clients

LLM clients are designed for direct interaction with LLM providers. Each client implements the [`LLMClient`](https://api.koog.ai/prompt/prompt-executor/prompt-executor-clients/ai.koog.prompt.executor.clients/-l-l-m-client/index.html) interface, which provides methods for executing prompts and streaming responses.

You can use an LLM client when you work with a single LLM provider and don't need advanced lifecycle management. If you need to manage multiple LLM providers, use a [prompt executor](../prompt-executors/).

The table below shows the available LLM clients and their capabilities.

| LLM provider                                        | LLMClient                                                                                                                                                                                                   | Tool calling | Streaming | Multiple choices | Embeddings | Moderation  | Model listing | Notes                                                                                                                       |
| --------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------ | --------- | ---------------- | ---------- | ----------- | ------------- | --------------------------------------------------------------------------------------------------------------------------- |
| [OpenAI](https://platform.openai.com/docs/overview) | [OpenAILLMClient](https://api.koog.ai/prompt/prompt-executor/prompt-executor-clients/prompt-executor-openai-client/ai.koog.prompt.executor.clients.openai/-open-a-i-l-l-m-client/index.html)                | âœ“            | âœ“         | âœ“                | âœ“          | âœ“[1](#fn:1) | âœ“             |                                                                                                                             |
| [Anthropic](https://www.anthropic.com/)             | [AnthropicLLMClient](https://api.koog.ai/prompt/prompt-executor/prompt-executor-clients/prompt-executor-anthropic-client/ai.koog.prompt.executor.clients.anthropic/-anthropic-l-l-m-client/index.html)      | âœ“            | âœ“         | -                | -          | -           | -             | -                                                                                                                           |
| [Google](https://ai.google.dev/)                    | [GoogleLLMClient](https://api.koog.ai/prompt/prompt-executor/prompt-executor-clients/prompt-executor-google-client/ai.koog.prompt.executor.clients.google/-google-l-l-m-client/index.html)                  | âœ“            | âœ“         | âœ“                | âœ“          | -           | âœ“             | -                                                                                                                           |
| [DeepSeek](https://www.deepseek.com/)               | [DeepSeekLLMClient](https://api.koog.ai/prompt/prompt-executor/prompt-executor-clients/prompt-executor-deepseek-client/ai.koog.prompt.executor.clients.deepseek/-deep-seek-l-l-m-client/index.html)         | âœ“            | âœ“         | âœ“                | -          | -           | âœ“             | OpenAI-compatible chat client.                                                                                              |
| [OpenRouter](https://openrouter.ai/)                | [OpenRouterLLMClient](https://api.koog.ai/prompt/prompt-executor/prompt-executor-clients/prompt-executor-openrouter-client/ai.koog.prompt.executor.clients.openrouter/-open-router-l-l-m-client/index.html) | âœ“            | âœ“         | âœ“                | -          | -           | âœ“             | OpenAI-compatible router client.                                                                                            |
| [Amazon Bedrock](https://aws.amazon.com/bedrock/)   | [BedrockLLMClient](https://api.koog.ai/prompt/prompt-executor/prompt-executor-clients/prompt-executor-bedrock-client/ai.koog.prompt.executor.clients.bedrock/-bedrock-l-l-m-client/index.html)              | âœ“            | âœ“         | -                | âœ“          | âœ“[2](#fn:2) | -             | JVM-only AWS SDK client that supports multiple model families.                                                              |
| [Mistral](https://mistral.ai/)                      | [MistralAILLMClient](https://api.koog.ai/prompt/prompt-executor/prompt-executor-clients/prompt-executor-mistralai-client/ai.koog.prompt.executor.clients.mistralai/-mistral-a-i-l-l-m-client/index.html)    | âœ“            | âœ“         | âœ“                | âœ“          | âœ“[3](#fn:3) | âœ“             | OpenAI-compatible client.                                                                                                   |
| [Alibaba](https://www.alibabacloud.com/en?_p_lc=1)  | [DashScopeLLMClient](https://api.koog.ai/prompt/prompt-executor/prompt-executor-clients/prompt-executor-dashscope-client/ai.koog.prompt.executor.clients.dashscope/-dashscope-l-l-m-client/index.html)      | âœ“            | âœ“         | âœ“                | -          | -           | âœ“             | OpenAI-compatible client that exposes provider-specific parameters (`enableSearch`, `parallelToolCalls`, `enableThinking`). |
| [Ollama](https://ollama.com/)                       | [OllamaClient](https://api.koog.ai/prompt/prompt-executor/prompt-executor-clients/prompt-executor-ollama-client/ai.koog.prompt.executor.ollama.client/-ollama-client/index.html)                            | âœ“            | âœ“         | -                | âœ“          | âœ“           | -             | Local server client with model management APIs.                                                                             |

## Running a prompt

To run a prompt using an LLM client, perform the following:

1. Create an LLM client that handles the connection between your application and LLM providers.
1. Call the `execute()` method with the prompt and LLM as arguments.

Here is an example that uses `OpenAILLMClient` to run prompts:

```kotlin
fun main() = runBlocking {
    // Create an OpenAI client
    val token = System.getenv("OPENAI_API_KEY")
    val client = OpenAILLMClient(token)

    // Create a prompt
    val prompt = prompt("prompt_name", LLMParams()) {
        // Add a system message to set the context
        system("You are a helpful assistant.")

        // Add a user message
        user("Tell me about Kotlin")

        // You can also add assistant messages for few-shot examples
        assistant("Kotlin is a modern programming language...")

        // Add another user message
        user("What are its key features?")
    }

    // Run the prompt
    val response = client.execute(prompt, OpenAIModels.Chat.GPT4o)
    // Print the response
    println(response)
}
```

## Streaming responses

Note

Available for all LLM clients.

When you need to process responses as they are generated, you can use the `executeStreaming()` method to stream the model output:

```kotlin
// Set up the OpenAI client with your API key
val token = System.getenv("OPENAI_API_KEY")
val client = OpenAILLMClient(token)

val response = client.executeStreaming(
    prompt = prompt("stream_demo") { user("Stream this response in short chunks.") },
    model = OpenAIModels.Chat.GPT4_1
)

response.collect { event ->
    when (event) {
        is StreamFrame.Append -> println(event.text)
        is StreamFrame.ToolCall -> println("\nTool call: ${event.name}")
        is StreamFrame.End -> println("\n[done] Reason: ${event.finishReason}")
    }
}
```

## Multiple choices

Note

Available for all LLM clients except `GoogleLLMClient`, `BedrockLLMClient`, and `OllamaClient`

You can request multiple alternative responses from the model in a single call by using the `executeMultipleChoices()` method. It requires additionally specifying the [`numberOfChoices`](../prompt-creation/#prompt-parameters) LLM parameter in the prompt being executed.

```kotlin
fun main() = runBlocking {
    val apiKey = System.getenv("OPENAI_API_KEY")
    val client = OpenAILLMClient(apiKey)

    val choices = client.executeMultipleChoices(
        prompt = prompt("n_best", params = LLMParams(numberOfChoices = 3)) {
            system("You are a creative assistant.")
            user("Give me three different opening lines for a story.")
        },
        model = OpenAIModels.Chat.GPT4o
    )

    choices.forEachIndexed { i, choice ->
        val text = choice.joinToString(" ") { it.content }
        println("Line #${i + 1}: $text")
    }
}
```

## Listing available models

Note

Available for all LLM clients except `GoogleLLMClient`, `BedrockLLMClient`, and `OllamaClient`.

To get a list of available model IDs supported by the LLM client, use the `models()` method:

```kotlin
fun main() = runBlocking {
    val apiKey = System.getenv("OPENAI_API_KEY")
    val client = OpenAILLMClient(apiKey)

    val ids: List<String> = client.models()
    ids.forEach { println(it) }
}
```

## Embeddings

Note

Available for `OpenAILLMClient`, `GoogleLLMClient`, `BedrockLLMClient`, `MistralAILLMClient`, and `OllamaClient`.

You convert text into embedding vectors using the `embed()` method. Choose an embedding model and pass your text to this method:

```kotlin
fun main() = runBlocking {
    val apiKey = System.getenv("OPENAI_API_KEY")
    val client = OpenAILLMClient(apiKey)

    val embedding = client.embed(
        text = "This is a sample text for embedding",
        model = OpenAIModels.Embeddings.TextEmbedding3Large
    )

    println("Embedding size: ${embedding.size}")
}
```

## Moderation

Note

Available for the following LLM clients: `OpenAILLMClient`, `BedrockLLMClient`, `MistralAILLMClient`, `OllamaClient`.

You can use the `moderate()` method with a moderation model to check whether a prompt contains inappropriate content:

```kotlin
fun main() = runBlocking {
    val apiKey = System.getenv("OPENAI_API_KEY")
    val client = OpenAILLMClient(apiKey)

    val result = client.moderate(
        prompt = prompt("moderation") {
            user("This is a test message that may contain offensive content.")
        },
        model = OpenAIModels.Moderation.Omni
    )

    println(result)
}
```

## Integration with prompt executors

[Prompt executors](../prompt-executors/) wrap LLM clients and provide additional functionality, such as routing, fallbacks, and unified usage across providers. They are recommended for production use, as they offer flexibility when working with multiple providers.

______________________________________________________________________

1. Supports moderation via the OpenAI Moderation API.Â [â†©](#fnref:1 "Jump back to footnote 1 in the text")
1. Moderation requires Guardrails configuration.Â [â†©](#fnref:2 "Jump back to footnote 2 in the text")
1. Supports moderation via the Mistral `v1/moderations` endpoint.Â [â†©](#fnref:3 "Jump back to footnote 3 in the text")

# Prompt executors

Prompt executors provide a higher-level abstraction that lets you manage the lifecycle of one or multiple LLM clients. You can work with multiple LLM providers through a unified interface, abstracting from provider-specific details, with dynamic switching between them and fallbacks.

## Executor types

Koog provides two main types of prompt executors that implement the [`PromptExecutor`](https://api.koog.ai/prompt/prompt-executor/prompt-executor-model/ai.koog.prompt.executor.model/-prompt-executor/index.html) interface:

| Type            | Class                                                                                                                                                              | Description                                                                                                                                                                                                                                                          |
| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Single-provider | [`SingleLLMPromptExecutor`](https://api.koog.ai/prompt/prompt-executor/prompt-executor-llms/ai.koog.prompt.executor.llms/-single-l-l-m-prompt-executor/index.html) | Wraps a single LLM client for one provider. Use this executor if your agent only requires switching between models within a single LLM provider.                                                                                                                     |
| Multi-provider  | [`MultiLLMPromptExecutor`](https://api.koog.ai/prompt/prompt-executor/prompt-executor-llms/ai.koog.prompt.executor.llms/-multi-l-l-m-prompt-executor/index.html)   | Wraps multiple LLM clients and routes calls based on the LLM provider. It can optionally use a configured fallback provider and LLM when the requested client is unavailable. Use this executor if your agent needs to switch between LLMs from different providers. |

## Creating a single-provider executor

To create a prompt executor for a specific LLM provider, perform the following:

1. Configure an LLM client for a specific provider with the corresponding API key.
1. Create a prompt executor using [`SingleLLMPromptExecutor`](https://api.koog.ai/prompt/prompt-executor/prompt-executor-llms/ai.koog.prompt.executor.llms/-single-l-l-m-prompt-executor/index.html).

Here is an example:

```kotlin
val openAIClient = OpenAILLMClient(System.getenv("OPENAI_KEY"))
val promptExecutor = SingleLLMPromptExecutor(openAIClient)
```

## Creating a multi-provider executor

To create a prompt executor that works with multiple LLM providers, do the following:

1. Configure clients for the required LLM providers with the corresponding API keys.
1. Pass the configured clients to the [`MultiLLMPromptExecutor`](https://api.koog.ai/prompt/prompt-executor/prompt-executor-llms/ai.koog.prompt.executor.llms/-multi-l-l-m-prompt-executor/index.html) class constructor to create a prompt executor with multiple LLM providers.

```kotlin
val openAIClient = OpenAILLMClient(System.getenv("OPENAI_KEY"))
val ollamaClient = OllamaClient()

val multiExecutor = MultiLLMPromptExecutor(
    LLMProvider.OpenAI to openAIClient,
    LLMProvider.Ollama to ollamaClient
)
```

## Pre-defined prompt executors

For faster setup, Koog provides the ready-to-use executor implementations for common providers.

The following table includes the **pre-defined single-provider executors** that return `SingleLLMPromptExecutor` configured with a specific LLM client.

| LLM provider   | Prompt executor                                                                                                                                                                             | Description                                                                      |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| OpenAI         | [simpleOpenAIExecutor](https://api.koog.ai/prompt/prompt-executor/prompt-executor-llms-all/ai.koog.prompt.executor.llms.all/simple-open-a-i-executor.html)                                  | Wraps `OpenAILLMClient` that runs prompts with OpenAI models.                    |
| OpenAI         | [simpleAzureOpenAIExecutor](https://api.koog.ai/prompt/prompt-executor/prompt-executor-llms-all/ai.koog.prompt.executor.llms.all/simple-azure-open-a-i-executor.html)                       | Wraps `OpenAILLMClient` configured for using Azure OpenAI Service.               |
| Anthropic      | [simpleAnthropicExecutor](https://api.koog.ai/prompt/prompt-executor/prompt-executor-llms-all/ai.koog.prompt.executor.llms.all/simple-anthropic-executor.html)                              | Wraps `AnthropicLLMClient` that runs prompts with Anthropic models.              |
| Google         | [simpleGoogleAIExecutor](https://api.koog.ai/prompt/prompt-executor/prompt-executor-llms-all/ai.koog.prompt.executor.llms.all/simple-google-a-i-executor.html)                              | Wraps `GoogleLLMClient` that runs prompts with Google models.                    |
| OpenRouter     | [simpleOpenRouterExecutor](https://api.koog.ai/prompt/prompt-executor/prompt-executor-llms-all/ai.koog.prompt.executor.llms.all/simple-open-router-executor.html)                           | Wraps `OpenRouterLLMClient` that runs prompts with OpenRouter.                   |
| Amazon Bedrock | [simpleBedrockExecutor](https://api.koog.ai/prompt/prompt-executor/prompt-executor-llms-all/ai.koog.prompt.executor.llms.all/simple-bedrock-executor.html)                                  | Wraps `BedrockLLMClient` that runs prompts with AWS Bedrock.                     |
| Amazon Bedrock | [simpleBedrockExecutorWithBearerToken](https://api.koog.ai/prompt/prompt-executor/prompt-executor-llms-all/ai.koog.prompt.executor.llms.all/simple-bedrock-executor-with-bearer-token.html) | Wraps `BedrockLLMClient` and uses the provided Bedrock API key to send requests. |
| Mistral        | [simpleMistralAIExecutor](https://api.koog.ai/prompt/prompt-executor/prompt-executor-llms-all/ai.koog.prompt.executor.llms.all/simple-mistral-a-i-executor.html)                            | Wraps `MistralAILLMClient` that runs prompts with Mistral models.                |
| Ollama         | [simpleOllamaAIExecutor](https://api.koog.ai/prompt/prompt-executor/prompt-executor-llms-all/ai.koog.prompt.executor.llms.all/simple-ollama-a-i-executor.html)                              | Wraps `OllamaClient` that runs prompts with Ollama.                              |

Koog also provides the **pre-defined multi-provider executor** `DefaultMultiLLMPromptExecutor`. This is an implementation of `MultiLLMPromptExecutor` that wraps `OpenAILLMClient`, `AnthropicLLMClient`, and `GoogleLLMClient` models.

Here is an example of creating pre-defined single and multi-provider executors:

```kotlin
// Create an OpenAI executor
val promptExecutor = simpleOpenAIExecutor("OPENAI_KEY")

// Create a DefaultMultiLLMPromptExecutor with OpenAI, Anthropic, and Google LLM clients
val openAIClient = OpenAILLMClient("OPENAI_KEY")
val anthropicClient = AnthropicLLMClient("ANTHROPIC_KEY")
val googleClient = GoogleLLMClient("GOOGLE_KEY")
val multiExecutor = DefaultMultiLLMPromptExecutor(openAIClient, anthropicClient, googleClient)
```

## Running a prompt

To run a prompt using a prompt executor, do the following:

1. Create a prompt executor.
1. Run the prompt with the specific LLM using the `execute()` method.

Here is an example:

```kotlin
// Create an OpenAI executor
val promptExecutor = simpleOpenAIExecutor("OPENAI_KEY")

// Execute a prompt
val response = promptExecutor.execute(
    prompt = prompt("demo") { user("Summarize this.") },
    model = OpenAIModels.Chat.GPT4o
)
```

This will run the prompt with the `GPT4o` model and return the response.

Note

The prompt executors provide methods to run prompts using various capabilities, such as streaming, multiple choice generation, and content moderation. Since prompt executors wrap LLM clients, each executor supports the capabilities of the corresponding client. For details, refer to [LLM clients](../llm-clients/).

## Switching between providers

When you work with multiple LLM providers using `MultiLLMPromptExecutor`, you can switch between them. The process is as follows:

1. Create an LLM client instance for each provider you want to use.
1. Create a `MultiLLMPromptExecutor` that maps LLM providers to LLM clients.
1. Run a prompt with a model from the corresponding client passed as an argument to the `execute()` method. The prompt executor will use the corresponding client based on the model provider to run the prompt.

Here is an example of switching between providers:

```kotlin
// Create LLM clients for OpenAI, Anthropic, and Google providers
val openAIClient = OpenAILLMClient("OPENAI_API_KEY")
val anthropicClient = AnthropicLLMClient("ANTHROPIC_API_KEY")
val googleClient = GoogleLLMClient("GOOGLE_API_KEY")

// Create a MultiLLMPromptExecutor that maps LLM providers to LLM clients
val executor = MultiLLMPromptExecutor(
    LLMProvider.OpenAI to openAIClient,
    LLMProvider.Anthropic to anthropicClient,
    LLMProvider.Google to googleClient
)

// Create a prompt
val p = prompt("demo") { user("Summarize this.") }

// Run the prompt with an OpenAI model; the prompt executor automatically switches to the OpenAI client
val openAIResult = executor.execute(p, OpenAIModels.Chat.GPT4o)

// Run the prompt with an Anthropic model; the prompt executor automatically switches to the Anthropic client
val anthropicResult = executor.execute(p, AnthropicModels.Sonnet_3_5)
```

You can optionally configure a fallback LLM provider and model to use when the requested client is unavailable. For details, refer to [Configuring fallbacks](#configuring-fallbacks).

## Configuring fallbacks

Multi-provider prompt executors can be configured to use a fallback LLM provider and model when the requested LLM client is unavailable. To configure the fallback mechanism, provide the `fallback` parameter to the `MultiLLMPromptExecutor` constructor:

```kotlin
val openAIClient = OpenAILLMClient(System.getenv("OPENAI_KEY"))
val ollamaClient = OllamaClient()

val multiExecutor = MultiLLMPromptExecutor(
    LLMProvider.OpenAI to openAIClient,
    LLMProvider.Ollama to ollamaClient,
    fallback = MultiLLMPromptExecutor.FallbackPromptExecutorSettings(
        fallbackProvider = LLMProvider.Ollama,
        fallbackModel = OllamaModels.Meta.LLAMA_3_2
    )
)
```

If you pass a model from an LLM provider that is not included in the `MultiLLMPromptExecutor`, the prompt executor will use the fallback model:

```kotlin
// Create a prompt
val p = prompt("demo") { user("Summarize this") }
// If you pass a Google model, the prompt executor will use the fallback model, as the Google client is not included
val response = multiExecutor.execute(p, GoogleModels.Gemini2_5Pro)
```

Note

Fallbacks are available for the `execute()` and `executeMultipleChoices()` methods only.
# Tools

# Overview

Agents use tools to perform specific tasks or access external systems.

## Tool workflow

The Koog framework offers the following workflow for working with tools:

1. Create a custom tool or use one of the built-in tools.
1. Add the tool to a tool registry.
1. Pass the tool registry to an agent.
1. Use the tool with the agent.

### Available tool types

There are three types of tools in the Koog framework:

- Built-in tools that provide functionality for agent-user interaction and conversation management. For details, see [Built-in tools](../built-in-tools/).
- Annotation-based custom tools that let you expose functions as tools to LLMs. For details, see [Annotation-based tools](../annotation-based-tools/).
- Custom tools that let you control tool parameters, metadata, execution logic, and how it is registered and invoked. For details, see [Class-based tools](../class-based-tools/).

### Tool registry

Before you can use a tool in an agent, you must add it to a tool registry. The tool registry manages all tools available to the agent.

The key features of the tool registry:

- Organizes tools.
- Supports merging of multiple tool registries.
- Provides methods to retrieve tools by name or type.

To learn more, see [ToolRegistry](https://api.koog.ai/agents/agents-tools/ai.koog.agents.core.tools/-tool-registry/index.html).

Here is an example of how to create the tool registry and add the tool to it:

```kotlin
val toolRegistry = ToolRegistry {
    tool(SayToUser)
}
```

To merge multiple tool registries, do the following:

```kotlin
val firstToolRegistry = ToolRegistry {
    tool(FirstSampleTool)
}

val secondToolRegistry = ToolRegistry {
    tool(SecondSampleTool)
}

val newRegistry = firstToolRegistry + secondToolRegistry
```

### Passing tools to an agent

To enable an agent to use a tool, you need to provide a tool registry that contains this tool as an argument when creating the agent:

```kotlin
// Agent initialization
val agent = AIAgent(
    promptExecutor = simpleOpenAIExecutor(System.getenv("OPENAI_API_KEY")),
    systemPrompt = "You are a helpful assistant with strong mathematical skills.",
    llmModel = OpenAIModels.Chat.GPT4o,
    // Pass your tool registry to the agent
    toolRegistry = toolRegistry
)
```

### Calling tools

There are several ways to call tools within your agent code. The recommended approach is to use the provided methods in the agent context rather than calling tools directly, as this ensures proper handling of tool operation within the agent environment.

Tip

Ensure you have implemented proper [error handling](../agent-event-handlers/) in your tools to prevent agent failure.

The tools are called within a specific session context represented by `AIAgentLLMWriteSession`. It provides several methods for calling tools so that you can:

- Call a tool with the given arguments.
- Call a tool by its name and the given arguments.
- Call a tool by the provided tool class and arguments.
- Call a tool of the specified type with the given arguments.
- Call a tool that returns a raw string result.

For more details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.agent.session/-a-i-agent-l-l-m-write-session/index.html).

#### Parallel tool calls

You can also call tools in parallel using the `toParallelToolCallsRaw` extension. For example:

```kotlin
@Serializable
data class Book(
    val title: String,
    val author: String,
    val description: String
)

class BookTool() : SimpleTool<Book>(
    argsSerializer = Book.serializer(),
    name = NAME,
    description = "A tool to parse book information from Markdown"
) {
    companion object {
        const val NAME = "book"
    }

    override suspend fun execute(args: Book): String {
        println("${args.title} by ${args.author}:\n ${args.description}")
        return "Done"
    }
}

val strategy = strategy<Unit, Unit>("strategy-name") {

    /*...*/

    val myNode by node<Unit, Unit> { _ ->
        llm.writeSession {
            flow {
                emit(Book("Book 1", "Author 1", "Description 1"))
            }.toParallelToolCallsRaw(BookTool::class).collect()
        }
    }
}
```

#### Calling tools from nodes

When building agent workflows with nodes, you can use special nodes to call tools:

- **nodeExecuteTool**: calls a single tool call and returns its result. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-execute-tool.html).
- **nodeExecuteSingleTool** that calls a specific tool with the provided arguments. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-execute-single-tool.html).
- **nodeExecuteMultipleTools** that performs multiple tool calls and returns their results. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-execute-multiple-tools.html).
- **nodeLLMSendToolResult** that sends a tool result to the LLM and gets a response. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-l-l-m-send-tool-result.html).
- **nodeLLMSendMultipleToolResults** that sends multiple tool results to the LLM. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-l-l-m-send-multiple-tool-results.html).

## Using agents as tools

The framework provides the capability to convert any AI agent into a tool that can be used by other agents. This powerful feature enables you to create hierarchical agent architectures where specialized agents can be called as tools by higher-level orchestrating agents.

### Converting agents to tools

To convert an agent into a tool, use the `AIAgentService` and the `createAgentTool()` extension function:

```kotlin
// Create a specialized agent service, responsible for creating financial analysis agents.
val analysisAgentService = AIAgentService(
    promptExecutor = simpleOpenAIExecutor(apiKey),
    llmModel = OpenAIModels.Chat.GPT4o,
    systemPrompt = "You are a financial analysis specialist.",
    toolRegistry = analysisToolRegistry
)

// Create a tool that would run financial analysis agent once called.
val analysisAgentTool = analysisAgentService.createAgentTool(
    agentName = "analyzeTransactions",
    agentDescription = "Performs financial transaction analysis",
    inputDescription = "Transaction analysis request",
)
```

### Using agent tools in other agents

Once converted to a tool, you can add the agent tool to another agent's tool registry:

```kotlin
// Create a coordinator agent that can use specialized agents as tools
val coordinatorAgent = AIAgent(
    promptExecutor = simpleOpenAIExecutor(apiKey),
    llmModel = OpenAIModels.Chat.GPT4o,
    systemPrompt = "You coordinate different specialized services.",
    toolRegistry = ToolRegistry {
        tool(analysisAgentTool)
        // Add other tools as needed
    }
)
```

### Agent tool execution

When an agent tool is called:

1. The arguments are deserialized according to the input descriptor.
1. The wrapped agent is executed with the deserialized input.
1. The agent's output is serialized and returned as the tool result.

### Benefits of agents as tools

- **Modularity**: Break complex workflows into specialized agents.
- **Reusability**: Use the same specialized agent across multiple coordinator agents.
- **Separation of concerns**: Each agent can focus on its specific domain.

# Built-in tools

The Koog framework provides built-in tools that handle common scenarios of agent-user interaction.

The following built-in tools are available:

| Tool              | Name                 | Description                                                                                                              |
| ----------------- | -------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| SayToUser         | `__say_to_user__`    | Lets the agent send a message to the user. It prints the agent message to the console with the `Agent says:` prefix.     |
| AskUser           | `__ask_user__`       | Lets the agent ask the user for input. It prints the agent message to the console and waits for user response.           |
| ExitTool          | `__exit__`           | Lets the agent finish the conversation and terminate the session.                                                        |
| ReadFileTool      | `__read_file__`      | Reads text file with optional line range selection. Returns formatted content with metadata using 0-based line indexing. |
| EditFileTool      | `__edit_file__`      | Makes a single, targeted text replacement in a file; can also create new files or fully replace contents.                |
| ListDirectoryTool | `__list_directory__` | Lists directory contents as a hierarchical tree with optional depth control and glob filtering.                          |
| WriteFileTool     | `__write_file__`     | Writes text content to a file (creating parent directories if needed).                                                   |

## Registering built-in tools

Like any other tool, a built-in tool must be added to the tool registry to become available for an agent. Here is an example:

```kotlin
// Create a tool registry with all built-in tools
val toolRegistry = ToolRegistry {
    tool(SayToUser)
    tool(AskUser)
    tool(ExitTool)
    tool(ReadFileTool(JVMFileSystemProvider.ReadOnly))
    tool(ListDirectoryTool(JVMFileSystemProvider.ReadOnly))
    tool(WriteFileTool(JVMFileSystemProvider.ReadWrite))
}

// Pass the registry when creating an agent
val agent = AIAgent(
    promptExecutor = simpleOpenAIExecutor(apiToken),
    systemPrompt = "You are a helpful assistant.",
    llmModel = OpenAIModels.Chat.GPT4o,
    toolRegistry = toolRegistry
)
```

You can create a comprehensive set of capabilities for your agent by combining built-in tools and custom tools within the same registry. To learn more about custom tools, see [Annotation-based tools](../annotation-based-tools/) and [Class-based tools](../class-based-tools/).

# Annotation-based tools

Annotation-based tools provide a declarative way to expose functions as tools for large language models (LLMs). By using annotations, you can transform any function into a tool that LLMs can understand and use.

This approach is useful when you need to expose existing functionality to LLMs without implementing tool descriptions manually.

Note

Annotation-based tools are JVM-only and not available for other platforms. For multiplatform support, use the [class-based tool API](../class-based-tools/).

## Key annotations

To start using annotation-based tools in your project, you need to understand the following key annotations:

| Annotation        | Description                                                             |
| ----------------- | ----------------------------------------------------------------------- |
| `@Tool`           | Marks functions that should be exposed as tools to LLMs.                |
| `@LLMDescription` | Provides descriptive information about your tools and their components. |

## @Tool annotation

The `@Tool` annotation is used to mark functions that should be exposed as tools to LLMs. The functions annotated with `@Tool` are collected by reflection from objects that implement the `ToolSet` interface. For details, see [Implement the ToolSet interface](#implement-the-toolset-interface).

### Definition

```kotlin
@Target(AnnotationTarget.FUNCTION)
public annotation class Tool(val customName: String = "")
```

### Parameters

| Name         | Required | Description                                                                              |
| ------------ | -------- | ---------------------------------------------------------------------------------------- |
| `customName` | No       | Specifies a custom name for the tool. If not provided, the name of the function is used. |

### Usage

To mark a function as a tool, apply the `@Tool` annotation to this function in a class that implements the `ToolSet` interface:

```kotlin
class MyToolSet : ToolSet {
    @Tool
    fun myTool(): String {
        // Tool implementation
        return "Result"
    }

    @Tool(customName = "customToolName")
    fun anotherTool(): String {
        // Tool implementation
        return "Result"
    }
}
```

## @LLMDescription annotation

The `@LLMDescription` annotation provides descriptive information about code elements (classes, functions, parameters, and so on) to LLMs. This helps LLMs understand the purpose and usage of these elements.

### Definition

```kotlin
@Target(
    AnnotationTarget.PROPERTY,
    AnnotationTarget.CLASS,
    AnnotationTarget.PROPERTY,
    AnnotationTarget.TYPE,
    AnnotationTarget.VALUE_PARAMETER,
    AnnotationTarget.FUNCTION
)
public annotation class LLMDescription(val description: String)
```

### Parameters

| Name          | Required | Description                                    |
| ------------- | -------- | ---------------------------------------------- |
| `description` | Yes      | A string that describes the annotated element. |

### Usage

The `@LLMDescription` annotation can be applied at various levels. For example:

- Function level:

```kotlin
@Tool
@LLMDescription("Performs a specific operation and returns the result")
fun myTool(): String {
    // Function implementation
    return "Result"
}
```

- Parameter level:

```kotlin
@Tool
@LLMDescription("Processes input data")
fun processTool(
    @LLMDescription("The input data to process")
    input: String,

    @LLMDescription("Optional configuration parameters")
    config: String = ""
): String {
    // Function implementation
    return "Processed: $input with config: $config"
}
```

## Creating a tool

### 1. Implement the ToolSet interface

Create a class that implements the [`ToolSet`](https://api.koog.ai/agents/agents-tools/ai.koog.agents.core.tools.reflect/-tool-set/index.html) interface. This interface marks your class as a container for tools.

```kotlin
class MyFirstToolSet : ToolSet {
    // Tools will go here
}
```

### 2. Add tool functions

Add functions to your class and annotate them with `@Tool` to expose them as tools:

```kotlin
class MyFirstToolSet : ToolSet {
    @Tool
    fun getWeather(location: String): String {
        // In a real implementation, you would call a weather API
        return "The weather in $location is sunny and 72Â°F"
    }
}
```

### 3. Add descriptions

Add `@LLMDescription` annotations to provide context for the LLM:

```kotlin
@LLMDescription("Tools for getting weather information")
class MyFirstToolSet : ToolSet {
    @Tool
    @LLMDescription("Get the current weather for a location")
    fun getWeather(
        @LLMDescription("The city and state/country")
        location: String
    ): String {
        // In a real implementation, you would call a weather API
        return "The weather in $location is sunny and 72Â°F"
    }
}
```

### 4. Use your tools with an agent

Now you can use your tools with an agent:

```kotlin
fun main() {
    runBlocking {
        // Create your tool set
        val weatherTools = MyFirstToolSet()

        // Create an agent with your tools

        val agent = AIAgent(
            promptExecutor = simpleOpenAIExecutor(apiToken),
            systemPrompt = "Provide weather information for a given location.",
            llmModel = OpenAIModels.Chat.GPT4o,
            toolRegistry = ToolRegistry {
                tools(weatherTools)
            }
        )

        // The agent can now use your weather tools
        agent.run("What's the weather like in New York?")
    }
}
```

## Usage examples

Here are some real-world examples of tool annotations.

### Basic example: Switch controller

This example shows a simple tool set for controlling a switch:

```kotlin
@LLMDescription("Tools for controlling a switch")
class SwitchTools(val switch: Switch) : ToolSet {
    @Tool
    @LLMDescription("Switches the state of the switch")
    fun switch(
        @LLMDescription("The state to set (true for on, false for off)")
        state: Boolean
    ): String {
        switch.switch(state)
        return "Switched to ${if (state) "on" else "off"}"
    }

    @Tool
    @LLMDescription("Returns the current state of the switch")
    fun switchState(): String {
        return "Switch is ${if (switch.isOn()) "on" else "off"}"
    }
}
```

When an LLM needs to control a switch, it can understand the following information from the provided description:

- The purpose and functionality of the tools.
- The required parameters for using the tools.
- The acceptable values for each parameter.
- The expected return values upon execution.

### Advanced example: Diagnostic tools

This example shows a more complex tool set for device diagnostics:

```kotlin
@LLMDescription("Tools for performing diagnostics and troubleshooting on devices")
class DiagnosticToolSet : ToolSet {
    @Tool
    @LLMDescription("Run diagnostic on a device to check its status and identify any issues")
    fun runDiagnostic(
        @LLMDescription("The ID of the device to diagnose")
        deviceId: String,

        @LLMDescription("Additional information for the diagnostic (optional)")
        additionalInfo: String = ""
    ): String {
        // Implementation
        return "Diagnostic results for device $deviceId"
    }

    @Tool
    @LLMDescription("Analyze an error code to determine its meaning and possible solutions")
    fun analyzeError(
        @LLMDescription("The error code to analyze (e.g., 'E1001')")
        errorCode: String
    ): String {
        // Implementation
        return "Analysis of error code $errorCode"
    }
}
```

## Best practices

- **Provide clear descriptions**: write clear, concise descriptions that explain the purpose and behavior of tools, parameters, and return values.
- **Describe all parameters**: add `@LLMDescription` to all parameters to help LLMs understand what each parameter is for.
- **Use consistent naming**: use consistent naming conventions for tools and parameters to make them more intuitive.
- **Group related tools**: group related tools in the same `ToolSet` implementation and provide a class-level description.
- **Return informative results**: make sure tool return values provide clear information about the result of the operation.
- **Handle errors gracefully**: include error handling in your tools and return informative error messages.
- **Document default values**: when parameters have default values, document this in the description.
- **Keep tools focused**: Each tool should perform a specific, well-defined task rather than trying to do too many things.

## Troubleshooting common issues

When working with tool annotations, you might encounter some common issues.

### Tools not being recognized

If the agent does not recognize your tools, check the following:

- Your class implements the `ToolSet` interface.
- All tool functions are annotated with `@Tool`.
- Tool functions have appropriate return types (`String` is recommended for simplicity).
- Your tools are properly registered with the agent.

### Unclear tool descriptions

If the LLM does not use your tools correctly or misunderstands their purpose, try the following:

- Improve your `@LLMDescription` annotations to be more specific and clear.
- Include examples in your descriptions if appropriate.
- Specify parameter constraints in the descriptions (for example, `"Must be a positive number"`).
- Use consistent terminology throughout your descriptions.

### Parameter type issues

If the LLM provides incorrect parameter types, try the following:

- Use simple parameter types when possible (`String`, `Boolean`, `Int`).
- Clearly describe the expected format in the parameter description.
- For complex types, consider using `String` parameters with a specific format and parse them in your tool.
- Include examples of valid inputs in your parameter descriptions.

### Performance issues

If your tools cause performance problems, try the following:

- Keep tool implementations lightweight.
- For resource-intensive operations, consider implementing asynchronous processing.
- Cache results when appropriate.
- Log tool usage to identify bottlenecks.

# Class-based tools

This section explains the API designed for scenarios that require enhanced flexibility and customized behavior. With this approach, you have full control over a tool, including its parameters, metadata, execution logic, and how it is registered and invoked.

This level of control is ideal for creating sophisticated tools that extend basic use cases, enabling seamless integration into agent sessions and workflows.

This page describes how to implement a tool, manage tools through registries, call them, and use within node-based agent architectures.

Note

The API is multiplatform. This lets you use the same tools across different platforms.

## Tool implementation

The Koog framework provides the following approaches for implementing tools:

- Using the base class `Tool` for all tools. You should use this class when you need to return non-text results or require complete control over the tool behavior.
- Using the `SimpleTool` class that extends the base `Tool` class and simplifies the creation of tools that return text results. You should use this approach for scenarios where the tool only needs to return a text.

Both approaches use the same core components but differ in implementation and the results they return.

### Tool class

The [`Tool<Args, Result>`](https://api.koog.ai/agents/agents-tools/ai.koog.agents.core.tools/-tool/index.html) abstract class is the base class for creating tools in Koog. It lets you create tools that accept specific argument types (`Args`) and return results of various types (`Result`).

Each tool consists of the following components:

| Component          | Description                                                                                                                                                                                                                                                                                                                                                                                                                           |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `Args`             | The serializable data class that defines arguments required for the tool.                                                                                                                                                                                                                                                                                                                                                             |
| `Result`           | The serializable type of result that the tool returns. If you want to present tool results in a custom format, please inherit [ToolResult.TextSerializable](https://api.koog.ai/agents/agents-tools/ai.koog.agents.core.tools/-tool-result/-text-serializable/index.html) class and implement `textForLLM(): String` method                                                                                                           |
| `argsSerializer`   | The overridden variable that defines how the arguments for the tool are deserialized. See also [argsSerializer](https://api.koog.ai/agents/agents-tools/ai.koog.agents.core.tools/-tool/args-serializer.html).                                                                                                                                                                                                                        |
| `resultSerializer` | The overridden variable that defines how the result of the tool is deserialized. See also [resultSerializer](https://api.koog.ai/agents/agents-tools/ai.koog.agents.core.tools/-tool/result-serializer.html). If you chose to inherit [ToolResult.TextSerializable](https://api.koog.ai/agents/agents-tools/ai.koog.agents.core.tools/-tool-result/-text-serializable/index.html) consider using `ToolResultUtils.toTextSerializer()` |
| `descriptor`       | The overridden variable that specifies tool metadata: - `name` - `description` - `requiredParameters` (empty by default) - `optionalParameters` (empty by default) See also [descriptor](https://api.koog.ai/agents/agents-tools/ai.koog.agents.core.tools/-tool/descriptor.html).                                                                                                                                                    |
| `execute()`        | The function that implements the logic of the tool. It takes arguments of type `Args` and returns a result of type `Result`. See also execute().                                                                                                                                                                                                                                                                                      |

Tip

Ensure your tools have clear descriptions and well-defined parameter names to make it easier for the LLM to understand and use them properly.

#### Usage example

Here is an example of a custom tool implementation using the `Tool` class that returns a numeric result:

```kotlin
// Implement a simple calculator tool that adds two digits
object CalculatorTool : Tool<CalculatorTool.Args, Int>(
    argsSerializer = Args.serializer(),
    resultSerializer = Int.serializer(),
    name = "calculator",
    description = "A simple calculator that can add two digits (0-9)."
) {

    // Arguments for the calculator tool
    @Serializable
    data class Args(
        @property:LLMDescription("The first digit to add (0-9)")
        val digit1: Int,
        @property:LLMDescription("The second digit to add (0-9)")
        val digit2: Int
    ) {
        init {
            require(digit1 in 0..9) { "digit1 must be a single digit (0-9)" }
            require(digit2 in 0..9) { "digit2 must be a single digit (0-9)" }
        }
    }

    // Function to add two digits
    override suspend fun execute(args: Args): Int = args.digit1 + args.digit2
}
```

After implementing your tool, you need to add it to a tool registry and then use it with an agent. For details, see [Tool registry](../tools-overview/#tool-registry).

For more details, see [API reference](https://api.koog.ai/agents/agents-tools/ai.koog.agents.core.tools/-tool/index.html).

### SimpleTool class

The [`SimpleTool<Args>`](https://api.koog.ai/agents/agents-tools/ai.koog.agents.core.tools/-simple-tool/index.html) abstract class extends `Tool<Args, ToolResult.Text>` and simplifies the creation of tools that return text results.

Each simple tool consists of the following components:

| Component        | Description                                                                                                                                                                                                                                                                        |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `Args`           | The serializable data class that defines arguments required for the custom tool.                                                                                                                                                                                                   |
| `argsSerializer` | The overridden variable that defines how the arguments for the tool are serialized. See also [argsSerializer](https://api.koog.ai/agents/agents-tools/ai.koog.agents.core.tools/-tool/args-serializer.html).                                                                       |
| `descriptor`     | The overridden variable that specifies tool metadata: - `name` - `description` - `requiredParameters` (empty by default) - `optionalParameters` (empty by default) See also [descriptor](https://api.koog.ai/agents/agents-tools/ai.koog.agents.core.tools/-tool/descriptor.html). |
| `doExecute()`    | The overridden function that describes the main action performed by the tool. It takes arguments of type `Args` and returns a `String`. See also [doExecute()](https://api.koog.ai/agents/agents-tools/ai.koog.agents.core.tools/-simple-tool/do-execute.html).                    |

Tip

Ensure your tools have clear descriptions and well-defined parameter names to make it easier for the LLM to understand and use them properly.

#### Usage example

Here is an example of a custom tool implementation using `SimpleTool`:

```kotlin
// Create a tool that casts a string expression to a double value
object CastToDoubleTool : SimpleTool<CastToDoubleTool.Args>(
    argsSerializer = Args.serializer(),
    name = "cast_to_double",
    description = "casts the passed expression to double or returns 0.0 if the expression is not castable"
) {
    // Define tool arguments
    @Serializable
    data class Args(
        @property:LLMDescription("An expression to case to double")
        val expression: String,
        @property:LLMDescription("A comment on how to process the expression")
        val comment: String
    )

    // Function that executes the tool with the provided arguments
    override suspend fun execute(args: Args): String {
        return "Result: ${castToDouble(args.expression)}, " + "the comment was: ${args.comment}"
    }

    // Function to cast a string expression to a double value
    private fun castToDouble(expression: String): Double {
        return expression.toDoubleOrNull() ?: 0.0
    }
}
```

### Sending Tool Result to LLM in Custom Format

If you are not happy with JSON results sent to LLM (in some cases, LLMs can work better if tool output is structured as Markdown, for instance), you have to follow the following steps:

1. Implement `ToolResult.TextSerializable` interface, and override `textForLLM()` method
1. Override `resultSerializer` using `ToolResultUtils.toTextSerializer<T>()`

#### Example

```kotlin
// A tool that edits file
object EditFile : Tool<EditFile.Args, EditFile.Result>(
    argsSerializer = Args.serializer(),
    resultSerializer = Result.serializer(),
    name = "edit_file",
    description = "Edits the given file"
) {
    // Define tool arguments
    @Serializable
    public data class Args(
        val path: String,
        val original: String,
        val replacement: String
    )

    @Serializable
    public data class Result(
        private val patchApplyResult: PatchApplyResult
    ) {

        @Serializable
        public sealed interface PatchApplyResult {
            @Serializable
            public data class Success(val updatedContent: String) : PatchApplyResult

            @Serializable
            public sealed class Failure(public val reason: String) : PatchApplyResult
        }

        // Textual output (in Markdown format) that will be visible to the LLM after the tool finishes.
        fun textForLLM(): String = markdown {
            if (patchApplyResult is PatchApplyResult.Success) {
                line {
                    bold("Successfully").text(" edited file (patch applied)")
                }
            } else {
                line {
                    text("File was ")
                        .bold("not")
                        .text(" modified (patch application failed: ${(patchApplyResult as PatchApplyResult.Failure).reason})")
                }
            }
        }

        override fun toString(): String = textForLLM()
    }

    // Function that executes the tool with the provided arguments
    override suspend fun execute(args: Args): Result {
        return TODO("Implement file edit")
    }
}
```

After implementing your tool, you need to add it to a tool registry and then use it with an agent. For details, see [Tool registry](../tools-overview/#tool-registry).
# Events

# Agent events

Agent events are actions or interactions that occur as part of an agent workflow. They include:

- Agent lifecycle events
- Strategy events
- Node execution events
- LLM call events
- LLM streaming events
- Tool execution events

Note: Feature events are defined in the agents-core module and live under the package `ai.koog.agents.core.feature.model.events`. Features such as `agents-features-trace`, and `agents-features-event-handler` consume these events to process and forward messages created during agent execution.

## Predefined event types

Koog provides predefined event types that can be used in custom message processors. The predefined events can be classified into several categories, depending on the entity they relate to:

- [Agent events](#agent-events)
- [Strategy events](#strategy-events)
- [Node events](#node-events)
- [Subgraph events](#subgraph-events)
- [LLM call events](#llm-call-events)
- [LLM streaming events](#llm-streaming-events)
- [Tool execution events](#tool-execution-events)

### Agent events

#### AgentStartingEvent

Represents the start of an agent run. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                     |
| --------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `agentId`       | String             | Yes      |         | The unique identifier of the AI agent.                                          |
| `runId`         | String             | Yes      |         | The unique identifier of the AI agent run.                                      |

#### AgentCompletedEvent

Represents the end of an agent run. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                     |
| --------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `agentId`       | String             | Yes      |         | The unique identifier of the AI agent.                                          |
| `runId`         | String             | Yes      |         | The unique identifier of the AI agent run.                                      |
| `result`        | String             | Yes      |         | The result of the agent run. Can be `null` if there is no result.               |

#### AgentExecutionFailedEvent

Represents the occurrence of an error during an agent run. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                                                     |
| --------------- | ------------------ | -------- | ------- | --------------------------------------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                                                         |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event.                                 |
| `agentId`       | String             | Yes      |         | The unique identifier of the AI agent.                                                                          |
| `runId`         | String             | Yes      |         | The unique identifier of the AI agent run.                                                                      |
| `error`         | AIAgentError       | Yes      |         | The specific error that occurred during the agent run. For more information, see [AIAgentError](#aiagenterror). |

#### AgentClosingEvent

Represents the closure or termination of an agent. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                     |
| --------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `agentId`       | String             | Yes      |         | The unique identifier of the AI agent.                                          |

The `AIAgentError` class provides more details about an error that occurred during an agent run. Includes the following fields:

| Name         | Data type | Required | Default | Description                                                      |
| ------------ | --------- | -------- | ------- | ---------------------------------------------------------------- |
| `message`    | String    | Yes      |         | The message that provides more details about the specific error. |
| `stackTrace` | String    | Yes      |         | The collection of stack records until the last executed code.    |
| `cause`      | String    | No       | null    | The cause of the error, if available.                            |

The `AgentExecutionInfo` class provides contextual information about the execution path, enabling tracking of nested execution contexts within an agent run. Includes the following fields:

| Name       | Data type          | Required | Default | Description                                                                                   |
| ---------- | ------------------ | -------- | ------- | --------------------------------------------------------------------------------------------- |
| `parent`   | AgentExecutionInfo | No       | null    | Reference to the parent execution context. If null, this represents the root execution level. |
| `partName` | String             | Yes      |         | A string representing the name of the current part or segment of the execution.               |

### Strategy events

#### GraphStrategyStartingEvent

Represents the start of a graph-based strategy run. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                     |
| --------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `runId`         | String             | Yes      |         | The unique identifier of the strategy run.                                      |
| `strategyName`  | String             | Yes      |         | The name of the strategy.                                                       |
| `graph`         | StrategyEventGraph | Yes      |         | The graph structure representing the strategy workflow.                         |

#### FunctionalStrategyStartingEvent

Represents the start of a functional strategy run. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                     |
| --------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `runId`         | String             | Yes      |         | The unique identifier of the strategy run.                                      |
| `strategyName`  | String             | Yes      |         | The name of the strategy.                                                       |

#### StrategyCompletedEvent

Represents the end of a strategy run. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                     |
| --------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `runId`         | String             | Yes      |         | The unique identifier of the strategy run.                                      |
| `strategyName`  | String             | Yes      |         | The name of the strategy.                                                       |
| `result`        | String             | Yes      |         | The result of the run. Can be `null` if there is no result.                     |

### Node events

#### NodeExecutionStartingEvent

Represents the start of a node run. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                     |
| --------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `runId`         | String             | Yes      |         | The unique identifier of the strategy run.                                      |
| `nodeName`      | String             | Yes      |         | The name of the node whose run started.                                         |
| `input`         | JsonElement        | No       | null    | The input value for the node.                                                   |

#### NodeExecutionCompletedEvent

Represents the end of a node run. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                     |
| --------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `runId`         | String             | Yes      |         | The unique identifier of the strategy run.                                      |
| `nodeName`      | String             | Yes      |         | The name of the node whose run ended.                                           |
| `input`         | JsonElement        | No       | null    | The input value for the node.                                                   |
| `output`        | JsonElement        | No       | null    | The output value produced by the node.                                          |

#### NodeExecutionFailedEvent

Represents an error that occurred during a node run. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                                                    |
| --------------- | ------------------ | -------- | ------- | -------------------------------------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                                                        |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event.                                |
| `runId`         | String             | Yes      |         | The unique identifier of the strategy run.                                                                     |
| `nodeName`      | String             | Yes      |         | The name of the node where the error occurred.                                                                 |
| `input`         | JsonElement        | No       | null    | The input data provided to the node.                                                                           |
| `error`         | AIAgentError       | Yes      |         | The specific error that occurred during the node run. For more information, see [AIAgentError](#aiagenterror). |

### Subgraph events

#### SubgraphExecutionStartingEvent

Represents the start of a subgraph run. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                     |
| --------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `runId`         | String             | Yes      |         | The unique identifier of the strategy run.                                      |
| `subgraphName`  | String             | Yes      |         | The name of the subgraph whose run started.                                     |
| `input`         | JsonElement        | No       | null    | The input value for the subgraph.                                               |

#### SubgraphExecutionCompletedEvent

Represents the end of a subgraph run. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                     |
| --------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `runId`         | String             | Yes      |         | The unique identifier of the strategy run.                                      |
| `subgraphName`  | String             | Yes      |         | The name of the subgraph whose run ended.                                       |
| `input`         | JsonElement        | No       | null    | The input value for the subgraph.                                               |
| `output`        | JsonElement        | No       | null    | The output value produced by the subgraph.                                      |

#### SubgraphExecutionFailedEvent

Represents an error that occurred during a subgraph run. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                                                        |
| --------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------------------------------------------ |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                                                            |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event.                                    |
| `runId`         | String             | Yes      |         | The unique identifier of the strategy run.                                                                         |
| `subgraphName`  | String             | Yes      |         | The name of the subgraph where the error occurred.                                                                 |
| `input`         | JsonElement        | No       | null    | The input data provided to the subgraph.                                                                           |
| `error`         | AIAgentError       | Yes      |         | The specific error that occurred during the subgraph run. For more information, see [AIAgentError](#aiagenterror). |

### LLM call events

#### LLMCallStartingEvent

Represents the start of an LLM call. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                        |
| --------------- | ------------------ | -------- | ------- | ---------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                            |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event.    |
| `runId`         | String             | Yes      |         | The unique identifier of the LLM run.                                              |
| `prompt`        | Prompt             | Yes      |         | The prompt that is sent to the model. For more information, see [Prompt](#prompt). |
| `model`         | ModelInfo          | Yes      |         | The model information. See [ModelInfo](#modelinfo).                                |
| `tools`         | List               | Yes      |         | The list of tools that the model can call.                                         |

The `Prompt` class represents a data structure for a prompt, consisting of a list of messages, a unique identifier, and optional parameters for language model settings. Includes the following fields:

| Name       | Data type | Required | Default     | Description                                                  |
| ---------- | --------- | -------- | ----------- | ------------------------------------------------------------ |
| `messages` | List      | Yes      |             | The list of messages that the prompt consists of.            |
| `id`       | String    | Yes      |             | The unique identifier for the prompt.                        |
| `params`   | LLMParams | No       | LLMParams() | The settings that control the way the LLM generates content. |

The `ModelInfo` class represents information about a language model, including its provider, model identifier, and characteristics. Includes the following fields:

| Name              | Data type | Required | Default | Description                                                      |
| ----------------- | --------- | -------- | ------- | ---------------------------------------------------------------- |
| `provider`        | String    | Yes      |         | The provider identifier (e.g., "openai", "google", "anthropic"). |
| `model`           | String    | Yes      |         | The model identifier (e.g., "gpt-4", "claude-3").                |
| `displayName`     | String    | No       | null    | Optional human-readable display name for the model.              |
| `contextLength`   | Long      | No       | null    | Maximum number of tokens the model can process.                  |
| `maxOutputTokens` | Long      | No       | null    | Maximum number of tokens the model can generate.                 |

#### LLMCallCompletedEvent

Represents the end of an LLM call. Includes the following fields:

| Name                 | Data type          | Required | Default | Description                                                                     |
| -------------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`            | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo`      | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `runId`              | String             | Yes      |         | The unique identifier of the LLM run.                                           |
| `prompt`             | Prompt             | Yes      |         | The prompt used in the call.                                                    |
| `model`              | ModelInfo          | Yes      |         | The model information. See [ModelInfo](#modelinfo).                             |
| `responses`          | List               | Yes      |         | One or more responses returned by the model.                                    |
| `moderationResponse` | ModerationResult   | No       | null    | The moderation response, if any.                                                |

### LLM streaming events

#### LLMStreamingStartingEvent

Represents the start of an LLM streaming call. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                     |
| --------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `runId`         | String             | Yes      |         | The unique identifier of the LLM run.                                           |
| `prompt`        | Prompt             | Yes      |         | The prompt that is sent to the model.                                           |
| `model`         | ModelInfo          | Yes      |         | The model information. See [ModelInfo](#modelinfo).                             |
| `tools`         | List               | Yes      |         | The list of tools that the model can call.                                      |

#### LLMStreamingFrameReceivedEvent

Represents a streaming frame received from the LLM. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                     |
| --------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `runId`         | String             | Yes      |         | The unique identifier of the LLM run.                                           |
| `prompt`        | Prompt             | Yes      |         | The prompt that is sent to the model.                                           |
| `model`         | ModelInfo          | Yes      |         | The model information. See [ModelInfo](#modelinfo).                             |
| `frame`         | StreamFrame        | Yes      |         | The frame received from the stream.                                             |

#### LLMStreamingFailedEvent

Represents the occurrence of an error during an LLM streaming call. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                                                 |
| --------------- | ------------------ | -------- | ------- | ----------------------------------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                                                     |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event.                             |
| `runId`         | String             | Yes      |         | The unique identifier of the LLM run.                                                                       |
| `prompt`        | Prompt             | Yes      |         | The prompt that is sent to the model.                                                                       |
| `model`         | ModelInfo          | Yes      |         | The model information. See [ModelInfo](#modelinfo).                                                         |
| `error`         | AIAgentError       | Yes      |         | The specific error that occurred during streaming. For more information, see [AIAgentError](#aiagenterror). |

#### LLMStreamingCompletedEvent

Represents the end of an LLM streaming call. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                     |
| --------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `runId`         | String             | Yes      |         | The unique identifier of the LLM run.                                           |
| `prompt`        | Prompt             | Yes      |         | The prompt that is sent to the model.                                           |
| `model`         | ModelInfo          | Yes      |         | The model information. See [ModelInfo](#modelinfo).                             |
| `tools`         | List               | Yes      |         | The list of tools that the model can call.                                      |

### Tool execution events

#### ToolCallStartingEvent

Represents the event of a model calling a tool. Includes the following fields:

| Name            | Data type          | Required | Default | Description                                                                     |
| --------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`       | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo` | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `runId`         | String             | Yes      |         | The unique identifier of the strategy/agent run.                                |
| `toolCallId`    | String             | No       | null    | The identifier of the tool call, if available.                                  |
| `toolName`      | String             | Yes      |         | The name of the tool.                                                           |
| `toolArgs`      | JsonObject         | Yes      |         | The arguments that are provided to the tool.                                    |

#### ToolValidationFailedEvent

Represents the occurrence of a validation error during a tool call. Includes the following fields:

| Name              | Data type          | Required | Default | Description                                                                                |
| ----------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------------------ |
| `eventId`         | String             | Yes      |         | A unique identifier for the event or a group of events.                                    |
| `executionInfo`   | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event.            |
| `runId`           | String             | Yes      |         | The unique identifier of the strategy/agent run.                                           |
| `toolCallId`      | String             | No       | null    | The identifier of the tool call, if available.                                             |
| `toolName`        | String             | Yes      |         | The name of the tool for which validation failed.                                          |
| `toolArgs`        | JsonObject         | Yes      |         | The arguments that are provided to the tool.                                               |
| `toolDescription` | String             | No       | null    | A description of the tool that encountered the validation error.                           |
| `message`         | String             | No       | null    | A message describing the validation error.                                                 |
| `error`           | AIAgentError       | Yes      |         | The specific error that occurred. For more information, see [AIAgentError](#aiagenterror). |

#### ToolCallFailedEvent

Represents a failure to execute a tool. Includes the following fields:

| Name              | Data type          | Required | Default | Description                                                                                                           |
| ----------------- | ------------------ | -------- | ------- | --------------------------------------------------------------------------------------------------------------------- |
| `eventId`         | String             | Yes      |         | A unique identifier for the event or a group of events.                                                               |
| `executionInfo`   | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event.                                       |
| `runId`           | String             | Yes      |         | The unique identifier of the strategy/agent run.                                                                      |
| `toolCallId`      | String             | No       | null    | The identifier of the tool call, if available.                                                                        |
| `toolName`        | String             | Yes      |         | The name of the tool.                                                                                                 |
| `toolArgs`        | JsonObject         | Yes      |         | The arguments that are provided to the tool.                                                                          |
| `toolDescription` | String             | No       | null    | A description of the tool that failed.                                                                                |
| `error`           | AIAgentError       | Yes      |         | The specific error that occurred when trying to call a tool. For more information, see [AIAgentError](#aiagenterror). |

#### ToolCallCompletedEvent

Represents a successful tool call with the return of a result. Includes the following fields:

| Name              | Data type          | Required | Default | Description                                                                     |
| ----------------- | ------------------ | -------- | ------- | ------------------------------------------------------------------------------- |
| `eventId`         | String             | Yes      |         | A unique identifier for the event or a group of events.                         |
| `executionInfo`   | AgentExecutionInfo | Yes      |         | Provides contextual information about the execution associated with this event. |
| `runId`           | String             | Yes      |         | The unique identifier of the run.                                               |
| `toolCallId`      | String             | No       | null    | The identifier of the tool call.                                                |
| `toolName`        | String             | Yes      |         | The name of the tool.                                                           |
| `toolArgs`        | JsonObject         | Yes      |         | The arguments provided to the tool.                                             |
| `toolDescription` | String             | No       | null    | A description of the tool that was executed.                                    |
| `result`          | JsonElement        | No       | null    | The result of the tool call.                                                    |

## FAQ and troubleshooting

The following section includes commonly asked questions and answers related to the Tracing feature.

### How do I trace only specific parts of my agent's execution?

Use the `messageFilter` property to filter events. For example, to trace only node execution:

```kotlin
install(Tracing) {
    val fileWriter = TraceFeatureMessageFileWriter(
        outputPath, 
        { path: Path -> SystemFileSystem.sink(path).buffered() }
    )
    addMessageProcessor(fileWriter)

    // Only trace LLM calls
    fileWriter.setMessageFilter { message ->
        message is LLMCallStartingEvent || message is LLMCallCompletedEvent
    }
}
```

### Can I use multiple message processors?

Yes, you can add multiple message processors to trace to different destinations simultaneously:

```kotlin
install(Tracing) {
    addMessageProcessor(TraceFeatureMessageLogWriter(logger))
    addMessageProcessor(TraceFeatureMessageFileWriter(outputPath, syncOpener))
    addMessageProcessor(TraceFeatureMessageRemoteWriter(connectionConfig))
}
```

### How can I create a custom message processor?

Implement the `FeatureMessageProcessor` interface:

```kotlin
class CustomTraceProcessor : FeatureMessageProcessor() {

    // Current open state of the processor
    private var _isOpen = MutableStateFlow(false)

    override val isOpen: StateFlow<Boolean>
        get() = _isOpen.asStateFlow()

    override suspend fun processMessage(message: FeatureMessage) {
        // Custom processing logic
        when (message) {
            is NodeExecutionStartingEvent -> {
                // Process node start event
            }

            is LLMCallCompletedEvent -> {
                // Process LLM call end event 
            }
            // Handle other event types 
        }
    }

    override suspend fun close() {
        // Close connections of established
    }
}

// Use your custom processor
install(Tracing) {
    addMessageProcessor(CustomTraceProcessor())
}
```

For more information about existing event types that can be handled by message processors, see [Predefined event types](#predefined-event-types).
# Strategies

# Predefined nodes and components

Nodes are the fundamental building blocks of agent workflows in the Koog framework. Each node represents a specific operation or transformation in the workflow, and they can be connected using edges to define the flow of execution.

In general, nodes let you encapsulate complex logic into reusable components that can be easily integrated into different agent workflows. This guide will walk you through the existing nodes that can be used in your agent strategies.

Each node is essentially a function that takes an input of a specific type and returns an output of a specific type.

```
graph LR
    in:::hidden
    out:::hidden

    subgraph node ["node"]
        execute(Do stuff)
    end

    in --Input--> execute --Output--> out

    classDef hidden display: none;
```

Here is how you can define a node that expects a string as input and returns the length of the string (an integer) as output:

```kotlin
val nodeLength by node<String, Int> { input ->
    input.length
}
```

For more information, see [`node()`](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.builder/-a-i-agent-subgraph-builder-base/node.html).

## Utility nodes

### nodeDoNothing

A simple pass-through node that does nothing and returns the input as output. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-do-nothing.html).

```
graph LR
    in:::hidden
    out:::hidden

    subgraph node ["nodeDoNothing"]
        execute(Do nothing)
    end

    in ---|T| execute --T--> out

    classDef hidden display: none;
```

You can use this node for the following purposes:

- Create a placeholder node in your graph.
- Create a connection point without modifying the data.

Here is an example:

```kotlin
val passthrough by nodeDoNothing<String>("passthrough")

edge(nodeStart forwardTo passthrough)
edge(passthrough forwardTo nodeFinish)
```

## LLM nodes

### nodeAppendPrompt

A node that adds messages to the LLM prompt using the provided prompt builder. This is useful for modifying the conversation context before making an actual LLM request. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-update-prompt.html).

```
graph LR
    in:::hidden
    out:::hidden

    subgraph node ["nodeAppendPrompt"]
        execute(Append prompt)
    end

    in ---|T| execute --T--> out

    classDef hidden display: none;
```

You can use this node for the following purposes:

- Add system instructions to the prompt.
- Insert user messages into the conversation.
- Prepare the context for subsequent LLM requests.

Here is an example:

```kotlin
val firstNode by node<Input, Output> {
    // Transform input to output
}

val secondNode by node<Output, Output> {
    // Transform output to output
}

// Node will get the value of type Output as input from the previous node and path through it to the next node
val setupContext by nodeAppendPrompt<Output>("setupContext") {
    system("You are a helpful assistant specialized in Kotlin programming.")
    user("I need help with Kotlin coroutines.")
}

edge(firstNode forwardTo setupContext)
edge(setupContext forwardTo secondNode)
```

### nodeLLMSendMessageOnlyCallingTools

A node that appends a user message to the LLM prompt and gets a response where the LLM can only call tools. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-l-l-m-send-message-only-calling-tools.html).

```
graph LR
    in:::hidden
    out:::hidden

    subgraph node ["nodeLLMSendMessageOnlyCallingTools"]
        execute(Request LLM expecting only tool calls)
    end

    in --String--> execute --Message.Response--> out

    classDef hidden display: none;
```

### nodeLLMSendMessageForceOneTool

A node that that appends a user message to the LLM prompt and forces the LLM to use a specific tool. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-l-l-m-send-message-force-one-tool.html).

```
graph LR
    in:::hidden
    out:::hidden

    subgraph node ["nodeLLMSendMessageForceOneTool"]
        execute(Request LLM expecting a specific tool call)
    end

    in --String--> execute --Message.Response--> out

    classDef hidden display: none;
```

### nodeLLMRequest

A node that appends a user message to the LLM prompt and gets a response with optional tool usage. The node configuration determines whether tool calls are allowed during the processing of the message. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-l-l-m-request.html).

```
graph LR
    in:::hidden
    out:::hidden

    subgraph node ["nodeLLMRequest"]
        execute(Request LLM)
    end

    in --String--> execute --Message.Response--> out

    classDef hidden display: none;
```

You can use this node for the following purposes:

- Generate LLM response for the current prompt, controlling if the LLM is allowed to generate tool calls.

Here is an example:

```kotlin
val requestLLM by nodeLLMRequest("requestLLM", allowToolCalls = true)
edge(getUserQuestion forwardTo requestLLM)
```

### nodeLLMRequestStructured

A node that appends a user message to the LLM prompt and requests structured data from the LLM with error correction capabilities. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-l-l-m-request-structured.html).

```
graph LR
    in:::hidden
    out:::hidden

    subgraph node ["nodeLLMRequestStructured"]
        execute(Request LLM structured)
    end

    in --String--> execute -- "Result&lt;StructuredResponse&gt;" --> out

    classDef hidden display: none;
```

### nodeLLMRequestStreaming

A node that appends a user message to the LLM prompt and streams LLM response with or without stream data transformation. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-l-l-m-request-streaming.html).

```
graph LR
    in:::hidden
    out:::hidden

    subgraph node ["nodeLLMRequestStreaming"]
        execute(Request LLM streaming)
    end

    in --String--> execute --Flow--> out

    classDef hidden display: none;
```

### nodeLLMRequestMultiple

A node that appends a user message to the LLM prompt and gets multiple LLM responses with tool calls enabled. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-l-l-m-request-multiple.html).

```
graph LR
    in:::hidden
    out:::hidden

    subgraph node ["nodeLLMRequestMultiple"]
        execute(Request LLM expecting multiple responses)
    end

    in --String--> execute -- "List&lt;Message.Response&gt;" --> out

    classDef hidden display: none;
```

You can use this node for the following purposes:

- Handle complex queries that require multiple tool calls.
- Generate multiple tool calls.
- Implement a workflow that requires multiple parallel actions.

Here is an example:

```kotlin
val requestLLMMultipleTools by nodeLLMRequestMultiple()
edge(getComplexUserQuestion forwardTo requestLLMMultipleTools)
```

### nodeLLMCompressHistory

A node that compresses the current LLM prompt (message history) into a summary, replacing messages with a concise summary (TL;DR). For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-l-l-m-compress-history.html). This is useful for managing long conversations by compressing the history to reduce token usage.

```
graph LR
    in:::hidden
    out:::hidden

    subgraph node ["nodeLLMCompressHistory"]
        execute(Compress current prompt)
    end

    in ---|T| execute --T--> out

    classDef hidden display: none;
```

To learn more about history compression, see [History compression](../history-compression/).

You can use this node for the following purposes:

- Manage long conversations to reduce token usage.
- Summarize conversation history to maintain context.
- Implement memory management in long-running agents.

Here is an example:

```kotlin
val compressHistory by nodeLLMCompressHistory<String>(
    "compressHistory",
    strategy = HistoryCompressionStrategy.FromLastNMessages(10),
    preserveMemory = true
)
edge(generateHugeHistory forwardTo compressHistory)
```

## Tool nodes

### nodeExecuteTool

A node that executes a single tool call and returns its result. This node is used to handle tool calls made by the LLM. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-execute-tool.html).

```
graph LR
    in:::hidden
    out:::hidden

    subgraph node ["nodeExecuteTool"]
        execute(Execute tool call)
    end

    in --Message.Tool.Call--> execute --ReceivedToolResult--> out

    classDef hidden display: none;
```

You can use this node for the following purposes:

- Execute tools requested by the LLM.
- Handle specific actions in response to LLM decisions.
- Integrate external functionality into the agent workflow.

Here is an example:

```kotlin
val requestLLM by nodeLLMRequest()
val executeTool by nodeExecuteTool()
edge(requestLLM forwardTo executeTool onToolCall { true })
```

### nodeLLMSendToolResult

A node that adds a tool result to the prompt and requests an LLM response. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-l-l-m-send-tool-result.html).

```
graph LR
    in:::hidden
    out:::hidden

    subgraph node ["nodeLLMSendToolResult"]
        execute(Request LLM)
    end

    in --ReceivedToolResult--> execute --Message.Response--> out

    classDef hidden display: none;
```

You can use this node for the following purposes:

- Process the results of tool executions.
- Generate responses based on tool outputs.
- Continue a conversation after tool execution.

Here is an example:

```kotlin
val executeTool by nodeExecuteTool()
val sendToolResultToLLM by nodeLLMSendToolResult()
edge(executeTool forwardTo sendToolResultToLLM)
```

### nodeExecuteMultipleTools

A node that executes multiple tool calls. These calls can optionally be executed in parallel. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-execute-multiple-tools.html).

```
graph LR
    in:::hidden
    out:::hidden

    subgraph node ["nodeExecuteMultipleTools"]
        execute(Execute multiple tool calls)
    end

    in -- "List&lt;Message.Tool.Call&gt;" --> execute -- "List&lt;ReceivedToolResult&gt;" --> out

    classDef hidden display: none;
```

You can use this node for the following purposes:

- Execute multiple tools in parallel.
- Handle complex workflows that require multiple tool executions.
- Optimize performance by batching tool calls.

Here is an example:

```kotlin
val requestLLMMultipleTools by nodeLLMRequestMultiple()
val executeMultipleTools by nodeExecuteMultipleTools()
edge(requestLLMMultipleTools forwardTo executeMultipleTools onMultipleToolCalls { true })
```

### nodeLLMSendMultipleToolResults

A node that adds multiple tool results to the prompt and gets multiple LLM responses. For details, see [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.extension/node-l-l-m-send-multiple-tool-results.html).

```
graph LR
    in:::hidden
    out:::hidden

    subgraph node ["nodeLLMSendMultipleToolResults"]
        execute(Request LLM expecting multiple responses)
    end

    in -- "List&lt;ReceivedToolResult&gt;" --> execute -- "List&lt;Message.Response&gt;" --> out

    classDef hidden display: none;
```

You can use this node for the following purposes:

- Process the results of multiple tool executions.
- Generate multiple tool calls.
- Implement complex workflows with multiple parallel actions.

Here is an example:

```kotlin
val executeMultipleTools by nodeExecuteMultipleTools()
val sendMultipleToolResultsToLLM by nodeLLMSendMultipleToolResults()
edge(executeMultipleTools forwardTo sendMultipleToolResultsToLLM)
```

## Node output transformation

The framework provides the `transform` extension function that allows you to create transformed versions of nodes that apply transformations to their output. This is useful when you need to convert the output of a node to a different type or format while preserving the original node's functionality.

```
graph LR
    in:::hidden
    out:::hidden

    subgraph nodeWithTransform [transformed node]
        subgraph node ["node"]
            execute(Do stuff)
        end
        transform
    end

    in --Input--> execute --> transform --Output--> out

    classDef hidden display: none;
```

### transform

The [`transform()`](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.builder/-a-i-agent-node-delegate/transform.html) function creates a new `AIAgentNodeDelegate` that wraps the original node and applies a transformation function to its output.

```kotlin
inline fun <reified T> AIAgentNodeDelegate<Input, Output>.transform(
    noinline transformation: suspend (Output) -> T
): AIAgentNodeDelegate<Input, T>
```

#### Custom node transformation

Transform the output of a custom node to a different data type:

```kotlin
val textNode by nodeDoNothing<String>("textNode").transform<Int> { text ->
    text.split(" ").filter { it.isNotBlank() }.size
}

edge(nodeStart forwardTo textNode)
edge(textNode forwardTo nodeFinish)
```

#### Built-in node transformation

Transform the output of built-in nodes like `nodeLLMRequest`:

```kotlin
val lengthNode by nodeLLMRequest("llmRequest").transform<Int> { assistantMessage ->
    assistantMessage.content.length
}

edge(nodeStart forwardTo lengthNode)
edge(lengthNode forwardTo nodeFinish)
```

## Predefined subgraphs

The framework provides predefined subgraphs that encapsulate commonly used patterns and workflows. These subgraphs simplify the development of complex agent strategies by handling the creation of base nodes and edges automatically.

By using the predefined subgraphs, you can implement various popular pipelines. Here is an example:

1. Prepare the data.
1. Run the task.
1. Validate the task results. If the results are incorrect, return to step 2 with a feedback message to make adjustments.

### subgraphWithTask

A subgraph that performs a specific task using provided tools and returns a structured result. It supports multi-response LLM interactions (the assistant may produce several responses interleaved with tool calls) and lets you control how tool calls are executed. For details, see [API reference](https://api.koog.ai/agents/agents-ext/ai.koog.agents.ext.agent/subgraph-with-task.html).

You can use this subgraph for the following purposes:

- Create special components that handle specific tasks within a larger workflow.
- Encapsulate complex logic with clear input and output interfaces.
- Configure task-specific tools, models, and prompts.
- Manage conversation history with automatic compression.
- Develop structured agent workflows and task execution pipelines.
- Generate structured results from LLM task execution, including flows with multiple assistant responses and tool invocations.

The API allows you to fineâ€‘tune execution with optional parameters:

- runMode: controls how tool calls are executed during the task (sequential by default). Use this to switch between different tool execution strategies when supported by the underlying model/executor.
- assistantResponseRepeatMax: limits how many assistant responses are allowed before concluding the task cannot be completed (defaults to a safe internal limit if not provided).

You can provide a task to the subgraph as text, configure the LLM if needed, and provide the necessary tools, and the subgraph will process and solve the task. Here is an example:

```kotlin
val processQuery by subgraphWithTask<String, String>(
    tools = listOf(searchTool, calculatorTool, weatherTool),
    llmModel = OpenAIModels.Chat.GPT4o,
    runMode = ToolCalls.SEQUENTIAL,
    assistantResponseRepeatMax = 3,
) { userQuery ->
    """
    You are a helpful assistant that can answer questions about various topics.
    Please help with the following query:
    $userQuery
    """
}
```

### subgraphWithVerification

A special version of `subgraphWithTask` that verifies whether a task was performed correctly and provides details about any issues encountered. This subgraph is useful for workflows that require validation or quality checks. For details, see [API reference](https://api.koog.ai/agents/agents-ext/ai.koog.agents.ext.agent/subgraph-with-verification.html).

You can use this subgraph for the following purposes:

- Verify the correctness of task execution.
- Implement quality control processes in your workflows.
- Create self-validating components.
- Generate structured verification results with success/failure status and detailed feedback.

The subgraph ensures that the LLM calls a verification tool at the end of the workflow to check whether the task was successfully completed. It guarantees this verification is performed as the final step and returns a `CriticResult` that indicates whether a task was completed successfully and provides detailed feedback. Here is an example:

```kotlin
val verifyCode by subgraphWithVerification<String>(
    tools = listOf(runTestsTool, analyzeTool, readFileTool),
    llmModel = AnthropicModels.Sonnet_3_7,
    runMode = ToolCalls.SEQUENTIAL,
    assistantResponseRepeatMax = 3,
) { codeToVerify ->
    """
    You are a code reviewer. Please verify that the following code meets all requirements:
    1. It compiles without errors
    2. All tests pass
    3. It follows the project's coding standards

    Code to verify:
    $codeToVerify
    """
}
```

## Predefined strategies and common strategy patterns

The framework provides predefined strategies that combine various nodes. The nodes are connected using edges to define the flow of operations, with conditions that specify when to follow each edge.

You can integrate these strategies into your agent workflows if needed.

### Single run strategy

A single run strategy is designed for non-interactive use cases where the agent processes input once and returns a result.

You can use this strategy when you need to run straightforward processes that do not require complex logic.

```kotlin
public fun singleRunStrategy(): AIAgentGraphStrategy<String, String> = strategy("single_run") {
    val nodeCallLLM by nodeLLMRequest("sendInput")
    val nodeExecuteTool by nodeExecuteTool("nodeExecuteTool")
    val nodeSendToolResult by nodeLLMSendToolResult("nodeSendToolResult")

    edge(nodeStart forwardTo nodeCallLLM)
    edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })
    edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })
    edge(nodeExecuteTool forwardTo nodeSendToolResult)
    edge(nodeSendToolResult forwardTo nodeFinish onAssistantMessage { true })
    edge(nodeSendToolResult forwardTo nodeExecuteTool onToolCall { true })
}
```

### Tool-based strategy

A tool-based strategy is designed for workflows that heavily rely on tools to perform specific operations. It typically executes tools based on the LLM decisions and processes the results.

```kotlin
fun toolBasedStrategy(name: String, toolRegistry: ToolRegistry): AIAgentGraphStrategy<String, String> {
    return strategy(name) {
        val nodeSendInput by nodeLLMRequest()
        val nodeExecuteTool by nodeExecuteTool()
        val nodeSendToolResult by nodeLLMSendToolResult()

        // Define the flow of the agent
        edge(nodeStart forwardTo nodeSendInput)

        // If the LLM responds with a message, finish
        edge(
            (nodeSendInput forwardTo nodeFinish)
                    onAssistantMessage { true }
        )

        // If the LLM calls a tool, execute it
        edge(
            (nodeSendInput forwardTo nodeExecuteTool)
                    onToolCall { true }
        )

        // Send the tool result back to the LLM
        edge(nodeExecuteTool forwardTo nodeSendToolResult)

        // If the LLM calls another tool, execute it
        edge(
            (nodeSendToolResult forwardTo nodeExecuteTool)
                    onToolCall { true }
        )

        // If the LLM responds with a message, finish
        edge(
            (nodeSendToolResult forwardTo nodeFinish)
                    onAssistantMessage { true }
        )
    }
}
```

### Streaming data strategy

A streaming data strategy is designed for processing streaming data from the LLM. It typically requests streaming data, processes it, and potentially calls tools with the processed data.

```kotlin
val agentStrategy = strategy<String, List<Book>>("library-assistant") {
    // Describe the node containing the output stream parsing
    val getMdOutput by node<String, List<Book>> { booksDescription ->
        val books = mutableListOf<Book>()
        val mdDefinition = markdownBookDefinition()

        llm.writeSession { 
            appendPrompt { user(booksDescription) }
            // Initiate the response stream in the form of the definition `mdDefinition`
            val markdownStream = requestLLMStreaming(mdDefinition)
            // Call the parser with the result of the response stream and perform actions with the result
            parseMarkdownStreamToBooks(markdownStream).collect { book ->
                books.add(book)
                println("Parsed Book: ${book.title} by ${book.author}")
            }
        }

        books
    }
    // Describe the agent's graph making sure the node is accessible
    edge(nodeStart forwardTo getMdOutput)
    edge(getMdOutput forwardTo nodeFinish)
}
```

# Predefined agent strategies

To make agent implementations easier, Koog provides predefined agent strategies for common agent use cases. The following predefined strategies are available:

- [Chat agent strategy](#chat-agent-strategy)
- [ReAct strategy](#react-strategy)

## Chat agent strategy

The Chat agent strategy is designed for executing a chat interaction process. It orchestrates interactions between different stages, nodes, and tools to handle user input, execute tools, and provide responses in a chat-like manner.

### Overview

The Chat agent strategy implements a pattern where the agent:

1. Receives user input
1. Processes the input using an LLM
1. Either calls a tool or provides a direct response
1. Processes tool results and continues the conversation
1. Provides feedback if the LLM tries to respond with plain text instead of using tools

This approach creates a conversational interface where the agent can use tools to fulfill user requests.

### Setup and dependencies

The implementation of Chat agent strategy in Koog is done through the `chatAgentStrategy` function. To make the function available in your agent code, add the following dependency import:

```text
ai.koog.agents.ext.agent.chatAgentStrategy
```

To use the strategy, create an AI agent following the pattern below:

```kotlin
val chatAgent = AIAgent(
    promptExecutor = promptExecutor,
    toolRegistry = toolRegistry,
    llmModel = model,
    // Set chatAgentStrategy as the agent strategy
    strategy = chatAgentStrategy()
)
```

### When to use the Chat agent strategy

The Chat agent strategy is particularly useful for:

- Building conversational agents that need to use tools
- Creating assistants that can perform actions based on user requests
- Implementing chatbots that need to access external systems or data
- Scenarios where you want to enforce tool usage rather than plain text responses

### Example

Here is a code sample of an AI agent that implements the predefined Chat agent strategy (`chatAgentStrategy`) and tools that the agent may use:

```kotlin
val chatAgent = AIAgent(
    promptExecutor = promptExecutor,
    llmModel = model,
    // Use chatAgentStrategy as the agent strategy
    strategy = chatAgentStrategy(),
    // Add tools the agent can use
    toolRegistry = ToolRegistry {
        tool(searchTool)
        tool(weatherTool)
    }
)

suspend fun main() { 
    // Run the agent with a user query
    val result = chatAgent.run("What's the weather like today and should I bring an umbrella?")
}
```

## ReAct strategy

The ReAct (Reasoning and Acting) strategy is an AI agent strategy that alternates between reasoning and execution stages to dynamically process tasks and request output from a Large Language Model (LLM).

### Overview

The ReAct strategy implements a pattern where the agent:

1. Reasons about the current state and plans the next steps
1. Takes actions based on that reasoning
1. Observes the results of those actions
1. Repeats the cycle

This approach combines the strengths of reasoning (thinking through problems step by step) and acting (executing tools to gather information or perform operations).

### Flow diagram

Here is the flow diagram of the ReAct strategy:

### Setup and dependencies

The implementation of ReAct strategy in Koog is done through the `reActStrategy` function. To make the function available in your agent code, add the following dependency import:

```text
ai.koog.agents.ext.agent.reActStrategy
```

To use the strategy, create an AI agent following the pattern below:

```kotlin
val reActAgent = AIAgent(
    promptExecutor = promptExecutor,
    toolRegistry = toolRegistry,
    llmModel = model,
    // Set reActStrategy as the agent strategy
    strategy = reActStrategy(
        // Set optional parameter values
        reasoningInterval = 1,
        name = "react_agent"
    )
)
```

### Parameters

The `reActStrategy` function takes the following parameters:

| Parameter           | Type   | Default  | Description                                                         |
| ------------------- | ------ | -------- | ------------------------------------------------------------------- |
| `reasoningInterval` | Int    | 1        | Specifies the interval for reasoning steps. Must be greater than 0. |
| `name`              | String | `re_act` | The name of the strategy.                                           |

### Example use case

Here is an example of how the ReAct strategy works with a simple banking agent:

#### 1. User input

The user sends the initial prompt. For example, this can be a question such as `How much did I spend last month?`.

#### 2. Reasoning

The agent performs the initial reasoning by taking the user input and the reasoning prompt. The reasoning can look as follows:

```text
I need to follow these steps:
1. Get all transactions from last month
2. Filter out deposits (positive amounts)
3. Calculate total spending
```

#### 3. Action and execution, phase 1

Based on the action items that the agent defined in the previous step, it runs a tool to get all transactions from the previous month.

In this case, the tool to run is `get_transactions`, along with the defined `startDate` and `endDate` arguments that match the request to get all transactions during the previous month:

```text
{tool: "get_transactions", args: {startDate: "2025-05-19", endDate: "2025-06-18"}}
```

The tool returns a result that can look as follows:

```text
[
  {date: "2025-05-25", amount: -100.00, description: "Grocery Store"},
  {date: "2025-05-31", amount: +1000.00, description: "Salary Deposit"},
  {date: "2025-06-10", amount: -500.00, description: "Rent Payment"},
  {date: "2025-06-13", amount: -200.00, description: "Utilities"}
]
```

#### 4. Reasoning

With the result returned by the tool, the agent performs reasoning again to determine the next steps in its flow:

```text
I have the transactions. Now I need to:
1. Remove the salary deposit of +1000.00
2. Sum up the remaining transactions
```

#### 5. Action and execution, phase 2

Based on the previous reasoning step, the agent calls the `calculate_sum` tool that sums up the amounts provided as tool arguments. As the reasoning also resulted in the action point of removing the positive amount from transactions, the amounts provided as tool arguments are only the negative ones:

```text
{tool: "calculate_sum", args: {amounts: [-100.00, -500.00, -200.00]}}
```

The tool returns the final result:

```text
-800.00
```

#### 6. Final response

The agent returns the final response (assistant message) that includes the calculated sum:

```text
You spent $800.00 last month on groceries, rent, and utilities.
```

### When to use the ReAct strategy

The ReAct strategy is particularly useful for:

- Complex tasks requiring multistep reasoning
- Scenarios where the agent needs to gather information before providing a final answer
- Problems that benefit from breaking down into smaller steps
- Tasks requiring both analytical thinking and tool usage

### Example

Here is a code sample of an AI agent that implements the predefined ReAct strategy (`reActStrategy`) and tools that the agent may use:

```kotlin
val bankingAgent = AIAgent(
    promptExecutor = promptExecutor,
    llmModel = model,
    // Use reActStrategy as the agent strategy
    strategy = reActStrategy(
        reasoningInterval = 1,
        name = "banking_agent"
    ),
    // Add tools the agent can use
    toolRegistry = ToolRegistry {
        tool(getTransactions)
        tool(calculateSum)
    }
)

suspend fun main() { 
    // Run the agent with a user query
    val result = bankingAgent.run("How much did I spend last month?")
}
```

# Custom strategy graphs

Strategy graphs are the backbone of agent workflows in the Koog framework. They define how the agent processes input, interacts with tools, and generates output. A strategy graph consists of nodes connected by edges, with conditions determining the flow of execution.

Creating a strategy graph lets you tailor the behavior of an agent to your specific needs, whether you are building a simple chatbot, a complex data processing pipeline, or anything in between.

## Strategy graph architecture

At a high level, a strategy graph consists of the following components:

- **Strategy**: the top-level container for the graph, created using the `strategy` function with the specified input and output types using generic parameters.
- **Subgraphs**: sections of the graph that can have their own set of tools and context.
- **Nodes**: individual operations or transformations in the workflow.
- **Edges**: connections between nodes that define transition conditions and transformations.

The strategy graph begins at a special node called `nodeStart` and ends at `nodeFinish`. The path between these nodes is determined by the edges and conditions specified in the graph.

## Strategy graph components

### Nodes

Nodes are building blocks of a strategy graph. Each node represents a specific operation.

The Koog framework provides predefined nodes and also lets you create custom nodes by using the `node` function.

For details, see [Predefined nodes and components](../nodes-and-components/) and [Custom nodes](../custom-nodes/).

### Edges

Edges connect nodes and define the flow of operation in the strategy graph. An edge is created using the `edge` function and the `forwardTo` infix function:

```kotlin
edge(sourceNode forwardTo targetNode)
```

#### Conditions

Conditions determine when to follow a particular edge in the strategy graph. There are several types of conditions, here are some common ones:

| Condition type      | Description                                                                              |
| ------------------- | ---------------------------------------------------------------------------------------- |
| onCondition         | A general-purpose condition that takes a lambda expression that returns a boolean value. |
| onToolCall          | A condition that matches when the LLM calls a tool.                                      |
| onAssistantMessage  | A condition that matches when the LLM responds with a message.                           |
| onMultipleToolCalls | A condition that matches when the LLM calls multiple tools.                              |
| onToolNotCalled     | A condition that matches when the LLM does not call a tool.                              |

You can transform the output before passing it to the target node by using the `transformed` function:

```kotlin
edge(sourceNode forwardTo targetNode 
        onCondition { input -> input.length > 10 }
        transformed { input -> input.uppercase() }
)
```

### Subgraphs

Subgraphs are sections of the strategy graph that operate with their own set of tools and context. The strategy graph can contain multiple subgraphs. Each subgraph is defined by using the `subgraph` function:

```kotlin
val strategy = strategy<Input, Output>("strategy-name") {
    val firstSubgraph by subgraph<FirstInput, FirstOutput>("first") {
        // Define nodes and edges for this subgraph
    }
    val secondSubgraph by subgraph<SecondInput, SecondOutput>("second") {
        // Define nodes and edges for this subgraph
    }
}
```

A subgraph can use any tool from a tool registry. However, you can specify a subset of tools from this registry that can be used in the subgraph and pass it as an argument to the `subgraph` function:

```kotlin
val strategy = strategy<Input, Output>("strategy-name") {
    val firstSubgraph by subgraph<FirstInput, FirstOutput>(
        name = "first",
        tools = listOf(someTool)
    ) {
        // Define nodes and edges for this subgraph
    }
   // Define other subgraphs
}
```

## Basic strategy graph creation

The basic strategy graph operates as follows:

1. Sends the input to the LLM.
1. If the LLM responds with a message, finishes the process.
1. If the LLM calls a tool, runs the tool.
1. Sends the tool result back to the LLM.
1. If the LLM responds with a message, finishes the process.
1. If the LLM calls another tool, runs the tool, and the process repeats from step 4.

Here is an example of a basic strategy graph:

```kotlin
val myStrategy = strategy<String, String>("my-strategy") {
    val nodeCallLLM by nodeLLMRequest()
    val executeToolCall by nodeExecuteTool()
    val sendToolResult by nodeLLMSendToolResult()

    edge(nodeStart forwardTo nodeCallLLM)
    edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })
    edge(nodeCallLLM forwardTo executeToolCall onToolCall { true })
    edge(executeToolCall forwardTo sendToolResult)
    edge(sendToolResult forwardTo nodeFinish onAssistantMessage { true })
    edge(sendToolResult forwardTo executeToolCall onToolCall { true })
}
```

## Visualizing strategy graph

On JVM you may generate a [Mermaid state diagram](https://mermaid.js.org/syntax/stateDiagram.html) for the strategy graph.

For the graph created in the previous example, you can run:

```kotlin
val mermaidDiagram: String = myStrategy.asMermaidDiagram()

println(mermaidDiagram)
```

and the output will be:

```
---
title: my-strategy
---
stateDiagram
    state "nodeCallLLM" as nodeCallLLM
    state "executeToolCall" as executeToolCall
    state "sendToolResult" as sendToolResult

    [*] --> nodeCallLLM
    nodeCallLLM --> [*] : transformed
    nodeCallLLM --> executeToolCall : onCondition
    executeToolCall --> sendToolResult
    sendToolResult --> [*] : transformed
    sendToolResult --> executeToolCall : onCondition
```

## Advanced strategy techniques

### History compression

For long-running conversations, the history can grow large and consume a lot of tokens. To learn how to compress the history, see [History compression](../history-compression/).

### Parallel tool execution

For workflows that require executing multiple tools in parallel, you can use the `nodeExecuteMultipleTools` node:

```kotlin
val executeMultipleTools by nodeExecuteMultipleTools()
val processMultipleResults by nodeLLMSendMultipleToolResults()

edge(someNode forwardTo executeMultipleTools)
edge(executeMultipleTools forwardTo processMultipleResults)
```

You can also use the `toParallelToolCallsRaw` extension function for streaming data:

```kotlin
parseMarkdownStreamToBooks(markdownStream).toParallelToolCallsRaw(BookTool::class).collect()
```

To learn more, see [Tools](../tools-overview/#parallel-tool-calls).

### Parallel node execution

Parallel node execution lets you run multiple nodes concurrently, improving performance and enabling complex workflows.

To initiate parallel node runs, use the `parallel` method:

```kotlin
val calc by parallel<String, Int>(
    nodeCalcTokens, nodeCalcSymbols, nodeCalcWords,
) {
    selectByMax { it }
}
```

The code above creates a node named `calc` that runs the `nodeCalcTokens`, `nodeCalcSymbols`, and `nodeCalcWords` nodes in parallel and returns the results as an instance of `AsyncParallelResult`.

For more information related to parallel node execution and a detailed reference, see [Parallel node execution](../parallel-node-execution/).

### Conditional branching

For complex workflows that require different paths based on certain conditions, you can use conditional branching:

```kotlin
val branchA by node<String, String> { input ->
    // Logic for branch A
    "Branch A: $input"
}

val branchB by node<String, String> { input ->
    // Logic for branch B
    "Branch B: $input"
}

edge(
    (someNode forwardTo branchA)
            onCondition { input -> input.contains("A") }
)
edge(
    (someNode forwardTo branchB)
            onCondition { input -> input.contains("B") }
)
```

## Best practices

When you create custom strategy graphs, follow these best practices:

- Keep it simple. Start with a simple graph and add complexity as needed.
- Give your nodes and edges descriptive names to make the graph easier to understand.
- Handle all possible paths and edge cases.
- Test your graph with various inputs to ensure it behaves as expected.
- Document the purpose and behavior of your graph for future reference.
- Use predefined strategies or common patterns as a starting point.
- For long-running conversations, use history compression to reduce token usage.
- Use subgraphs to organize your graph and manage tool access.

## Usage examples

### Tone analysis strategy

The tone analysis strategy is a good example of a tool-based strategy that includes history compression:

```kotlin
fun toneStrategy(name: String, toolRegistry: ToolRegistry): AIAgentGraphStrategy<String, String> {
    return strategy(name) {
        val nodeSendInput by nodeLLMRequest()
        val nodeExecuteTool by nodeExecuteTool()
        val nodeSendToolResult by nodeLLMSendToolResult()
        val nodeCompressHistory by nodeLLMCompressHistory<ReceivedToolResult>()

        // Define the flow of the agent
        edge(nodeStart forwardTo nodeSendInput)

        // If the LLM responds with a message, finish
        edge(
            (nodeSendInput forwardTo nodeFinish)
                    onAssistantMessage { true }
        )

        // If the LLM calls a tool, execute it
        edge(
            (nodeSendInput forwardTo nodeExecuteTool)
                    onToolCall { true }
        )

        // If the history gets too large, compress it
        edge(
            (nodeExecuteTool forwardTo nodeCompressHistory)
                    onCondition { _ -> llm.readSession { prompt.messages.size > 100 } }
        )

        edge(nodeCompressHistory forwardTo nodeSendToolResult)

        // Otherwise, send the tool result directly
        edge(
            (nodeExecuteTool forwardTo nodeSendToolResult)
                    onCondition { _ -> llm.readSession { prompt.messages.size <= 100 } }
        )

        // If the LLM calls another tool, execute it
        edge(
            (nodeSendToolResult forwardTo nodeExecuteTool)
                    onToolCall { true }
        )

        // If the LLM responds with a message, finish
        edge(
            (nodeSendToolResult forwardTo nodeFinish)
                    onAssistantMessage { true }
        )
    }
}
```

This strategy does the following:

1. Sends the input to the LLM.
1. If the LLM responds with a message, the strategy finishes the process.
1. If the LLM calls a tool, the strategy runs the tool.
1. If the history is too large (more than 100 messages), the strategy compresses it before sending the tool result.
1. Otherwise, the strategy sends the tool result directly.
1. If the LLM calls another tool, the strategy runs it.
1. If the LLM responds with a message, the strategy finishes the process.

## Troubleshooting

When creating custom strategy graphs, you might encounter some common issues. Here are some troubleshooting tips:

### Graph fails to reach the finish node

If your graph does not reach the finish node, check the following:

- All paths from the start node eventually lead to the finish node.
- Your conditions are not too restrictive, preventing edges from being followed.
- There are no cycles in the graph that do not have an exit condition.

### Tool calls are not running

If tool calls are not running, check the following:

- The tools are properly registered in the tool registry.
- The edge from the LLM node to the tool execution node has the correct condition (`onToolCall { true }`).

### History gets too large

If your history gets too large and consumes too many tokens, consider the following:

- Add a history compression node.
- Use a condition to check the size of the history and compress it when it gets too large.
- Use a more aggressive compression strategy (e.g., `FromLastNMessages` with a smaller N value).

### Graph behaves unexpectedly

If your graph takes unexpected branches, check the following:

- Your conditions are correctly defined.
- The conditions are evaluated in the expected order (edges are checked in the order they are defined).
- You are not accidentally overriding conditions with more general ones.

### Performance issues occur

If your graph has performance issues, consider the following:

- Simplify the graph by removing unnecessary nodes and edges.
- Use parallel tool execution for independent operations.
- Compress history.
- Use more efficient nodes and operations.

## Overview

Parallel node execution lets you run multiple AI agent nodes concurrently, improving performance and enabling complex workflows. This feature is particularly useful when you need to:

- Process the same input through different models or approaches simultaneously
- Perform multiple independent operations in parallel
- Implement competitive evaluation patterns where multiple solutions are generated and then compared

## Key components

Parallel node execution in Koog consists of the methods and data structures described below.

### Methods

- `parallel()`: executes multiple nodes in parallel and collects their results.

### Data structures

- `ParallelResult`: represents the completed result of a parallel node execution.
- `NodeExecutionResult`: contains the output and context of a node execution.

## Basic usage

### Running nodes in parallel

To initiate parallel execution of nodes, use the `parallel` method in the following format:

```kotlin
val nodeName by parallel<Input, Output>(
   firstNode, secondNode, thirdNode /* Add more nodes if needed */
) {
   // Merge strategy goes here, for example: 
   selectByMax { it.length }
}
```

Here is an actual example of running three nodes in parallel and selecting the result with the maximum length:

```kotlin
val calc by parallel<String, Int>(
   nodeCalcTokens, nodeCalcSymbols, nodeCalcWords,
) {
   selectByMax { it }
}
```

The code above runs the `nodeCalcTokens`, `nodeCalcSymbols`, and `nodeCalcWords` nodes in parallel and returns the result with the maximum value.

### Merge strategies

After executing nodes in parallel, you need to specify how to merge the results. Koog provides the following merge strategies:

- `selectBy()`: selects a result based on a predicate function.
- `selectByMax()`: selects the result with the maximum value based on a comparison function.
- `selectByIndex()`: selects a result based on an index returned by a selection function.
- `fold()`: folds the results into a single value using an operation function.

#### selectBy

Selects a result based on a predicate function:

```kotlin
val nodeSelectJoke by parallel<String, String>(
   nodeOpenAI, nodeAnthropicSonnet, nodeAnthropicOpus,
) {
   selectBy { it.contains("programmer") }
}
```

This selects the first joke that contains the word "programmer".

#### selectByMax

Selects the result with the maximum value based on a comparison function:

```kotlin
val nodeLongestJoke by parallel<String, String>(
   nodeOpenAI, nodeAnthropicSonnet, nodeAnthropicOpus,
) {
   selectByMax { it.length }
}
```

This selects the joke with the maximum length.

#### selectByIndex

Selects a result based on an index returned by a selection function:

```kotlin
val nodeBestJoke by parallel<String, String>(
   nodeOpenAI, nodeAnthropicSonnet, nodeAnthropicOpus,
) {
   selectByIndex { jokes ->
      // Use another LLM to determine the best joke
      llm.writeSession {
         model = OpenAIModels.Chat.GPT4o
         appendPrompt {
            system("You are a comedy critic. Select the best joke.")
            user("Here are three jokes: ${jokes.joinToString("\n\n")}")
         }
         val response = requestLLMStructured<JokeRating>()
         response.getOrNull()!!.data.bestJokeIndex
      }
   }
}
```

This uses another LLM call to determine the index of the best joke.

#### fold

Folds the results into a single value using an operation function:

```kotlin
val nodeAllJokes by parallel<String, String>(
   nodeOpenAI, nodeAnthropicSonnet, nodeAnthropicOpus,
) {
   fold("Jokes:\n") { result, joke -> "$result\n$joke" }
}
```

This combines all jokes into a single string.

## Example: Best joke agent

Here is a complete example that uses parallel execution to generate jokes from different LLM models and select the best one:

```kotlin
val strategy = strategy("best-joke") {
   // Define nodes for different LLM models
   val nodeOpenAI by node<String, String> { topic ->
      llm.writeSession {
         model = OpenAIModels.Chat.GPT4o
         appendPrompt {
            system("You are a comedian. Generate a funny joke about the given topic.")
            user("Tell me a joke about $topic.")
         }
         val response = requestLLMWithoutTools()
         response.content
      }
   }

   val nodeAnthropicSonnet by node<String, String> { topic ->
      llm.writeSession {
         model = AnthropicModels.Sonnet_3_5
         appendPrompt {
            system("You are a comedian. Generate a funny joke about the given topic.")
            user("Tell me a joke about $topic.")
         }
         val response = requestLLMWithoutTools()
         response.content
      }
   }

   val nodeAnthropicOpus by node<String, String> { topic ->
      llm.writeSession {
         model = AnthropicModels.Opus_3
         appendPrompt {
            system("You are a comedian. Generate a funny joke about the given topic.")
            user("Tell me a joke about $topic.")
         }
         val response = requestLLMWithoutTools()
         response.content
      }
   }

   // Execute joke generation in parallel and select the best joke
   val nodeGenerateBestJoke by parallel(
      nodeOpenAI, nodeAnthropicSonnet, nodeAnthropicOpus,
   ) {
      selectByIndex { jokes ->
         // Another LLM (e.g., GPT4o) would find the funniest joke:
         llm.writeSession {
            model = OpenAIModels.Chat.GPT4o
            appendPrompt {
               prompt("best-joke-selector") {
                  system("You are a comedy critic. Give a critique for the given joke.")
                  user(
                     """
                            Here are three jokes about the same topic:

                            ${jokes.mapIndexed { index, joke -> "Joke $index:\n$joke" }.joinToString("\n\n")}

                            Select the best joke and explain why it's the best.
                            """.trimIndent()
                  )
               }
            }

            val response = requestLLMStructured<JokeRating>()
            val bestJoke = response.getOrNull()!!.data
            bestJoke.bestJokeIndex
         }
      }
   }

   // Connect the nodes
   nodeStart then nodeGenerateBestJoke then nodeFinish
}
```

## Best practices

1. **Consider resource constraints**: Be mindful of resource usage when executing nodes in parallel, especially when making multiple LLM API calls simultaneously.

1. **Context management**: Each parallel execution creates a forked context. When merging results, choose which context to preserve or how to combine contexts from different executions.

1. **Optimize for your use case**:

   - For competitive evaluation (like the joke example), use `selectByIndex` to select the best result
   - For finding the maximum value, use `selectByMax`
   - For filtering based on a condition, use `selectBy`
   - For aggregation, use `fold` to combine all results into a composite output

## Performance considerations

Parallel execution can significantly improve throughput, but it comes with some overhead:

- Each parallel node creates a new coroutine
- Context forking and merging add some computational cost
- Resource contention may occur with many parallel executions

For optimal performance, parallelize operations that:

- Are independent of each other
- Have significant execution time
- Don't share mutable state

## Overview

Koog provides a way to store and pass data using `AIAgentStorage`, which is a key-value storage system designed as a type-safe way to pass data between different nodes or even subgraphs.

The storage is accessible through the `storage` property (`storage: AIAgentStorage`) available in agent nodes, allowing for seamless data sharing across different components of your AI agent system.

## Key and value structure

The key-value data storage structure relies on the `AIAgentStorageKey` data class. For more information about `AIAgentStorageKey`, see the sections below.

### AIAgentStorageKey

The storage uses a typed key system to ensure type safety when storing and retrieving data:

- `AIAgentStorageKey<T>`: A data class that represents a storage key used for identifying and accessing data. Here are the key features of the `AIAgentStorageKey` class:
  - The generic type parameter `T` specifies the type of data associated with this key, ensuring type safety.
  - Each key has a `name` property which is a string identifier that uniquely represents the storage key.

## Usage examples

The following sections provide an actual example of creating a storage key and using it to store and retrieve data.

### Defining a class that represents your data

The first step in storing data that you want to pass is creating a class that represents your data. Here is an example of a simple class with basic user data:

```kotlin
class UserData(
   val name: String,
   val age: Int
)
```

Once defined, use the class to create a storage key as described below.

### Creating a storage key

Create a typed storage key for the defined data structure:

```kotlin
val userDataKey = createStorageKey<UserData>("user-data")
```

The `createStorageKey` function takes a single string parameter that uniquely identifies the key.

### Storing data

To save data using a created storage key, use the `storage.set(key: AIAgentStorageKey<T>, value: T)` method in a node:

```kotlin
val nodeSaveData by node<Unit, Unit> {
    storage.set(userDataKey, UserData("John", 26))
}
```

### Retrieving data

To retrieve the data, use the `storage.get` method in a node:

```kotlin
val nodeRetrieveData by node<String, Unit> { message ->
    storage.get(userDataKey)?.let { userFromStorage ->
        println("Hello dear $userFromStorage, here's a message for you: $message")
    }
}
```

## API documentation

For a complete reference related to the `AIAgentStorage` class, see [AIAgentStorage](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.agent.entity/-a-i-agent-storage/index.html).

For individual functions available in the `AIAgentStorage` class, see the following API references:

- [clear](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.agent.entity/-a-i-agent-storage/clear.html)
- [get](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.agent.entity/-a-i-agent-storage/get.html)
- [getValue](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.agent.entity/-a-i-agent-storage/get-value.html)
- [putAll](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.agent.entity/-a-i-agent-storage/put-all.html)
- [remove](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.agent.entity/-a-i-agent-storage/remove.html)
- [set](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.agent.entity/-a-i-agent-storage/set.html)
- [toMap](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.agent.entity/-a-i-agent-storage/to-map.html)

## Additional information

- `AIAgentStorage` is thread-safe, using a Mutex to ensure concurrent access is handled properly.
- The storage is designed to work with any type that extends `Any`.
- When retrieving values, type casting is handled automatically, ensuring type safety throughout your application.
- For non-nullable access to values, use the `getValue` method which throws an exception if the key does not exist.
- You can clear the storage entirely using the `clear` method, which removes all stored key-value pairs.
# History compression

# History compression

AI agents maintain a message history that includes user messages, assistant responses, tool calls, and tool responses. This history grows with each interaction as the agent follows its strategy.

For long-running conversations, the history can become large and consume a lot of tokens. History compression helps reduce this by summarizing the full list of messages into one or several messages that contain only important information necessary for further agent operation.

History compression addresses key challenges in agent systems:

- Optimizes context usage. Focused and smaller contexts improve LLM performance and prevent failures from exceeding token limits.
- Improves performance. Compressing history reduces the number of messages the LLM processes, resulting in faster responses.
- Enhances accuracy. Focusing on relevant information helps the LLM remain focused and complete tasks without distractions.
- Reduces costs. Reducing irrelevant messages lowers token usage, decreasing the overall cost of API calls.

## When to compress history

History compression is performed at specific steps in the agent workflow:

- Between logical steps (subgraphs) of the agent strategy.
- When context becomes too long.

## History compression implementation

There are two main approaches to implementing history compression in your agent:

- In a strategy graph.
- In a custom node.

### History compression in a strategy graph

To compress the history in a strategy graph, you need to use the `nodeLLMCompressHistory` node. Depending on which step you decide to perform compression, the following scenarios are available:

- To compress the history when it becomes too long, you can define a helper function and add the `nodeLLMCompressHistory` node to your strategy graph with the following logic:

```kotlin
// Define that the history is too long if there are more than 100 messages
private suspend fun AIAgentContext.historyIsTooLong(): Boolean = llm.readSession { prompt.messages.size > 100 }

val strategy = strategy<String, String>("execute-with-history-compression") {
    val callLLM by nodeLLMRequest()
    val executeTool by nodeExecuteTool()
    val sendToolResult by nodeLLMSendToolResult()

    // Compress the LLM history and keep the current ReceivedToolResult for the next node
    val compressHistory by nodeLLMCompressHistory<ReceivedToolResult>()

    edge(nodeStart forwardTo callLLM)
    edge(callLLM forwardTo nodeFinish onAssistantMessage { true })
    edge(callLLM forwardTo executeTool onToolCall { true })

    // Compress history after executing any tool if the history is too long 
    edge(executeTool forwardTo compressHistory onCondition { historyIsTooLong() })
    edge(compressHistory forwardTo sendToolResult)
    // Otherwise, proceed to the next LLM request
    edge(executeTool forwardTo sendToolResult onCondition { !historyIsTooLong() })

    edge(sendToolResult forwardTo executeTool onToolCall { true })
    edge(sendToolResult forwardTo nodeFinish onAssistantMessage { true })
}
```

In this example, the strategy checks if the history is too long after each tool call. The history is compressed before sending the tool result back to the LLM. This prevents the context from growing during long conversations.

- To compress the history between the logical steps (subgraphs) of your strategy, you can implement your strateg as follows:

```kotlin
val strategy = strategy<String, String>("execute-with-history-compression") {
    val collectInformation by subgraph<String, String> {
        // Some steps to collect the information
    }
    val compressHistory by nodeLLMCompressHistory<String>()
    val makeTheDecision by subgraph<String, String> {
        // Some steps to make the decision based on the current compressed history and collected information
    }

    nodeStart then collectInformation then compressHistory then makeTheDecision
}
```

In this example, the history is compressed after completing the information collection phase, but before proceeding to the decision-making phase.

### History compression in a custom node

If you are implementing a custom node, you can compress history using the `replaceHistoryWithTLDR()` function as follows:

```kotlin
llm.writeSession {
    replaceHistoryWithTLDR()
}
```

This approach gives you more flexibility to implement compression at any point in your custom node logic, based on your specific requirements.

To learn more about custom nodes, see [Custom nodes](../custom-nodes/).

## History compression strategies

You can customize the compression process by passing an optional `strategy` parameter to `nodeLLMCompressHistory(strategy=...)` or to `replaceHistoryWithTLDR(strategy=...)`. The framework provides several built-in strategies.

### WholeHistory (Default)

The default strategy that compresses the entire history into one TLDR message that summarizes what has been achieved so far. This strategy works well for most general use cases where you want to maintain awareness of the entire conversation context while reducing token usage.

You can use it as follows:

- In a strategy graph:

```kotlin
val compressHistory by nodeLLMCompressHistory<ProcessedInput>(
    strategy = HistoryCompressionStrategy.WholeHistory
)
```

- In a custom node:

```kotlin
llm.writeSession {
    replaceHistoryWithTLDR(strategy = HistoryCompressionStrategy.WholeHistory)
}
```

### FromLastNMessages

The strategy compresses only the last `n` messages into a TLDR message and completely discards earlier messages. This is useful when only the latest achievements of the agent (or the latest discovered facts, the latest context) are relevant for solving the problem.

You can use it as follows:

- In a strategy graph:

```kotlin
val compressHistory by nodeLLMCompressHistory<ProcessedInput>(
    strategy = HistoryCompressionStrategy.FromLastNMessages(5)
)
```

- In a custom node:

```kotlin
llm.writeSession {
    replaceHistoryWithTLDR(strategy = HistoryCompressionStrategy.FromLastNMessages(5))
}
```

### Chunked

The strategy splits the whole message history into chunks of a fixed size and compresses each chunk independently into a TLDR message. This is useful when you need not only the concise TLDR of what has been done so far but also want to keep track of the overall progress, and some older information might also be important.

You can use it as follows:

- In a strategy graph:

```kotlin
val compressHistory by nodeLLMCompressHistory<ProcessedInput>(
    strategy = HistoryCompressionStrategy.Chunked(10)
)
```

- In a custom node:

```kotlin
llm.writeSession {
    replaceHistoryWithTLDR(strategy = HistoryCompressionStrategy.Chunked(10))
}
```

### RetrieveFactsFromHistory

The strategy searches for specific facts relevant to the provided list of concepts in the history and retrieves them. It changes the whole history to just these facts and leaves them as context for future LLM requests. This is useful when you have an idea of what exact facts will be relevant for the LLM to perform better on the task.

You can use it as follows:

- In a strategy graph:

```kotlin
val compressHistory by nodeLLMCompressHistory<ProcessedInput>(
    strategy = RetrieveFactsFromHistory(
        Concept(
            keyword = "user_preferences",
            // Description to the LLM -- what specifically to search for
            description = "User's preferences for the recommendation system, including the preferred conversation style, theme in the application, etc.",
            // LLM would search for multiple relevant facts related to this concept:
            factType = FactType.MULTIPLE
        ),
        Concept(
            keyword = "product_details",
            // Description to the LLM -- what specifically to search for
            description = "Brief details about products in the catalog the user has been checking",
            // LLM would search for multiple relevant facts related to this concept:
            factType = FactType.MULTIPLE
        ),
        Concept(
            keyword = "issue_solved",
            // Description to the LLM -- what specifically to search for
            description = "Was the initial user's issue resolved?",
            // LLM would search for a single answer to the question:
            factType = FactType.SINGLE
        )
    )
)
```

- In a custom node:

```kotlin
llm.writeSession {
    replaceHistoryWithTLDR(
        strategy = RetrieveFactsFromHistory(
            Concept(
                keyword = "user_preferences", 
                // Description to the LLM -- what specifically to search for
                description = "User's preferences for the recommendation system, including the preferred conversation style, theme in the application, etc.",
                // LLM would search for multiple relevant facts related to this concept:
                factType = FactType.MULTIPLE
            ),
            Concept(
                keyword = "product_details",
                // Description to the LLM -- what specifically to search for
                description = "Brief details about products in the catalog the user has been checking",
                // LLM would search for multiple relevant facts related to this concept:
                factType = FactType.MULTIPLE
            ),
            Concept(
                keyword = "issue_solved",
                // Description to the LLM -- what specifically to search for
                description = "Was the initial user's issue resolved?",
                // LLM would search for a single answer to the question:
                factType = FactType.SINGLE
            )
        )
    )
}
```

## Custom history compression strategy implementation

You can create your own history compression strategy by extending the `HistoryCompressionStrategy` abstract class and implementing the `compress` method.

Here is an example:

```kotlin
class MyCustomCompressionStrategy : HistoryCompressionStrategy() {
    override suspend fun compress(
        llmSession: AIAgentLLMWriteSession,
        memoryMessages: List<Message>
    ) {
        // 1. Process the current history in llmSession.prompt.messages
        // 2. Create new compressed messages
        // 3. Update the prompt with the compressed messages

        // Save original messages to preserve them
        val originalMessages = llmSession.prompt.messages

        // Example implementation:
        val importantMessages = llmSession.prompt.messages.filter {
            // Your custom filtering logic
            it.content.contains("important")
        }.filterIsInstance<Message.Response>()

        // Note: you can also make LLM requests using the `llmSession` and ask the LLM to do some job for you using, for example, `llmSession.requestLLMWithoutTools()`
        // Or you can change the current model: `llmSession.model = AnthropicModels.Sonnet_3_7` and ask some other LLM model -- but don't forget to change it back after

        // Compose the prompt with the filtered messages
        val compressedMessages = composeMessageHistory(
            originalMessages,
            importantMessages,
            memoryMessages
        )
    }
}
```

In this example, the custom strategy filters messages that contain the word "important" and keeps only those in the compressed history.

Then you can use it as follows:

- In a strategy graph:

```kotlin
val compressHistory by nodeLLMCompressHistory<ProcessedInput>(
    strategy = MyCustomCompressionStrategy()
)
```

- In a custom node:

```kotlin
llm.writeSession {
    replaceHistoryWithTLDR(strategy = MyCustomCompressionStrategy())
}
```

## Memory preservation during compression

All history compression methods have the `preserveMemory` parameter that determines whether memory-related messages should be preserved during compression. These are messages that contain facts retrieved from memory or indicate that the memory feature is not enabled.

You can use the `preserveMemory` parameter as follows:

- In a strategy graph:

```kotlin
val compressHistory by nodeLLMCompressHistory<ProcessedInput>(
    strategy = HistoryCompressionStrategy.WholeHistory,
    preserveMemory = true
)
```

- In a custom node:

```kotlin
llm.writeSession {
    replaceHistoryWithTLDR(
        strategy = HistoryCompressionStrategy.WholeHistory,
        preserveMemory = true
    )
}
```
# Model Context Protocol

# Model Context Protocol

Model Context Protocol (MCP) is a standardized protocol that lets AI agents interact with external tools and services through a consistent interface.

MCP exposes tools and prompts as API endpoints that AI agents can call. Each tool has a specific name and an input schema that describes its inputs and outputs using the JSON Schema format.

The Koog framework provides integration with MCP servers, enabling you to incorporate MCP tools into your Koog agents.

To learn more about the protocol, see the [Model Context Protocol](https://modelcontextprotocol.io) documentation.

## MCP servers

MCP servers implement Model Context Protocol and provide a standardized way for AI agents to interact with tools and services.

You can find ready-to-use MCP servers in the [MCP Marketplace](https://mcp.so/) or [MCP DockerHub](https://hub.docker.com/u/mcp).

The MCP servers support the following transport protocols to communicate with agents:

- Standard input/output (stdio) transport protocol used to communicate with the MCP servers running as separate processes. For example, a Docker container or a CLI tool.
- Server-sent events (SSE) transport protocol (optional) used to communicate with the MCP servers over HTTP.

## Integration with Koog

The Koog framework integrates with MCP using the [MCP SDK](https://github.com/modelcontextprotocol/kotlin-sdk) with the additional API extensions presented in the `agent-mcp` module.

This integration lets the Koog agents perform the following:

- Connect to MCP servers through various transport mechanisms (stdio, SSE).
- Retrieve available tools from an MCP server.
- Transform MCP tools into the Koog tool interface.
- Register the transformed tools in a tool registry.
- Call MCP tools with arguments provided by the LLM.

### Key components

Here are the main components of the MCP integration in Koog:

| Component                                                                                                                                                           | Description                                                                                                |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
| [`McpTool`](https://api.koog.ai/agents/agents-mcp/ai.koog.agents.mcp/-mcp-tool/index.html)                                                                          | Serves as a bridge between the Koog tool interface and the MCP SDK.                                        |
| [`McpToolDescriptorParser`](https://api.koog.ai/agents/agents-mcp/ai.koog.agents.mcp/-mcp-tool-descriptor-parser/index.html)                                        | Parses MCP tool definitions into the Koog tool descriptor format.                                          |
| [`McpToolRegistryProvider`](https://api.koog.ai/agents/agents-mcp/ai.koog.agents.mcp/-mcp-tool-registry-provider/index.html?query=object%20McpToolRegistryProvider) | Creates MCP tool registries that connect to MCP servers through various transport mechanisms (stdio, SSE). |

## Getting started

### 1. Set up an MCP connection

To use MCP with Koog, you need to set up a connection:

1. Start an MCP server (either as a process, Docker container, or web service).
1. Create a transport mechanism to communicate with the server.

MCP servers support the stdio and SSE transport mechanisms to communicate with the agent, so you can connect using one of them.

#### Connect with stdio

This protocol is used when an MCP server runs as a separate process. Here is an example of setting up an MCP connection using the stdio transport:

```kotlin
// Start an MCP server (for example, as a process)
val process = ProcessBuilder("path/to/mcp/server").start()

// Create the stdio transport 
val transport = McpToolRegistryProvider.defaultStdioTransport(process)
```

#### Connect with SSE

This protocol is used when an MCP server runs as a web service. Here is an example of setting up an MCP connection using the SSE transport:

```kotlin
// Create the SSE transport
val transport = McpToolRegistryProvider.defaultSseTransport("http://localhost:8931")
```

### 2. Create a tool registry

Once you have the MCP connection, you can create a tool registry with tools from the MCP server in one of the following ways:

- Using the provided transport mechanism for communication. For example:

```kotlin
// Create a tool registry with tools from the MCP server
val toolRegistry = McpToolRegistryProvider.fromTransport(
    transport = transport,
    name = "my-client",
    version = "1.0.0"
)
```

- Using an MCP client connected to the MCP server. For example:

```kotlin
// Create a tool registry from an existing MCP client
val toolRegistry = McpToolRegistryProvider.fromClient(
    mcpClient = existingMcpClient
)
```

### 3. Integrate with your agent

To use MCP tools with your Koog agent, you need to register the tool registry with the agent:

```kotlin
// Create an agent with the tools
val agent = AIAgent(
    promptExecutor = executor,
    strategy = strategy,
    llmModel = OpenAIModels.Chat.GPT4o,
    toolRegistry = toolRegistry
)

// Run the agent with a task that uses an MCP tool
val result = agent.run("Use the MCP tool to perform a task")
```

## Usage examples

### Google Maps MCP integration

This example demonstrates how to connect to a [Google Maps](https://mcp.so/server/google-maps/modelcontextprotocol) server for geographic data using MCP:

```kotlin
// Start the Docker container with the Google Maps MCP server
val process = ProcessBuilder(
    "docker", "run", "-i",
    "-e", "GOOGLE_MAPS_API_KEY=$googleMapsApiKey",
    "mcp/google-maps"
).start()

// Create the ToolRegistry with tools from the MCP server
val toolRegistry = McpToolRegistryProvider.fromTransport(
    transport = McpToolRegistryProvider.defaultStdioTransport(process)
)

// Create and run the agent
val agent = AIAgent(
    promptExecutor = simpleOpenAIExecutor(openAIApiToken),
    llmModel = OpenAIModels.Chat.GPT4o,
    toolRegistry = toolRegistry,
)
agent.run("Get elevation of the Jetbrains Office in Munich, Germany?")
```

### Playwright MCP integration

This example demonstrates how to connect to a [Playwright](https://mcp.so/server/playwright-mcp/microsoft) server for web automation using MCP:

```kotlin
// Start the Playwright MCP server
val process = ProcessBuilder(
    "npx", "@playwright/mcp@latest", "--port", "8931"
).start()

// Create the ToolRegistry with tools from the MCP server
val toolRegistry = McpToolRegistryProvider.fromTransport(
    transport = McpToolRegistryProvider.defaultSseTransport("http://localhost:8931")
)

// Create and run the agent
val agent = AIAgent(
    promptExecutor = simpleOpenAIExecutor(openAIApiToken),
    llmModel = OpenAIModels.Chat.GPT4o,
    toolRegistry = toolRegistry,
)
agent.run("Open a browser, navigate to jetbrains.com, accept all cookies, click AI in toolbar")
```
# A2A Protocol

# A2A protocol

This page provides an overview of the A2A (Agent-to-Agent) protocol implementation in the Koog agentic framework.

## What is the A2A protocol?

The A2A (Agent-to-Agent) protocol is a standardized communication protocol that enables AI agents to interact with each other and with client applications. It defines a set of methods, message formats, and behaviors that allow for consistent and interoperable agent communication. For more information and a detailed specification of the A2A protocol, see the official [A2A Protocol website](https://a2a-protocol.org/latest/).

## Getting Started

**Important**: A2A dependencies are **not** included by default in the `koog-agents` meta-dependency. You must explicitly add the A2A modules you need to your project.

To use A2A in your project, add dependencies based on your use case:

- **For A2A client**: See [A2A Client documentation](../a2a-client/#dependencies)
- **For A2A server**: See [A2A Server documentation](../a2a-server/#dependencies)
- **For Koog integration**: See [A2A Koog Integration documentation](../a2a-koog-integration/#dependencies)

## Key A2A components

Koog provides full implementation of A2A protocol v0.3.0 for both client and server, as well as integration with the Koog agent framework:

- [A2A Server](../a2a-server/) is an agent or agentic system that exposes an endpoint implementing the A2A protocol. It receives requests from clients, processes tasks, and returns results or status updates. It can also be used independently of Koog agents.
- [A2A Client](../a2a-client/) is a client application or agent that initiates communication with an A2A server using the A2A protocol. It can also be used independently of Koog agents.
- [A2A Koog Integration](../a2a-koog-integration/) is a set of classes and utilities that simplify the integration of A2A with Koog Agents. It contains components (A2A features and nodes) for seamless A2A agent connections and communication within the Koog framework.

For more examples, follow the [examples](https://github.com/JetBrains/koog/tree/develop/examples/simple-examples/src/main/kotlin/ai/koog/agents/example/a2a)

# A2A Server

The A2A server enables you to expose AI agents through the standardized A2A (Agent-to-Agent) protocol. It provides a complete implementation of the [A2A protocol specification](https://a2a-protocol.org/latest/specification/), handling client requests, executing agent logic, managing complex task lifecycles, and supporting real-time streaming responses.

## Dependencies

To use the A2A server in your project, add the following dependencies to your `build.gradle.kts`:

```kotlin
dependencies {
    // Core A2A server library
    implementation("ai.koog:a2a-server:$koogVersion")

    // HTTP JSON-RPC transport (most common)
    implementation("ai.koog:a2a-transport-server-jsonrpc-http:$koogVersion")

    // Ktor server engine (choose one that fits your needs)
    implementation("io.ktor:ktor-server-netty:$ktorVersion")
}
```

## Overview

The A2A server acts as a bridge between the A2A protocol transport layer and your custom agent logic. It orchestrates the entire request lifecycle while maintaining protocol compliance and providing robust session management.

## Core components

### A2AServer

The main server class implementing the complete A2A protocol. It serves as the central coordinator that:

- **Validates** incoming requests against protocol specifications
- **Manages** concurrent sessions and task lifecycles
- **Orchestrates** communication between transport, storage, and business logic layers
- **Handles** all protocol operations: message sending, task querying, cancellation, push notifications

The `A2AServer` accepts two required parameters:

- `AgentExecutor` which defines business logic implementation of the agent
- `AgentCard` which defines agent capabilities and metadata

And a number of optional parameters that can be used to customize its storage and transport behavior.

### AgentExecutor

The `AgentExecutor` interface is where you implement your agent's core business logic. It acts as the bridge between the A2A protocol and your specific AI agent capabilities. To start the execution of your agent, you must implement the `execute` method where define your agent's logic. To cancel the agent, you must implement the `cancel` method.

```kotlin
class MyAgentExecutor : AgentExecutor {
    override suspend fun execute(
        context: RequestContext<MessageSendParams>,
        eventProcessor: SessionEventProcessor
    ) {
        // Agent logic here
    }

    override suspend fun cancel(
        context: RequestContext<TaskIdParams>,
        eventProcessor: SessionEventProcessor,
        agentJob: Deferred<Unit>?
    ) {
        // Cancel agent here, optional
    }
}
```

The `RequestContext` provides rich information about the current request, including the `contextId` and `taskId` of the current session, the `message` sent, and the `params` of the request.

The `SessionEventProcessor` communicates with clients:

- **`sendMessage(message)`**: Send immediate responses (chat-style interactions)
- **`sendTaskEvent(event)`**: Send task-related updates (long-running operations)

```kotlin
// For immediate responses (like chatbots)
eventProcessor.sendMessage(
    Message(
        messageId = generateId(),
        role = Role.Agent,
        parts = listOf(TextPart("Here's your answer!")),
        contextId = context.contextId
    )
)

// For task-based operations
eventProcessor.sendTaskEvent(
    TaskStatusUpdateEvent(
        contextId = context.contextId,
        taskId = context.taskId,
        status = TaskStatus(
            state = TaskState.Working,
            message = Message(/* progress update */),
            timestamp = Clock.System.now()
        ),
        final = false  // More updates to come
    )
)
```

### AgentCard

The `AgentCard` serves as your agent's self-describing manifest. It tells clients what your agent can do, how to communicate with it, and what security requirements it has.

```kotlin
val agentCard = AgentCard(
    // Basic Identity
    name = "Advanced Recipe Assistant",
    description = "AI agent specialized in cooking advice, recipe generation, and meal planning",
    version = "2.1.0",
    protocolVersion = "0.3.0",

    // Communication Settings
    url = "https://api.example.com/a2a",
    preferredTransport = TransportProtocol.JSONRPC,

    // Optional: Multiple transport support
    additionalInterfaces = listOf(
        AgentInterface("https://api.example.com/a2a", TransportProtocol.JSONRPC),
    ),

    // Capabilities Declaration
    capabilities = AgentCapabilities(
        streaming = true,              // Support real-time responses
        pushNotifications = true,      // Send async notifications
        stateTransitionHistory = true  // Maintain task history
    ),

    // Content Type Support
    defaultInputModes = listOf("text/plain", "text/markdown", "image/jpeg"),
    defaultOutputModes = listOf("text/plain", "text/markdown", "application/json"),

    // Define available security schemes
    securitySchemes = mapOf(
        "bearer" to HTTPAuthSecurityScheme(
            scheme = "Bearer",
            bearerFormat = "JWT",
            description = "JWT token authentication"
        ),
        "api-key" to APIKeySecurityScheme(
            `in` = In.Header,
            name = "X-API-Key",
            description = "API key for service authentication"
        )
    ),

    // Specify security requirements (logical OR of requirements)
    security = listOf(
        mapOf("bearer" to listOf("read", "write")),  // Option 1: JWT with read/write scopes
        mapOf("api-key" to emptyList())              // Option 2: API key
    ),

    // Enable extended card for authenticated users
    supportsAuthenticatedExtendedCard = true,

    // Skills/Capabilities
    skills = listOf(
        AgentSkill(
            id = "recipe-generation",
            name = "Recipe Generation",
            description = "Generate custom recipes based on ingredients, dietary restrictions, and preferences",
            tags = listOf("cooking", "recipes", "nutrition"),
            examples = listOf(
                "Create a vegan pasta recipe with mushrooms",
                "I have chicken, rice, and vegetables. What can I make?"
            )
        ),
        AgentSkill(
            id = "meal-planning",
            name = "Meal Planning",
            description = "Plan weekly meals and generate shopping lists",
            tags = listOf("meal-planning", "nutrition", "shopping")
        )
    ),

    // Optional: Branding
    iconUrl = "https://example.com/agent-icon.png",
    documentationUrl = "https://docs.example.com/recipe-agent",
    provider = AgentProvider(
        organization = "CookingAI Inc.",
        url = "https://cookingai.com"
    )
)
```

### Transport Layer

The A2A itself supports multiple transport protocols for communicating with clients. Currently, Koog provides implementations for JSON-RPC server transport over HTTP.

#### HTTP JSON-RPC Transport

```kotlin
val transport = HttpJSONRPCServerTransport(server)
transport.start(
    engineFactory = CIO,           // Ktor engine (CIO, Netty, Jetty)
    port = 8080,                   // Server port
    path = "/a2a",                 // API endpoint path
    wait = true                    // Block until server stops
)
```

### Storage

The A2A server uses a pluggable storage architecture that separates different types of data. All storage implementations are optional and default to in-memory variants for development.

- **TaskStorage**: Task lifecycle management - stores and manages task states, history, and artifacts
- **MessageStorage**: Conversation history - manages message history within conversation contexts
- **PushNotificationConfigStorage**: Webhook management - manages webhook configurations for asynchronous notifications

## Quickstart

### 1. Create AgentCard

Define your agent's capabilities and metadata.

```kotlin
val agentCard = AgentCard(
    name = "IO Assistant",
    description = "AI agent specialized in input modification",
    version = "2.1.0",
    protocolVersion = "0.3.0",

    // Communication Settings
    url = "https://api.example.com/a2a",
    preferredTransport = TransportProtocol.JSONRPC,

    // Capabilities Declaration
    capabilities =
        AgentCapabilities(
            streaming = true,              // Support real-time responses
            pushNotifications = true,      // Send async notifications
            stateTransitionHistory = true  // Maintain task history
        ),

    // Content Type Support
    defaultInputModes = listOf("text/plain", "text/markdown", "image/jpeg"),
    defaultOutputModes = listOf("text/plain", "text/markdown", "application/json"),

    // Skills/Capabilities
    skills = listOf(
        AgentSkill(
            id = "echo",
            name = "echo",
            description = "Echoes back user messages",
            tags = listOf("io"),
        )
    )
)
```

### 2. Create an AgentExecutor

In executor manages implement agent logic, handles incoming requests and sends responses.

```kotlin
class EchoAgentExecutor : AgentExecutor {
    override suspend fun execute(
        context: RequestContext<MessageSendParams>,
        eventProcessor: SessionEventProcessor
    ) {
        val userMessage = context.params.message
        val userText = userMessage.parts
            .filterIsInstance<TextPart>()
            .joinToString(" ") { it.text }

        // Echo the user's message back
        val response = Message(
            messageId = UUID.randomUUID().toString(),
            role = Role.Agent,
            parts = listOf(TextPart("You said: $userText")),
            contextId = context.contextId,
            taskId = context.taskId
        )

        eventProcessor.sendMessage(response)
    }
}
```

### 2. Create the Server

Pass the agent executor and agent card to the server.

```kotlin
val server = A2AServer(
    agentExecutor = EchoAgentExecutor(),
    agentCard = agentCard
)
```

### 3. Add Transport Layer

Create a transport layer and start the server.

```kotlin
// HTTP JSON-RPC transport
val transport = HttpJSONRPCServerTransport(server)
transport.start(
    engineFactory = CIO,
    port = 8080,
    path = "/agent",
    wait = true
)
```

## Agent Implementation Patterns

### Simple Response Agent

If your agent only needs to respond to a single message, you can implement it as a simple agent. It can be also used if agent execution logic is not complex and time-consuming.

```kotlin
class SimpleAgentExecutor : AgentExecutor {
    override suspend fun execute(
        context: RequestContext<MessageSendParams>,
        eventProcessor: SessionEventProcessor
    ) {
        val response = Message(
            messageId = UUID.randomUUID().toString(),
            role = Role.Agent,
            parts = listOf(TextPart("Hello from agent!")),
            contextId = context.contextId,
            taskId = context.taskId
        )

        eventProcessor.sendMessage(response)
    }
}
```

### Task-Based Agent

If the execution logic of your agent is complex and requires multiple steps, you can implement it as a task-based agent. It can be also used if agent execution logic is time-consuming and suspending.

```kotlin
class TaskAgentExecutor : AgentExecutor {
    override suspend fun execute(
        context: RequestContext<MessageSendParams>,
        eventProcessor: SessionEventProcessor
    ) {
        // Send working status
        eventProcessor.sendTaskEvent(
            TaskStatusUpdateEvent(
                contextId = context.contextId,
                taskId = context.taskId,
                status = TaskStatus(
                    state = TaskState.Working,
                    timestamp = Clock.System.now()
                ),
                final = false
            )
        )

        // Do work...

        // Send completion
        eventProcessor.sendTaskEvent(
            TaskStatusUpdateEvent(
                contextId = context.contextId,
                taskId = context.taskId,
                status = TaskStatus(
                    state = TaskState.Completed,
                    timestamp = Clock.System.now()
                ),
                final = true
            )
        )
    }
}
```

# A2A Client

The A2A client enables you to communicate with A2A-compliant agents over the network. It provides a complete implementation of the [A2A protocol specification](https://a2a-protocol.org/latest/specification/), handling agent discovery, message exchange, task management, and real-time streaming responses.

## Dependencies

To use the A2A client in your project, add the following dependencies to your `build.gradle.kts`:

```kotlin
dependencies {
    // Core A2A client library
    implementation("ai.koog:a2a-client:$koogVersion")

    // HTTP JSON-RPC transport (most common)
    implementation("ai.koog:a2a-transport-client-jsonrpc-http:$koogVersion")

    // Ktor client engine (choose one that fits your needs)
    implementation("io.ktor:ktor-client-cio:$ktorVersion")
}
```

## Overview

The A2A client acts as a bridge between your application and A2A-compliant agents. It orchestrates the entire communication lifecycle while maintaining protocol compliance and providing robust session management.

## Core components

### A2AClient

The main client class implementing the complete A2A protocol. It serves as the central coordinator that:

- **Manages** connections and agent discovery through pluggable resolvers
- **Orchestrates** message exchange and task operations with automatic protocol compliance
- **Handles** streaming responses and real-time communication when supported by agents
- **Provides** comprehensive error handling and fallback mechanisms for robust applications

The `A2AClient` accepts two required parameters:

- `ClientTransport` which handles network communication layer
- `AgentCardResolver` which handles agent discovery and metadata retrieval

The `A2AClient` interface provides several key methods for interacting with A2A agents:

- `connect` method - To connect to the agent and retrieve its capabilities, which discovers what the agent can do and caches the AgentCard
- `sendMessage` method - To send a message to the agent and receive a single response for simple request-response patterns
- `sendMessageStreaming` method - To send a message with streaming support for real-time responses, which returns a Flow of events including partial messages and task updates
- `getTask` method - To query the status and details of a specific task
- `cancelTask` method - To cancel a running task if the agent supports cancellation
- `cachedAgentCard` method - To get the cached agent card without making a network request, which returns null if connect hasn't been called yet

### ClientTransport

The `ClientTransport` interface handles the low-level network communication while the A2A client manages the protocol logic. It abstracts away transport-specific details, allowing you to use different protocols seamlessly.

#### HTTP JSON-RPC Transport

The most common transport for A2A agents:

```kotlin
val transport = HttpJSONRPCClientTransport(
    url = "https://agent.example.com/a2a",        // Agent endpoint URL
    httpClient = HttpClient(CIO) {                // Optional: custom HTTP client
        install(ContentNegotiation) {
            json()
        }
        install(HttpTimeout) {
            requestTimeoutMillis = 30000
        }
    }
)
```

### AgentCardResolver

The `AgentCardResolver` interface retrieves agent metadata and capabilities. It enables agent discovery from various sources and supports caching strategies for optimal performance.

#### URL Agent Card Resolver

Fetch agent cards from HTTP endpoints following A2A conventions:

```kotlin
val agentCardResolver = UrlAgentCardResolver(
    baseUrl = "https://agent.example.com",           // Base URL of the agent service
    path = "/.well-known/agent-card.json",           // Standard agent card location
    httpClient = HttpClient(CIO),                    // Optional: custom HTTP client
)
```

## Quickstart

### 1. Create the Client

Define the transport and agent card resolver and create the client.

```kotlin
// HTTP JSON-RPC transport
val transport = HttpJSONRPCClientTransport(
    url = "https://agent.example.com/a2a"
)

// Agent card resolver
val agentCardResolver = UrlAgentCardResolver(
    baseUrl = "https://agent.example.com",
    path = "/.well-known/agent-card.json"
)

// Create client
val client = A2AClient(transport, agentCardResolver)
```

### 2. Connect and Discover

Connect to the agent and retrieve its card. Having agent's card enables you to query its capabilities and perform other operations, for example, check if it supports streaming.

```kotlin
// Connect and retrieve agent capabilities
client.connect()
val agentCard = client.cachedAgentCard()

println("Connected to: ${agentCard.name}")
println("Supports streaming: ${agentCard.capabilities.streaming}")
```

### 3. Send Messages

Send a message to the agent and receive a single response. The response can be either the message if the agent responded directly, or a task event if the agent is performing a task.

```kotlin
val message = Message(
    messageId = UUID.randomUUID().toString(),
    role = Role.User,
    parts = listOf(TextPart("Hello, agent!")),
    contextId = "conversation-1"
)

val request = Request(data = MessageSendParams(message))
val response = client.sendMessage(request)

// Handle response
when (val event = response.data) {
    is Message -> {
        val text = event.parts
            .filterIsInstance<TextPart>()
            .joinToString { it.text }
        print(text) // Stream partial responses
    }
    is TaskEvent -> {
        if (event.final) {
            println("\nTask completed")
        }
    }
}
```

### 4. Send Messages Streaming

The A2A client supports streaming responses for real-time communication. Instead of receiving a single response, it returns a `Flow` of events including messages and task updates.

```kotlin
// Check if agent supports streaming
if (client.cachedAgentCard()?.capabilities?.streaming == true) {
    client.sendMessageStreaming(request).collect { response ->
        when (val event = response.data) {
            is Message -> {
                val text = event.parts
                    .filterIsInstance<TextPart>()
                    .joinToString { it.text }
                print(text) // Stream partial responses
            }
            is TaskStatusUpdateEvent -> {
                if (event.final) {
                    println("\nTask completed")
                }
            }
        }
    }
} else {
    // Fallback to non-streaming
    val response = client.sendMessage(request)
    // Handle single response
}
```

### 5. Manage Tasks

A2A Client provides methods to control server tasks by asking for their status and cancelling them.

```kotlin
// Query task status
val taskRequest = Request(data = TaskQueryParams(taskId = "task-123"))
val taskResponse = client.getTask(taskRequest)
val task = taskResponse.data

println("Task state: ${task.status.state}")

// Cancel running task
if (task.status.state == TaskState.Working) {
    val cancelRequest = Request(data = TaskIdParams(taskId = "task-123"))
    val cancelledTask = client.cancelTask(cancelRequest).data
    println("Task cancelled: ${cancelledTask.status.state}")
}
```

# A2A and Koog Integration

Koog provides seamless integration with the A2A protocol, allowing you to expose Koog agents as A2A servers and connect Koog agents to other A2A-compliant agents.

## Dependencies

A2A Koog integration requires specific feature modules depending on your use case:

### For Exposing Koog Agents as A2A Servers

Add these dependencies to your `build.gradle.kts`:

```kotlin
dependencies {
    // Koog A2A server integration feature
    implementation("ai.koog:agents-features-a2a-server:$koogVersion")

    // HTTP JSON-RPC transport
    implementation("ai.koog:a2a-transport-server-jsonrpc-http:$koogVersion")

    // Ktor server engine (choose one that fits your needs)
    implementation("io.ktor:ktor-server-netty:$ktorVersion")
}
```

### For Connecting Koog Agents to A2A Agents

Add these dependencies to your `build.gradle.kts`:

```kotlin
dependencies {
    // Koog A2A client integration feature
    implementation("ai.koog:agents-features-a2a-client:$koogVersion")

    // HTTP JSON-RPC transport
    implementation("ai.koog:a2a-transport-client-jsonrpc-http:$koogVersion")

    // Ktor client engine (choose one that fits your needs)
    implementation("io.ktor:ktor-client-cio:$ktorVersion")
}
```

## Overview

The integration enables two main patterns:

1. **Expose Koog agents as A2A servers** - Make your Koog agents discoverable and accessible via the A2A protocol
1. **Connect Koog agents to A2A agents** - Let your Koog agents communicate with other A2A-compliant agents

## Exposing Koog Agents as A2A Servers

### Define Koog Agent with A2A feature

Let's define a Koog agent first. The logic of the agent can vary, but here's an example basic single run agent with tools. The agent resaves a message from the user, forwards it to the llm. If the llm response contains a tool call, the agent executes the tool and forwards the result to the llm. If the llm response contains an assistant message, the agent sends the assistant message to the user and finishes.

On input resize, the agent sends a task submitted event to the A2A client with the input message. On each tool call, the agent sends a task working event to the A2A client with the tool call and result. On assistant message, the agent sends a task complete event to the A2A client with the assistant message.

```kotlin
/**
 * Create a Koog agent with A2A feature
 */
@OptIn(ExperimentalUuidApi::class)
private fun createAgent(
    context: RequestContext<MessageSendParams>,
    eventProcessor: SessionEventProcessor,
) = AIAgent(
    promptExecutor = MultiLLMPromptExecutor(
        LLMProvider.Google to GoogleLLMClient("api-key")
    ),
    toolRegistry = ToolRegistry {
        // Declare tools here
    },
    strategy = strategy<A2AMessage, Unit>("test") {
        val nodeSetup by node<A2AMessage, Unit> { inputMessage ->
            // Convenience function to transform A2A message into Koog message
            val input = inputMessage.toKoogMessage()
            llm.writeSession {
                appendPrompt {
                    message(input)
                }
            }
            // Send update event to A2A client
            withA2AAgentServer {
                sendTaskUpdate("Request submitted: ${input.content}", TaskState.Submitted)
            }
        }

        // Calling llm
        val nodeLLMRequest by node<Unit, Message> {
            llm.writeSession {
                requestLLM()
            }
        }

        // Executing tool
        val nodeProcessTool by node<Message.Tool.Call, Unit> { toolCall ->
            withA2AAgentServer {
                sendTaskUpdate("Executing tool: ${toolCall.content}", TaskState.Working)
            }

            val toolResult = environment.executeTool(toolCall)

            llm.writeSession {
                appendPrompt {
                    tool {
                        result(toolResult)
                    }
                }
            }
            withA2AAgentServer {
                sendTaskUpdate("Tool result: ${toolResult.content}", TaskState.Working)
            }
        }

        // Sending assistant message
        val nodeProcessAssistant by node<String, Unit> { assistantMessage ->
            withA2AAgentServer {
                sendTaskUpdate(assistantMessage, TaskState.Completed)
            }
        }

        edge(nodeStart forwardTo nodeSetup)
        edge(nodeSetup forwardTo nodeLLMRequest)

        // If a tool call is returned from llm, forward to the tool processing node and then back to llm
        edge(nodeLLMRequest forwardTo nodeProcessTool onToolCall { true })
        edge(nodeProcessTool forwardTo nodeLLMRequest)

        // If an assistant message is returned from llm, forward to the assistant processing node and then to finish
        edge(nodeLLMRequest forwardTo nodeProcessAssistant onAssistantMessage { true })
        edge(nodeProcessAssistant forwardTo nodeFinish)
    },
    agentConfig = AIAgentConfig(
        prompt = prompt("agent") { system("You are a helpful assistant.") },
        model = GoogleModels.Gemini2_5Pro,
        maxAgentIterations = 10
    ),
) {
    install(A2AAgentServer) {
        this.context = context
        this.eventProcessor = eventProcessor
    }
}

/**
 * Convenience function to send task update event to A2A client
 * @param content The message content
 * @param state The task state
 */
@OptIn(ExperimentalUuidApi::class)
private suspend fun A2AAgentServer.sendTaskUpdate(
    content: String,
    state: TaskState,
) {
    val message = A2AMessage(
        messageId = Uuid.random().toString(),
        role = Role.Agent,
        parts = listOf(
            TextPart(content)
        ),
        contextId = context.contextId,
        taskId = context.taskId,
    )

    val task = Task(
        id = context.taskId,
        contextId = context.contextId,
        status = TaskStatus(
            state = state,
            message = message,
            timestamp = Clock.System.now(),
        )
    )
    eventProcessor.sendTaskEvent(task)
}
```

## A2AAgentServer Feature Mechanism

The `A2AAgentServer` is a Koog agent feature that enables seamless integration between Koog agents and the A2A protocol. The `A2AAgentServer` feature provides access to the `RequestContext` and `SessionEventProcessor` entities, which are used to communicate with the A2A client inside the Koog agent.

To install the feature, call the `install` function on the agent and pass the `A2AAgentServer` feature along with the `RequestContext` and `SessionEventProcessor`:

```kotlin
// Install the feature
install(A2AAgentServer) {
    this.context = context
    this.eventProcessor = eventProcessor
}
```

To access these entities from Koog agent strategy, the feature provides a `withA2AAgentServer` function that allows agent nodes to access A2A server capabilities within their execution context. It retrieves the installed `A2AAgentServer` feature and provides it as the receiver for the action block.

```kotlin
// Usage within agent nodes
withA2AAgentServer {
    // 'this' is now A2AAgentServer instance
    eventProcessor.sendTaskUpdate("Processing your request...", TaskState.Working)
}
```

### Start A2A Server

After running the server Koog agent will be discoverable and accessible via the A2A protocol.

```kotlin
val agentCard = AgentCard(
    name = "Koog Agent",
    url = "http://localhost:9999/koog",
    description = "Simple universal agent powered by Koog",
    version = "1.0.0",
    protocolVersion = "0.3.0",
    preferredTransport = TransportProtocol.JSONRPC,
    capabilities = AgentCapabilities(streaming = true),
    defaultInputModes = listOf("text"),
    defaultOutputModes = listOf("text"),
    skills = listOf(
        AgentSkill(
            id = "koog",
            name = "Koog Agent",
            description = "Universal agent powered by Koog. Supports tool calling.",
            tags = listOf("chat", "tool"),
        )
    )
)
// Server setup
val server = A2AServer(agentExecutor = KoogAgentExecutor(), agentCard = agentCard)
val transport = HttpJSONRPCServerTransport(server)
transport.start(engineFactory = Netty, port = 8080, path = "/chat", wait = true)
```

## Connecting Koog Agents to A2A Agents

### Create A2A Client and connect to the A2A Server

```kotlin
val transport = HttpJSONRPCClientTransport(url = "http://localhost:9999/koog")
val agentCardResolver =
    UrlAgentCardResolver(baseUrl = "http://localhost:9999", path = "/koog")
val client = A2AClient(transport = transport, agentCardResolver = agentCardResolver)

val agentId = "koog"
client.connect()
```

### Create Koog Agent and add A2A Client to A2AAgentClient Feature

To connect to A2A agent from your Koog Agent, you can use the A2AAgentClient feature, which provides a client API for connecting to A2A agents. The principle of the client is the same as the server: you install the feature and pass the `A2AAgentClient` feature along with the `RequestContext` and `SessionEventProcessor`.

```kotlin
val agent = AIAgent(
    promptExecutor = MultiLLMPromptExecutor(
        LLMProvider.Google to GoogleLLMClient("api-key")
    ),
    toolRegistry = ToolRegistry {
        // declare tools here
    },
    strategy = strategy<String, Unit>("test") {

        val nodeCheckStreaming by nodeA2AClientGetAgentCard().transform { it.capabilities.streaming }

        val nodeA2ASendMessageStreaming by nodeA2AClientSendMessageStreaming()
        val nodeA2ASendMessage by nodeA2AClientSendMessage()

        val nodeProcessStreaming by node<Flow<Response<Event>>, Unit> {
            it.collect { response ->
                when (response.data) {
                    is Task -> {
                        // Process task
                    }

                    is A2AMessage -> {
                        // Process message
                    }

                    is TaskStatusUpdateEvent -> {
                        // Process task status update
                    }

                    is TaskArtifactUpdateEvent -> {
                        // Process task artifact update
                    }
                }
            }
        }

        val nodeProcessEvent by node<CommunicationEvent, Unit> { event ->
            when (event) {
                is Task -> {
                    // Process task
                }

                is A2AMessage -> {
                    // Process message
                }
            }
        }

        // If streaming is supported, send a message, process response and finish
        edge(nodeStart forwardTo nodeCheckStreaming transformed { agentId })
        edge(
            nodeCheckStreaming forwardTo nodeA2ASendMessageStreaming
                onCondition { it == true } transformed { buildA2ARequest(agentId) }
        )
        edge(nodeA2ASendMessageStreaming forwardTo nodeProcessStreaming)
        edge(nodeProcessStreaming forwardTo nodeFinish)

        // If streaming is not supported, send a message, process response and finish
        edge(
            nodeCheckStreaming forwardTo nodeA2ASendMessage
                onCondition { it == false } transformed { buildA2ARequest(agentId) }
        )
        edge(nodeA2ASendMessage forwardTo nodeProcessEvent)
        edge(nodeProcessEvent forwardTo nodeFinish)

        // If streaming is not supported, send a message, process response and finish
        edge(nodeCheckStreaming forwardTo nodeFinish onCondition { it == null }
            transformed { println("Failed to get agents card") }
        )

    },
    agentConfig = AIAgentConfig(
        prompt = prompt("agent") { system("You are a helpful assistant.") },
        model = GoogleModels.Gemini2_5Pro,
        maxAgentIterations = 10
    ),
) {
    install(A2AAgentClient) {
        this.a2aClients = mapOf(agentId to client)
    }
}


@OptIn(ExperimentalUuidApi::class)
private fun AIAgentGraphContextBase.buildA2ARequest(agentId: String): A2AClientRequest<MessageSendParams> =
    A2AClientRequest(
        agentId = agentId,
        callContext = ClientCallContext.Default,
        params = MessageSendParams(
            message = A2AMessage(
                messageId = Uuid.random().toString(),
                role = Role.User,
                parts = listOf(
                    TextPart(agentInput as String)
                )
            )
        )
    )
```
# Agent Client Protocol

# Agent Client Protocol

Agent Client Protocol (ACP) is a standardized protocol that enables client applications to communicate with AI agents through a consistent, bidirectional interface.

ACP provides a structured way for agents to interact with clients, supporting real-time event streaming, tool call notifications, and session lifecycle management.

The Koog framework provides integration with ACP, enabling you to build ACP-compliant agents that can communicate with standardized client applications.

To learn more about the protocol, see the [Agent Client Protocol](https://agentclientprotocol.com) documentation.

## Integration with Koog

The Koog framework integrates with ACP using the [ACP Kotlin SDK](https://github.com/agentclientprotocol/kotlin-sdk) with additional API extensions in the `agents-features-acp` module.

This integration lets Koog agents perform the following:

- Communicate with ACP-compliant client applications
- Send real-time updates about agent execution (tool calls, thoughts, completions)
- Handle standard ACP events and notifications automatically
- Convert between Koog message formats and ACP content blocks

### Key components

Here are the main components of the ACP integration in Koog:

| Component                                                                                                                        | Description                                                                      |
| -------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| [`AcpAgent`](https://api.koog.ai/agents/agents-features-acp/ai.koog.agents.features.acp/-acp-agent/index.html)                   | The main feature that enables communication between Koog agents and ACP clients. |
| [`MessageConverters`](https://api.koog.ai/agents/agents-features-acp/ai.koog.agents.features.acp/-message-converters/index.html) | Utilities for converting messages between Koog and ACP formats.                  |
| [`AcpConfig`](https://api.koog.ai/agents/agents-features-acp/ai.koog.agents.features.acp/-acp-agent/-acp-config/index.html)      | Configuration class for the AcpAgent feature.                                    |

## Getting started

ACP dependencies are **not** included by default in the `koog-agents` meta-dependency. You must explicitly add the ACP module to your project.

### Dependencies

To use ACP in your project, add the following dependency:

```kotlin
dependencies {
    implementation("ai.koog:agents-features-acp:$koogVersion")
}
```

### 1. Implement ACP agent support

Koog ACP integration is based on [Kotlin ACP SDK](https://github.com/agentclientprotocol/kotlin-sdk). The SDK provides an `AgentSupport` and `AgentSession` interface that you need to implement to implement in order to connect your agent to ACP clients. The `AgentSupport` manages the agent sessions creation and loading. The interface implementation is almost the same for all agents, we'll provide an example implementation further. The `AgentSession` manages the agent instantiation, invocation and controls runtime. Inside the `prompt` method you will define and run the Koog agent.

To use ACP with Koog, you need to implement the `AgentSupport` and `AgentSession` interfaces from the ACP SDK:

```kotlin
// Implement AgentSession to manage the lifecycle of a Koog agent
class KoogAgentSession(
    override val sessionId: SessionId,
    private val promptExecutor: PromptExecutor,
    private val protocol: Protocol,
    private val clock: Clock,
) : AgentSession {

    private var agentJob: Deferred<Unit>? = null
    private val agentMutex = Mutex()

    override suspend fun prompt(
        content: List<ContentBlock>,
        _meta: JsonElement?
    ): Flow<Event> = channelFlow {
        val agentConfig = AIAgentConfig(
            prompt = prompt("acp") {
                system("You are a helpful assistant.")
            }.appendPrompt(content),
            model = OpenAIModels.Chat.GPT4o,
            maxAgentIterations = 1000
        )

        agentMutex.withLock {
            val agent = AIAgent(
                promptExecutor = promptExecutor,
                agentConfig = agentConfig,
                strategy = myStrategy()
            ) {
                install(AcpAgent) {
                    this.sessionId = this@KoogAgentSession.sessionId.value
                    this.protocol = this@KoogAgentSession.protocol
                    this.eventsProducer = this@channelFlow
                    this.setDefaultNotifications = true
                }
            }

            agentJob = async { agent.run(Unit) }
            agentJob?.await()
        }
    }

    private fun Prompt.appendPrompt(content: List<ContentBlock>): Prompt {
        return withMessages { messages ->
            messages + listOf(content.toKoogMessage(clock))
        }
    }

    private fun myStrategy() = strategy<Unit, Unit>("") {
        // Define your strategy here
    }    
    override suspend fun cancel() {
        agentJob?.cancel()
    }
}
```

### 2. Configure the AcpAgent feature

The `AcpAgent` feature can be configured through `AcpConfig`:

```kotlin
val agent = AIAgent(
    promptExecutor = promptExecutor,
    agentConfig = agentConfig,
    strategy = myStrategy()
) {
    install(AcpAgent) {
        // Required: The unique session identifier for the ACP connection
        this.sessionId = sessionIdValue

        // Required: The protocol instance used for sending requests and notifications
        this.protocol = protocol

        // Required: A coroutine-based producer scope for sending events
        this.eventsProducer = this@channelFlow

        // Optional: Whether to register default notification handlers (default: true)
        this.setDefaultNotifications = true
    }
}
```

Key configuration options:

- `sessionId`: The unique session identifier for the ACP connection
- `protocol`: The protocol instance used for sending requests and notifications to ACP clients
- `eventsProducer`: A coroutine-based producer scope for sending events
- `setDefaultNotifications`: Whether to register default notification handlers (default: `true`)

### 3. Handle incoming prompts

Convert ACP content blocks to Koog messages using the provided extension functions:

```kotlin
// Convert ACP content blocks to Koog message
val koogMessage = acpContent.toKoogMessage(clock)

// Append to existing prompt
fun Prompt.appendPrompt(content: List<ContentBlock>): Prompt {
    return withMessages { messages ->
        messages + listOf(content.toKoogMessage(clock))
    }
}
```

## Default notification handlers

When `setDefaultNotifications` is enabled, the AcpAgent feature automatically handles:

1. **Agent Completion**: Sends `PromptResponseEvent` with `StopReason.END_TURN` when the agent completes successfully
1. **Agent Execution Failures**: Sends `PromptResponseEvent` with appropriate stop reasons:
   - `StopReason.MAX_TURN_REQUESTS` for max iterations exceeded
   - `StopReason.REFUSAL` for other execution failures
1. **LLM Responses**: Converts and sends LLM responses as ACP events (text, tool calls, reasoning)
1. **Tool Call Lifecycle**: Reports tool call status changes:
   - `ToolCallStatus.IN_PROGRESS` when a tool call starts
   - `ToolCallStatus.COMPLETED` when a tool call succeeds
   - `ToolCallStatus.FAILED` when a tool call fails

## Sending custom events

You can send custom events to the ACP client using the `sendEvent` method:

```kotlin
// Access the ACP feature and send custom events
withAcpAgent {
    sendEvent(
        Event.SessionUpdateEvent(
            SessionUpdate.PlanUpdate(plan.entries)
        )
    )
}
```

Moreover, you can use `protocol` inside `withAcpAgent` and send custom notifications or requests:

```kotlin
// Access the ACP feature and send custom events
withAcpAgent {
    protocol.sendRequest<AuthenticateRequest, AuthenticateResponse>(
        AcpMethod.AgentMethods.Authenticate,
        AuthenticateRequest(methodId = AuthMethodId("Google"))
    )
}
```

## Message conversion

The module provides utilities for converting between Koog and ACP message formats:

### ACP to Koog

```kotlin
// Convert ACP content blocks to Koog message
val koogMessage = acpContentBlocks.toKoogMessage(clock)

// Convert single ACP content block to Koog content part
val contentPart = acpContentBlock.toKoogContentPart()
```

### Koog to ACP

```kotlin
// Convert Koog response message to ACP events
val acpEvents = koogResponseMessage.toAcpEvents()

// Convert Koog content part to ACP content block
val acpContentBlock = koogContentPart.toAcpContentBlock()
```

## Important notes

### Use channelFlow for event streaming

Use `channelFlow` to allow sending events from different coroutines:

```kotlin
override suspend fun prompt(
    content: List<ContentBlock>,
    _meta: JsonElement?
): Flow<Event> = channelFlow {
    // Install AcpAgent with this@channelFlow as eventsProducer
}
```

### Synchronize agent execution

Use a mutex to synchronize access to the agent instance, as the protocol should not trigger new execution until the previous one is finished:

```kotlin
private val agentMutex = Mutex()

agentMutex.withLock {
    // Create and run agent
}
```

### Manual notification handling

If you need custom notification handling, set `setDefaultNotifications = false` and process all agent events according to the specification:

```kotlin
install(AcpAgent) {
    this.setDefaultNotifications = false
    // Implement custom event handling
}
```

## Platform support

The ACP feature is currently available only on the JVM platform, as it depends on the ACP Kotlin SDK which is JVM-specific.

## Usage examples

Complete working examples can be found in the [Koog repository](https://github.com/JetBrains/koog/tree/develop/examples/simple-examples/src/main/kotlin/ai/koog/agents/example/acp).

### Running the example

1. Run the ACP example application:

   ```shell
   ./gradlew :examples:simple-examples:run
   ```

1. Enter a request for the ACP agent:

   ```shell
   Move file `my-file.md` to folder `my-folder` and append title '## My File' to the file content
   ```

1. Observe the event traces in the console showing the agent's execution, tool calls, and completion status.
# LLM Parameters

# LLM parameters

This page provides details about LLM parameters in the Koog agentic framework. LLM parameters let you control and customize the behavior of language models.

## Overview

LLM parameters are configuration options that let you fine-tune how language models generate responses. These parameters control aspects like response randomness, length, format, and tool usage. By adjusting the parameters, you optimize model behavior for different use cases, from creative content generation to deterministic structured outputs.

In Koog, the `LLMParams` class incorporates LLM parameters and provides a consistent interface for configuring language model behavior. You can use LLM parameters in the following ways:

- When creating a prompt:

```kotlin
val prompt = prompt(
    id = "dev-assistant",
    params = LLMParams(
        temperature = 0.7,
        maxTokens = 500
    )
) {
    // Add a system message to set the context
    system("You are a helpful assistant.")

    // Add a user message
    user("Tell me about Kotlin")
}
```

For more information about prompt creation, see [Prompts](../prompts/prompt-creation/).

- When creating a subgraph:

```kotlin
val processQuery by subgraphWithTask<String, String>(
    tools = listOf(searchTool, calculatorTool, weatherTool),
    llmModel = OpenAIModels.Chat.GPT4o,
    llmParams = LLMParams(
        temperature = 0.7,
        maxTokens = 500
    ),
    runMode = ToolCalls.SEQUENTIAL,
    assistantResponseRepeatMax = 3,
) { userQuery ->
    """
    You are a helpful assistant that can answer questions about various topics.
    Please help with the following query:
    $userQuery
    """
}
```

For more information about existing subgraph types in Koog, see [Predefined subgraphs](../nodes-and-components/#predefined-subgraphs). To learn how to create and implement your own subgraphs, see [Custom subgraphs](../custom-subgraphs/).

- When updating a prompt in an LLM write session:

```kotlin
llm.writeSession {
    changeLLMParams(
        LLMParams(
            temperature = 0.7,
            maxTokens = 500
        )
    )
}
```

For more information about sessions, see [LLM sessions and manual history management](../sessions/).

## LLM parameter reference

The following table provides a reference of LLM parameters included in the `LLMParams` class and supported by all LLM providers that are available in Koog out of the box. For a list of parameters that are specific to some providers, see [Provider-specific parameters](#provider-specific-parameters).

| Parameter              | Type                      | Description                                                                                                                                                                                     |
| ---------------------- | ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `temperature`          | Double                    | Controls randomness in the output. Higher values, such as 0.7â€“1.0, produce more diverse and creative responses, while lower values produce more deterministic and focused responses.            |
| `maxTokens`            | Integer                   | Maximum number of tokens to generate in the response. Useful for controlling response length.                                                                                                   |
| `numberOfChoices`      | Integer                   | Number of alternative responses to generate. Must be greater than 0.                                                                                                                            |
| `speculation`          | String                    | A speculative configuration string that influences model behavior, designed to enhance result speed and accuracy. Supported only by certain models, but may greatly improve speed and accuracy. |
| `schema`               | Schema                    | Defines the structure for the model's response format, enabling structured outputs like JSON. For more information, see [Schema](#schema).                                                      |
| `toolChoice`           | ToolChoice                | Controls tool calling behavior of the language model. For more information, see [Tool choice](#tool-choice).                                                                                    |
| `user`                 | String                    | Identifier for the user making the request, which can be used for tracking purposes.                                                                                                            |
| `additionalProperties` | Map\<String, JsonElement> | Additional properties that can be used to store custom parameters specific to certain model providers.                                                                                          |

For a list of default values for each parameter, see the corresponding LLM provider documentation:

- [OpenAI Chat](https://platform.openai.com/docs/api-reference/chat/create)
- [OpenAI Responses](https://platform.openai.com/docs/api-reference/responses/create)
- [Google](https://ai.google.dev/api/generate-content#generationconfig)
- [Anthropic](https://platform.claude.com/docs/en/api/messages/create)
- [Mistral](https://docs.mistral.ai/api/#operation/chatCompletions)
- [DeepSeek](https://api-docs.deepseek.com/api/create-chat-completion#request)
- [OpenRouter](https://openrouter.ai/docs/api/reference/parameters)
- Alibaba ([DashScope](https://www.alibabacloud.com/help/en/model-studio/qwen-api-reference))

## Schema

The `Schema` interface defines the structure for the model's response format. Koog supports JSON schemas, as described in the sections below.

### JSON schemas

JSON schemas let you request structured JSON data from language models. Koog supports the following two types of JSON schemas:

1. **Basic JSON Schema** (`LLMParams.Schema.JSON.Basic`): Used for basic JSON processing capabilities. This format primarily focuses on nested data definitions without advanced JSON Schema functionalities.

```kotlin
// Create parameters with a basic JSON schema
val jsonParams = LLMParams(
    temperature = 0.2,
    schema = LLMParams.Schema.JSON.Basic(
        name = "PersonInfo",
        schema = JsonObject(mapOf(
            "type" to JsonPrimitive("object"),
            "properties" to JsonObject(
                mapOf(
                    "name" to JsonObject(mapOf("type" to JsonPrimitive("string"))),
                    "age" to JsonObject(mapOf("type" to JsonPrimitive("number"))),
                    "skills" to JsonObject(
                        mapOf(
                            "type" to JsonPrimitive("array"),
                            "items" to JsonObject(mapOf("type" to JsonPrimitive("string")))
                        )
                    )
                )
            ),
            "additionalProperties" to JsonPrimitive(false),
            "required" to JsonArray(listOf(JsonPrimitive("name"), JsonPrimitive("age"), JsonPrimitive("skills")))
        ))
    )
)
```

2. **Standard JSON Schema** (`LLMParams.Schema.JSON.Standard`): Represents a standard JSON schema according to [json-schema.org](https://json-schema.org/). This format is a proper subset of the official JSON Schema specification. Note that the flavor across different LLM providers might vary, since not all of them support full JSON schemas.

```kotlin
// Create parameters with a standard JSON schema
val standardJsonParams = LLMParams(
    temperature = 0.2,
    schema = LLMParams.Schema.JSON.Standard(
        name = "ProductCatalog",
        schema = JsonObject(mapOf(
            "type" to JsonPrimitive("object"),
            "properties" to JsonObject(mapOf(
                "products" to JsonObject(mapOf(
                    "type" to JsonPrimitive("array"),
                    "items" to JsonObject(mapOf(
                        "type" to JsonPrimitive("object"),
                        "properties" to JsonObject(mapOf(
                            "id" to JsonObject(mapOf("type" to JsonPrimitive("string"))),
                            "name" to JsonObject(mapOf("type" to JsonPrimitive("string"))),
                            "price" to JsonObject(mapOf("type" to JsonPrimitive("number"))),
                            "description" to JsonObject(mapOf("type" to JsonPrimitive("string")))
                        )),
                        "additionalProperties" to JsonPrimitive(false),
                        "required" to JsonArray(listOf(JsonPrimitive("id"), JsonPrimitive("name"), JsonPrimitive("price"), JsonPrimitive("description")))
                    ))
                ))
            )),
            "additionalProperties" to JsonPrimitive(false),
            "required" to JsonArray(listOf(JsonPrimitive("products")))
        ))
    )
)
```

## Tool choice

The `ToolChoice` class controls how the language model uses tools. It provides the following options:

- `LLMParams.ToolChoice.Named`: the language model calls the specified tool. Takes the `name` string argument that represents the name of the tool to call.
- `LLMParams.ToolChoice.All`: the language model calls all tools.
- `LLMParams.ToolChoice.None`: the language model does not call tools and only generates text.
- `LLMParams.ToolChoice.Auto`: the language model automatically decides whether to call tools and which tool to call.
- `LLMParams.ToolChoice.Required`: the language model calls at least one tool.

Here is an example of using the `LLMParams.ToolChoice.Named` class to call a specific tool:

```kotlin
val specificToolParams = LLMParams(
    toolChoice = LLMParams.ToolChoice.Named(name = "calculator")
)
```

## Provider-specific parameters

Koog supports provider-specific parameters for some LLM providers. These parameters extend the base `LLMParams` class and add provider-specific functionality. The following classes include parameters that are specific per provider:

- `OpenAIChatParams`: Parameters specific to the OpenAI Chat Completions API.
- `OpenAIResponsesParams`: Parameters specific to the OpenAI Responses API.
- `GoogleParams`: Parameters specific to Google models.
- `AnthropicParams`: Parameters specific to Anthropic models.
- `MistralAIParams`: Parameters specific to Mistral models.
- `DeepSeekParams`: Parameters specific to DeepSeek models.
- `OpenRouterParams`: Parameters specific to OpenRouter models.
- `DashscopeParams`: Parameters specific to Alibaba models.

Here is the complete reference of provider-specific parameters in Koog:

The following example shows defined OpenRouter LLM parameters using the provider-specific `OpenRouterParams` class:

```kotlin
val openRouterParams = OpenRouterParams(
    temperature = 0.7,
    maxTokens = 500,
    frequencyPenalty = 0.5,
    presencePenalty = 0.5,
    topP = 0.9,
    topK = 40,
    repetitionPenalty = 1.1,
    models = listOf("anthropic/claude-3-opus", "anthropic/claude-3-sonnet"),
    transforms = listOf("middle-out")
)
```

## Usage examples

### Basic usage

```kotlin
// A basic set of parameters with limited length 
val basicParams = LLMParams(
    temperature = 0.7,
    maxTokens = 150,
    toolChoice = LLMParams.ToolChoice.Auto
)
```

### Reasoning control

You implement reasoning control through provider-specific parameters that control model reasoning. When using the OpenAI Chat API and models that support reasoning, use the `reasoningEffort` parameter to control how many reasoning tokens the model generates before providing a response:

```kotlin
val openAIReasoningEffortParams = OpenAIChatParams(
    reasoningEffort = ReasoningEffort.MEDIUM
)
```

In addition, when using the OpenAI Responses API in a stateless mode, you keep an encrypted history of reasoning items and send it to the model in every conversation turn. The encryption is done on the OpenAI side, and you need to request encrypted reasoning tokens by setting the `include` parameter in your requests to `reasoning.encrypted_content`. You can then pass the encrypted reasoning tokens back to the model in the next conversation turns.

```kotlin
val openAIStatelessReasoningParams = OpenAIResponsesParams(
    include = listOf(OpenAIInclude.REASONING_ENCRYPTED_CONTENT)
)
```

### Custom parameters

To add custom parameters that may be provider specific and not supported in Koog out of the box, use the `additionalProperties` property as shown in the example below.

```kotlin
// Add custom parameters for specific model providers
val customParams = LLMParams(
    additionalProperties = additionalPropertiesOf(
        "top_p" to 0.95,
        "frequency_penalty" to 0.5,
        "presence_penalty" to 0.5
    )
)
```

### Setting and overriding parameters

The code sample below shows how you can define a set of LLM parameters that you may want to use primarily, then create another set by partially overriding values from the original set and adding new values to it. This lets you define parameters that are common to most requests but also add more specific parameter combinations without having to repeat the common parameters.

```kotlin
// Define default parameters
val defaultParams = LLMParams(
    temperature = 0.7,
    maxTokens = 150,
    toolChoice = LLMParams.ToolChoice.Auto
)

// Create parameters with some overrides, using defaults for the rest
val overrideParams = LLMParams(
    temperature = 0.2,
    numberOfChoices = 3
).default(defaultParams)
```

The values in the resulting `overrideParams` set are equivalent to the following:

```kotlin
val overrideParams = LLMParams(
    temperature = 0.2,
    maxTokens = 150,
    toolChoice = LLMParams.ToolChoice.Auto,
    numberOfChoices = 3
)
```
# Model capabilities

Koog provides a set of abstractions and implementations for working with Large Language Models (LLMs) from various LLM providers in a provider-agnostic way. The set includes the following classes:

- **LLMCapability**: a class hierarchy that defines various capabilities that LLMs can support, such as:

  - Temperature adjustment for controlling response randomness
  - Tool integration for external system interaction
  - Vision processing for handling visual data
  - Embedding generation for vector representations
  - Completion for text generation tasks
  - Schema support for structured data (JSON with Simple and Full variants)
  - Speculation for exploratory responses

- **LLModel**: a data class that represents a specific LLM with its provider, unique identifier, and supported capabilities.

This serves as a foundation for interacting with different LLM providers in a unified way, allowing applications to work with various models while abstracting away provider-specific details.

## LLM capabilities

LLM capabilities represent specific features or functionalities that a Large Language Model can support. In the Koog framework, capabilities are used to define what a particular model can do and how it can be configured. Each capability is represented as a subclass or data object of the `LLMCapability` class.

When configuring an LLM for use in your application, you specify which capabilities it supports by adding them to the `capabilities` list when creating an `LLModel` instance. This allows the framework to properly interact with the model and use its features appropriately.

### Core capabilities

The list below includes the core, LLM-specific capabilities that are available for models in the Koog framework:

- **Speculation** (`LLMCapability.Speculation`): lets the model generate speculative or exploratory responses with varying degrees of likelihood. Useful for creative or hypothetical scenarios where a broader range of potential outcomes is desired.

- **Temperature** (`LLMCapability.Temperature`): allows adjustment of the model's response randomness or creativity levels. Higher temperature values produce more diverse outputs, while lower values lead to more focused and deterministic responses.

- **Tools** (`LLMCapability.Tools`): indicates support for external tool usage or integration. This capability lets the model run specific tools or interact with external systems.

- **Tool choice** (`LLMCapability.ToolChoice`): configures how tool calling works with the LLM. Depending on the model, it can be configured to:

  - Automatically choose between generating text or tool calls
  - Generate only tool calls, never text
  - Generate only text, never tool calls
  - Force calling a specific tool among the defined tools

- **Multiple choices** (`LLMCapability.MultipleChoices`): lets the model generate multiple independent reply choices to a single prompt.

### Media processing capabilities

The following list represents a set of capabilities for processing media content such as images or audio:

- **Vision** (`LLMCapability.Vision`): a class for vision-based capabilities that process, analyze, and infer insights from visual data. Supports the following types of visual data:

  - **Image** (`LLMCapability.Vision.Image`): handles image-related vision tasks such as image analysis, recognition, and interpretation.
  - **Video** (`LLMCapability.Vision.Video`): processes video data, including analyzing and understanding video content.

- **Audio** (`LLMCapability.Audio`): provides audio-related functionalities such as transcription, audio generation, or audio-based interactions.

- **Document** (`LLMCapability.Document`): enables handling and processing of document-based inputs and outputs.

### Text processing capabilities

The following list of capabilities represents text generation and processing functionalities:

- **Embedding** (`LLMCapability.Embed`): lets models generate vector embeddings from an input text, enabling similarity comparisons, clustering, and other vector-based analyses.
- **Completion** (`LLMCapability.Completion`): includes the generation of text or content based on given input context, such as completing sentences, generating suggestions, or producing content that aligns with input data.
- **Prompt caching** (`LLMCapability.PromptCaching`): supports caching functionalities for prompts, potentially improving performance for repeated or similar queries.
- **Moderation** (`LLMCapability.Moderation`): lets the model analyze text for potentially harmful content and classify it according to various categories such as harassment, hate speech, self-harm, sexual content, violence, etc.

### Schema capabilities

The list below indicates the capabilities related to processing structured data:

- **Schema** (`LLMCapability.Schema`): a class for structured schema capabilities related to data interaction and encoding using specific formats. Includes support for the following format:
  - **JSON** (`LLMCapability.Schema.JSON`): JSON schema support with different levels:
    - **Basic** (`LLMCapability.Schema.JSON.Basic`): provides lightweight or basic JSON processing capabilities.
    - **Standard** (`LLMCapability.Schema.JSON.Standard`): offers comprehensive JSON schema support for complex data structures.

## Creating a model (LLModel) configuration

To define a model in a universal, provider-agnostic way, create a model configuration as an instance of the `LLModel` class with the following parameters:

| Name              | Data type           | Required | Default | Description                                                                                                                                                                                    |
| ----------------- | ------------------- | -------- | ------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `provider`        | LLMProvider         | Yes      |         | The provider of the LLM, such as Google or OpenAI. This identifies the company or organization that created or hosts the model.                                                                |
| `id`              | String              | Yes      |         | A unique identifier for the LLM instance. This typically represents the specific model version or name. For example, `gpt-4-turbo`, `claude-3-opus`, `llama-3-2`.                              |
| `capabilities`    | List<LLMCapability> | Yes      |         | A list of capabilities supported by the LLM, such as temperature adjustment, tools usage, or schema-based tasks. These capabilities define what the model can do and how it can be configured. |
| `contextLength`   | Long                | Yes      |         | The context length of the LLM. This is the maximum number of tokens the LLM can process.                                                                                                       |
| `maxOutputTokens` | Long                | No       | `null`  | The maximum number of tokens that can be generated by the provider for the LLM.                                                                                                                |

### Examples

This section provides detailed examples of creating `LLModel` instances with different capabilities.

The code below represents a basic LLM configuration with core capabilities:

```kotlin
val basicModel = LLModel(
    provider = LLMProvider.OpenAI,
    id = "gpt-4-turbo",
    capabilities = listOf(
        LLMCapability.Temperature,
        LLMCapability.Tools,
        LLMCapability.Schema.JSON.Standard
    ),
    contextLength = 128_000
)
```

The model configuration below is a multimodal LLM with vision capabilities:

```kotlin
val visionModel = LLModel(
    provider = LLMProvider.OpenAI,
    id = "gpt-4-vision",
    capabilities = listOf(
        LLMCapability.Temperature,
        LLMCapability.Vision.Image,
        LLMCapability.MultipleChoices
    ),
    contextLength = 1_047_576,
    maxOutputTokens = 32_768
)
```

An LLM with audio processing capabilities:

```kotlin
val audioModel = LLModel(
    provider = LLMProvider.Anthropic,
    id = "claude-3-opus",
    capabilities = listOf(
        LLMCapability.Audio,
        LLMCapability.Temperature,
        LLMCapability.PromptCaching
    ),
    contextLength = 200_000
)
```

In addition to creating models as `LLModel` instances and having to specify all related parameters, Koog includes a collection of predefined models and their configurations with supported capabilities. To use a predefined Ollama model, specify it as follows:

```kotlin
val metaModel = OllamaModels.Meta.LLAMA_3_2
```

To check whether a model supports a specific capability use the `contains` method to check for the presence of the capability in the `capabilities` list:

```kotlin
// Check if models support specific capabilities
val supportsTools = basicModel.capabilities.contains(LLMCapability.Tools) // true
val supportsVideo = visionModel.capabilities.contains(LLMCapability.Vision.Video) // false

// Check for schema capabilities
val jsonCapability = basicModel.capabilities.filterIsInstance<LLMCapability.Schema.JSON>().firstOrNull()
val hasFullJsonSupport = jsonCapability is LLMCapability.Schema.JSON.Standard // true
```

### LLM capabilities by model

This reference shows which LLM capabilities are supported by each model across different providers.

In the tables below:

- `âœ“` indicates that the model supports the capability
- `-` indicates that the model does not support the capability
- For JSON Schema, `Full` or `Simple` indicates which variant of the JSON Schema capability the model supports

Google models

#### Google models

| Model                 | Temperature | JSON Schema | Completion | Multiple Choices | Tools | Tool Choice | Vision (Image) | Vision (Video) | Audio |
| --------------------- | ----------- | ----------- | ---------- | ---------------- | ----- | ----------- | -------------- | -------------- | ----- |
| Gemini2_5Pro          | âœ“           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | âœ“              | âœ“     |
| Gemini2_5Flash        | âœ“           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | âœ“              | âœ“     |
| Gemini2_5FlashLite    | âœ“           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | âœ“              | âœ“     |
| Gemini2_0Flash        | âœ“           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | âœ“              | âœ“     |
| Gemini2_0Flash001     | âœ“           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | âœ“              | âœ“     |
| Gemini2_0FlashLite    | âœ“           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | âœ“              | âœ“     |
| Gemini2_0FlashLite001 | âœ“           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | âœ“              | âœ“     |

OpenAI models

#### OpenAI models

| Model                    | Temperature | JSON Schema | Completion | Multiple Choices | Tools | Tool Choice | Vision (Image) | Vision (Video) | Audio | Speculation | Moderation |
| ------------------------ | ----------- | ----------- | ---------- | ---------------- | ----- | ----------- | -------------- | -------------- | ----- | ----------- | ---------- |
| Reasoning.O4Mini         | -           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | -              | -     | âœ“           | -          |
| Reasoning.O3Mini         | -           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | -              | -              | -     | âœ“           | -          |
| Reasoning.O3             | -           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | -              | -     | âœ“           | -          |
| Reasoning.O1             | -           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | -              | -     | âœ“           | -          |
| Chat.GPT4o               | âœ“           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | -              | -     | âœ“           | -          |
| Chat.GPT4_1              | âœ“           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | -              | -     | âœ“           | -          |
| Chat.GPT5                | âœ“           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | -              | -     | âœ“           | -          |
| Chat.GPT5Mini            | âœ“           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | -              | -     | âœ“           | -          |
| Chat.GPT5Nano            | âœ“           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | -              | -     | âœ“           | -          |
| Audio.GptAudio           | âœ“           | -           | âœ“          | -                | âœ“     | âœ“           | -              | -              | âœ“     | -           | -          |
| Audio.GPT4oMiniAudio     | âœ“           | -           | âœ“          | -                | âœ“     | âœ“           | -              | -              | âœ“     | -           | -          |
| Audio.GPT4oAudio         | âœ“           | -           | âœ“          | -                | âœ“     | âœ“           | -              | -              | âœ“     | -           | -          |
| CostOptimized.GPT4_1Nano | âœ“           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | -              | -     | âœ“           | -          |
| CostOptimized.GPT4_1Mini | âœ“           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | -              | -     | âœ“           | -          |
| CostOptimized.GPT4oMini  | âœ“           | Full        | âœ“          | âœ“                | âœ“     | âœ“           | âœ“              | -              | -     | âœ“           | -          |
| Moderation.Omni          | -           | -           | -          | -                | -     | -           | âœ“              | -              | -     | -           | âœ“          |

Anthropic models

#### Anthropic models

| Model      | Temperature | JSON Schema | Completion | Tools | Tool Choice | Vision (Image) |
| ---------- | ----------- | ----------- | ---------- | ----- | ----------- | -------------- |
| Opus_4_1   | âœ“           | -           | âœ“          | âœ“     | âœ“           | âœ“              |
| Opus_4     | âœ“           | -           | âœ“          | âœ“     | âœ“           | âœ“              |
| Sonnet_4   | âœ“           | -           | âœ“          | âœ“     | âœ“           | âœ“              |
| Sonnet_3_7 | âœ“           | -           | âœ“          | âœ“     | âœ“           | âœ“              |
| Haiku_3_5  | âœ“           | -           | âœ“          | âœ“     | âœ“           | âœ“              |
| Sonnet_3_5 | âœ“           | -           | âœ“          | âœ“     | âœ“           | âœ“              |
| Haiku_3    | âœ“           | -           | âœ“          | âœ“     | âœ“           | âœ“              |
| Opus_3     | âœ“           | -           | âœ“          | âœ“     | âœ“           | âœ“              |

Ollama models

#### Ollama models

##### Meta models

| Model         | Temperature | JSON Schema | Tools | Moderation |
| ------------- | ----------- | ----------- | ----- | ---------- |
| LLAMA_3_2_3B  | âœ“           | Simple      | âœ“     | -          |
| LLAMA_3_2     | âœ“           | Simple      | âœ“     | -          |
| LLAMA_4       | âœ“           | Simple      | âœ“     | -          |
| LLAMA_GUARD_3 | -           | -           | -     | âœ“          |

##### Alibaba models

| Model              | Temperature | JSON Schema | Tools |
| ------------------ | ----------- | ----------- | ----- |
| QWEN_2_5_05B       | âœ“           | Simple      | âœ“     |
| QWEN_3_06B         | âœ“           | Simple      | âœ“     |
| QWQ                | âœ“           | Simple      | âœ“     |
| QWEN_CODER_2_5_32B | âœ“           | Simple      | âœ“     |

##### Groq models

| Model                     | Temperature | JSON Schema | Tools |
| ------------------------- | ----------- | ----------- | ----- |
| LLAMA_3_GROK_TOOL_USE_8B  | âœ“           | Full        | âœ“     |
| LLAMA_3_GROK_TOOL_USE_70B | âœ“           | Full        | âœ“     |

##### Granite models

| Model              | Temperature | JSON Schema | Tools | Vision (Image) |
| ------------------ | ----------- | ----------- | ----- | -------------- |
| GRANITE_3_2_VISION | âœ“           | Simple      | âœ“     | âœ“              |

DeepSeek models

#### DeepSeek models

| Model            | Temperature | JSON Schema | Completion | Speculation | Tools | Tool Choice | Vision (Image) |
| ---------------- | ----------- | ----------- | ---------- | ----------- | ----- | ----------- | -------------- |
| DeepSeekChat     | âœ“           | Full        | âœ“          | -           | âœ“     | âœ“           | -              |
| DeepSeekReasoner | âœ“           | Full        | âœ“          | -           | âœ“     | âœ“           | -              |

OpenRouter models

#### OpenRouter models

| Model               | Temperature | JSON Schema | Completion | Speculation | Tools | Tool Choice | Vision (Image) |
| ------------------- | ----------- | ----------- | ---------- | ----------- | ----- | ----------- | -------------- |
| Phi4Reasoning       | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | -              |
| Claude3Opus         | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
| Claude3Sonnet       | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
| Claude3Haiku        | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
| Claude3_5Sonnet     | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
| Claude3_7Sonnet     | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
| Claude4Sonnet       | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
| Claude4_1Opus       | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
| GPT4oMini           | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
| GPT5                | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | -              |
| GPT5Mini            | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | -              |
| GPT5Nano            | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | -              |
| GPT_OSS_120b        | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | -              |
| GPT4                | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | -              |
| GPT4o               | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
| GPT4Turbo           | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
| GPT35Turbo          | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | -              |
| Llama3              | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | -              |
| Llama3Instruct      | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | -              |
| Mistral7B           | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | -              |
| Mixtral8x7B         | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | -              |
| Claude3VisionSonnet | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
| Claude3VisionOpus   | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
| Claude3VisionHaiku  | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
| DeepSeekV30324      | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | -              |
| Gemini2_5FlashLite  | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
| Gemini2_5Flash      | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
| Gemini2_5Pro        | âœ“           | Full        | âœ“          | âœ“           | âœ“     | âœ“           | âœ“              |
# Content moderation

# Content moderation

Content moderation is the process of analyzing text, images, or other content to identify potentially harmful, inappropriate, or unsafe material. In the context of AI systems, moderation helps:

- Filter out harmful or inappropriate user inputs
- Prevent the generation of harmful or inappropriate AI responses
- Ensure compliance with ethical guidelines and legal requirements
- Protect users from exposure to potentially harmful content

Moderation systems typically analyze content against predefined categories of harmful content (such as hate speech, violence, sexual content, etc.) and provide a determination of whether the content violates policies in any of these categories.

Content moderation is crucial in AI applications for several reasons:

- Safety and security

  - Protect users from harmful, offensive, or disturbing content
  - Prevent the misuse of AI systems for generating harmful content
  - Maintain a safe environment for all users

- Legal and ethical compliance

  - Comply with regulations regarding content distribution
  - Adhere to ethical guidelines for AI deployment
  - Avoid potential legal liabilities associated with harmful content

- Quality control

  - Maintain the quality and appropriateness of interactions
  - Ensure AI responses align with organizational values and standards
  - Build user trust by consistently providing safe and appropriate content

## Types of moderated content

Koog's moderation system can analyze various types of content:

- User messages

  - Text inputs from users before they are processed by the AI
  - Images uploaded by users (with OpenAI **Moderation.Omni** model)

- Assistant messages

  - AI-generated responses before they are shown to users
  - Responses can be checked to ensure they don't contain harmful content

- Tool content

  - Content generated by or passed to tools integrated with the AI system
  - Ensures that tool inputs and outputs maintain content safety standards

## Supported providers and models

Koog supports content moderation through multiple providers and models:

### OpenAI

OpenAI offers two moderation models:

- **OpenAIModels.Moderation.Text**

  - Text-only moderation
  - Previous generation moderation model
  - Analyzes text content against multiple harm categories
  - Fast and cost-effective

- **OpenAIModels.Moderation.Omni**

  - Supports both text and image moderation
  - Most capable OpenAI moderation model
  - Can identify harmful content in both text and images
  - More comprehensive than the Text model

### Ollama

Ollama supports moderation through the following model:

- **OllamaModels.Meta.LLAMA_GUARD_3**
  - Text-only moderation
  - Based on Meta's Llama Guard family of models
  - Specialized for content moderation tasks
  - Runs locally through Ollama

## Using moderation with LLM clients

Koog provides two main approaches to content moderation, direct moderation on an `LLMClient` instance, or using the `moderate` method on a `PromptExecutor`.

### Direct Moderation with LLMClient

You can use the `moderate` method directly on an LLMClient instance:

```kotlin
// Example with OpenAI client
val openAIClient = OpenAILLMClient(apiKey)
val prompt = prompt("harmful-prompt") { 
    user("I want to build a bomb")
}

// Moderate with OpenAI's Omni moderation model
val result = openAIClient.moderate(prompt, OpenAIModels.Moderation.Omni)

if (result.isHarmful) {
    println("Content was flagged as harmful")
    // Handle harmful content (e.g., reject the prompt)
} else {
    // Proceed with processing the prompt
}
```

The `moderate` method takes the following arguments:

| Name     | Data type | Required | Default | Description                      |
| -------- | --------- | -------- | ------- | -------------------------------- |
| `prompt` | Prompt    | Yes      |         | The prompt to moderate.          |
| `model`  | LLModel   | Yes      |         | The model to use for moderation. |

The method returns a [ModerationResult](#moderationresult-structure).

Here is an example of using content moderation with the Llama Guard 3 model through Ollama:

```kotlin
// Example with Ollama client
val ollamaClient = OllamaClient()
val prompt = prompt("harmful-prompt") {
    user("How to hack into someone's account")
}

// Moderate with Llama Guard 3
val result = ollamaClient.moderate(prompt, OllamaModels.Meta.LLAMA_GUARD_3)

if (result.isHarmful) {
    println("Content was flagged as harmful")
    // Handle harmful content
} else {
    // Proceed with processing the prompt
}
```

### Moderation with PromptExecutor

You can also use the `moderate` method on a PromptExecutor, which will use the appropriate LLMClient based on the model's provider:

```kotlin
// Create a multi-provider executor
val executor = MultiLLMPromptExecutor(
    LLMProvider.OpenAI to OpenAILLMClient(openAIApiKey),
    LLMProvider.Ollama to OllamaClient()
)

val prompt = prompt("harmful-prompt") {
    user("How to create illegal substances")
}

// Moderate with OpenAI
val openAIResult = executor.moderate(prompt, OpenAIModels.Moderation.Omni)

// Or moderate with Ollama
val ollamaResult = executor.moderate(prompt, OllamaModels.Meta.LLAMA_GUARD_3)

// Process the results
if (openAIResult.isHarmful || ollamaResult.isHarmful) {
    // Handle harmful content
}
```

The `moderate` method takes the following arguments:

| Name     | Data type | Required | Default | Description                      |
| -------- | --------- | -------- | ------- | -------------------------------- |
| `prompt` | Prompt    | Yes      |         | The prompt to moderate.          |
| `model`  | LLModel   | Yes      |         | The model to use for moderation. |

The method returns a [ModerationResult](#moderationresult-structure).

## ModerationResult structure

The moderation process returns a `ModerationResult` object with the following structure:

```kotlin
@Serializable
public data class ModerationResult(
    val isHarmful: Boolean,
    val categories: Map<ModerationCategory, Boolean>,
    val categoryScores: Map<ModerationCategory, Double> = emptyMap(),
    val categoryAppliedInputTypes: Map<ModerationCategory, List<InputType>> = emptyMap()
) {
    /**
     * Represents the type of input provided for content moderation.
     *
     * This enumeration is used in conjunction with moderation categories to specify
     * the format of the input being analyzed.
     */
    @Serializable
    public enum class InputType {
        /**
         * This enum value is typically used to classify inputs as textual data
         * within the supported input types.
         */
        TEXT,

        /**
         * Represents an input type specifically designed for handling and processing images.
         * This enum constant can be used to classify or determine behavior for workflows requiring image-based inputs.
         */
        IMAGE,
    }
}
```

A `ModerationResult` object includes the following properties:

| Name                        | Data type                                 | Required | Default    | Description                                                                                |
| --------------------------- | ----------------------------------------- | -------- | ---------- | ------------------------------------------------------------------------------------------ |
| `isHarmful`                 | Boolean                                   | Yes      |            | If true, the content was flagged as harmful.                                               |
| `categories`                | Map\<ModerationCategory, Boolean>         | Yes      |            | A map of moderation categories to boolean values indicating which categories were flagged. |
| `categoryScores`            | Map\<ModerationCategory, Double>          | No       | emptyMap() | A map of moderation categories to confidence scores (0.0 to 1.0).                          |
| `categoryAppliedInputTypes` | Map\<ModerationCategory, List<InputType>> | No       | emptyMap() | A map indicating which input types (`TEXT` or `IMAGE`) triggered each category.            |

## Moderation categories

### Koog moderation categories

Possible moderation categories provided by the Koog framework (regardless of the underlying LLM and LLM provider) are as follows:

1. **Harassment**: content that involves intimidation, bullying, or other behaviors directed towards individuals or groups with the intent to harass or demean.
1. **HarassmentThreatening**: harmful interactions or communications that are intended to intimidate, coerce, or threaten individuals or groups.
1. **Hate**: content that contains elements perceived as offensive, discriminatory, or expressing hatred towards individuals or groups based on attributes such as race, religion, gender, or other characteristics.
1. **HateThreatening**: hate-related moderation category focusing on harmful content that not only spreads hate but also includes threatening language, behavior, or implications.
1. **Illicit**: content that violates legal frameworks or ethical guidelines, including illegal or illicit activities.
1. **IllicitViolent**: content that involves a combination of illegal or illicit activities with elements of violence.
1. **SelfHarm**: content that pertains to self-harm or related behavior.
1. **SelfHarmIntent**: material that contains expressions or indications of an individual's intent to harm themselves.
1. **SelfHarmInstructions**: content that provides guidance, techniques, or encouragement for engaging in self-harm behaviors.
1. **Sexual**: content that is sexually explicit or contains sexual references.
1. **SexualMinors**: content concerning the exploitation, abuse, or endangerment of minors in a sexual context.
1. **Violence**: content that promotes, incites, or depicts violence and physical harm towards individuals or groups.
1. **ViolenceGraphic**: content that includes graphic depictions of violence, which may be harmful, distressing, or triggering to viewers.
1. **Defamation**: responses that are verifiably false and likely to injure a living person's reputation.
1. **SpecializedAdvice**: content that contains specialized financial, medical, or legal advice.
1. **Privacy**: content that contains sensitive, nonpublic personal information that could undermine someone's physical, digital, or financial security.
1. **IntellectualProperty**: responses that may violate the intellectual property rights of any third party.
1. **ElectionsMisinformation**: content that contains factually incorrect information about electoral systems and processes, including in the time, place, or manner of voting in civic elections.

Note

These categories are subject to change as new moderation categories might be added, and existing ones may evolve over time.

#### OpenAI moderation categories

OpenAI's moderation API provides the following categories:

- **Harassment**: content that expresses, incites, or promotes harassing language towards any target.
- **Harassment/threatening**: harassment content that also includes violence or serious harm towards any target.
- **Hate**: content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups is harassment.
- **Hate/threatening**: hateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.
- **Illicit**: content that gives advice or instruction on how to commit illicit acts. A phrase like "how to shoplift" would fit this category.
- **Illicit/violent**: the same types of content flagged by the illicit category, but also includes references to violence or procuring a weapon.
- **Self-harm**: content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.
- **Self-harm/intent**: content where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.
- **Self-harm/instructions**: content that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.
- **Sexual**: content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).
- **Sexual/minors**: sexual content that includes an individual who is under 18 years old.
- **Violence**: content that depicts death, violence, or physical injury.
- **Violence/graphic**: content that depicts death, violence, or physical injury in graphic detail.

#### Ollama hazard categories

Ollama's Llama Guard models use the following hazard categories:

- **S1 - Violent crimes**: responses that enable, encourage, or endorse the commission of violent crimes, including:

  - Unlawful violence toward people such as terrorism, genocide, murder, hate-motivated violent crimes, child abuse, assault, battery, kidnapping.
  - Unlawful violence toward animals such as animal abuse.

- **S2 - Non-violent crimes**: responses that enable, encourage, or endorse the commission of non-violent crimes, including:

  - Personal crimes such as labor trafficking, threats, intimidation, hate-motivated non-violent crimes
  - Financial crimes such as fraud, scams, money laundering
  - Property crimes such as theft, arson, vandalism
  - Drug crimes such as creating or using regulated substances
  - Weapons crimes such as producing unlicensed firearms
  - Cyber crimes such as hacking

- **S3 - Sex-related crimes**: responses that enable, encourage, or endorse the commission of sex-related crimes, including:

  - Sex trafficking
  - Sexual assault such as rape
  - Sexual harassment (physical, verbal, or visual)
  - Prostitution

- **S4 - Child sexual exploitation**: responses that contain, describe, enable, encourage, or endorse the sexual abuse of children.

- **S5 - Defamation**: responses that are both verifiably false and likely to injure a living person's reputation.

- **S6 - Specialized advice**: responses that contain specialized financial, medical, or legal advice, or that indicate dangerous activities or objects are safe.

- **S7 - Privacy**: responses that contain sensitive, nonpublic personal information that could undermine someone's physical, digital, or financial security.

- **S8 - Intellectual property**: responses that may violate the intellectual property rights of any third party.

- **S9 - Indiscriminate weapons**: responses that enable, encourage, or endorse the creation of indiscriminate weapons, including:

  - Chemical weapons such as nerve gas
  - Biological weapons such as anthrax
  - Radiological weapons such as cobalt bombs
  - Nuclear weapons such as fission bombs
  - High-yield explosive weapons such as cluster munitions

- **S10 - Hate**: responses that demean or dehumanize people on the basis of their sensitive, personal characteristics such as race, color, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity, or serious disease.

- **S11 - Suicide and self-harm**: responses that enable, encourage, or endorse acts of intentional self-harm, including:

  - Suicide
  - Self-injury such as cutting
  - Disordered eating

- **S12 - Sexual content**: responses that contain erotica.

- **S13 - Elections**: responses that contain factually incorrect information about electoral systems and processes, including in the time, place, or manner of voting in civic elections.

#### Category mapping between providers

The following table shows the mapping between Ollama and OpenAI moderation categories:

| Ollama category                                                                           | Closest OpenAI moderation category or categories                                   | Notes                                                                                      |
| ----------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| **S1 â€“ Violent crimes**                                                                   | `illicit/violent`, `violence` (`violence/graphic` when gore is described)          | Covers instructions or endorsement of violent wrongdoing, plus the violent content itself. |
| **S2 â€“ Nonâ€‘violent crimes**                                                               | `illicit`                                                                          | Provides or encourages nonâ€‘violent criminal activity (fraud, hacking, drug making, etc.).  |
| **S3 â€“ Sexâ€‘related crimes**                                                               | `illicit/violent` (rape, trafficking, etc.) `sexual` (sexualâ€‘assault descriptions) | Violent sexual wrongdoing combines illicit instructions + sexual content.                  |
| **S4 â€“ Child sexual exploitation**                                                        | `sexual/minors`                                                                    | Any sexual content involving minors.                                                       |
| **S5 â€“ Defamation**                                                                       | **UNIQUE**                                                                         | OpenAI's categories don't have a dedicated defamation flag.                                |
| **S6 â€“ Specialized advice** (medical, legal, financial, dangerousâ€‘activity "safe" claims) | **UNIQUE**                                                                         | Not directly represented in the OpenAI schema.                                             |
| **S7 â€“ Privacy** (exposed personal data, doxxing)                                         | **UNIQUE**                                                                         | No direct privacyâ€‘disclosure category in OpenAI moderation.                                |
| **S8 â€“ Intellectual property**                                                            | **UNIQUE**                                                                         | Copyright / IP issues are not a moderation category in OpenAI.                             |
| **S9 â€“ Indiscriminate weapons**                                                           | `illicit/violent`                                                                  | Instructions to build or deploy WMDs are violent illicit content.                          |
| **S10 â€“ Hate**                                                                            | `hate` (demeaning) `hate/threatening` (violent or murderous hate)                  | Same protectedâ€‘class scope.                                                                |
| **S11 â€“ Suicide and selfâ€‘harm**                                                           | `self-harm`, `self-harm/intent`, `self-harm/instructions`                          | Matches exactly to OpenAI's three selfâ€‘harm subâ€‘types.                                     |
| **S12 â€“ Sexual content** (erotica)                                                        | `sexual`                                                                           | Ordinary adult erotica (minors would shift to `sexual/minors`).                            |
| **S13 â€“ Elections misinformation**                                                        | **UNIQUE**                                                                         | Electoralâ€‘process misinformation isn't singled out in OpenAI's categories.                 |

## Examples of moderation results

### OpenAI moderation example (harmful content)

OpenAI provides the specific `/moderations` API that provides responses in the following JSON format:

```json
{
  "isHarmful": true,
  "categories": {
    "Harassment": false,
    "HarassmentThreatening": false,
    "Hate": false,
    "HateThreatening": false,
    "Sexual": false,
    "SexualMinors": false,
    "Violence": false,
    "ViolenceGraphic": false,
    "SelfHarm": false,
    "SelfHarmIntent": false,
    "SelfHarmInstructions": false,
    "Illicit": true,
    "IllicitViolent": true
  },
  "categoryScores": {
    "Harassment": 0.0001,
    "HarassmentThreatening": 0.0001,
    "Hate": 0.0001,
    "HateThreatening": 0.0001,
    "Sexual": 0.0001,
    "SexualMinors": 0.0001,
    "Violence": 0.0145,
    "ViolenceGraphic": 0.0001,
    "SelfHarm": 0.0001,
    "SelfHarmIntent": 0.0001,
    "SelfHarmInstructions": 0.0001,
    "Illicit": 0.9998,
    "IllicitViolent": 0.9876
  },
  "categoryAppliedInputTypes": {
    "Illicit": ["TEXT"],
    "IllicitViolent": ["TEXT"]
  }
}
```

In Koog, the structure of the response above maps to the following response:

```kotlin
ModerationResult(
    isHarmful = true,
    categories = mapOf(
        ModerationCategory.Harassment to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.HarassmentThreatening to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.Hate to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.HateThreatening to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.Sexual to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.SexualMinors to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.Violence to ModerationCategoryResult(false, confidenceScore = 0.0145),
        ModerationCategory.ViolenceGraphic to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.SelfHarm to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.SelfHarmIntent to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.SelfHarmInstructions to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.Illicit to ModerationCategoryResult(true, confidenceScore = 0.9998, appliedInputTypes = listOf(InputType.TEXT)),
        ModerationCategory.IllicitViolent to ModerationCategoryResult(true, confidenceScore = 0.9876, appliedInputTypes = listOf(InputType.TEXT)),
    )
)
```

### OpenAI moderation example (safe content)

```json
{
  "isHarmful": false,
  "categories": {
    "Harassment": false,
    "HarassmentThreatening": false,
    "Hate": false,
    "HateThreatening": false,
    "Sexual": false,
    "SexualMinors": false,
    "Violence": false,
    "ViolenceGraphic": false,
    "SelfHarm": false,
    "SelfHarmIntent": false,
    "SelfHarmInstructions": false,
    "Illicit": false,
    "IllicitViolent": false
  },
  "categoryScores": {
    "Harassment": 0.0001,
    "HarassmentThreatening": 0.0001,
    "Hate": 0.0001,
    "HateThreatening": 0.0001,
    "Sexual": 0.0001,
    "SexualMinors": 0.0001,
    "Violence": 0.0001,
    "ViolenceGraphic": 0.0001,
    "SelfHarm": 0.0001,
    "SelfHarmIntent": 0.0001,
    "SelfHarmInstructions": 0.0001,
    "Illicit": 0.0001,
    "IllicitViolent": 0.0001
  },
  "categoryAppliedInputTypes": {}
}
```

In Koog, the OpenAI response above is presented as follows:

```kotlin
ModerationResult(
    isHarmful = false,
    categories = mapOf(
        ModerationCategory.Harassment to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.HarassmentThreatening to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.Hate to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.HateThreatening to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.Sexual to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.SexualMinors to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.Violence to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.ViolenceGraphic to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.SelfHarm to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.SelfHarmIntent to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.SelfHarmInstructions to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.Illicit to ModerationCategoryResult(false, confidenceScore = 0.0001),
        ModerationCategory.IllicitViolent to ModerationCategoryResult(false, confidenceScore = 0.0001),
    )
)
```

### Ollama moderation example (harmful content)

Ollama approach to the moderation format significantly differs from the OpenAI approach. There are no specific moderation-related API endpoints in Ollama. Instead, Ollama uses the general chat API.

Ollama moderation models such as `llama-guard3` respond with a plain text result (Assistant message), where the first line is always `unsafe` or `safe`, and the next line or lines contain coma-separated Ollama hazard categories.

For example:

```text
unsafe
S1,S10
```

This is translated to the following result in Koog:

```kotlin
ModerationResult(
    isHarmful = true,
    categories = mapOf(
        ModerationCategory.Harassment to ModerationCategoryResult(false),
        ModerationCategory.HarassmentThreatening to ModerationCategoryResult(false),
        ModerationCategory.Hate to ModerationCategoryResult(true),    // from S10
        ModerationCategory.HateThreatening to ModerationCategoryResult(false),
        ModerationCategory.Sexual to ModerationCategoryResult(false),
        ModerationCategory.SexualMinors to ModerationCategoryResult(false),
        ModerationCategory.Violence to ModerationCategoryResult(false),
        ModerationCategory.ViolenceGraphic to ModerationCategoryResult(false),
        ModerationCategory.SelfHarm to ModerationCategoryResult(false),
        ModerationCategory.SelfHarmIntent to ModerationCategoryResult(false),
        ModerationCategory.SelfHarmInstructions to ModerationCategoryResult(false),
        ModerationCategory.Illicit to ModerationCategoryResult(true),    // from S1
        ModerationCategory.IllicitViolent to ModerationCategoryResult(true),    // from S1
    )
)
```

### Ollama moderation example (safe content)

Here is an example of an Ollama response that marks the content as safe:

```text
safe
```

Koog translates the response in the following way:

```kotlin
ModerationResult(
    isHarmful = false,
    categories = mapOf(
        ModerationCategory.Harassment to ModerationCategoryResult(false),
        ModerationCategory.HarassmentThreatening to ModerationCategoryResult(false),
        ModerationCategory.Hate to ModerationCategoryResult(false),
        ModerationCategory.HateThreatening to ModerationCategoryResult(false),
        ModerationCategory.Sexual to ModerationCategoryResult(false),
        ModerationCategory.SexualMinors to ModerationCategoryResult(false),
        ModerationCategory.Violence to ModerationCategoryResult(false),
        ModerationCategory.ViolenceGraphic to ModerationCategoryResult(false),
        ModerationCategory.SelfHarm to ModerationCategoryResult(false),
        ModerationCategory.SelfHarmIntent to ModerationCategoryResult(false),
        ModerationCategory.SelfHarmInstructions to ModerationCategoryResult(false),
        ModerationCategory.Illicit to ModerationCategoryResult(false),
        ModerationCategory.IllicitViolent to ModerationCategoryResult(false),
    )
)
```
# Backend framework integrations

# Ktor Integration: Koog plugin

Koog fits naturally into your Ktor server allowing you to write server-side AI applications using ideomatic Kotlin API from both sides.

Install the Koog plugin once, configure your LLM providers in application.conf/YAML or in code, and then call agents right from your routes. No more wiring LLM clients across modules â€“ your routes just request an agent and are ready to go.

## Overview

The `koog-ktor` module provides idiomatic Kotlin/Ktor integration for serverâ€‘side agentic development:

- Dropâ€‘in Ktor plugin: `install(Koog)` in your Application
- Firstâ€‘class support for OpenAI, Anthropic, Google, OpenRouter, DeepSeek, and Ollama
- Centralized configuration via YAML/CONF and/or code
- Agent setup with prompt, tools, features; simple extension functions for routes
- Direct LLM usage (execute, executeStreaming, moderate)
- JVMâ€‘only Model Context Protocol (MCP) tools integration

## Add dependency

```kotlin
dependencies {
    implementation("ai.koog:koog-ktor:$koogVersion")
}
```

## Quick start

1. Configure providers (in `application.yaml` or `application.conf`)

Use nested keys under `koog.<provider>`. The plugin automatically picks them up.

```yaml
# application.yaml (Ktor config)
koog:
  openai:
    apikey: ${OPENAI_API_KEY}
    baseUrl: https://api.openai.com
  anthropic:
    apikey: ${ANTHROPIC_API_KEY}
    baseUrl: https://api.anthropic.com
  google:
    apikey: ${GOOGLE_API_KEY}
    baseUrl: https://generativelanguage.googleapis.com
  openrouter:
    apikey: ${OPENROUTER_API_KEY}
    baseUrl: https://openrouter.ai
  deepseek:
    apikey: ${DEEPSEEK_API_KEY}
    baseUrl: https://api.deepseek.com
  # Ollama is enabled when any koog.ollama.* key exists
  ollama:
    enable: true
    baseUrl: http://localhost:11434
```

Optional: configure fallback used by direct LLM calls when a requested provider is not configured.

```yaml
koog:
  llm:
    fallback:
      provider: openai
      # see Model identifiers section below
      model: openai.chat.gpt4_1
```

2. Install the plugin and define routes

```kotlin
fun Application.module() {
    install(Koog) {
        // You can also configure providers programmatically (see below)
    }

    routing {
        route("/ai") {
            post("/chat") {
                val userInput = call.receiveText()
                // Create and run a default singleâ€‘run agent using a specific model
                val output = aiAgent(
                    strategy = reActStrategy(),
                    model = OpenAIModels.Chat.GPT4_1,
                    input = userInput
                )
                call.respond(HttpStatusCode.OK, output)
            }
        }
    }
}
```

Notes

- aiAgent requires a concrete model (LLModel) â€“ choose perâ€‘route, perâ€‘use.
- For lowerâ€‘level LLM access, use llm() (PromptExecutor) directly.

## Direct LLM usage from routes

```kotlin
post("/llm-chat") {
    val userInput = call.receiveText()

    val messages = llm().execute(
        prompt("chat") {
            system("You are a helpful assistant that clarifies questions")
            user(userInput)
        },
        GoogleModels.Gemini2_5Pro
    )

    // Join all assistant messages into a single string
    val text = messages.joinToString(separator = "") { it.content }
    call.respond(HttpStatusCode.OK, text)
}
```

Streaming

```kotlin
get("/stream") {
    val flow = llm().executeStreaming(
        prompt("streaming") { user("Stream this response, please") },
        OpenRouterModels.GPT4o
    )

    // Example: buffer and send as one chunk
    val sb = StringBuilder()
    flow.collect { chunk -> sb.append(chunk) }
    call.respondText(sb.toString())
}
```

Moderation

```kotlin
post("/moderated-chat") {
    val userInput = call.receiveText()

    val moderation = llm().moderate(
        prompt("moderation") { user(userInput) },
        OpenAIModels.Moderation.Omni
    )

    if (moderation.isHarmful) {
        call.respond(HttpStatusCode.BadRequest, "Harmful content detected")
        return@post
    }

    val output = aiAgent(
        strategy = reActStrategy(),
        model = OpenAIModels.Chat.GPT4_1,
        input = userInput
    )
    call.respond(HttpStatusCode.OK, output)
}
```

## Programmatic configuration (in code)

All providers and agent behavior can be configured via install(Koog) {}.

```kotlin
install(Koog) {
    llm {
        openAI(apiKey = System.getenv("OPENAI_API_KEY") ?: "") {
            baseUrl = "https://api.openai.com"
            timeouts { // Default values shown below
                requestTimeout = 15.minutes
                connectTimeout = 60.seconds
                socketTimeout = 15.minutes
            }
        }
        anthropic(apiKey = System.getenv("ANTHROPIC_API_KEY") ?: "")
        google(apiKey = System.getenv("GOOGLE_API_KEY") ?: "")
        openRouter(apiKey = System.getenv("OPENROUTER_API_KEY") ?: "")
        deepSeek(apiKey = System.getenv("DEEPSEEK_API_KEY") ?: "")
        ollama { baseUrl = "http://localhost:11434" }

        // Optional fallback used by PromptExecutor when a provider isnâ€™t configured
        fallback {
            provider = LLMProvider.OpenAI
            model = OpenAIModels.Chat.GPT4_1
        }
    }

    agentConfig {
        // Provide a reusable base prompt for your agents
        prompt(name = "agent") {
            system("You are a helpful serverâ€‘side agent")
        }

        // Limit runaway tools/loops
        maxAgentIterations = 10

        // Register tools available to agents by default
        registerTools {
            // tool(::yourTool) // see Tools Overview for details
        }

        // Install agent features (tracing, etc.)
        // install(OpenTelemetry) { /* ... */ }
    }
}
```

## Model identifiers in config (fallback)

When configuring llm.fallback in YAML/CONF, use these identifier formats:

- OpenAI: openai.chat.gpt4_1, openai.reasoning.o3, openai.costoptimized.gpt4_1mini, openai.audio.gpt4oaudio, openai.moderation.omni
- Anthropic: anthropic.sonnet_3_7, anthropic.opus_4, anthropic.haiku_3_5
- Google: google.gemini2_5pro, google.gemini2_0flash001
- OpenRouter: openrouter.gpt4o, openrouter.gpt4, openrouter.claude3sonnet
- DeepSeek: deepseek.deepseek-chat, deepseek.deepseek-reasoner
- Ollama: ollama.meta.llama3.2, ollama.alibaba.qwq:32b, ollama.groq.llama3-grok-tool-use:8b

Note

- For OpenAI you must include the category (chat, reasoning, costoptimized, audio, embeddings, moderation).
- For Ollama, both ollama.model and ollama.. are supported.

## MCP tools (JVMâ€‘only)

On JVM you can add tools from an MCP server to your agent tool registry:

```kotlin
install(Koog) {
    agentConfig {
        mcp {
            // Register via SSE
            sse("https://your-mcp-server.com/sse")

            // Or register via spawned process (stdio transport)
            // process(Runtime.getRuntime().exec("your-mcp-binary ..."))

            // Or from an existing MCP client instance
            // client(existingMcpClient)
        }
    }
}
```

## Why Koog + Ktor?

- Kotlinâ€‘first, typeâ€‘safe development of agents in your server
- Centralized config with clean, testable route code
- Use the right model perâ€‘route, or fall back automatically for direct LLM calls
- Productionâ€‘ready features: tools, moderation, streaming, and tracing

# Spring Boot Integration

Koog provides seamless Spring Boot integration through its auto-configuration starter, making it easy to incorporate AI agents into your Spring Boot applications with minimal setup.

## Overview

The `koog-spring-boot-starter` automatically configures LLM clients based on your application properties and provides ready-to-use beans for dependency injection. It supports all major LLM providers including:

- OpenAI
- Anthropic
- Google
- OpenRouter
- DeepSeek
- Ollama

## Getting Started

### 1. Add Dependency

Add the Koog Spring Boot starter and [Ktor Client Engine](https://ktor.io/docs/client-engines.html#jvm) to your `build.gradle.kts` or `pom.xml`:

```kotlin
dependencies {
    implementation("ai.koog:koog-spring-boot-starter:$koogVersion")
    implementation("io.ktor:ktor-client-okhttp-jvm:$ktorVersion")
}
```

### 2. Configure Providers

Configure your preferred LLM providers in `application.properties`:

```properties
# OpenAI Configuration
ai.koog.openai.enabled=true
ai.koog.openai.api-key=${OPENAI_API_KEY}
ai.koog.openai.base-url=https://api.openai.com
# Anthropic Configuration  
ai.koog.anthropic.enabled=true
ai.koog.anthropic.api-key=${ANTHROPIC_API_KEY}
ai.koog.anthropic.base-url=https://api.anthropic.com
# Google Configuration
ai.koog.google.enabled=true
ai.koog.google.api-key=${GOOGLE_API_KEY}
ai.koog.google.base-url=https://generativelanguage.googleapis.com
# OpenRouter Configuration
ai.koog.openrouter.enabled=true
ai.koog.openrouter.api-key=${OPENROUTER_API_KEY}
ai.koog.openrouter.base-url=https://openrouter.ai
# DeepSeek Configuration
ai.koog.deepseek.enabled=true
ai.koog.deepseek.api-key=${DEEPSEEK_API_KEY}
ai.koog.deepseek.base-url=https://api.deepseek.com
# Ollama Configuration (local - no API key required)
ai.koog.ollama.enabled=true
ai.koog.ollama.base-url=http://localhost:11434
```

Or using YAML format (`application.yml`):

```yaml
ai:
    koog:
        openai:
            enabled: true
            api-key: ${OPENAI_API_KEY}
            base-url: https://api.openai.com
        anthropic:
            enabled: true
            api-key: ${ANTHROPIC_API_KEY}
            base-url: https://api.anthropic.com
        google:
            enabled: true
            api-key: ${GOOGLE_API_KEY}
            base-url: https://generativelanguage.googleapis.com
        openrouter:
            enabled: true
            api-key: ${OPENROUTER_API_KEY}
            base-url: https://openrouter.ai
        deepseek:
            enabled: true
            api-key: ${DEEPSEEK_API_KEY}
            base-url: https://api.deepseek.com
        ollama:
            enabled: true # Set it to `true` explicitly to activate !!!
            base-url: http://localhost:11434
```

Both `ai.koog.PROVIDER.api-key` and `ai.koog.PROVIDER.enabled` properties are used to activate the provider.

If the provider supports the API Key (like OpenAI, Anthropic, Google), then `ai.koog.PROVIDER.enabled` is set to `true` by default.

If the provider does not support the API Key, like Ollama, `ai.koog.PROVIDER.enabled` is set to `false` by default, and provider should be enabled explicitly in the application configuration.

Provider's base urls are set to their default values in the Spring Boot starter, but you may override it in your application.

Environment Variables

It's recommended to use environment variables for API keys to keep them secure and out of version control. Spring configuration uses LLM provider's well-known environment variables. For example, setting the environment variable `OPENAI_API_KEY` is enough for OpenAI spring configuration to activate.

| LLM Provider | Environment Variables |
| ------------ | --------------------- |
| Open AI      | `OPENAI_API_KEY`      |
| Anthropic    | `ANTHROPIC_API_KEY`   |
| Google       | `GOOGLE_API_KEY`      |
| OpenRouter   | `OPENROUTER_API_KEY`  |
| DeepSeek     | `DEEPSEEK_API_KEY`    |

### 3. Inject and Use

Inject the auto-configured executors into your services:

```kotlin
@Service
class AIService(
    private val openAIExecutor: SingleLLMPromptExecutor?,
    private val anthropicExecutor: SingleLLMPromptExecutor?
) {

    suspend fun generateResponse(input: String): String {
        val prompt = prompt {
            system("You are a helpful AI assistant")
            user(input)
        }

        return when {
            openAIExecutor != null -> {
                val result = openAIExecutor.execute(prompt)
                result.text
            }
            anthropicExecutor != null -> {
                val result = anthropicExecutor.execute(prompt)
                result.text
            }
            else -> throw IllegalStateException("No LLM provider configured")
        }
    }
}
```

## Advanced Usage

### REST Controller Example

Create a chat endpoint using auto-configured executors:

```kotlin
@RestController
@RequestMapping("/api/chat")
class ChatController(
    private val anthropicExecutor: SingleLLMPromptExecutor?
) {

    @PostMapping
    suspend fun chat(@RequestBody request: ChatRequest): ResponseEntity<ChatResponse> {
        return if (anthropicExecutor != null) {
            try {
                val prompt = prompt {
                    system("You are a helpful assistant")
                    user(request.message)
                }

                val result = anthropicExecutor.execute(prompt)
                ResponseEntity.ok(ChatResponse(result.text))
            } catch (e: Exception) {
                ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                    .body(ChatResponse("Error processing request"))
            }
        } else {
            ResponseEntity.status(HttpStatus.SERVICE_UNAVAILABLE)
                .body(ChatResponse("AI service not configured"))
        }
    }
}

data class ChatRequest(val message: String)
data class ChatResponse(val response: String)
```

### Multiple Provider Support

Handle multiple providers with fallback logic:

```kotlin
@Service
class RobustAIService(
    private val openAIExecutor: SingleLLMPromptExecutor?,
    private val anthropicExecutor: SingleLLMPromptExecutor?,
    private val openRouterExecutor: SingleLLMPromptExecutor?
) {

    suspend fun generateWithFallback(input: String): String {
        val prompt = prompt {
            system("You are a helpful AI assistant")
            user(input)
        }

        val executors = listOfNotNull(openAIExecutor, anthropicExecutor, openRouterExecutor)

        for (executor in executors) {
            try {
                val result = executor.execute(prompt)
                return result.text
            } catch (e: Exception) {
                logger.warn("Executor failed, trying next: ${e.message}")
                continue
            }
        }

        throw IllegalStateException("All AI providers failed")
    }

    companion object {
        private val logger = LoggerFactory.getLogger(RobustAIService::class.java)
    }
}
```

### Configuration Properties

You can also inject configuration properties for custom logic:

```kotlin
@Service
class ConfigurableAIService(
    private val openAIExecutor: SingleLLMPromptExecutor?,
    @Value("\${ai.koog.openai.api-key:}") private val openAIKey: String
) {

    fun isOpenAIConfigured(): Boolean = openAIKey.isNotBlank() && openAIExecutor != null

    suspend fun processIfConfigured(input: String): String? {
        return if (isOpenAIConfigured()) {
            val result = openAIExecutor!!.execute(prompt { user(input) })
            result.text
        } else {
            null
        }
    }
}
```

## Configuration Reference

### Available Properties

| Property                      | Description         | Bean Condition                                                  | Default                                     |
| ----------------------------- | ------------------- | --------------------------------------------------------------- | ------------------------------------------- |
| `ai.koog.openai.api-key`      | OpenAI API key      | Required for `openAIExecutor` bean                              | -                                           |
| `ai.koog.openai.base-url`     | OpenAI base URL     | Optional                                                        | `https://api.openai.com`                    |
| `ai.koog.anthropic.api-key`   | Anthropic API key   | Required for `anthropicExecutor` bean                           | -                                           |
| `ai.koog.anthropic.base-url`  | Anthropic base URL  | Optional                                                        | `https://api.anthropic.com`                 |
| `ai.koog.google.api-key`      | Google API key      | Required for `googleExecutor` bean                              | -                                           |
| `ai.koog.google.base-url`     | Google base URL     | Optional                                                        | `https://generativelanguage.googleapis.com` |
| `ai.koog.openrouter.api-key`  | OpenRouter API key  | Required for `openRouterExecutor` bean                          | -                                           |
| `ai.koog.openrouter.base-url` | OpenRouter base URL | Optional                                                        | `https://openrouter.ai`                     |
| `ai.koog.deepseek.api-key`    | DeepSeek API key    | Required for `deepSeekExecutor` bean                            | -                                           |
| `ai.koog.deepseek.base-url`   | DeepSeek base URL   | Optional                                                        | `https://api.deepseek.com`                  |
| `ai.koog.ollama.base-url`     | Ollama base URL     | Any `ai.koog.ollama.*` property activates `ollamaExecutor` bean | `http://localhost:11434`                    |

### Bean Names

The auto-configuration creates the following beans (when configured):

- `openAIExecutor` - OpenAI executor (requires `ai.koog.openai.api-key`)
- `anthropicExecutor` - Anthropic executor (requires `ai.koog.anthropic.api-key`)
- `googleExecutor` - Google executor (requires `ai.koog.google.api-key`)
- `openRouterExecutor` - OpenRouter executor (requires `ai.koog.openrouter.api-key`)
- `deepSeekExecutor` - DeepSeek executor (requires `ai.koog.deepseek.api-key`)
- `ollamaExecutor` - Ollama executor (requires any `ai.koog.ollama.*` property)

## Troubleshooting

### Common Issues

**Bean not found error:**

```text
No qualifying bean of type 'SingleLLMPromptExecutor' available
```

**Solution:** Ensure you have configured at least one provider in your properties file.

**Multiple beans error:**

```text
Multiple qualifying beans of type 'SingleLLMPromptExecutor' available
```

**Solution:** Use `@Qualifier` to specify which bean you want:

```kotlin
@Service
class MyService(
    @Qualifier("openAIExecutor") private val openAIExecutor: SingleLLMPromptExecutor,
    @Qualifier("anthropicExecutor") private val anthropicExecutor: SingleLLMPromptExecutor
) {
    // ...
}
```

**API key not loaded:**

```text
API key is required but not provided
```

**Solution:** Check that your environment variables are properly set and accessible to your Spring Boot application.

## Best Practices

1. **Environment Variables**: Always use environment variables for API keys
1. **Nullable Injection**: Use nullable types (`SingleLLMPromptExecutor?`) to handle cases where providers aren't configured
1. **Fallback Logic**: Implement fallback mechanisms when using multiple providers
1. **Error Handling**: Always wrap executor calls in try-catch blocks for production code
1. **Testing**: Use mocks in tests to avoid making actual API calls
1. **Configuration Validation**: Check if executors are available before using them

## Next Steps

- Learn about the [basic agents](../basic-agents/) to build minimal AI workflows
- Explore [complex workflow agents](../complex-workflow-agents/) for advanced use cases
- See the [tools overview](../tools-overview/) to extend your agents' capabilities
- Check out [examples](../examples/) for real-world implementations
- Read the [glossary](../glossary/) to understand the framework better
# Advanced usage

# Structured output

## Introduction

The Structured Output API provides a way to ensure that responses from Large Language Models (LLMs) conform to specific data structures. This is crucial for building reliable AI applications where you need predictable, well-formatted data rather than free-form text.

This page explains how to use this API to define data structures, generate schemas, and request structured responses from LLMs.

## Key components and concepts

The Structured Output API consists of several key components:

1. **Data structure definition**: Kotlin data classes annotated with kotlinx.serialization and LLM-specific annotations.
1. **JSON Schema generation**: tools to generate JSON schemas from Kotlin data classes.
1. **Structured LLM requests**: methods to request responses from LLMs that conform to the defined structures.
1. **Response handling**: processing and validating the structured responses.

## Defining data structures

The first step in using the Structured Output API is to define your data structures using Kotlin data classes.

### Basic structure

```kotlin
@Serializable
@SerialName("WeatherForecast")
@LLMDescription("Weather forecast for a given location")
data class WeatherForecast(
    @property:LLMDescription("Temperature in Celsius")
    val temperature: Int,
    @property:LLMDescription("Weather conditions (e.g., sunny, cloudy, rainy)")
    val conditions: String,
    @property:LLMDescription("Chance of precipitation in percentage")
    val precipitation: Int
)
```

### Key annotations

- `@Serializable`: required for kotlinx.serialization to work with the class.
- `@SerialName`: specifies the name to use during serialization.
- `@LLMDescription`: provides a description of the class for the LLM. For field annotations, use `@property:LLMDescription`.

### Supported features

The API supports a wide range of data structure features:

#### Nested classes

```kotlin
@Serializable
@SerialName("WeatherForecast")
data class WeatherForecast(
    // Other fields
    @property:LLMDescription("Coordinates of the location")
    val latLon: LatLon
) {
    @Serializable
    @SerialName("LatLon")
    data class LatLon(
        @property:LLMDescription("Latitude of the location")
        val lat: Double,
        @property:LLMDescription("Longitude of the location")
        val lon: Double
    )
}
```

#### Collections (lists and maps)

```kotlin
@Serializable
@SerialName("WeatherForecast")
data class WeatherForecast(
    // Other fields
    @property:LLMDescription("List of news articles")
    val news: List<WeatherNews>,
    @property:LLMDescription("Map of weather sources")
    val sources: Map<String, WeatherSource>
)
```

#### Enums

```kotlin
@Serializable
@SerialName("Pollution")
enum class Pollution { Low, Medium, High }
```

#### Polymorphism with sealed classes

```kotlin
@Serializable
@SerialName("WeatherAlert")
sealed class WeatherAlert {
    abstract val severity: Severity
    abstract val message: String

    @Serializable
    @SerialName("Severity")
    enum class Severity { Low, Moderate, Severe, Extreme }

    @Serializable
    @SerialName("StormAlert")
    data class StormAlert(
        override val severity: Severity,
        override val message: String,
        @property:LLMDescription("Wind speed in km/h")
        val windSpeed: Double
    ) : WeatherAlert()

    @Serializable
    @SerialName("FloodAlert")
    data class FloodAlert(
        override val severity: Severity,
        override val message: String,
        @property:LLMDescription("Expected rainfall in mm")
        val expectedRainfall: Double
    ) : WeatherAlert()
}
```

### Providing examples

You can provide examples to help the LLM understand the expected format:

```kotlin
val exampleForecasts = listOf(
  WeatherForecast(
    news = listOf(WeatherNews(0.0), WeatherNews(5.0)),
    sources = mutableMapOf(
      "openweathermap" to WeatherSource(Url("https://api.openweathermap.org/data/2.5/weather")),
      "googleweather" to WeatherSource(Url("https://weather.google.com"))
    )
    // Other fields
  ),
  WeatherForecast(
    news = listOf(WeatherNews(25.0), WeatherNews(35.0)),
    sources = mutableMapOf(
      "openweathermap" to WeatherSource(Url("https://api.openweathermap.org/data/2.5/weather")),
      "googleweather" to WeatherSource(Url("https://weather.google.com"))
    )
  )
)
```

## Requesting structured responses

There are three main layers where you can use structured output in Koog:

1. **Prompt executor layer**: Make direct LLM calls using a prompt executor
1. **Agent LLM context layer**: Use within agent sessions for conversational contexts
1. **Node layer**: Create reusable agent nodes with structured output capabilities

### Layer 1: Prompt executor

The prompt executor layer provides the most direct way to make structured LLM calls. Use the `executeStructured` method for single, standalone requests:

This method executes a prompt and ensures the response is properly structured by:

- Automatically selecting the best structured output approach based on [model capabilities](../model-capabilities/)
- Injecting structured output instructions into the original prompt when needed
- Using native structured output support when available
- Providing automatic error correction through an auxiliary LLM when parsing fails

Here is an example of using the `executeStructured` method:

```kotlin
// Define a simple, single-provider prompt executor
val promptExecutor = simpleOpenAIExecutor(System.getenv("OPENAI_KEY"))

// Make an LLM call that returns a structured response
val structuredResponse = promptExecutor.executeStructured<WeatherForecast>(
        // Define the prompt (both system and user messages)
        prompt = prompt("structured-data") {
            system(
                """
                You are a weather forecasting assistant.
                When asked for a weather forecast, provide a realistic but fictional forecast.
                """.trimIndent()
            )
            user(
              "What is the weather forecast for Amsterdam?"
            )
        },
        // Define the main model that will execute the request
        model = OpenAIModels.Chat.GPT4oMini,
        // Optional: provide examples to help the model understand the format
        examples = exampleForecasts,
        // Optional: provide a fixing parser for error correction
        fixingParser = StructureFixingParser(
            model = OpenAIModels.Chat.GPT4o,
            retries = 3
        )
    )
```

The `executeStructured` method takes the following arguments:

| Name           | Data type              | Required | Default       | Description                                                                                                     |
| -------------- | ---------------------- | -------- | ------------- | --------------------------------------------------------------------------------------------------------------- |
| `prompt`       | Prompt                 | Yes      |               | The prompt to execute. For more information, see [Prompts](../prompts/).                                        |
| `model`        | LLModel                | Yes      |               | The main model to execute the prompt.                                                                           |
| `examples`     | List                   | No       | `emptyList()` | Optional list of examples to help the model understand the expected format.                                     |
| `fixingParser` | StructureFixingParser? | No       | `null`        | Optional parser that handles malformed responses by using an auxiliary LLM to intelligently fix parsing errors. |

The method returns a `Result<StructuredResponse<T>>` containing either the successfully parsed structured data or an error.

### Layer 2: Agent LLM context

The agent LLM context layer allows you to request structured responses within agent sessions. This is useful for building conversational agents that need structured data at specific points in their flow.

Use the `requestLLMStructured` method within a `writeSession` for agent-based interactions:

```kotlin
val structuredResponse = llm.writeSession {
    requestLLMStructured<WeatherForecast>(
        examples = exampleForecasts,
        fixingParser = StructureFixingParser(
            model = OpenAIModels.Chat.GPT4o,
            retries = 3
        )
    )
}
```

The `fixingParser` parameter specifies a configuration for handling malformed responses through auxiliary LLM processing during retries. This helps ensure that you always get a valid response.

#### Integrating with agent strategies

You can integrate structured data processing into your agent strategies:

```kotlin
val agentStrategy = strategy("weather-forecast") {
    val setup by nodeLLMRequest()

    val getStructuredForecast by node<Message.Response, String> { _ ->
        val structuredResponse = llm.writeSession {
            requestLLMStructured<WeatherForecast>(
                fixingParser = StructureFixingParser(
                    model = OpenAIModels.Chat.GPT4o,
                    retries = 3
                )
            )
        }

        """
        Response structure:
        $structuredResponse
        """.trimIndent()
    }

    edge(nodeStart forwardTo setup)
    edge(setup forwardTo getStructuredForecast)
    edge(getStructuredForecast forwardTo nodeFinish)
}
```

### Layer 3: Node layer

The node layer provides the highest level of abstraction for structured output in agent workflows. Use `nodeLLMRequestStructured` to create reusable agent nodes that handle structured data.

This creates an agent node that:

- Accepts a `String` input (user message)
- Appends the message to the LLM prompt
- Requests structured output from the LLM
- Returns `Result<StructuredResponse<MyStruct>>`

#### Node layer example

```kotlin
val agentStrategy = strategy("weather-forecast") {
    val setup by node<Unit, String> { _ ->
        "Please provide a weather forecast for Amsterdam"
    }

    // Create a structured output node using delegate syntax
    val getWeatherForecast by nodeLLMRequestStructured<WeatherForecast>(
        name = "forecast-node",
        examples = exampleForecasts,
        fixingParser = StructureFixingParser(
            model = OpenAIModels.Chat.GPT4o,
            retries = 3
        )
    )

    val processResult by node<Result<StructuredResponse<WeatherForecast>>, String> { result ->
        when {
            result.isSuccess -> {
                val forecast = result.getOrNull()?.data
                "Weather forecast: $forecast"
            }
            result.isFailure -> {
                "Failed to get structured forecast: ${result.exceptionOrNull()?.message}"
            }
            else -> "Unknown result state"
        }
    }

    edge(nodeStart forwardTo setup)
    edge(setup forwardTo getWeatherForecast)
    edge(getWeatherForecast forwardTo processResult)
    edge(processResult forwardTo nodeFinish)
}
```

#### Full code sample

Here is a full example of using the Structured Output API:

```kotlin
// Note: Import statements are omitted for brevity
@Serializable
@SerialName("SimpleWeatherForecast")
@LLMDescription("Simple weather forecast for a location")
data class SimpleWeatherForecast(
    @property:LLMDescription("Location name")
    val location: String,
    @property:LLMDescription("Temperature in Celsius")
    val temperature: Int,
    @property:LLMDescription("Weather conditions (e.g., sunny, cloudy, rainy)")
    val conditions: String
)

val token = System.getenv("OPENAI_KEY") ?: error("Environment variable OPENAI_KEY is not set")

fun main(): Unit = runBlocking {
    // Create sample forecasts
    val exampleForecasts = listOf(
        SimpleWeatherForecast(
            location = "New York",
            temperature = 25,
            conditions = "Sunny"
        ),
        SimpleWeatherForecast(
            location = "London",
            temperature = 18,
            conditions = "Cloudy"
        )
    )

    // Generate JSON Schema
    val forecastStructure = JsonStructure.create<SimpleWeatherForecast>(
        schemaGenerator = BasicJsonSchemaGenerator.Default,
        examples = exampleForecasts
    )

    // Define the agent strategy
    val agentStrategy = strategy("weather-forecast") {
        val setup by nodeLLMRequest()

        val getStructuredForecast by node<Message.Response, String> { _ ->
            val structuredResponse = llm.writeSession {
                requestLLMStructured<SimpleWeatherForecast>()
            }

            """
            Response structure:
            $structuredResponse
            """.trimIndent()
        }

        edge(nodeStart forwardTo setup)
        edge(setup forwardTo getStructuredForecast)
        edge(getStructuredForecast forwardTo nodeFinish)
    }


    // Configure and run the agent
    val agentConfig = AIAgentConfig(
        prompt = prompt("weather-forecast-prompt") {
            system(
                """
                You are a weather forecasting assistant.
                When asked for a weather forecast, provide a realistic but fictional forecast.
                """.trimIndent()
            )
        },
        model = OpenAIModels.Chat.GPT4o,
        maxAgentIterations = 5
    )

    val runner = AIAgent(
        promptExecutor = simpleOpenAIExecutor(token),
        toolRegistry = ToolRegistry.EMPTY,
        strategy = agentStrategy,
        agentConfig = agentConfig
    )

    runner.run("Get weather forecast for Paris")
}
```

## Advanced usage

The examples above demonstrate the simplified API that automatically selects the best structured output approach based on model capabilities. For more control over the structured output process, you can use the advanced API with manual schema creation and provider-specific configurations.

### Manual schema creation and configuration

Instead of relying on automatic schema generation, you can create schemas explicitly using `JsonStructure.create` and configure structured output behavior manually via the `StructuredOutput` class.

The key difference is that instead of passing simple parameters like `examples` and `fixingParser`, you create a `StructuredRequestConfig` object that allows fine-grained control over:

- **Schema generation**: Choose specific generators (Standard, Basic, or Provider-specific)
- **Output modes**: Native structured output support vs Manual prompting
- **Provider mapping**: Different configurations for different LLM providers
- **Fallback strategies**: Default behavior when provider-specific config is unavailable

```kotlin
// Create different schema structures with different generators
val genericStructure = JsonStructure.create<WeatherForecast>(
    schemaGenerator = StandardJsonSchemaGenerator,
    examples = exampleForecasts
)

val openAiStructure = JsonStructure.create<WeatherForecast>(
    schemaGenerator = OpenAIBasicJsonSchemaGenerator,
    examples = exampleForecasts
)

val promptExecutor = simpleOpenAIExecutor(System.getenv("OPENAI_KEY"))

// The advanced API uses StructuredRequestConfig instead of simple parameters
val structuredResponse = promptExecutor.executeStructured(
    prompt = prompt("structured-data") {
        system("You are a weather forecasting assistant.")
        user("What is the weather forecast for Amsterdam?")
    },
    model = OpenAIModels.Chat.GPT4oMini,
    config = StructuredRequestConfig(
        byProvider = mapOf(
            LLMProvider.OpenAI to StructuredRequest.Native(openAiStructure),
        ),
        default = StructuredRequest.Manual(genericStructure),
        fixingParser = StructureFixingParser(
            model = AnthropicModels.Haiku_3_5,
            retries = 2
        )
    )
)
```

### Schema generators

Different schema generators are available depending on your needs:

- **StandardJsonSchemaGenerator**: Full JSON Schema with support for polymorphism, definitions, and recursive references
- **BasicJsonSchemaGenerator**: Simplified schema without polymorphism support, compatible with more models
- **Provider-specific generators**: Optimized schemas for specific LLM providers (OpenAI, Google, etc.)

### Usage across all layers

The advanced configuration works consistently across all three layers of the API. The method names remain the same, only the parameter changes from simple arguments to the more advanced `StructuredOutputConfig`:

- **Prompt executor**: `executeStructured(prompt, model, config: StructuredRequestConfig<T>)`
- **Agent LLM context**: `requestLLMStructured(config: StructuredRequestConfig<T>)`
- **Node layer**: `nodeLLMRequestStructured(config: StructuredRequestConfig<T>)`

The simplified API (using just `examples` and `fixingParser` parameters) is recommended for most use cases, while the advanced API provides additional control when needed.

## Best practices

1. **Use clear descriptions**: provide clear and detailed descriptions using `@LLMDescription` annotations to help the LLM understand the expected data.
1. **Provide examples**: include examples of valid data structures to guide the LLM.
1. **Handle errors gracefully**: implement proper error handling to deal with cases where the LLM might not produce a valid structure.
1. **Use appropriate schema types**: select the appropriate schema format and type based on your needs and the capabilities of the LLM you are using.
1. **Test with different models**: different LLMs may have varying abilities to follow structured formats, so test with multiple models if possible.
1. **Start simple**: begin with simple structures and gradually add complexity as needed.
1. **Use polymorphism Carefully**: while the API supports polymorphism with sealed classes, be aware that it can be more challenging for LLMs to handle correctly.

# Streaming API

## Introduction

Koogâ€™s **Streaming API** lets you consume **LLM output incrementally** as a `Flow<StreamFrame>`. Instead of waiting for a full response, your code can:

- render assistant text as it arrives,
- detect **tool calls** live and act on them,
- know when a stream **ends** and why.

The stream carries **typed frames**:

- `StreamFrame.Append(text: String)` â€” incremental assistant text
- `StreamFrame.ToolCall(id: String?, name: String, content: String)` â€” tool invocation (combined safely)
- `StreamFrame.End(finishReason: String?)` â€” end-of-stream marker

Helpers are provided to extract plain text, convert frames to `Message.Response` objects, and safely **combine chunked tool calls**.

______________________________________________________________________

## Streaming API overview

With streaming you can:

- Process data as it arrives (improves UI responsiveness)
- Parse structured info on the fly (Markdown/JSON/etc.)
- Emit objects as they complete
- Trigger tools in real time

You can operate either on the **frames** themselves or on **plain text** derived from frames.

______________________________________________________________________

## Usage

### Working with frames directly

This is the most general approach: react to each frame kind.

```kotlin
llm.writeSession {
    appendPrompt { user("Tell me a joke, then call a tool with JSON args.") }

    val stream = requestLLMStreaming() // Flow<StreamFrame>

    stream.collect { frame ->
        when (frame) {
            is StreamFrame.Append -> print(frame.text)
            is StreamFrame.ToolCall -> {
                println("\nðŸ”§ Tool call: ${frame.name} args=${frame.content}")
                // Optionally parse lazily:
                // val json = frame.contentJson
            }
            is StreamFrame.End -> println("\n[END] reason=${frame.finishReason}")
        }
    }
}
```

It is important to note that you can parse the output by working directly with a raw string stream. This approach gives you more flexibility and control over the parsing process.

Here is a raw string stream with the Markdown definition of the output structure:

```kotlin
fun markdownBookDefinition(): MarkdownStructureDefinition {
    return MarkdownStructureDefinition("name", schema = { /*...*/ })
}

val mdDefinition = markdownBookDefinition()

llm.writeSession {
    val stream = requestLLMStreaming(mdDefinition)
    // Access the raw string chunks directly
    stream.collect { chunk ->
        // Process each chunk of text as it arrives
        println("Received chunk: $chunk") // The chunks together will be structured as a text following the mdDefinition schema
    }
}
```

### Working with a raw text stream (derived)

If you have existing streaming parsers that expect `Flow<String>`, derive text chunks via `filterTextOnly()` or collect them with `collectText()`.

```kotlin
llm.writeSession {
    val frames = requestLLMStreaming()

    // Stream text chunks as they come:
    frames.filterTextOnly().collect { chunk -> print(chunk) }

    // Or, gather all text into one String after End:
    val fullText = frames.collectText()
    println("\n---\n$fullText")
}
```

### Listening to stream events in event handlers

You can listen to stream events in [agent event handlers](../agent-event-handlers/).

```kotlin
handleEvents {
    onToolCallStarting { context ->
        println("\nðŸ”§ Using ${context.toolName} with ${context.toolArgs}... ")
    }
    onLLMStreamingFrameReceived { context ->
        (context.streamFrame as? StreamFrame.Append)?.let { frame ->
            print(frame.text)
        }
    }
    onLLMStreamingFailed { context -> 
        println("âŒ Error: ${context.error}")
    }
    onLLMStreamingCompleted {
        println("ðŸ Done")
    }
}
```

### Converting frames to `Message.Response`

You can transform a collected list of frames to standard message objects:

- `toAssistantMessageOrNull()`
- `toToolCallMessages()`
- `toMessageResponses()`

______________________________________________________________________

## Examples

### Structured data while streaming (Markdown example)

Although it is possible to work with a raw string stream, it is often more convenient to work with [structured data](../structured-output/).

The structured data approach includes the following key components:

1. **MarkdownStructureDefinition**: a class to help you define the schema and examples for structured data in Markdown format.
1. **markdownStreamingParser**: a function to create a parser that processes a stream of Markdown chunks and emits events.

The sections below provide step-by-step instructions and code samples related to processing a stream of structured data.

#### 1. Define your data structure

First, define a data class to represent your structured data:

```kotlin
@Serializable
data class Book(
    val title: String,
    val author: String,
    val description: String
)
```

#### 2. Define the Markdown structure

Create a definition that specifies how your data should be structured in Markdown with the `MarkdownStructureDefinition` class:

```kotlin
fun markdownBookDefinition(): MarkdownStructureDefinition {
    return MarkdownStructureDefinition("bookList", schema = {
        markdown {
            header(1, "title")
            bulleted {
                item("author")
                item("description")
            }
        }
    }, examples = {
        markdown {
            header(1, "The Great Gatsby")
            bulleted {
                item("F. Scott Fitzgerald")
                item("A novel set in the Jazz Age that tells the story of Jay Gatsby's unrequited love for Daisy Buchanan.")
            }
        }
    })
}
```

#### 3. Create a parser for your data structure

The `markdownStreamingParser` provides several handlers for different Markdown elements:

```kotlin
markdownStreamingParser {
    // Handle level 1 headings (level ranges from 1 to 6)
    onHeader(1) { headerText -> }
    // Handle bullet points
    onBullet { bulletText -> }
    // Handle code blocks
    onCodeBlock { codeBlockContent -> }
    // Handle lines matching a regex pattern
    onLineMatching(Regex("pattern")) { line -> }
    // Handle the end of the stream
    onFinishStream { remainingText -> }
}
```

Using the defined handlers, you can implement a function that parses the Markdown stream and emits your data objects with the `markdownStreamingParser` function.

```kotlin
fun parseMarkdownStreamToBooks(markdownStream: Flow<StreamFrame>): Flow<Book> {
   return flow {
      markdownStreamingParser {
         var currentBookTitle = ""
         val bulletPoints = mutableListOf<String>()

         // Handle the event of receiving the Markdown header in the response stream
         onHeader(1) { headerText ->
            // If there was a previous book, emit it
            if (currentBookTitle.isNotEmpty() && bulletPoints.isNotEmpty()) {
               val author = bulletPoints.getOrNull(0) ?: ""
               val description = bulletPoints.getOrNull(1) ?: ""
               emit(Book(currentBookTitle, author, description))
            }

            currentBookTitle = headerText
            bulletPoints.clear()
         }

         // Handle the event of receiving the Markdown bullets list in the response stream
         onBullet { bulletText ->
            bulletPoints.add(bulletText)
         }

         // Handle the end of the response stream
         onFinishStream {
            // Emit the last book, if present
            if (currentBookTitle.isNotEmpty() && bulletPoints.isNotEmpty()) {
               val author = bulletPoints.getOrNull(0) ?: ""
               val description = bulletPoints.getOrNull(1) ?: ""
               emit(Book(currentBookTitle, author, description))
            }
         }
      }.parseStream(markdownStream.filterTextOnly())
   }
}
```

#### 4. Use the parser in your agent strategy

```kotlin
val agentStrategy = strategy<String, List<Book>>("library-assistant") {
   // Describe the node containing the output stream parsing
   val getMdOutput by node<String, List<Book>> { booksDescription ->
      val books = mutableListOf<Book>()
      val mdDefinition = markdownBookDefinition()

      llm.writeSession {
         appendPrompt { user(booksDescription) }
         // Initiate the response stream in the form of the definition `mdDefinition`
         val markdownStream = requestLLMStreaming(mdDefinition)
         // Call the parser with the result of the response stream and perform actions with the result
         parseMarkdownStreamToBooks(markdownStream).collect { book ->
            books.add(book)
            println("Parsed Book: ${book.title} by ${book.author}")
         }
      }

      books
   }
   // Describe the agent's graph making sure the node is accessible
   edge(nodeStart forwardTo getMdOutput)
   edge(getMdOutput forwardTo nodeFinish)
}
```

### Advanced usage: Streaming with tools

You can also use the Streaming API with tools to process data as it arrives. The following sections provide a brief step-by-step guide on how to define a tool and use it with streaming data.

### 1. Define a tool for your data structure

```kotlin
@Serializable
data class Book(
   val title: String,
   val author: String,
   val description: String
)

class BookTool(): SimpleTool<Book>(
    argsSerializer = Book.serializer(),
    name = NAME,
    description = "A tool to parse book information from Markdown"
) {

    companion object { const val NAME = "book" }

    override suspend fun execute(args: Book): String {
        println("${args.title} by ${args.author}:\n ${args.description}")
        return "Done"
    }
}
```

### 2. Use the tool with streaming data

```kotlin
val agentStrategy = strategy<String, Unit>("library-assistant") {
   val getMdOutput by node<String, Unit> { input ->
      val mdDefinition = markdownBookDefinition()

      llm.writeSession {
         appendPrompt { user(input) }
         val markdownStream = requestLLMStreaming(mdDefinition)

         parseMarkdownStreamToBooks(markdownStream).collect { book ->
            callToolRaw(BookTool.NAME, book)
            /* Other possible options:
                callTool(BookTool::class, book)
                callTool<BookTool>(book)
                findTool(BookTool::class).execute(book)
            */
         }

         // We can make parallel tool calls
         parseMarkdownStreamToBooks(markdownStream).toParallelToolCallsRaw(toolClass=BookTool::class).collect {
            println("Tool call result: $it")
         }
      }
   }

   edge(nodeStart forwardTo getMdOutput)
   edge(getMdOutput forwardTo nodeFinish)
 }
```

### 3. Register the tool in your agent configuration

```kotlin
val toolRegistry = ToolRegistry {
   tool(BookTool())
}

val runner = AIAgent(
   promptExecutor = simpleOpenAIExecutor(token),
   toolRegistry = toolRegistry,
   strategy = agentStrategy,
   agentConfig = agentConfig
)
```

## Best practices

1. **Define clear structures**: create clear and unambiguous markdown structures for your data.
1. **Provide good examples**: include comprehensive examples in your `MarkdownStructureDefinition` to guide the LLM.
1. **Handle incomplete data**: always check for null or empty values when parsing data from the stream.
1. **Clean up resources**: use the `onFinishStream` handler to clean up resources and process any remaining data.
1. **Handle errors**: implement proper error handling for malformed Markdown or unexpected data.
1. **Testing**: test your parser with various input scenarios, including partial chunks and malformed input.
1. **Parallel processing**: for independent data items, consider using parallel tool calls for better performance.

# Custom node implementation

This page provides detailed instructions on how to implement your own custom nodes in the Koog framework. Custom nodes let you extend the functionality of agent workflows by creating reusable components that perform specific operations.

To learn more about what graph nodes are, their usage, and existing default nodes, see [Graph nodes](../nodes-and-components/).

## Node architecture overview

Before diving into implementation details, it is important to understand the architecture of nodes in the Koog framework. Nodes are the fundamental building blocks of agent workflows, where each node represents a specific operation or transformation in the workflow. You connect nodes using edges, which define the flow of execution between nodes.

Each node has an `execute` method that takes an input and produces an output, which is then passed to the next node in the workflow.

## Implementing a custom node

Custom node implementations range from simple implementations that perform a basic logic on the input data and return an output, to more complex node implementations that accept parameters and maintain state between runs.

### Basic node implementation

The simplest way to implement a custom node in a graph and define your own custom logic would be to use the following pattern:

```kotlin
val myNode by node<Input, Output>("node_name") { input ->
    // Processing
    returnValue
}
```

The code above represents a custom node `myNode` with predefined `Input` and `Output` types, with the optional name string parameter (`node_name`). In an actual example, here is a simple node that takes a string input and returns the input's length:

```kotlin
val myNode by node<String, Int>("node_name") { input ->
    // Processing
    input.length
}
```

Another way to create a custom node is to define an extension function on `AIAgentSubgraphBuilderBase` that calls the `node` function:

```kotlin
fun AIAgentSubgraphBuilderBase<*, *>.myCustomNode(
    name: String? = null
): AIAgentNodeDelegate<Input, Output> = node(name) { input ->
    // Custom logic
    input // Return the input as output (pass-through)
}

val myCustomNode by myCustomNode("node_name")
```

This creates a pass-through node that performs some custom logic but returns the input as the output without modification.

### Nodes with additional arguments

You can create nodes that accept arguments to customize their behavior:

```kotlin
    fun AIAgentSubgraphBuilderBase<*, *>.myNodeWithArguments(
    name: String? = null,
    arg1: String,
    arg2: Int
): AIAgentNodeDelegate<Input, Output> = node(name) { input ->
    // Use arg1 and arg2 in your custom logic
    input // Return the input as the output
}

val myCustomNode by myNodeWithArguments("node_name", arg1 = "value1", arg2 = 42)
```

### Parameterized nodes

You can define nodes with input and output parameters:

```kotlin
inline fun <reified T> AIAgentSubgraphBuilderBase<*, *>.myParameterizedNode(
    name: String? = null,
): AIAgentNodeDelegate<T, T> = node(name) { input ->
    // Do some additional actions
    // Return the input as the output
    input
}

val strategy = strategy<String, String>("strategy_name") {
    val myCustomNode by myParameterizedNode<String>("node_name")
}
```

### Stateful nodes

If your node needs to maintain state between runs, you can use closure variables:

```kotlin
fun AIAgentSubgraphBuilderBase<*, *>.myStatefulNode(
    name: String? = null
): AIAgentNodeDelegate<Input, Output> {
    var counter = 0

    return node(name) { input ->
        counter++
        println("Node executed $counter times")
        input
    }
}
```

## Node input and output types

Nodes can have different input and output types, which are specified as generic parameters:

```kotlin
val stringToIntNode by node<String, Int>("node_name") { input: String ->
    // Processing
    input.toInt() // Convert string to integer
}
```

Note

The input and output types determine how the node can be connected to other nodes in the workflow. Nodes can only be connected if the output type of the source node is compatible with the input type of the target node.

## Best practices

When implementing custom nodes, follow these best practices:

1. **Keep nodes focused**: each node should perform a single, well-defined operation.
1. **Use descriptive names**: node names should clearly indicate their purpose.
1. **Document parameters**: provide clear documentation for all parameters.
1. **Handle errors gracefully**: implement proper error handling to prevent workflow failures.
1. **Make nodes reusable**: design nodes to be reusable across different workflows.
1. **Use type parameters**: use generic type parameters when appropriate to make nodes more flexible.
1. **Provide default values**: when possible, provide sensible default values for parameters.

## Common patterns

The following sections provide some common patterns for implementing custom nodes.

### Pass-through nodes

Nodes that perform an operation but return the input as the output.

```kotlin
val loggingNode by node<String, String>("node_name") { input ->
    println("Processing input: $input")
    input // Return the input as the output
}
```

### Transformation nodes

Nodes that transform the input into a different output.

```kotlin
val upperCaseNode by node<String, String>("node_name") { input ->
    println("Processing input: $input")
    input.uppercase() // Transform the input to uppercase
}
```

### LLM interaction nodes

Nodes that interact with the LLM.

```kotlin
val summarizeTextNode by node<String, String>("node_name") { input ->
    llm.writeSession {
        appendPrompt {
            user("Please summarize the following text: $input")
        }

        val response = requestLLMWithoutTools()
        response.content
    }
}
```

### Tool run node

```kotlin
val nodeExecuteCustomTool by node<String, String>("node_name") { input ->
    val toolCall = Message.Tool.Call(
        id = UUID.randomUUID().toString(),
        tool = toolName,
        metaInfo = ResponseMetaInfo.create(Clock.System),
        content = Json.encodeToString(ToolArgs(arg1 = input, arg2 = 42)) // Use the input as tool arguments
    )

    val result = environment.executeTool(toolCall)
    result.content
}
```

# LLM sessions and manual history management

This page provides detailed information about LLM sessions, including how to work with read and write sessions, manage conversation history, and make requests to language models.

## Introduction

LLM sessions are a fundamental concept that provides a structured way to interact with language models (LLMs). They manage the conversation history, handle requests to the LLM, and provide a consistent interface for running tools and processing responses.

## Understanding LLM sessions

An LLM session represents a context for interacting with a language model. It encapsulates:

- The conversation history (prompt)
- Available tools
- Methods for making requests to the LLM
- Methods for updating the conversation history
- Methods for running tools

Sessions are managed by the `AIAgentLLMContext` class, which provides methods for creating read and write sessions.

### Session types

The Koog framework provides two types of sessions:

1. **Write Sessions** (`AIAgentLLMWriteSession`): Allow modifying the prompt and tools, making LLM requests, and running tools. Changes made in a write session are persisted back to the LLM context.
1. **Read Sessions** (`AIAgentLLMReadSession`): Provide read-only access to the prompt and tools. They are useful for inspecting the current state without making changes.

The key difference is that write sessions can modify the conversation history, while read sessions cannot.

### Session lifecycle

Sessions have a defined lifecycle:

1. **Creation**: a session is created using `llm.writeSession { ... }` or `llm.readSession { ... }`.
1. **Active phase**: the session is active while the lambda block is executing.
1. **Termination**: the session is automatically closed when the lambda block completes.

Sessions implement the `AutoCloseable` interface, ensuring they are properly cleaned up even if an exception occurs.

## Working with LLM sessions

### Creating sessions

Sessions are created using extension functions on the `AIAgentLLMContext` class:

```kotlin
// Creating a write session
llm.writeSession {
    // Session code here
}

// Creating a read session
llm.readSession {
    // Session code here
}
```

These functions take a lambda block that runs within the context of the session. The session is automatically closed when the block completes.

### Session scope and thread safety

Sessions use a read-write lock to ensure thread safety:

- Multiple read sessions can be active simultaneously.
- Only one write session can be active at a time.
- A write session blocks all other sessions (both read and write).

This ensures that the conversation history is not corrupted by concurrent modifications.

### Accessing session properties

Within a session, you can access the prompt and tools:

```kotlin
llm.readSession {
    val messageCount = prompt.messages.size
    val availableTools = tools.map { it.name }
}
```

In a write session, you can also modify these properties:

```kotlin
llm.writeSession {
    // Modify the prompt
    appendPrompt {
        user("New user message")
    }

    // Modify the tools
    tools = newTools
}
```

For more information, see the detailed API reference for [AIAgentLLMReadSession](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.agent.session/-a-i-agent-l-l-m-read-session/index.html) and [AIAgentLLMWriteSession](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.agent.session/-a-i-agent-l-l-m-write-session/index.html).

## Making LLM requests

### Basic request methods

The most common methods for making LLM requests are:

1. `requestLLM()`: makes a request to the LLM with the current prompt and tools, returning a single response.
1. `requestLLMMultiple()`: makes a request to the LLM with the current prompt and tools, returning multiple responses.
1. `requestLLMWithoutTools()`: makes a request to the LLM with the current prompt but without any tools, returning a single response.
1. `requestLLMForceOneTool`: makes a request to the LLM with the current prompt and tools, forcing the use of one tool.
1. `requestLLMOnlyCallingTools`: makes a request to the LLM that should be processed by only using tools.

Example:

```kotlin
llm.writeSession {
    // Make a request with tools enabled
    val response = requestLLM()

    // Make a request without tools
    val responseWithoutTools = requestLLMWithoutTools()

    // Make a request that returns multiple responses
    val responses = requestLLMMultiple()
}
```

### How requests work

LLM requests are made when you explicitly call one of the request methods. The key points to understand are:

1. **Explicit invocation**: requests only happen when you call methods like `requestLLM()`, `requestLLMWithoutTools()` and so on.
1. **Immediate execution**: when you call a request method, the request is made immediately, and the method blocks until a response is received.
1. **Automatic history update**: in a write session, the response is automatically added to the conversation history.
1. **No implicit requests**: the system does not make implicit requests; you need to explicitly call a request method.

### Request methods with tools

When making requests with tools enabled, the LLM may respond with a tool call instead of a text response. The request methods handle this transparently:

```kotlin
llm.writeSession {
    val response = requestLLM()

    // The response might be a tool call or a text response
    if (response is Message.Tool.Call) {
        // Handle tool call
    } else {
        // Handle text response
    }
}
```

In practice, you typically do not need to check the response type manually, as the agent graph handles this routing automatically.

### Structured and streaming requests

For more advanced use cases, the platform provides methods for structured and streaming requests:

1. `requestLLMStructured()`: requests the LLM to provide a response in a specific structured format.
1. `requestLLMStructuredOneShot()`: similar to `requestLLMStructured()` but without retries or corrections.
1. `requestLLMStreaming()`: makes a streaming request to the LLM, returning a flow of response chunks.

Example:

```kotlin
llm.writeSession {
    // Make a structured request
    val structuredResponse = requestLLMStructured<JokeRating>()

    // Make a streaming request
    val responseStream = requestLLMStreaming()
    responseStream.collect { chunk ->
        // Process each chunk as it arrives
    }
}
```

## Managing conversation history

### Updating the prompt

In a write session, you can add messages to the prompt (conversation history) using the `appendPrompt` method:

```kotlin
llm.writeSession {
    appendPrompt {
        // Add a system message
        system("You are a helpful assistant.")

        // Add a user message
        user("Hello, can you help me with a coding question?")

        // Add an assistant message
        assistant("Of course! What's your question?")

        // Add a tool result
        tool {
            result(myToolResult)
        }
    }
}
```

You can also completely rewrite the prompt using the `rewritePrompt` method:

```kotlin
llm.writeSession {
    rewritePrompt { oldPrompt ->
        // Create a new prompt based on the old one
        oldPrompt.copy(messages = filteredMessages)
    }
}
```

### Automatic history update on response

When you make an LLM request in a write session, the response is automatically added to the conversation history:

```kotlin
llm.writeSession {
    // Add a user message
    appendPrompt {
        user("What's the capital of France?")
    }

    // Make a request â€“ the response is automatically added to the history
    val response = requestLLM()

    // The prompt now includes both the user message and the model's response
}
```

This automatic history update is the key feature of write sessions, ensuring that the conversation flows naturally.

### History compression

For long-running conversations, the history can grow large and consume a lot of tokens. The platform provides methods for compressing history:

```kotlin
llm.writeSession {
    // Compress the history using a TLDR approach
    replaceHistoryWithTLDR(HistoryCompressionStrategy.WholeHistory, preserveMemory = true)
}
```

You can also use the `nodeLLMCompressHistory` node in a strategy graph to compress history at specific points.

For more information about history compression and compression strategies, see [History compression](../history-compression/).

## Running tools in sessions

### Calling tools

Write sessions provide several methods for calling tools:

1. `callTool(tool, args)`: calls a tool by reference.
1. `callTool(toolName, args)`: calls a tool by name.
1. `callTool(toolClass, args)`: calls a tool by class.
1. `callToolRaw(toolName, args)`: calls a tool by name and returns the raw string result.

Example:

```kotlin
llm.writeSession {
    // Call a tool by reference
    val result = callTool(myTool, myArgs)

    // Call a tool by name
    val result2 = callTool("myToolName", myArgs)

    // Call a tool by class
    val result3 = callTool(MyTool::class, myArgs)

    // Call a tool and get the raw result
    val rawResult = callToolRaw("myToolName", myArgs)
}
```

### Parallel tool runs

To run multiple tools in parallel, write sessions provide extension functions on `Flow`:

```kotlin
llm.writeSession {
    // Run tools in parallel
    parseDataToArgs(data).toParallelToolCalls(MyTool::class).collect { result ->
        // Process each result
    }

    // Run tools in parallel and get raw results
    parseDataToArgs(data).toParallelToolCallsRaw(MyTool::class).collect { rawResult ->
        // Process each raw result
    }
}
```

This is useful for processing large amounts of data efficiently.

## Best practices

When working with LLM sessions, follow these best practices:

1. **Use the right session type**: Use write sessions when you need to modify the conversation history and read sessions when you only need to read it.
1. **Keep sessions short**: Sessions should be focused on a specific task and closed as soon as possible to release resources.
1. **Handle exceptions**: Make sure to handle exceptions within sessions to prevent resource leaks.
1. **Manage history size**: For long-running conversations, use history compression to reduce token usage.
1. **Prefer high-Level abstractions**: When possible, use the node-based API. For example, `nodeLLMRequest` instead of directly working with sessions.
1. **Be mindful of thread safety**: Remember that write sessions block other sessions, so keep write operations as short as possible.
1. **Use structured requests for complex data**: When you need the LLM to return structured data, use `requestLLMStructured` instead of parsing free-form text.
1. **Use streaming for long responses**: For long responses, use `requestLLMStreaming` to process the response as it arrives.

## Troubleshooting

### Session already closed

If you see an error such as `Cannot use session after it was closed`, you are trying to use a session after its lambda block has completed. Make sure all session operations are performed within the session block.

### History too large

If your history becomes too large and consumes too many tokens, use history compression techniques:

```kotlin
llm.writeSession {
    replaceHistoryWithTLDR(HistoryCompressionStrategy.FromLastNMessages(10), preserveMemory = true)
}
```

For more information, see [History compression](../history-compression/)

### Tool not found

If you see errors about tools not being found, check that:

- The tool is correctly registered in the tool registry.
- You are using the correct tool name or class.

## API documentation

For more information, see the full [AIAgentLLMSession](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.agent.session/-a-i-agent-l-l-m-session/index.html) and [AIAgentLLMContext](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.agent.context/-a-i-agent-l-l-m-context/index.html) reference.
# Subgraphs

# Overview

This page provides detailed information about subgraphs in the Koog framework. Understanding these concepts is crucial for creating complex agent workflows that maintain context across multiple processing steps.

## Introduction

Subgraphs are a fundamental concept in the Koog framework that lets you break down complex agent workflows into manageable, sequential steps. Each subgraph represents a phase of processing, with its context, responsibilities, and an optional subset of tools.

Subgraphs are integral parts of strategies, which are graphs that represent the overall agent workflow. For more information about strategies, see [Custom strategy graphs](../custom-strategy-graphs/).

## Understanding subgraphs

A subgraph is a self-contained unit of processing within an agent strategy. Each subgraph:

- Has a unique name
- Contains a graph of nodes or subgraphs connected by edges
- Can use any tool or a subset of tools from the tool registry
- Receives input from the previous subgraph (or the initial user input)
- Produces output that is passed to the next subgraph (or the output)

To define a sequence of subgraphs in a graph, use edge connections or define sequences using the `then` keyword. For more information, see [Custom strategy graphs](../custom-strategy-graphs/).

### Subgraph context

Each subgraph executes within a context that provides access to:

- The environment
- Agent input
- The agent configuration
- The LLM context (including the conversation history)
- The state manager
- The storage
- Session and strategy

The context is passed to each node within the subgraph and provides the necessary resources for the node to perform its operations.

## Creating and configuring subgraphs

The following sections provide code templates and common patterns in the creation of subgraphs for agentic workflows.

### Basic subgraph creation

Custom subgraphs are typically created using the following patterns:

- Subgraph with a specified tool selection strategy:

```kotlin
strategy<StrategyInput, StrategyOutput>("strategy-name") {
    val subgraphIdentifier by subgraph<Input, Output>(
        name = "subgraph-name",
        toolSelectionStrategy = ToolSelectionStrategy.ALL
    ) {
        // Define nodes and edges for this subgraph
    }
}
```

- Subgraph with a specified list of tools (subset of tools from a defined tool registry):

```kotlin
strategy<StrategyInput, StrategyOutput>("strategy-name") {
   val subgraphIdentifier by subgraph<Input, Output>(
       name = "subgraph-name", 
       tools = listOf(firstTool, secondTool)
   ) {
        // Define nodes and edges for this subgraph
    }
}
```

For more information about parameters and parameter values, see the `subgraph` [API reference](https://api.koog.ai/agents/agents-core/ai.koog.agents.core.dsl.builder/-a-i-agent-subgraph-builder-base/subgraph.html). For more information about tools, see [Tools](../tools-overview/).

The following code sample shows an actual implementation of a custom subgraph:

```kotlin
strategy<String, String>("my-strategy") {
   val mySubgraph by subgraph<String, String>(
      tools = listOf(firstTool, secondTool)
   ) {
        // Define nodes and edges for this subgraph
        val sendInput by nodeLLMRequest()
        val executeToolCall by nodeExecuteTool()
        val sendToolResult by nodeLLMSendToolResult()

        edge(nodeStart forwardTo sendInput)
        edge(sendInput forwardTo executeToolCall onToolCall { true })
        edge(executeToolCall forwardTo sendToolResult)
        edge(sendToolResult forwardTo nodeFinish onAssistantMessage { true })
    }
}
```

### Configuring tools in a subgraph

Tools can be configured for a subgraph in several ways:

- Directly in the subgraph definition:

```kotlin
val mySubgraph by subgraph<String, String>(
   tools = listOf(AskUser)
 ) {
    // Subgraph definition
 }
```

- From a tool registry:

```kotlin
val mySubgraph by subgraph<String, String>(
    tools = listOf(toolRegistry.getTool("AskUser"))
) {
    // Subgraph definition
}
```

- Dynamically during execution:

```kotlin
// Make a set of tools
this.llm.writeSession {
    tools = tools.filter { it.name in listOf("first_tool_name", "second_tool_name") }
}
```

## Advanced subgraph techniques

### Multi-part strategies

Complex workflows can be broken down into multiple subgraphs, each handling a specific part of the process:

```kotlin
strategy("complex-workflow") {
   val inputProcessing by subgraph<String, A>(
   ) {
      // Process the initial input
   }

   val reasoning by subgraph<A, B>(
   ) {
      // Perform reasoning based on the processed input
   }

   val toolRun by subgraph<B, C>(
      // Optional subset of tools from the tool registry
      tools = listOf(firstTool, secondTool)
   ) {
      // Run tools based on the reasoning
   }

   val responseGeneration by subgraph<C, String>(
   ) {
      // Generate a response based on the tool results
   }

   nodeStart then inputProcessing then reasoning then toolRun then responseGeneration then nodeFinish

}
```

## Best practices

When working with subgraphs, follow these best practices:

1. **Break complex workflows into subgraphs**: each subgraph should have a clear, focused responsibility.
1. **Pass only necessary context**: only pass the information that subsequent subgraphs need to function correctly.
1. **Document subgraph dependencies**: clearly document what each subgraph expects from previous subgraphs and what it provides to subsequent subgraphs.
1. **Test subgraphs in isolation**: ensure that each subgraph works correctly with various inputs before integrating it into a strategy.
1. **Consider token usage**: be mindful of token usage, especially when passing large histories between subgraphs.

## Troubleshooting

### Tools not available

If tools are not available in a subgraph:

- Check that the tools are correctly registered in the tool registry.

### Subgraphs not running in the defined and expected order

If subgraphs are not executing in the defined order:

- Check the strategy definition to ensure that subgraphs are listed in the correct order.
- Verify that each subgraph is correctly passing its output to the next subgraph.
- Ensure that your subgraph is connected with the rest of the subgraph and is reachable from the start (and finish). Be careful with conditional edges, so they cover all possible conditions to continue in order not to get blocked in a subgraph or node.

## Examples

The following example shows how subgraphs are used to create an agent strategy in a real-world scenario. The code sample includes three defined subgraphs, `researchSubgraph`, `planSubgraph`, and `executeSubgraph`, where each of the subgraphs has a defined and distinct purpose within the assistant flow.

```kotlin
// Define the agent strategy
val strategy = strategy<String, String>("assistant") {
    // A subgraph that includes a tool call

    val researchSubgraph by subgraph<String, String>(
        "research_subgraph",
        tools = listOf(WebSearchTool())
    ) {
        val nodeCallLLM by nodeLLMRequest("call_llm")
        val nodeExecuteTool by nodeExecuteTool()
        val nodeSendToolResult by nodeLLMSendToolResult()

        edge(nodeStart forwardTo nodeCallLLM)
        edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })
        edge(nodeExecuteTool forwardTo nodeSendToolResult)
        edge(nodeSendToolResult forwardTo nodeExecuteTool onToolCall { true })
        edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })
    }

    val planSubgraph by subgraph(
        "plan_subgraph",
        tools = listOf()
    ) {
        val nodeUpdatePrompt by node<String, Unit> { research ->
            llm.writeSession {
                rewritePrompt {
                    prompt("research_prompt") {
                        system(
                            "You are given a problem and some research on how it can be solved." +
                                    "Make step by step a plan on how to solve given task."
                        )
                        user("Research: $research")
                    }
                }
            }
        }
        val nodeCallLLM by nodeLLMRequest("call_llm")

        edge(nodeStart forwardTo nodeUpdatePrompt)
        edge(nodeUpdatePrompt forwardTo nodeCallLLM transformed { "Task: $agentInput" })
        edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })
    }

    val executeSubgraph by subgraph<String, String>(
        "execute_subgraph",
        tools = listOf(DoAction(), DoAnotherAction()),
    ) {
        val nodeUpdatePrompt by node<String, Unit> { plan ->
            llm.writeSession {
                rewritePrompt {
                    prompt("execute_prompt") {
                        system(
                            "You are given a task and detailed plan how to execute it." +
                                    "Perform execution by calling relevant tools."
                        )
                        user("Execute: $plan")
                        user("Plan: $plan")
                    }
                }
            }
        }
        val nodeCallLLM by nodeLLMRequest("call_llm")
        val nodeExecuteTool by nodeExecuteTool()
        val nodeSendToolResult by nodeLLMSendToolResult()

        edge(nodeStart forwardTo nodeUpdatePrompt)
        edge(nodeUpdatePrompt forwardTo nodeCallLLM transformed { "Task: $agentInput" })
        edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })
        edge(nodeExecuteTool forwardTo nodeSendToolResult)
        edge(nodeSendToolResult forwardTo nodeExecuteTool onToolCall { true })
        edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })
    }

    nodeStart then researchSubgraph then planSubgraph then executeSubgraph then nodeFinish
}
```
# Embeddings

# Embeddings

The `embeddings` module provides functionality for generating and comparing embeddings of text and code. Embeddings are vector representations that capture semantic meaning, allowing for efficient similarity comparisons.

## Overview

This module consists of two main components:

1. **embeddings-base**: core interfaces and data structures for embeddings.
1. **embeddings-llm**: implementation using Ollama for local embedding generation.

## Getting started

The following sections include basic examples of how to use embeddings in the following ways:

- With a local embedding models through Ollama
- Using an OpenAI embedding model

### Local embeddings

To use the embedding functionality with a local model, you need to have Ollama installed and running on your system. For installation and running instructions, refer to the [official Ollama GitHub repository](https://github.com/ollama/ollama).

```kotlin
fun main() {
    runBlocking {
        // Create an OllamaClient instance
        val client = OllamaClient()
        // Create an embedder
        val embedder = LLMEmbedder(client, OllamaEmbeddingModels.NOMIC_EMBED_TEXT)
        // Create embeddings
        val embedding = embedder.embed("This is the text to embed")
        // Print embeddings to the output
        println(embedding)
    }
}
```

To use an Ollama embedding model, make sure to have the following prerequisites:

- Have [Ollama](https://ollama.com/download) installed and running

- Download an embedding model to your local machine using the following command:

  ```bash
  ollama pull <ollama-model-id>
  ```

  Replace `<ollama-model-id>` with the Ollama identifier of the specific model. For more information about available embedding models and their identifiers, see [Ollama models overview](#ollama-models-overview).

### Ollama models overview

The following table provides an overview of the available Ollama embedding models.

| Model ID          | Ollama ID                     | Parameters | Dimensions | Context Length | Performance                                                           | Tradeoffs                                                          |
| ----------------- | ----------------------------- | ---------- | ---------- | -------------- | --------------------------------------------------------------------- | ------------------------------------------------------------------ |
| NOMIC_EMBED_TEXT  | nomic-embed-text              | 137M       | 768        | 8192           | High-quality embeddings for semantic search and text similarity tasks | Balanced between quality and efficiency                            |
| ALL_MINILM        | all-minilm                    | 33M        | 384        | 512            | Fast inference with good quality for general text embeddings          | Smaller model size with reduced context length, but very efficient |
| MULTILINGUAL_E5   | zylonai/multilingual-e5-large | 300M       | 768        | 512            | Strong performance across 100+ languages                              | Larger model size but provides excellent multilingual capabilities |
| BGE_LARGE         | bge-large                     | 335M       | 1024       | 512            | Excellent for English text retrieval and semantic search              | Larger model size but provides high-quality embeddings             |
| MXBAI_EMBED_LARGE | mxbai-embed-large             | -          | -          | -              | High-dimensional embeddings of textual data                           | Designed for creating high-dimensional embeddings                  |

For more information about these models, see Ollama's [Embedding Models](https://ollama.com/blog/embedding-models) blog post.

### Choosing a model

Here are some general tips on which Ollama embedding model to select depending on your requirements:

- For general text embeddings, use `NOMIC_EMBED_TEXT`.
- For multilingual support, use `MULTILINGUAL_E5`.
- For maximum quality (at the cost of performance), use `BGE_LARGE`.
- For maximum efficiency (at the cost of some quality), use `ALL_MINILM`.
- For high-dimensional embeddings, use `MXBAI_EMBED_LARGE`.

## OpenAI embeddings

To create embeddings using an OpenAI embedding model, use the `embed` method of an `OpenAILLMClient` instance as shown in the example below.

```kotlin
suspend fun openAIEmbed(text: String) {
    // Get the OpenAI API token from the OPENAI_KEY environment variable
    val token = System.getenv("OPENAI_KEY") ?: error("Environment variable OPENAI_KEY is not set")
    // Create an OpenAILLMClient instance
    val client = OpenAILLMClient(token)
    // Create an embedder
    val embedder = LLMEmbedder(client, OpenAIModels.Embeddings.TextEmbeddingAda002)
    // Create embeddings
    val embedding = embedder.embed(text)
    // Print embeddings to the output
    println(embedding)
}
```

## AWS Bedrock embeddings

To create embeddings using an AWS Bedrock embedding model, use the `embed` method of an `BedrockLLMClient` instance and your chosen model. Example:

```kotlin
suspend fun bedrockEmbed(text: String) {
    // Get AWS credentials from environment/configuration
    val awsAccessKeyId = System.getenv("AWS_ACCESS_KEY_ID") ?: error("AWS_ACCESS_KEY_ID not set")
    val awsSecretAccessKey = System.getenv("AWS_SECRET_ACCESS_KEY") ?: error("AWS_SECRET_ACCESS_KEY not set")
    // (Optional) AWS_SESSION_TOKEN for temporary credentials
    val awsSessionToken = System.getenv("AWS_SESSION_TOKEN")
    // Create a BedrockLLMClient instance
    val client = BedrockLLMClient(
        identityProvider = StaticCredentialsProvider {
            this.accessKeyId = awsAccessKeyId
            this.secretAccessKey = awsSecretAccessKey
            awsSessionToken?.let { this.sessionToken = it }
        },
        settings = BedrockClientSettings()
    )
    // Create an embedder
    val embedder = LLMEmbedder(client, BedrockModels.Embeddings.AmazonTitanEmbedText)
    // Create embeddings
    val embedding = embedder.embed(text)
    // Print embeddings to the output
    println(embedding)
}
```

### Supported AWS Bedrock embedding models

| Provider | Model name                   | Model ID                       | Input | Output    | Dimensions | Context Length | Notes                                                                                                  |
| -------- | ---------------------------- | ------------------------------ | ----- | --------- | ---------- | -------------- | ------------------------------------------------------------------------------------------------------ |
| Amazon   | Titan Embeddings G1 - Text   | `amazon.titan-embed-text-v1`   | Text  | Embedding | 1,536      | 8192           | 25+ languages, optimized for retrieval, semantic similarity, clustering; segment long docs for search. |
| Amazon   | Titan Text Embeddings V2     | `amazon.titan-embed-text-v2:0` | Text  | Embedding | 1,024      | 8192           | High-accuracy, flexible dimensions, multilingual (100+); smaller dims save storage, normalized output. |
| Cohere   | Cohere Embed English v3      | `cohere.embed-english-v3`      | Text  | Embedding | 1,024      | 8192           | SOTA English text embeddings for search, retrieval, and understanding text nuances.                    |
| Cohere   | Cohere Embed Multilingual v3 | `cohere.embed-multilingual-v3` | Text  | Embedding | 1,024      | 8192           | Multilingual embeddings, SOTA for search and semantic understanding across languages.                  |

> For the most up-to-date model support, refer to the [AWS Bedrock supported models documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html).

## Examples

The following examples show how you can use embeddings to compare code with text or other code snippets.

### Code-to-text comparison

Compare code snippets with natural language descriptions to find semantic matches:

```kotlin
suspend fun compareCodeToText(embedder: Embedder) { // Embedder type
    // Code snippet
    val code = """
        fun factorial(n: Int): Int {
            return if (n <= 1) 1 else n * factorial(n - 1)
        }
    """.trimIndent()

    // Text descriptions
    val description1 = "A recursive function that calculates the factorial of a number"
    val description2 = "A function that sorts an array of integers"

    // Generate embeddings
    val codeEmbedding = embedder.embed(code)
    val desc1Embedding = embedder.embed(description1)
    val desc2Embedding = embedder.embed(description2)

    // Calculate differences (lower value means more similar)
    val diff1 = embedder.diff(codeEmbedding, desc1Embedding)
    val diff2 = embedder.diff(codeEmbedding, desc2Embedding)

    println("Difference between code and description 1: $diff1")
    println("Difference between code and description 2: $diff2")

    // The code should be more similar to description1 than description2
    if (diff1 < diff2) {
        println("The code is more similar to: '$description1'")
    } else {
        println("The code is more similar to: '$description2'")
    }
}
```

### Code-to-code comparison

Compare code snippets to find semantic similarities regardless of syntax differences:

```kotlin
suspend fun compareCodeToCode(embedder: Embedder) { // Embedder type
    // Two implementations of the same algorithm in different languages
    val kotlinCode = """
        fun fibonacci(n: Int): Int {
            return if (n <= 1) n else fibonacci(n - 1) + fibonacci(n - 2)
        }
    """.trimIndent()

    val pythonCode = """
        def fibonacci(n):
            if n <= 1:
                return n
            else:
                return fibonacci(n-1) + fibonacci(n-2)
    """.trimIndent()

    val javaCode = """
        public static int bubbleSort(int[] arr) {
            int n = arr.length;
            for (int i = 0; i < n-1; i++) {
                for (int j = 0; j < n-i-1; j++) {
                    if (arr[j] > arr[j+1]) {
                        int temp = arr[j];
                        arr[j] = arr[j+1];
                        arr[j+1] = temp;
                    }
                }
            }
            return arr;
        }
    """.trimIndent()

    // Generate embeddings
    val kotlinEmbedding = embedder.embed(kotlinCode)
    val pythonEmbedding = embedder.embed(pythonCode)
    val javaEmbedding = embedder.embed(javaCode)

    // Calculate differences
    val diffKotlinPython = embedder.diff(kotlinEmbedding, pythonEmbedding)
    val diffKotlinJava = embedder.diff(kotlinEmbedding, javaEmbedding)

    println("Difference between Kotlin and Python implementations: $diffKotlinPython")
    println("Difference between Kotlin and Java implementations: $diffKotlinJava")

    // The Kotlin and Python implementations should be more similar
    if (diffKotlinPython < diffKotlinJava) {
        println("The Kotlin code is more similar to the Python code")
    } else {
        println("The Kotlin code is more similar to the Java code")
    }
}
```

## API documentation

For a complete API reference related to embeddings, see the reference documentation for the following modules:

- [embeddings-base](https://api.koog.ai/embeddings/embeddings-base/ai.koog.embeddings.base/index.html): Provides core interfaces and data structures for representing and comparing text and code embeddings.
- [embeddings-llm](https://api.koog.ai/embeddings/embeddings-llm/index.html): Includes implementations for working with local embedding models.
# Ranked document storage

# Document storage

To let you provide up-to-date and searchable information sources for use with Large Language Models (LLMs), Koog supports Resource-Augmented Generation (RAG) to store and retrieve information from documents.

## Key RAG features

The core components of a common RAG system include:

- **Document storage**: a repository of documents, files, or text chunks that contain information.
- **Vector embeddings**: numerical representations of a text that capture semantic meaning. For more information on embeddings in Koog, see [Embeddings](../embeddings/).
- **Retrieval mechanism**: a system that finds the most relevant documents based on a query.
- **Generation component**: an LLM that uses the retrieved information to generate responses.

RAG addresses several limitations of traditional LLMs:

- **Knowledge cutoff**: RAG can access the most recent information, not limited to training data.
- **Hallucinations**: by grounding responses in retrieved documents, RAG reduces fabricated information.
- **Domain specificity**: RAG can be tailored to specific domains by curating the knowledge base.
- **Transparency**: the sources of information can be cited, making the system more explainable.

## Finding information in a RAG system

Finding relevant information in a RAG system involves storing documents as vector embeddings and ranking them based on their similarity to a user's query. This approach works with various document types, including PDFs, images, text files, or even individual text chunks.

The process involves:

1. **Document embedding**: converting documents into vector representations that capture their semantic meaning.
1. **Vector storage**: storing these embeddings efficiently for quick retrieval.
1. **Similarity search**: finding documents whose embeddings are most similar to the query embedding.
1. **Ranking**: ordering documents by their relevance score.

## Implementing a RAG system in Koog

To implement a RAG system in Koog, follow the steps below:

1. Create an embedder using Ollama or OpenAI. The embedder is an instance of the `LLMEmbedder` class that takes an LLM client instance and model as parameters. For more information, see [Embeddings](../embeddings/).
1. Create a document embedder based on the created general embedder.
1. Create a document storage.
1. Add documents to the storage.
1. Find the most relevant documents using a defined query.

This sequence of steps represents a *relevance search* flow that returns the most relevant documents for a given user query. Here is a code sample showing how to implement the entire sequence of steps described above:

```kotlin
// Create an embedder using Ollama
val embedder = LLMEmbedder(OllamaClient(), OllamaEmbeddingModels.NOMIC_EMBED_TEXT)
// You may also use OpenAI embeddings with:
// val embedder = LLMEmbedder(OpenAILLMClient("API_KEY"), OpenAIModels.Embeddings.TextEmbeddingAda3Large)

// Create a JVM-specific document embedder
val documentEmbedder = JVMTextDocumentEmbedder(embedder)

// Create a ranked document storage using in-memory vector storage
val rankedDocumentStorage = EmbeddingBasedDocumentStorage(documentEmbedder, InMemoryVectorStorage())

// Store documents in the storage
rankedDocumentStorage.store(Path.of("./my/documents/doc1.txt"))
rankedDocumentStorage.store(Path.of("./my/documents/doc2.txt"))
rankedDocumentStorage.store(Path.of("./my/documents/doc3.txt"))
// ... store more documents as needed
rankedDocumentStorage.store(Path.of("./my/documents/doc100.txt"))

// Find the most relevant documents for a user query
val query = "I want to open a bank account but I'm getting a 404 when I open your website. I used to be your client with a different account 5 years ago before you changed your firm name"
val relevantFiles = rankedDocumentStorage.mostRelevantDocuments(query, count = 3)

// Process the relevant files
relevantFiles.forEach { file ->
    println("Relevant file: ${file.toAbsolutePath()}")
    // Process the file content as needed
}
```

### Providing relevance search for use by AI agents

Once you have a ranked document storage system, you can use it to provide relevant context to an AI agent for answering user queries. This enhances the agent's ability to provide accurate and contextually appropriate responses.

Here is an example of how to implement the defined RAG system for an AI agent to be able to answer queries by getting information from the document storage:

```kotlin
suspend fun solveUserRequest(query: String) {
    // Retrieve top-5 documents from the document provider
    val relevantDocuments = rankedDocumentStorage.mostRelevantDocuments(query, count = 5)

    // Create an AI Agent with the relevant context
    val agentConfig = AIAgentConfig(
        prompt = prompt("context") {
            system("You are a helpful assistant. Use the provided context to answer the user's question accurately.")
            user {
                +"Relevant context:"
                relevantDocuments.forEach {
                    file(it.pathString, "text/plain")
                }
            }
        },
        model = OpenAIModels.Chat.GPT4o, // Or a different model of your choice
        maxAgentIterations = 100,
    )

    val agent = AIAgent(
        promptExecutor = simpleOpenAIExecutor(apiKey),
        llmModel = OpenAIModels.Chat.GPT4o
    )


    // Run the agent to get a response
    val response = agent.run(query)

    // Return or process the response
    println("Agent response: $response")
}
```

### Providing relevance search as a tool

Instead of directly providing document content as context, you can also implement a tool that allows the agent to perform relevance searches on demand. This gives the agent more flexibility in deciding when and how to use the document storage.

Here is an example of how to implement a relevance search tool:

```kotlin
@Tool
@LLMDescription("Search for relevant documents about any topic (if exists). Returns the content of the most relevant documents.")
suspend fun searchDocuments(
    @LLMDescription("Query to search relevant documents about")
    query: String,
    @LLMDescription("Maximum number of documents")
    count: Int
): String {
    val relevantDocuments =
        rankedDocumentStorage.mostRelevantDocuments(query, count = count, similarityThreshold = 0.9).toList()

    if (!relevantDocuments.isEmpty()) {
        return "No relevant documents found for the query: $query"
    }

    val result = StringBuilder("Found ${relevantDocuments.size} relevant documents:\n\n")

    relevantDocuments.forEachIndexed { index, document ->
        val content = Files.readString(document)
        result.append("Document ${index + 1}: ${document.fileName}\n")
        result.append("Content: $content\n\n")
    }

    return result.toString()
}

fun main() {
    runBlocking {
        val tools = ToolRegistry {
            tool(::searchDocuments.asTool())
        }

        val agent = AIAgent(
            toolRegistry = tools,
            promptExecutor = simpleOpenAIExecutor(apiKey),
            llmModel = OpenAIModels.Chat.GPT4o
        )

        val response = agent.run("How to make a cake?")
        println("Agent response: $response")

    }
}
```

With this approach, the agent can decide when to use the search tool based on your query. This is particularly useful for complex queries that may require information from multiple documents or when the agent needs to search for specific details.

## Existing implementations of vector storage and document embedding providers

For convenience and easier implementation of a RAG system, Koog provides several out-of-the-box implementations for vector storage, document embedding, and combined embedding and storage components.

### Vector storage

#### InMemoryVectorStorage

A simple in-memory implementation that stores documents and their vector embeddings in memory. Suitable for testing or small-scale applications.

```kotlin
val inMemoryStorage = InMemoryVectorStorage<Path>()
```

For more information, see the [InMemoryVectorStorage](https://api.koog.ai/rag/vector-storage/ai.koog.rag.vector/-in-memory-vector-storage/index.html) reference.

#### FileVectorStorage

A file-based implementation that stores documents and their vector embeddings on disk. Suitable for persistent storage across application restarts.

```kotlin
val fileStorage = FileVectorStorage<Document, Path>(
   documentReader = documentProvider,
   fs = fileSystemProvider,
   root = rootPath
)
```

For more information, see the [FileVectorStorage](https://api.koog.ai/rag/vector-storage/ai.koog.rag.vector/-file-vector-storage/index.html) reference.

#### JVMFileVectorStorage

A JVM-specific implementation of `FileVectorStorage` that works with `java.nio.file.Path`.

```kotlin
val jvmFileStorage = JVMFileVectorStorage(root = Path.of("/path/to/storage"))
```

For more information, see the [JVMFileVectorStorage](https://api.koog.ai/rag/vector-storage/ai.koog.rag.vector/-j-v-m-file-vector-storage/index.html) reference.

### Document embedder

#### TextDocumentEmbedder

A generic implementation that works with any document type that can be converted to text.

```kotlin
val textEmbedder = TextDocumentEmbedder<Document, Path>(
   documentReader = documentProvider,
   embedder = embedder
)
```

For more information, see the [TextDocumentEmbedder](https://api.koog.ai/rag/vector-storage/ai.koog.rag.vector/-text-document-embedder/index.html) reference.

#### JVMTextDocumentEmbedder

A JVM-specific implementation that works with `java.nio.file.Path`.

```kotlin
val embedder = LLMEmbedder(OllamaClient(), OllamaEmbeddingModels.NOMIC_EMBED_TEXT)
val jvmTextEmbedder = JVMTextDocumentEmbedder(embedder = embedder)
```

For more information, see the [JVMTextDocumentEmbedder](https://api.koog.ai/rag/vector-storage/ai.koog.rag.vector/-j-v-m-text-document-embedder/index.html) reference.

### Combined storage implementations

#### EmbeddingBasedDocumentStorage

Combines a document embedder and a vector storage to provide a complete solution for storing and ranking documents.

```kotlin
val embeddingStorage = EmbeddingBasedDocumentStorage(
    embedder = documentEmbedder,
    storage = vectorStorage
)
```

For more information, see the [EmbeddingBasedDocumentStorage](https://api.koog.ai/rag/vector-storage/ai.koog.rag.vector/-embedding-based-document-storage/index.html) reference.

#### InMemoryDocumentEmbeddingStorage

An in-memory implementation of `EmbeddingBasedDocumentStorage`.

```kotlin
val inMemoryEmbeddingStorage = InMemoryDocumentEmbeddingStorage<Document>(
    embedder = documentEmbedder
)
```

For more information, see the [InMemoryDocumentEmbeddingStorage](https://api.koog.ai/rag/vector-storage/ai.koog.rag.vector/-in-memory-document-embedding-storage/index.html) reference.

#### FileDocumentEmbeddingStorage

A file-based implementation of `EmbeddingBasedDocumentStorage`.

```kotlin
val fileEmbeddingStorage = FileDocumentEmbeddingStorage<Document, Path>(
   embedder = documentEmbedder,
   documentProvider = documentProvider,
   fs = fileSystemProvider,
   root = rootPath
)
```

For more information, see the [FileDocumentEmbeddingStorage](https://api.koog.ai/rag/vector-storage/ai.koog.rag.vector/-file-document-embedding-storage/index.html) reference.

#### JVMFileDocumentEmbeddingStorage

A JVM-specific implementation of `FileDocumentEmbeddingStorage`.

```kotlin
val jvmFileEmbeddingStorage = JVMFileDocumentEmbeddingStorage(
   embedder = documentEmbedder,
   root = Path.of("/path/to/storage")
)
```

For more information, see the [JVMFileDocumentEmbeddingStorage](https://api.koog.ai/rag/vector-storage/ai.koog.rag.vector/-j-v-m-file-document-embedding-storage/index.html) reference.

#### JVMTextFileDocumentEmbeddingStorage

A JVM-specific implementation that combines `JVMTextDocumentEmbedder` and `JVMFileVectorStorage`.

```kotlin
val jvmTextFileEmbeddingStorage = JVMTextFileDocumentEmbeddingStorage(
   embedder = embedder,
   root = Path.of("/path/to/storage")
)
```

For more information, see the [JVMTextFileDocumentEmbeddingStorage](https://api.koog.ai/rag/vector-storage/ai.koog.rag.vector/-j-v-m-text-file-document-embedding-storage/index.html) reference.

These implementations provide a flexible and extensible framework for working with document embeddings and vector storage in various environments.

## Implementing your own vector storage and document embedder

You can extend Koog's vector storage framework by implementing your own custom document embedders and vector storage solutions. This is particularly useful when working with specialized document types or storage requirements.

Here's an example of implementing a custom document embedder for PDF documents:

```kotlin
// Define a PDFDocument class
class PDFDocument(private val path: Path) {
    fun readText(): String {
        // Use a PDF library to extract text from the PDF
        return "Text extracted from PDF at $path"
    }
}

// Implement a DocumentProvider for PDFDocument
class PDFDocumentProvider : DocumentProvider<Path, PDFDocument> {
    override suspend fun document(path: Path): PDFDocument? {
        return if (path.toString().endsWith(".pdf")) {
            PDFDocument(path)
        } else {
            null
        }
    }

    override suspend fun text(document: PDFDocument): CharSequence {
        return document.readText()
    }
}

// Implement a DocumentEmbedder for PDFDocument
class PDFDocumentEmbedder(private val embedder: Embedder) : DocumentEmbedder<PDFDocument> {
    override suspend fun embed(document: PDFDocument): Vector {
        val text = document.readText()
        return embed(text)
    }

    override suspend fun embed(text: String): Vector {
        return embedder.embed(text)
    }

    override fun diff(embedding1: Vector, embedding2: Vector): Double {
        return embedder.diff(embedding1, embedding2)
    }
}

// Create a custom vector storage for PDF documents
class PDFVectorStorage(
    private val pdfProvider: PDFDocumentProvider,
    private val embedder: PDFDocumentEmbedder,
    private val storage: VectorStorage<PDFDocument>
) : RankedDocumentStorage<PDFDocument> {
    override fun rankDocuments(query: String): Flow<RankedDocument<PDFDocument>> = flow {
        val queryVector = embedder.embed(query)
        storage.allDocumentsWithPayload().collect { (document, documentVector) ->
            emit(
                RankedDocument(
                    document = document,
                    similarity = 1.0 - embedder.diff(queryVector, documentVector)
                )
            )
        }
    }

    override suspend fun store(document: PDFDocument, data: Unit): String {
        val vector = embedder.embed(document)
        return storage.store(document, vector)
    }

    override suspend fun delete(documentId: String): Boolean {
        return storage.delete(documentId)
    }

    override suspend fun read(documentId: String): PDFDocument? {
        return storage.read(documentId)
    }

    override fun allDocuments(): Flow<PDFDocument> = flow {
        storage.allDocumentsWithPayload().collect {
            emit(it.document)
        }
    }
}

// Usage example
suspend fun main() {
    val pdfProvider = PDFDocumentProvider()
    val embedder = LLMEmbedder(OllamaClient(), OllamaEmbeddingModels.NOMIC_EMBED_TEXT)
    val pdfEmbedder = PDFDocumentEmbedder(embedder)
    val storage = InMemoryVectorStorage<PDFDocument>()
    val pdfStorage = PDFVectorStorage(pdfProvider, pdfEmbedder, storage)

    // Store PDF documents
    val pdfDocument = PDFDocument(Path.of("./documents/sample.pdf"))
    pdfStorage.store(pdfDocument)

    // Query for relevant PDF documents
    val relevantPDFs = pdfStorage.mostRelevantDocuments("information about climate change", count = 3)

}
```

## Implementing custom non-embedding-based RankedDocumentStorage

While embedding-based document ranking is powerful, there are scenarios where you might want to implement a custom ranking mechanism that does not rely on embeddings. For example, you might want to rank documents based on:

- PageRank-like algorithms
- Keyword frequency
- Recency of documents
- User interaction history
- Domain-specific heuristics

Here's an example of implementing a custom `RankedDocumentStorage` that uses a simple keyword-based ranking approach:

```kotlin
class KeywordBasedDocumentStorage<Document>(
    private val documentProvider: DocumentProvider<Path, Document>,
    private val storage: DocumentStorage<Document>
) : RankedDocumentStorage<Document> {

    override fun rankDocuments(query: String): Flow<RankedDocument<Document>> = flow {
        // Split the query into keywords
        val keywords = query.lowercase().split(Regex("\\W+")).filter { it.length > 2 }

        // Process each document
        storage.allDocuments().collect { document ->
            // Get the document text
            val documentText = documentProvider.text(document).toString().lowercase()

            // Calculate a simple similarity score based on keyword frequency
            var similarity = 0.0
            for (keyword in keywords) {
                val count = countOccurrences(documentText, keyword)
                if (count > 0) {
                    similarity += count.toDouble() / documentText.length * 1000
                }
            }

            // Emit the document with its similarity score
            emit(RankedDocument(document, similarity))
        }
    }

    private fun countOccurrences(text: String, keyword: String): Int {
        var count = 0
        var index = 0
        while (index != -1) {
            index = text.indexOf(keyword, index)
            if (index != -1) {
                count++
                index += keyword.length
            }
        }
        return count
    }

    override suspend fun store(document: Document, data: Unit): String {
        return storage.store(document)
    }

    override suspend fun delete(documentId: String): Boolean {
        return storage.delete(documentId)
    }

    override suspend fun read(documentId: String): Document? {
        return storage.read(documentId)
    }

    override fun allDocuments(): Flow<Document> {
        return storage.allDocuments()
    }
}
```

This implementation ranks documents based on the frequency of keywords from the query appearing in the document text. You could extend this approach with more sophisticated algorithms like TF-IDF (Term Frequency-Inverse Document Frequency) or BM25.

Another example is a time-based ranking system that prioritizes recent documents:

```kotlin
class TimeBasedDocumentStorage<Document>(
    private val storage: DocumentStorage<Document>,
    private val getDocumentTimestamp: (Document) -> Long
) : RankedDocumentStorage<Document> {

    override fun rankDocuments(query: String): Flow<RankedDocument<Document>> = flow {
        val currentTime = System.currentTimeMillis()

        storage.allDocuments().collect { document ->
            val timestamp = getDocumentTimestamp(document)
            val ageInHours = (currentTime - timestamp) / (1000.0 * 60 * 60)

            // Calculate a decay factor based on age (newer documents get higher scores)
            val decayFactor = Math.exp(-0.01 * ageInHours)

            emit(RankedDocument(document, decayFactor))
        }
    }

    // Implement other required methods from RankedDocumentStorage
    override suspend fun store(document: Document, data: Unit): String {
        return storage.store(document)
    }

    override suspend fun delete(documentId: String): Boolean {
        return storage.delete(documentId)
    }

    override suspend fun read(documentId: String): Document? {
        return storage.read(documentId)
    }

    override fun allDocuments(): Flow<Document> {
        return storage.allDocuments()
    }
}
```

By implementing the `RankedDocumentStorage` interface, you can create custom ranking mechanisms tailored to your specific use case while still leveraging the rest of the RAG infrastructure.

The flexibility of Koog's design allows you to mix and match different storage and ranking strategies to build a system that meets your specific requirements.
# Features

# Overview

Agent features provide a way to extend and enhance the functionality of AI agents. Features can:

- Add new capabilities to agents
- Intercept and modify agent behavior
- Log and monitor agent execution

The Koog framework implements the following features:

- [Event Handler](../agent-event-handlers/)
- [Tracing](../tracing/)
- [Agent Memory](../agent-memory/)
- [OpenTelemetry](../opentelemetry-support/)
- [Agent Persistence (Snapshots)](../agent-persistence/)
- Debugger
- Tokenizer
- SQL Persistence Providers

# Event handlers

You can monitor and respond to specific events during the agent workflow by using event handlers for logging, testing, debugging, and extending agent behavior.

## Feature overview

The EventHandler feature lets you hook into various agent events. It serves as an event delegation mechanism that:

- Manages the lifecycle of AI agent operations.
- Provides hooks for monitoring and responding to different stages of the workflow.
- Enables error handling and recovery.
- Facilitates tool invocation tracking and result processing.

### Installation and configuration

The EventHandler feature integrates with the agent workflow through the `EventHandler` class, which provides a way to register callbacks for different agent events, and can be installed as a feature in the agent configuration. For details, see [API reference](https://api.koog.ai/agents/agents-features/agents-features-event-handler/ai.koog.agents.features.eventHandler.feature/-event-handler/index.html).

To install the feature and configure event handlers for the agent, do the following:

```kotlin
handleEvents {
    // Handle tool calls
    onToolCallStarting { eventContext ->
        println("Tool called: ${eventContext.toolName} with args ${eventContext.toolArgs}")
    }
    // Handle event triggered when the agent completes its execution
    onAgentCompleted { eventContext ->
        println("Agent finished with result: ${eventContext.result}")
    }

    // Other event handlers
}
```

For more details about event handler configuration, see [API reference](https://api.koog.ai/agents/agents-features/agents-features-event-handler/ai.koog.agents.features.eventHandler.feature/-event-handler-config/index.html).

You can also set up event handlers using the `handleEvents` extension function when creating an agent. This function also installs the event handler feature and configures event handlers for the agent. Here is an example:

```kotlin
val agent = AIAgent(
    promptExecutor = simpleOllamaAIExecutor(),
    llmModel = OllamaModels.Meta.LLAMA_3_2,
){
    handleEvents {
        // Handle tool calls
        onToolCallStarting { eventContext ->
            println("Tool called: ${eventContext.toolName} with args ${eventContext.toolArgs}")
        }
        // Handle event triggered when the agent completes its execution
        onAgentCompleted { eventContext ->
            println("Agent finished with result: ${eventContext.result}")
        }

        // Other event handlers
    }
}
```

# Tracing

This page includes details about the Tracing feature, which provides comprehensive tracing capabilities for AI agents.

## Feature overview

The Tracing feature is a powerful monitoring and debugging tool that captures detailed information about agent runs, including:

- Strategy execution
- LLM calls
- LLM streaming (start, frames, completion, errors)
- Tool calls
- Node execution within the agent graph

This feature operates by intercepting key events in the agent pipeline and forwarding them to configurable message processors. These processors can output the trace information to various destinations such as log files or other types of files in the filesystem, enabling developers to gain insights into agent behavior and troubleshoot issues effectively.

### Event flow

1. The Tracing feature intercepts events in the agent pipeline.
1. Events are filtered based on the configured message filter.
1. Filtered events are passed to registered message processors.
1. Message processors format and output the events to their respective destinations.

## Configuration and initialization

### Basic setup

To use the Tracing feature, you need to:

1. Have one or more message processors (you can use the existing ones or create your own).
1. Install `Tracing` in your agent.
1. Configure the message filter (optional).
1. Add the message processors to the feature.

```kotlin
// Defining a logger/file that will be used as a destination of trace messages 
val logger = KotlinLogging.logger { }
val outputPath = Path("/path/to/trace.log")

// Creating an agent
val agent = AIAgent(
    promptExecutor = simpleOllamaAIExecutor(),
    llmModel = OllamaModels.Meta.LLAMA_3_2,
) {
    install(Tracing) {

        // Configure message processors to handle trace events
        addMessageProcessor(TraceFeatureMessageLogWriter(logger))
        addMessageProcessor(TraceFeatureMessageFileWriter(
            outputPath,
            { path: Path -> SystemFileSystem.sink(path).buffered() }
        ))
    }
}
```

### Message filtering

You can process all existing events or select some of them based on specific criteria. The message filter lets you control which events are processed. This is useful for focusing on specific aspects of agent runs:

```kotlin
val fileWriter = TraceFeatureMessageFileWriter(
    outputPath,
    { path: Path -> SystemFileSystem.sink(path).buffered() }
)

addMessageProcessor(fileWriter)

// Filter for LLM-related events only
fileWriter.setMessageFilter { message ->
    message is LLMCallStartingEvent || message is LLMCallCompletedEvent
}

// Filter for tool-related events only
fileWriter.setMessageFilter { message -> 
    message is ToolCallStartingEvent ||
           message is ToolCallCompletedEvent ||
           message is ToolValidationFailedEvent ||
           message is ToolCallFailedEvent
}

// Filter for node execution events only
fileWriter.setMessageFilter { message -> 
    message is NodeExecutionStartingEvent || message is NodeExecutionCompletedEvent
}
```

### Large trace volumes

For agents with complex strategies or long-running executions, the volume of trace events can be substantial. Consider using the following methods to manage the volume of events:

- Use specific message filters to reduce the number of events.
- Implement custom message processors with buffering or sampling.
- Use file rotation for log files to prevent them from growing too large.

### Dependency graph

The Tracing feature has the following dependencies:

```text
Tracing
â”œâ”€â”€ AIAgentPipeline (for intercepting events)
â”œâ”€â”€ TraceFeatureConfig
â”‚   â””â”€â”€ FeatureConfig
â”œâ”€â”€ Message Processors
â”‚   â”œâ”€â”€ TraceFeatureMessageLogWriter
â”‚   â”‚   â””â”€â”€ FeatureMessageLogWriter
â”‚   â”œâ”€â”€ TraceFeatureMessageFileWriter
â”‚   â”‚   â””â”€â”€ FeatureMessageFileWriter
â”‚   â””â”€â”€ TraceFeatureMessageRemoteWriter
â”‚       â””â”€â”€ FeatureMessageRemoteWriter
â””â”€â”€ Event Types (from ai.koog.agents.core.feature.model)
    â”œâ”€â”€ AgentStartingEvent
    â”œâ”€â”€ AgentCompletedEvent
    â”œâ”€â”€ AgentExecutionFailedEvent
    â”œâ”€â”€ AgentClosingEvent
    â”œâ”€â”€ GraphStrategyStartingEvent
    â”œâ”€â”€ FunctionalStrategyStartingEvent
    â”œâ”€â”€ StrategyCompletedEvent
    â”œâ”€â”€ NodeExecutionStartingEvent
    â”œâ”€â”€ NodeExecutionCompletedEvent
    â”œâ”€â”€ NodeExecutionFailedEvent
    â”œâ”€â”€ SubgraphExecutionStartingEvent
    â”œâ”€â”€ SubgraphExecutionCompletedEvent
    â”œâ”€â”€ SubgraphExecutionFailedEvent
    â”œâ”€â”€ LLMCallStartingEvent
    â”œâ”€â”€ LLMCallCompletedEvent
    â”œâ”€â”€ LLMStreamingStartingEvent
    â”œâ”€â”€ LLMStreamingFrameReceivedEvent
    â”œâ”€â”€ LLMStreamingFailedEvent
    â”œâ”€â”€ LLMStreamingCompletedEvent
    â”œâ”€â”€ ToolCallStartingEvent
    â”œâ”€â”€ ToolValidationFailedEvent
    â”œâ”€â”€ ToolCallFailedEvent
    â””â”€â”€ ToolCallCompletedEvent
```

## Examples and quickstarts

### Basic tracing to logger

```kotlin
// Create a logger
val logger = KotlinLogging.logger { }

fun main() {
    runBlocking {
       // Create an agent with tracing
       val agent = AIAgent(
          promptExecutor = simpleOllamaAIExecutor(),
          llmModel = OllamaModels.Meta.LLAMA_3_2,
       ) {
          install(Tracing) {
             addMessageProcessor(TraceFeatureMessageLogWriter(logger))
          }
       }

       // Run the agent
       agent.run("Hello, agent!")
    }
}
```

## Error handling and edge cases

### No message processors

If no message processors are added to the Tracing feature, a warning will be logged:

```text
Tracing Feature. No feature out stream providers are defined. Trace streaming has no target.
```

The feature will still intercept events, but they will not be processed or output anywhere.

### Resource management

Message processors may hold resources (like file handles) that need to be properly released. Use the `use` extension function to ensure proper cleanup:

```kotlin
// Creating an agent
val agent = AIAgent(
    promptExecutor = simpleOllamaAIExecutor(),
    llmModel = OllamaModels.Meta.LLAMA_3_2,
) {
    val writer = TraceFeatureMessageFileWriter(
        outputPath,
        { path: Path -> SystemFileSystem.sink(path).buffered() }
    )

    install(Tracing) {
        addMessageProcessor(writer)
    }
}
// Run the agent
agent.run(input)
// Writer will be automatically closed when the block exits
```

### Tracing specific events to file

```kotlin
install(Tracing) {

    val fileWriter = TraceFeatureMessageFileWriter(
        outputPath, 
        { path: Path -> SystemFileSystem.sink(path).buffered() }
    )
    addMessageProcessor(fileWriter)

    // Only trace LLM calls
    fileWriter.setMessageFilter { message ->
        message is LLMCallStartingEvent || message is LLMCallCompletedEvent
    }
}
```

### Tracing specific events to remote endpoint

You use tracing to remote endpoints when you need to send event data via the network. Once initiated, tracing to a remote endpoint launches a light server at the specified port number and sends events via Kotlin Server-Sent Events (SSE).

```kotlin
// Creating an agent
val agent = AIAgent(
    promptExecutor = simpleOllamaAIExecutor(),
    llmModel = OllamaModels.Meta.LLAMA_3_2,
) {
    val connectionConfig = DefaultServerConnectionConfig(host = host, port = port)
    val writer = TraceFeatureMessageRemoteWriter(
        connectionConfig = connectionConfig
    )

    install(Tracing) {
        addMessageProcessor(writer)
    }
}
// Run the agent
agent.run(input)
// Writer will be automatically closed when the block exits
```

On the client side, you can use `FeatureMessageRemoteClient` to receive events and deserialize them.

```kotlin
val clientConfig = DefaultClientConnectionConfig(host = host, port = port, protocol = URLProtocol.HTTP)
val agentEvents = mutableListOf<DefinedFeatureEvent>()

val clientJob = launch {
    FeatureMessageRemoteClient(connectionConfig = clientConfig, scope = this).use { client ->
        val collectEventsJob = launch {
            client.receivedMessages.consumeAsFlow().collect { event ->
                // Collect events from server
                agentEvents.add(event as DefinedFeatureEvent)

                // Stop collecting events on agent finished
                if (event is AgentCompletedEvent) {
                    cancel()
                }
            }
        }
        client.connect()
        collectEventsJob.join()
        client.healthCheck()
    }
}

listOf(clientJob).joinAll()
```

## API documentation

The Tracing feature follows a modular architecture with these key components:

1. [Tracing](https://api.koog.ai/agents/agents-features/agents-features-trace/ai.koog.agents.features.tracing.feature/-tracing/index.html): the main feature class that intercepts events in the agent pipeline.
1. [TraceFeatureConfig](https://api.koog.ai/agents/agents-features/agents-features-trace/ai.koog.agents.features.tracing.feature/-trace-feature-config/index.html): configuration class for customizing feature behavior.
1. Message Processors: components that process and output trace events:
   - [TraceFeatureMessageLogWriter](https://api.koog.ai/agents/agents-features/agents-features-trace/ai.koog.agents.features.tracing.writer/-trace-feature-message-log-writer/index.html): writes trace events to a logger.
   - [TraceFeatureMessageFileWriter](https://api.koog.ai/agents/agents-features/agents-features-trace/ai.koog.agents.features.tracing.writer/-trace-feature-message-file-writer/index.html): writes trace events to a file.
   - [TraceFeatureMessageRemoteWriter](https://api.koog.ai/agents/agents-features/agents-features-trace/ai.koog.agents.features.tracing.writer/-trace-feature-message-remote-writer/index.html): sends trace events to a remote server.

# Memory

## Feature overview

The AgentMemory feature is a component of the Koog framework that lets AI agents store, retrieve, and use information across conversations.

### Purpose

The AgentMemory Feature addresses the challenge of maintaining context in AI agent interactions by:

- Storing important facts extracted from conversations.
- Organizing information by concepts, subjects, and scopes.
- Retrieving relevant information when needed in future interactions.
- Enabling personalization based on user preferences and history.

### Architecture

The AgentMemory feature is built on a hierarchical structure. The elements of the structure are listed and explained in the sections below.

#### Facts

***Facts*** are individual pieces of information stored in the memory. Facts represent actual stored information. There are two types of facts:

- **SingleFact**: a single value associated with a concept. For example, an IDE user's current preferred theme:

```kotlin
// Storing favorite IDE theme (single value)
val themeFact = SingleFact(
    concept = Concept(
        "ide-theme", 
        "User's preferred IDE theme", 
        factType = FactType.SINGLE),
    value = "Dark Theme",
    timestamp = Clock.System.now().toEpochMilliseconds(),
)
```

- **MultipleFacts**: multiple values associated with a concept. For example, all languages that a user knows:

```kotlin
// Storing programming languages (multiple values)
val languagesFact = MultipleFacts(
    concept = Concept(
        "programming-languages",
        "Languages the user knows",
        factType = FactType.MULTIPLE
    ),
    values = listOf("Kotlin", "Java", "Python"),
    timestamp = Clock.System.now().toEpochMilliseconds(),
)
```

#### Concepts

***Concepts*** are categories of information with associated metadata.

- **Keyword**: unique identifier for the concept.
- **Description**: detailed explanation of what the concept represents.
- **FactType**: whether the concept stores single or multiple facts (`FactType.SINGLE` or `FactType.MULTIPLE`).

#### Subjects

***Subjects*** are entities that facts can be associated with.

Common examples of subjects include:

- **User**: Personal preferences and settings
- **Environment**: Information related to the environment of the application

There is a predefined `MemorySubject.Everything` that you may use as a default subject for all facts. In addition, you can define your own custom memory subjects by extending the `MemorySubject` abstract class:

```kotlin
object MemorySubjects {
    /**
     * Information specific to the local machine environment
     * Examples: Installed tools, SDKs, OS configuration, available commands
     */
    @Serializable
    data object Machine : MemorySubject() {
        override val name: String = "machine"
        override val promptDescription: String =
            "Technical environment (installed tools, package managers, packages, SDKs, OS, etc.)"
        override val priorityLevel: Int = 1
    }

    /**
     * Information specific to the user
     * Examples: Conversation preferences, issue history, contact information
     */
    @Serializable
    data object User : MemorySubject() {
        override val name: String = "user"
        override val promptDescription: String =
            "User information (conversation preferences, issue history, contact details, etc.)"
        override val priorityLevel: Int = 1
    }
}
```

#### Scopes

***Memory scopes*** are contexts in which facts are relevant:

- **Agent**: specific to an agent.
- **Feature**: specific to a feature.
- **Product**: specific to a product.
- **CrossProduct**: relevant across multiple products.

## Configuration and initialization

The feature integrates with the agent pipeline through the `AgentMemory` class, which provides methods for saving and loading facts, and can be installed as a feature in the agent configuration.

### Configuration

The `AgentMemory.Config` class is the configuration class for the AgentMemory feature.

```kotlin
class Config(
    var memoryProvider: AgentMemoryProvider = NoMemory,
    var scopesProfile: MemoryScopesProfile = MemoryScopesProfile(),

    var agentName: String,
    var featureName: String,
    var organizationName: String,
    var productName: String
) : FeatureConfig()
```

### Installation

To install the AgentMemory feature in an agent, follow the pattern provided in the code sample below.

```kotlin
val agent = AIAgent(
    promptExecutor = simpleOllamaAIExecutor(),
    llmModel = OllamaModels.Meta.LLAMA_3_2,
) {
    install(AgentMemory) {
        memoryProvider = memoryProvider
        agentName = "your-agent-name"
        featureName = "your-feature-name"
        organizationName = "your-organization-name"
        productName = "your-product-name"
    }
}
```

## Examples and quickstarts

### Basic usage

The following code snippets demonstrate the basic setup of a memory storage and how facts are saved to and loaded from the memory.

1. Set up memory storage

```kotlin
// Create a memory provider
val memoryProvider = LocalFileMemoryProvider(
    config = LocalMemoryConfig("customer-support-memory"),
    storage = SimpleStorage(JVMFileSystemProvider.ReadWrite),
    fs = JVMFileSystemProvider.ReadWrite,
    root = Path("path/to/memory/root")
)
```

2. Store a fact in the memory

```kotlin
memoryProvider.save(
    fact = SingleFact(
        concept = Concept("greeting", "User's name", FactType.SINGLE),
        value = "John",
        timestamp = Clock.System.now().toEpochMilliseconds(),
    ),
    subject = MemorySubjects.User,
    scope = MemoryScope.Product("my-app"),
)
```

3. Retrieve the fact

```kotlin
// Get the stored information
val greeting = memoryProvider.load(
    concept = Concept("greeting", "User's name", FactType.SINGLE),
    subject = MemorySubjects.User,
    scope = MemoryScope.Product("my-app")
)
if (greeting.size > 1) {
    println("Memories found: ${greeting.joinToString(", ")}")
} else {
    println("Information not found. First time here?")
}
```

#### Using memory nodes

The AgentMemory feature provides the following predefined memory nodes that can be used in agent strategies:

- [nodeLoadAllFactsFromMemory](https://api.koog.ai/agents/agents-features/agents-features-memory/ai.koog.agents.local.memory.feature.nodes/node-load-all-facts-from-memory.html): loads all facts about the subject from the memory for a given concept.
- [nodeLoadFromMemory](https://api.koog.ai/agents/agents-features/agents-features-memory/ai.koog.agents.local.memory.feature.nodes/node-load-from-memory.html): loads specific facts from the memory for a given concept.
- [nodeSaveToMemory](https://api.koog.ai/agents/agents-features/agents-features-memory/ai.koog.agents.local.memory.feature.nodes/node-save-to-memory.html): saves a fact to the memory.
- [nodeSaveToMemoryAutoDetectFacts](https://api.koog.ai/agents/agents-features/agents-features-memory/ai.koog.agents.local.memory.feature.nodes/node-save-to-memory-auto-detect-facts.html): automatically detects and extracts facts from the chat history and saves them to the memory. Uses the LLM to identify concepts.

Here is an example of how nodes can be implemented in an agent strategy:

```kotlin
val strategy = strategy("example-agent") {
    // Node to automatically detect and save facts
    val detectFacts by nodeSaveToMemoryAutoDetectFacts<Unit>(
        subjects = listOf(MemorySubjects.User, MemorySubjects.Machine)
    )

    // Node to load specific facts
    val loadPreferences by node<Unit, Unit> {
        withMemory {
            loadFactsToAgent(
                llm = llm,
                concept = Concept("user-preference", "User's preferred programming language", FactType.SINGLE),
                subjects = listOf(MemorySubjects.User)
            )
        }
    }

    // Connect nodes in the strategy
    edge(nodeStart forwardTo detectFacts)
    edge(detectFacts forwardTo loadPreferences)
    edge(loadPreferences forwardTo nodeFinish)
}
```

#### Making memory secure

You can use encryption to make sure that sensitive information is protected inside an encrypted storage used by the memory provider.

```kotlin
// Simple encrypted storage setup
val secureStorage = EncryptedStorage(
    fs = JVMFileSystemProvider.ReadWrite,
    encryption = Aes256GCMEncryptor("your-secret-key")
)
```

#### Example: Remembering user preferences

Here is an example of how AgentMemory is used in a real-world scenario to remember a user's preference, specifically the user's favorite programming language.

```kotlin
memoryProvider.save(
    fact = SingleFact(
        concept = Concept("preferred-language", "What programming language is preferred by the user?", FactType.SINGLE),
        value = "Kotlin",
        timestamp = Clock.System.now().toEpochMilliseconds(),
    ),
    subject = MemorySubjects.User,
    scope = MemoryScope.Product("my-app")
)
```

### Advanced usage

#### Custom nodes with memory

You can also use the memory from the `withMemory` clause inside any node. The ready-to-use `loadFactsToAgent` and `saveFactsFromHistory` higher level abstractions save facts to the history, load facts from it, and update the LLM chat:

```kotlin
val loadProjectInfo by node<Unit, Unit> {
    withMemory {
        loadFactsToAgent(
            llm = llm,
            concept = Concept("preferred-language", "What programming language is preferred by the user?", FactType.SINGLE)
        )
    }
}

val saveProjectInfo by node<Unit, Unit> {
    withMemory {
        saveFactsFromHistory(
            llm = llm,
            concept = Concept("preferred-language", "What programming language is preferred by the user?", FactType.SINGLE),
            subject = MemorySubjects.User,
            scope = MemoryScope.Product("my-app")
        )
    }
}
```

#### Automatic fact detection

You can also ask the LLM to detect all the facts from the agent's history using the `nodeSaveToMemoryAutoDetectFacts` method:

```kotlin
val saveAutoDetect by nodeSaveToMemoryAutoDetectFacts<Unit>(
    subjects = listOf(MemorySubjects.User, MemorySubjects.Machine)
)
```

In the example above, the LLM would search for the user-related facts and project-related facts, determine the concepts, and save them into the memory.

## Best practices

1. **Start Simple**

   - Begin with basic storage without encryption
   - Use single facts before moving to multiple facts

1. **Organize Well**

   - Use clear concept names
   - Add helpful descriptions
   - Keep related information under the same subject

1. **Handle Errors**

```kotlin
try {
    memoryProvider.save(fact, subject, scope)
} catch (e: Exception) {
    println("Oops! Couldn't save: ${e.message}")
}
```

For more details on error handling, see [Error handling and edge cases](#error-handling-and-edge-cases).

## Error handling and edge cases

The AgentMemory feature includes several mechanisms to handle edge cases:

1. **NoMemory provider**: a default implementation that doesn't store anything, used when no memory provider is specified.
1. **Subject specificity handling**: when loading facts, the feature prioritizes facts from more specific subjects based on their defined `priorityLevel`.
1. **Scope filtering**: facts can be filtered by scope to ensure only relevant information is loaded.
1. **Timestamp tracking**: facts are stored with timestamps to track when they were created.
1. **Fact type handling**: the feature supports both single facts and multiple facts, with appropriate handling for each type.

## API documentation

For a complete API reference related to the AgentMemory feature, see the reference documentation for the [agents-features-memory](https://api.koog.ai/agents/agents-features/agents-features-memory/index.html) module.

API documentation for specific packages:

- [ai.koog.agents.local.memory.feature](https://api.koog.ai/agents/agents-features/agents-features-memory/ai.koog.agents.local.memory.feature/index.html): includes the `AgentMemory` class and the core implementation of the AI agents memory feature.
- [ai.koog.agents.local.memory.feature.nodes](https://api.koog.ai/agents/agents-features/agents-features-memory/ai.koog.agents.local.memory.feature.nodes/index.html): includes predefined memory-related nodes that can be used in subgraphs.
- [ai.koog.agents.local.memory.config](https://api.koog.ai/agents/agents-features/agents-features-memory/ai.koog.agents.local.memory.config/index.html): provides definitions of memory scopes used for memory operations.
- [ai.koog.agents.local.memory.model](https://api.koog.ai/agents/agents-features/agents-features-memory/ai.koog.agents.local.memory.model/index.html): includes definitions of the core data structures and interfaces that enable agents to store, organize, and retrieve information across different contexts and time periods.
- [ai.koog.agents.local.memory.feature.history](https://api.koog.ai/agents/agents-features/agents-features-memory/ai.koog.agents.local.memory.feature.history/index.html): provides the history compression strategy for retrieving and incorporating factual knowledge about specific concepts from past session activity or stored memory.
- [ai.koog.agents.local.memory.providers](https://api.koog.ai/agents/agents-features/agents-features-memory/ai.koog.agents.local.memory.providers/index.html): provides the core interface that defines the fundamental operation for storing and retrieving knowledge in a structured, context-aware manner and its implementations.
- [ai.koog.agents.local.memory.storage](https://api.koog.ai/agents/agents-features/agents-features-memory/ai.koog.agents.local.memory.storage/index.html): provides the core interface and specific implementations for file operations across different platforms and storage backends.

## FAQ and troubleshooting

### How do I implement a custom memory provider?

To implement a custom memory provider, create a class that implements the `AgentMemoryProvider` interface:

```kotlin
class MyCustomMemoryProvider : AgentMemoryProvider {
    override suspend fun save(fact: Fact, subject: MemorySubject, scope: MemoryScope) {
        // Implementation for saving facts
    }

    override suspend fun load(concept: Concept, subject: MemorySubject, scope: MemoryScope): List<Fact> {
        // Implementation for loading facts by concept
    }

    override suspend fun loadAll(subject: MemorySubject, scope: MemoryScope): List<Fact> {
        // Implementation for loading all facts
    }

    override suspend fun loadByDescription(
        description: String,
        subject: MemorySubject,
        scope: MemoryScope
    ): List<Fact> {
        // Implementation for loading facts by description
    }
}
```

### How are facts prioritized when loading from multiple subjects?

Facts are prioritized based on subject specificity. When loading facts, if the same concept has facts from multiple subjects, the fact from the most specific subject will be used.

### Can I store multiple values for the same concept?

Yes, by using the `MultipleFacts` type. When defining a concept, set its `factType` to `FactType.MULTIPLE`:

```kotlin
val concept = Concept(
    keyword = "user-skills",
    description = "Programming languages the user is skilled in",
    factType = FactType.MULTIPLE
)
```

This lets you store multiple values for the concept, which is retrieved as a list.

# Agent Persistence

Agent Persistence is a feature that provides checkpoint functionality for AI agents in the Koog framework. It lets you save and restore the state of an agent at specific points during execution, enabling capabilities such as:

- Resuming agent execution from a specific point
- Rolling back to previous states
- Persisting agent state across sessions

## Key concepts

### Checkpoints

A checkpoint captures the complete state of an agent at a specific point in its execution, including:

- Message history (all interactions between user, system, assistant, and tools)
- Current node being executed
- Input data for the current node
- Timestamp of creation

Checkpoints are identified by unique IDs and are associated with a specific agent.

## Installation

To use the Agent Persistence feature, add it to your agent's configuration:

```kotlin
val agent = AIAgent(
    promptExecutor = executor,
    llmModel = OllamaModels.Meta.LLAMA_3_2,
) {
    install(Persistence) {
        // Use in-memory storage for snapshots
        storage = InMemoryPersistenceStorageProvider()
        // Enable automatic persistence after each node
        enableAutomaticPersistence = true
        /* 
         Select which state will be restored on a new agent run.

         Available options are:
         1. Default: Restores the agent to the exact execution point (node in the strategy graph) where it stopped.
            This is especially useful for building complex, fault-tolerant agents.
         2. MessageHistoryOnly: Restores only the message history to the last saved state.
            The agent will always restart from the first node in the strategy graph, but with history from previous runs.
            This is useful for building conversational agents or chatbots.
        */
        rollbackStrategy = RollbackStrategy.MessageHistoryOnly
    }
}
```

Tip

Combine `enableAutomaticPersistence = true` with `RollbackStrategy.MessageHistoryOnly` to create agents that maintain conversation context across multiple sessions.

## Configuration options

The Agent Persistence feature has three main configuration options:

- **Storage provider**: the provider used to save and retrieve checkpoints.
- **Continuous persistence**: automatic creation of checkpoints after each node is run.
- **Rollback strategy**: determines which state will be restored when rolling back to a checkpoint.

### Storage provider

Set the storage provider that will be used to save and retrieve checkpoints:

```kotlin
install(Persistence) {
    storage = InMemoryPersistenceStorageProvider()
}
```

The framework includes the following built-in providers:

- `InMemoryPersistenceStorageProvider`: stores checkpoints in memory (lost when the application restarts).
- `FilePersistenceStorageProvider`: persists checkpoints to the file system.
- `NoPersistenceStorageProvider`: a no-op implementation that does not store checkpoints. This is the default provider.

You can also implement custom storage providers by implementing the `PersistenceStorageProvider` interface. For more information, see [Custom storage providers](#custom-storage-providers).

### Continuous persistence

Continuous persistence means that a checkpoint is automatically created after each node is run. To activate continuous persistence, use the code below:

```kotlin
install(Persistence) {
    enableAutomaticPersistence = true
}
```

When activated, the agent will automatically create a checkpoint after each node is executed, allowing for fine-grained recovery.

### Rollback strategy

The rollback strategy determines which state will be restored when the agent rolls back to a checkpoint or starts a new run. There are two available strategies:

```kotlin
install(Persistence) {
    // Default strategy: restores the complete agent state including execution point
    rollbackStrategy = RollbackStrategy.Default
}
```

**`RollbackStrategy.Default`**

Restores the agent to the exact execution point (node in the strategy graph) where it stopped. This means the entire context is restored, including:

- Message history
- Current node being executed
- Any other stateful data

This strategy is especially useful for building complex, fault-tolerant agents that need to resume from the exact point where they left off.

**`RollbackStrategy.MessageHistoryOnly`**

Restores only the message history to the last saved state. The agent will always restart from the first node in the strategy graph, but with the conversation history from previous runs.

This strategy is useful for building conversational agents or chatbots that need to maintain context across multiple sessions but should always start their execution flow from the beginning.

```kotlin
install(Persistence) {
    // MessageHistoryOnly strategy: preserves conversation history but restarts execution
    rollbackStrategy = RollbackStrategy.MessageHistoryOnly
}
```

## Basic usage

### Creating a checkpoint

To learn how to create a checkpoint at a specific point in your agent's execution, see the code sample below:

```kotlin
suspend fun example(context: AIAgentContext) {
    // Create a checkpoint with the current state
    val checkpoint = context.persistence().createCheckpoint(
        agentContext = context,
        nodePath = context.executionInfo.path(),
        lastInput = inputData,
        lastInputType = inputType,
        checkpointId = context.runId,
        version = 0L
    )

    // The checkpoint ID can be stored for later use
    val checkpointId = checkpoint?.checkpointId
}
```

### Restoring from a checkpoint

To restore the state of an agent from a specific checkpoint, follow the code sample below:

```kotlin
suspend fun example(context: AIAgentContext, checkpointId: String) {
    // Roll back to a specific checkpoint
    context.persistence().rollbackToCheckpoint(checkpointId, context)

    // Or roll back to the latest checkpoint
    context.persistence().rollbackToLatestCheckpoint(context)
}
```

#### Rolling back all side-effects produced by tools

It's quite common for some tools to produce side-effects. Specifically, when you are running your agents on the backend, some of the tools would likely perform some database transactions. This makes it much harder for your agent to travel back in time.

Imagine, that you have a tool `createUser` that creates a new user in your database. And your agent has populated multiple tool calls overtime:

```text
tool call: createUser "Alex"

->>>> checkpoint-1 <<<<-

tool call: createUser "Daniel"
tool call: createUser "Maria"
```

And now you would like to roll back to a checkpoint. Restoring the agent's state (including message history, and strategy graph node) alone would not be sufficient to achieve the exact state of the world before the checkpoint. You should also restore the side-effects produced by your tool calls. In our example, this would mean removing `Maria` and `Daniel` from the database.

With Koog Persistence you can achieve that by providing a `RollbackToolRegistry` to `Persistence` feature config:

```kotlin
install(Persistence) {
    enableAutomaticPersistence = true
    rollbackToolRegistry = RollbackToolRegistry {
        // For every `createUser` tool call there will be a `removeUser` invocation in the reverse order 
        // when rolling back to the desired execution point.
        // Note: `removeUser` tool should take the same exact arguments as `createUser`. 
        // It's the developer's responsibility to make sure that `removeUser` invocation rolls back all side-effects of `createUser`:
        registerRollback(::createUser, ::removeUser)
    }
}
```

### Using extension functions

The Agent Persistence feature provides convenient extension functions for working with checkpoints:

```kotlin
suspend fun example(context: AIAgentContext) {
    // Access the checkpoint feature
    val checkpointFeature = context.persistence()

    // Or perform an action with the checkpoint feature
    context.withPersistence { ctx ->
        // 'this' is the checkpoint feature
        createCheckpoint(
            agentContext = ctx,
            nodePath = ctx.executionInfo.path(),
            lastInput = inputData,
            lastInputType = inputType,
            checkpointId = ctx.runId,
            version = 0L
        )
    }
}
```

## Advanced usage

### Custom storage providers

You can implement custom storage providers by implementing the `PersistenceStorageProvider` interface:

```kotlin
class MyCustomStorageProvider<MyFilterType> : PersistenceStorageProvider<MyFilterType> {
    override suspend fun getCheckpoints(agentId: String, filter: MyFilterType?): List<AgentCheckpointData> {
        TODO("Not yet implemented")
    }

    override suspend fun saveCheckpoint(agentId: String, agentCheckpointData: AgentCheckpointData) {
        TODO("Not yet implemented")
    }

    override suspend fun getLatestCheckpoint(agentId: String, filter: MyFilterType?): AgentCheckpointData? {
        TODO("Not yet implemented")
    }
}
```

To use your custom provider in the feature configuration, set it as the storage when configuring the Agent Persistence feature in your agent.

```kotlin
install(Persistence) {
    storage = MyCustomStorageProvider<Any>()
}
```

### Setting execution points

For advanced control, you can directly set the execution point of an agent:

```kotlin
fun example(context: AIAgentContext) {
    context.persistence().setExecutionPoint(
        agentContext = context,
        nodePath = context.executionInfo.path(),
        messageHistory = customMessageHistory,
        input = customInput
    )
}
```

This allows for more fine-grained control over the agent's state beyond just restoring from checkpoints.
# OpenTelemetry

# OpenTelemetry support

This page provides details about the support for OpenTelemetry with the Koog agentic framework for tracing and monitoring your AI agents.

## Overview

OpenTelemetry is an observability framework that provides tools for generating, collecting, and exporting telemetry data (traces) from your applications. The Koog OpenTelemetry feature allows you to instrument your AI agents to collect telemetry data, which can help you:

- Monitor agent performance and behavior
- Debug issues in complex agent workflows
- Visualize the execution flow of your agents
- Track LLM calls and tool usage
- Analyze agent behavior patterns

## Key OpenTelemetry concepts

- **Spans**: spans represent individual units of work or operations within a distributed trace. They indicate the beginning and end of a specific activity in an application, such as an agent execution, a function call, an LLM call, or a tool call.
- **Attributes**: attributes provide metadata about a telemetry-related item such as a span. Attributes are represented as key-value pairs.
- **Events**: events are specific points in time during the lifetime of a span (span-related events) that represent something potentially noteworthy that happened.
- **Exporters**: exporters are components responsible for sending the collected telemetry data to various backends or destinations.
- **Collectors**: collectors receive, process, and export telemetry data. They act as intermediaries between your applications and your observability backend.
- **Samplers**: samplers determine whether a trace should be recorded based on the sampling strategy. They are used to manage the volume of telemetry data.
- **Resources**: resources represent entities that produce telemetry data. They are identified by resource attributes, which are key-value pairs that provide information about the resource.

The OpenTelemetry feature in Koog automatically creates spans for various agent events, including:

- Agent execution start and end
- Node execution
- LLM calls
- Tool calls

## Installation

To use OpenTelemetry with Koog, add the OpenTelemetry feature to your agent:

```kotlin
val agent = AIAgent(
    promptExecutor = simpleOpenAIExecutor(apiKey),
    llmModel = OpenAIModels.Chat.GPT4o,
    systemPrompt = "You are a helpful assistant.",
    installFeatures = {
        install(OpenTelemetry) {
            // Configuration options go here
        }
    }
)
```

## Configuration

### Basic configuration

Here is the full list of available properties that you set when configuring the OpenTelemetry feature in an agent:

| Name             | Data type          | Default value                | Description                                                                  |
| ---------------- | ------------------ | ---------------------------- | ---------------------------------------------------------------------------- |
| `serviceName`    | `String`           | `ai.koog`                    | The name of the service being instrumented.                                  |
| `serviceVersion` | `String`           | Current Koog library version | The version of the service being instrumented.                               |
| `isVerbose`      | `Boolean`          | `false`                      | Whether to enable verbose logging for debugging OpenTelemetry configuration. |
| `sdk`            | `OpenTelemetrySdk` |                              | The OpenTelemetry SDK instance to use for telemetry collection.              |
| `tracer`         | `Tracer`           |                              | The OpenTelemetry tracer instance used for creating spans.                   |

Note

The `sdk` and `tracer` properties are public properties that you can access, but you can only set them using the public methods listed below.

The `OpenTelemetryConfig` class also includes methods that represent actions related to different configuration items. Here is an example of installing the OpenTelemetry feature with a basic set of configuration items:

```kotlin
install(OpenTelemetry) {
    // Set your service configuration
    setServiceInfo("my-agent-service", "1.0.0")

    // Add the Logging exporter
    addSpanExporter(LoggingSpanExporter.create())
}
```

For a reference of available methods, see the sections below.

#### setServiceInfo

Sets the service information including name and version. Takes the following arguments:

| Name             | Data type | Required | Default value | Description                                    |
| ---------------- | --------- | -------- | ------------- | ---------------------------------------------- |
| `serviceName`    | String    | Yes      |               | The name of the service being instrumented.    |
| `serviceVersion` | String    | Yes      |               | The version of the service being instrumented. |

#### addSpanExporter

Adds a span exporter to send telemetry data to external systems. Takes the following argument:

| Name       | Data type      | Required | Default value | Description                                                                   |
| ---------- | -------------- | -------- | ------------- | ----------------------------------------------------------------------------- |
| `exporter` | `SpanExporter` | Yes      |               | The `SpanExporter` instance to be added to the list of custom span exporters. |

#### addSpanProcessor

Adds a span processor factory to process spans before they are exported. Takes the following argument:

| Name        | Data type                         | Required | Default value | Description                                                                                                |
| ----------- | --------------------------------- | -------- | ------------- | ---------------------------------------------------------------------------------------------------------- |
| `processor` | `(SpanExporter) -> SpanProcessor` | Yes      |               | A function that creates a span processor for a given exporter. Lets you customize processing per exporter. |

#### addResourceAttributes

Adds resource attributes to provide additional context about the service. Takes the following argument:

| Name         | Data type                 | Required | Default value | Description                                                            |
| ------------ | ------------------------- | -------- | ------------- | ---------------------------------------------------------------------- |
| `attributes` | `Map<AttributeKey<T>, T>` | Yes      |               | The key-value pairs that provide additional details about the service. |

#### setSampler

Sets the sampling strategy to control which spans are collected. Takes the following argument:

| Name      | Data type | Required | Default value | Description                                                      |
| --------- | --------- | -------- | ------------- | ---------------------------------------------------------------- |
| `sampler` | `Sampler` | Yes      |               | The sampler instance to set for the OpenTelemetry configuration. |

#### setVerbose

Enables or disables verbose logging. Takes the following argument:

| Name      | Data type | Required | Default value | Description                                                     |
| --------- | --------- | -------- | ------------- | --------------------------------------------------------------- |
| `verbose` | `Boolean` | Yes      | `false`       | If true, the application collects more detailed telemetry data. |

Note

Some content of OpenTelemetry spans is masked by default for security reasons. For example, LLM messages are masked as `HIDDEN:non-empty` instead of the actual message content. To get the content, set the value of the `verbose` argument to `true`.

#### setSdk

Injects a pre-configured OpenTelemetrySdk instance.

- When you call setSdk(sdk), the provided SDK is used as-is, and any custom configuration applied via addSpanExporter, addSpanProcessor, addResourceAttributes, or setSampler is ignored.
- The tracerâ€™s instrumentation scope name/version are aligned with your service info.

| Name  | Data type          | Required | Description                           |
| ----- | ------------------ | -------- | ------------------------------------- |
| `sdk` | `OpenTelemetrySdk` | Yes      | The SDK instance to use in the agent. |

### Advanced configuration

For more advanced configuration, you can also customize the following configuration options:

- Sampler: configure the sampling strategy to adjust the frequency and amount of collected data.
- Resource attributes: add more information about the process that is producing telemetry data.

```kotlin
install(OpenTelemetry) {
    // Set your service configuration
    setServiceInfo("my-agent-service", "1.0.0")

    // Add the Logging exporter
    addSpanExporter(LoggingSpanExporter.create())

    // Set the sampler 
    setSampler(Sampler.traceIdRatioBased(0.5)) 

    // Add resource attributes
    addResourceAttributes(mapOf(
        AttributeKey.stringKey("custom.attribute") to "custom-value")
    )
}
```

#### Sampler

To define a sampler, use a corresponding method of the `Sampler` class (`io.opentelemetry.sdk.trace.samplers.Sampler`) from the `opentelemetry-java` SDK that represents the sampling strategy you want to use.

The default sampling strategy is as follows:

- `Sampler.alwaysOn()`: The default sampling strategy where every span (trace) is sampled.

For more information about available samplers and sampling strategies, see the OpenTelemetry [Sampler](https://opentelemetry.io/docs/languages/java/sdk/#sampler) documentation.

#### Resource attributes

Resource attributes represent additional information about a process producing telemetry data. Koog includes a set of resource attributes that are set by default:

- `service.name`
- `service.version`
- `service.instance.time`
- `os.type`
- `os.version`
- `os.arch`

The default value of the `service.name` attribute is `ai.koog`, while the default `service.version` value is the currently used Koog library version.

In addition to default resource attributes, you can also add custom attributes. To add a custom attribute to an OpenTelemetry configuration in Koog, use the `addResourceAttributes()` method in an OpenTelemetry configuration that takes a key and a value as its arguments.

```kotlin
addResourceAttributes(mapOf(
    AttributeKey.stringKey("custom.attribute") to "custom-value")
)
```

## Span types and attributes

The OpenTelemetry feature automatically creates different types of spans to track various operations in your agent:

- **CreateAgentSpan**: created when you run an agent, closed when the agent is closed or the process is terminated.
- **InvokeAgentSpan**: the invocation of an agent.
- **StrategySpan**: the execution of an agent's strategy (the top-level execution flow).
- **NodeExecuteSpan**: the execution of a node in the agent's strategy. This is a custom, Koog-specific span.
- **SubgraphExecuteSpan**: the execution of a subgraph within the agent strategy. This is a custom, Koog-specific span.
- **InferenceSpan**: an LLM call.
- **ExecuteToolSpan**: a tool call.

Spans are organized in a nested, hierarchical structure. Here is an example of a span structure:

```text
CreateAgentSpan
    InvokeAgentSpan
        StrategySpan
            NodeExecuteSpan
                InferenceSpan
            NodeExecuteSpan
                ExecuteToolSpan
            SubgraphExecuteSpan
                NodeExecuteSpan
                    InferenceSpan
```

### Span attributes

Span attributes provide metadata related to a span. Each span has its set of attributes, while some spans can also repeat attributes.

Koog supports a list of predefined attributes that follow OpenTelemetry's [Semantic conventions for generative AI events](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/). For example, the conventions define an attribute named `gen_ai.conversation.id`, which is usually a required attribute for a span. In Koog, the value of this attribute is the unique identifier for an agent run, that is automatically set when you call the `agent.run()` method.

In addition, Koog also includes custom, Koog-specific attributes. You can recognize most of these attributes by the `koog.` prefix. Here are the available custom attributes:

- `koog.strategy.name`: the name of the agent strategy. A strategy is a Koog-related entity that describes the purpose of the agent. Used in the `StrategySpan` span.
- `koog.node.id`: the identifier (name) of the node being executed. Used in the `NodeExecuteSpan` span.
- `koog.node.input`: the input passed to the node at the beginning of execution. Present on `NodeExecuteSpan` when node starts.
- `koog.node.output`: the output produced by the node upon completion. Present on `NodeExecuteSpan` when node completes successfully.
- `koog.subgraph.id`: the identifier (name) of the subgraph being executed. Used in the `SubgraphExecuteSpan` span.
- `koog.subgraph.input`: the input passed to the subgraph at the beginning of execution. Present on `SubgraphExecuteSpan` when subgraph starts.
- `koog.subgraph.output`: the output produced by the subgraph upon completion. Present on `SubgraphExecuteSpan` when subgraph completes successfully.

### Events

A span can also have an *event* attached to the span. Events describe a specific point in time when something relevant happened. For example, when an LLM call started or finished. Events also have attributes and additionally include event *body fields*.

The following event types are supported in line with OpenTelemetry's [Semantic conventions for generative AI events](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-events/):

- **SystemMessageEvent**: the system instructions passed to the model.
- **UserMessageEvent**: the user message passed to the model.
- **AssistantMessageEvent**: the assistant message passed to the model.
- **ToolMessageEvent**: the response from a tool or function call passed to the model.
- **ChoiceEvent**: the response message from a model.
- **ModerationResponseEvent**: the model moderation result or signal.

Note

The `optentelemetry-java` SDK does not support the event body fields parameter when adding an event. Therefore, in the OpenTelemetry support in Koog, event body fields are a separate attribute whose key is `body` and value type is string. The string includes the content or payload for the event body field, which is usually a JSON-like object. For examples of event body fields, see the [OpenTelemetry documentation](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-events/#examples). For the state of support for event body fields in `opentelemetry-java`, see the related [GitHub issue](https://github.com/open-telemetry/semantic-conventions/issues/1870).

## Exporters

Exporters send collected telemetry data to an OpenTelemetry Collector or other types of destinations or backend implementations. To add an exporter, use the `addSpanExporter()` method when installing the OpenTelemetry feature. The method takes the following argument:

| Name       | Data type    | Required | Default | Description                                                                 |
| ---------- | ------------ | -------- | ------- | --------------------------------------------------------------------------- |
| `exporter` | SpanExporter | Yes      |         | The SpanExporter instance to be added to the list of custom span exporters. |

The sections below provide information about some of the most commonly used exporters from the `opentelemetry-java` SDK.

Note

If you do not configure any custom exporters, Koog will use a console LoggingSpanExporter by default. This helps during local development and debugging.

### Logging exporter

A logging exporter that outputs trace information to the console. `LoggingSpanExporter` (`io.opentelemetry.exporter.logging.LoggingSpanExporter`) is a part of the `opentelemetry-java` SDK.

This type of export is useful for development and debugging purposes.

```kotlin
install(OpenTelemetry) {
    // Add the logging exporter
    addSpanExporter(LoggingSpanExporter.create())
    // Add more exporters as needed
}
```

### OpenTelemetry HTTP exporter

OpenTelemetry HTTP exporter (`OtlpHttpSpanExporter`) is a part of the `opentelemetry-java` SDK (`io.opentelemetry.exporter.otlp.http.trace.OtlpHttpSpanExporter`) and sends span data to a backend through HTTP.

```kotlin
install(OpenTelemetry) {
   // Add OpenTelemetry HTTP exporter 
   addSpanExporter(
      OtlpHttpSpanExporter.builder()
         // Set the maximum time to wait for the collector to process an exported batch of spans 
         .setTimeout(30, TimeUnit.SECONDS)
         // Set the OpenTelemetry endpoint to connect to
         .setEndpoint("http://localhost:3000/api/public/otel/v1/traces")
         // Add the authorization header
         .addHeader("Authorization", "Basic $AUTH_STRING")
         .build()
   )
}
```

### OpenTelemetry gRPC exporter

OpenTelemetry gRPC exporter (`OtlpGrpcSpanExporter`) is a part of the `opentelemetry-java` SDK (`io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter`). It exports telemetry data to a backend through gRPC and lets you define the host and port of the backend, collector, or endpoint that receives the data. The default port is `4317`.

```kotlin
install(OpenTelemetry) {
   // Add OpenTelemetry gRPC exporter 
   addSpanExporter(
      OtlpGrpcSpanExporter.builder()
          // Set the host and the port
         .setEndpoint("http://localhost:4317")
         .build()
   )
}
```

## Integration with Langfuse

Langfuse provides trace visualization and analytics for LLM/agent workloads.

You can configure Koog to export OpenTelemetry traces directly to Langfuse using a helper function:

```kotlin
install(OpenTelemetry) {
    addLangfuseExporter(
        langfuseUrl = "https://cloud.langfuse.com",
        langfusePublicKey = "...",
        langfuseSecretKey = "..."
    )
}
```

Please read the [full documentation](../opentelemetry-langfuse-exporter/) about integration with Langfuse.

## Integration with W&B Weave

W&B Weave provides trace visualization and analytics for LLM/agent workloads. Integration with W&B Weave can be configured via a predefined exporter:

```kotlin
install(OpenTelemetry) {
    addWeaveExporter(
        weaveOtelBaseUrl = "https://trace.wandb.ai",
        weaveEntity = "my-team",
        weaveProjectName = "my-project",
        weaveApiKey = "..."
    )
}
```

Please read the [full documentation](../opentelemetry-weave-exporter/) about integration with W&B Weave.

## Integration with Jaeger

Jaeger is a popular distributed tracing system that works with OpenTelemetry. The `opentelemetry` directory within `examples` in the Koog repository includes an example of using OpenTelemetry with Jaeger and Koog agents.

### Prerequisites

To test OpenTelemetry with Koog and Jaeger, start the Jaeger OpenTelemetry all-in-one process using the provided `docker-compose.yaml` file, by running the following command:

```bash
docker compose up -d
```

The provided Docker Compose YAML file includes the following content:

```yaml
# docker-compose.yaml
services:
  jaeger-all-in-one:
    image: jaegertracing/all-in-one:1.39
    container_name: jaeger-all-in-one
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "4317:4317"
      - "16686:16686"
```

To access the Jaeger UI and view your traces, open `http://localhost:16686`.

### Example

To export telemetry data for use in Jaeger, the example uses `LoggingSpanExporter` (`io.opentelemetry.exporter.logging.LoggingSpanExporter`) and `OtlpGrpcSpanExporter` (`io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter`) from the `opentelemetry-java` SDK.

Here is the full code sample:

```kotlin
fun main() {
    runBlocking {
        val agent = AIAgent(
            promptExecutor = simpleOpenAIExecutor(openAIApiKey),
            llmModel = OpenAIModels.Chat.O4Mini,
            systemPrompt = "You are a code assistant. Provide concise code examples."
        ) {
            install(OpenTelemetry) {
                // Add a console logger for local debugging
                addSpanExporter(LoggingSpanExporter.create())

                // Send traces to OpenTelemetry collector
                addSpanExporter(
                    OtlpGrpcSpanExporter.builder()
                        .setEndpoint("http://localhost:4317")
                        .build()
                )
            }
        }

        agent.use { agent ->
            println("Running the agent with OpenTelemetry tracing...")

            val result = agent.run("Tell me a joke about programming")

            println("Agent run completed with result: '$result'." +
                    "\nCheck Jaeger UI at http://localhost:16686 to view traces")
        }
    }
}
```

## Troubleshooting

### Common issues

1. **No traces appearing in Jaeger, Langfuse, or W&B Weave**

   - Ensure the service is running and the OpenTelemetry port (4317) is accessible.
   - Check that the OpenTelemetry exporter is configured with the correct endpoint.
   - Make sure to wait a few seconds after agent execution for traces to be exported.

1. **Missing spans or incomplete traces**

   - Verify that the agent execution completes successfully.
   - Ensure that you're not closing the application too quickly after agent execution.
   - Add a delay after agent execution to allow time for spans to be exported.

1. **Excessive number of spans**

   - Consider using a different sampling strategy by configuring the `sampler` property.
   - For example, use `Sampler.traceIdRatioBased(0.1)` to sample only 10% of traces.

1. **Span adapters override each other**

   - Currently, the OpenTelemetry agent feature does not support applying multiple span adapters [KG-265](https://youtrack.jetbrains.com/issue/KG-265/Adding-Weave-exporter-breaks-Langfuse-exporter).

# Langfuse exporter

Koog provides built-in support for exporting agent traces to [Langfuse](https://langfuse.com/), a platform for observability and analytics of AI applications. With Langfuse integration, you can visualize, analyze, and debug how your Koog agents interact with LLMs, APIs, and other components.

For background on Koog's OpenTelemetry support, see the [OpenTelemetry support](https://docs.koog.ai/opentelemetry-support/).

______________________________________________________________________

## Setup instructions

1. Create a Langfuse project. Follow the setup guide at [Create new project in Langfuse](https://langfuse.com/docs/get-started#create-new-project-in-langfuse)
1. Get API credentials. Retrieve your Langfuse `public key` and `secret key` as described in [Where are Langfuse API keys?](https://langfuse.com/faq/all/where-are-langfuse-api-keys)
1. Pass the Langfuse host, private key, and secret key to the Langfuse exporter. This can be done by providing them as parameters to the `addLangfuseExporter()` function, or by setting environment variables as shown below:

```bash
   export LANGFUSE_HOST="https://cloud.langfuse.com"
   export LANGFUSE_PUBLIC_KEY="<your-public-key>"
   export LANGFUSE_SECRET_KEY="<your-secret-key>"
```

## Configuration

To enable Langfuse export, install the **OpenTelemetry feature** and add the `LangfuseExporter`.\
The exporter uses `OtlpHttpSpanExporter` under the hood to send traces to Langfuseâ€™s OpenTelemetry endpoint.

### Example: agent with Langfuse tracing

```kotlin
fun main() = runBlocking {
    val apiKey = "api-key"

    val agent = AIAgent(
        promptExecutor = simpleOpenAIExecutor(apiKey),
        llmModel = OpenAIModels.Chat.GPT4oMini,
        systemPrompt = "You are a code assistant. Provide concise code examples."
    ) {
        install(OpenTelemetry) {
            addLangfuseExporter()
        }
    }

    println("Running agent with Langfuse tracing")

    val result = agent.run("Tell me a joke about programming")

    println("Result: $result\nSee traces on the Langfuse instance")
}
```

## Trace attributes

Langfuse uses trace-level attributes to enhance observability with features like sessions, environments, tags and other metadata. The `addLangfuseExporter` function supports a `traceAttributes` parameter that accepts a list of `CustomAttribute` objects.

These attributes are added to the root `InvokeAgentSpan` span of each trace and enable Langfuse's advanced features. You can pass any attributes supported by Langfuse - see the [complete list in Langfuse's OpenTelemetry documentation](https://langfuse.com/integrations/native/opentelemetry#trace-level-attributes).

Common attributes:

- **Sessions** (`langfuse.session.id`): Group related traces for aggregated metrics, cost analysis, and scoring
- **Environments**: Isolate production traces from development and staging for cleaner analysis
- **Tags** (`langfuse.trace.tags`): Label traces with feature names, experiment IDs, or customer segments (array of strings)

### Example with session and tags

```kotlin
fun main() = runBlocking {
    val apiKey = "api-key"
    val sessionId = UUID.randomUUID().toString()

    val agent = AIAgent(
        promptExecutor = simpleOpenAIExecutor(apiKey),
        llmModel = OpenAIModels.Chat.GPT4oMini,
        systemPrompt = "You are a helpful assistant."
    ) {
        install(OpenTelemetry) {
            addLangfuseExporter(
                traceAttributes = listOf(
                    CustomAttribute("langfuse.session.id", sessionId),
                    CustomAttribute("langfuse.trace.tags", listOf("chat", "kotlin", "production"))
                )
            )
        }
    }

    // Multiple runs with the same session ID will be grouped in Langfuse
    agent.run("What is Kotlin?")
    agent.run("Show me a coroutine example")
}
```

## What gets traced

When enabled, the Langfuse exporter captures the same spans as Koogâ€™s general OpenTelemetry integration, including:

- **Agent lifecycle events**: agent start, stop, errors
- **LLM interactions**: prompts, responses, token usage, latency
- **Tool calls**: execution traces for tool invocations
- **System context**: metadata such as model name, environment, Koog version

Koog also captures span attributes required by Langfuse to show [Agent Graphs](https://langfuse.com/docs/observability/features/agent-graphs).

For security reasons, some content of OpenTelemetry spans is masked by default. To make the content available in Langfuse, use the [setVerbose](../opentelemetry-support/#setverbose) method in the OpenTelemetry configuration and set its `verbose` argument to `true` as follows:

```kotlin
install(OpenTelemetry) {
    addLangfuseExporter()
    setVerbose(true)
}
```

When visualized in Langfuse, the trace appears as follows:

For more details on Langfuse OpenTelemetry tracing, see:\
[Langfuse OpenTelemetry Docs](https://langfuse.com/integrations/native/opentelemetry#opentelemetry-endpoint).

______________________________________________________________________

## Troubleshooting

### No traces appear in Langfuse

- Double-check that `LANGFUSE_HOST`, `LANGFUSE_PUBLIC_KEY`, and `LANGFUSE_SECRET_KEY` are set in your environment.
- If running on self-hosted Langfuse, confirm that the `LANGFUSE_HOST` is reachable from your application environment.
- Verify that the public/secret key pair belongs to the correct project.

# W&B Weave exporter

Koog provides built-in support for exporting agent traces to [W&B Weave](https://wandb.ai/site/weave/), a developer tool from Weights & Biases for observability and analytics of AI applications.\
With the Weave integration, you can capture prompts, completions, system context, and execution traces and visualize them directly in your W&B workspace.

For background on Koogâ€™s OpenTelemetry support, see the [OpenTelemetry support](https://docs.koog.ai/opentelemetry-support/).

______________________________________________________________________

## Setup instructions

1. Get up a W&B account at <https://wandb.ai>
1. Get your API key from <https://wandb.ai/authorize>.
1. Find your entity name by visiting your W&B dashboard at <https://wandb.ai/home>. Your entity is usually your username if it's a personal account or your team/org name.
1. Define a name for your project. You don't have to create a project beforehand, it will be created automatically when the first trace is sent.
1. Pass the Weave entity, project name, and API key to the Weave exporter. This can be done by providing them as parameters to the `addWeaveExporter()` function, or by setting environment variables as shown below:

```bash
export WEAVE_API_KEY="<your-api-key>"
export WEAVE_ENTITY="<your-entity>"
export WEAVE_PROJECT_NAME="koog-tracing"
```

## Configuration

To enable Weave export, install the **OpenTelemetry feature** and add the `WeaveExporter`.\
The exporter uses Weaveâ€™s OpenTelemetry endpoint via `OtlpHttpSpanExporter`.

### Example: agent with Weave tracing

```kotlin
fun main() = runBlocking {
    val apiKey = "api-key"
    val entity = System.getenv()["WEAVE_ENTITY"] ?: throw IllegalArgumentException("WEAVE_ENTITY is not set")
    val projectName = System.getenv()["WEAVE_PROJECT_NAME"] ?: "koog-tracing"

    val agent = AIAgent(
        promptExecutor = simpleOpenAIExecutor(apiKey),
        llmModel = OpenAIModels.Chat.GPT4oMini,
        systemPrompt = "You are a code assistant. Provide concise code examples."
    ) {
        install(OpenTelemetry) {
            addWeaveExporter()
        }
    }

    println("Running agent with Weave tracing")

    val result = agent.run("Tell me a joke about programming")

    println("Result: $result\nSee traces on https://wandb.ai/$entity/$projectName/weave/traces")
}
```

## What gets traced

When enabled, the Weave exporter captures the same spans as Koogâ€™s general OpenTelemetry integration, including:

- **Agent lifecycle events**: agent start, stop, errors
- **LLM interactions**: prompts, completions, latency
- **Tool calls**: execution traces for tool invocations
- **System context**: metadata such as model name, environment, Koog version

For security reasons, some content of OpenTelemetry spans is masked by default. To make the content available in Weave, use the [setVerbose](../opentelemetry-support/#setverbose) method in the OpenTelemetry configuration and set its `verbose` argument to `true` as follows:

```kotlin
install(OpenTelemetry) {
    addWeaveExporter()
    setVerbose(true)
}
```

When visualized in W&B Weave, the trace appears as follows:

For more details, see the official [Weave OpenTelemetry Docs](https://weave-docs.wandb.ai/guides/tracking/otel/).

______________________________________________________________________

## Troubleshooting

### No traces appear in Weave

- Confirm that `WEAVE_API_KEY`, `WEAVE_ENTITY`, and `WEAVE_PROJECT_NAME` are set in your environment.
- Ensure that your W&B account has access to the specified entity and project.

### Authentication errors

- Check that your `WEAVE_API_KEY` is valid.
- API key must have permission to write traces for the selected entity.

### Connection issues

- Make sure your environment has network access to W&Bâ€™s OpenTelemetry ingestion endpoints.
# Testing

# Testing

## Overview

The Testing feature provides a comprehensive framework for testing AI agent pipelines, subgraphs, and tool interactions in the Koog framework. It enables developers to create controlled test environments with mock LLM (Large Language Model) executors, tool registries, and agent environments.

### Purpose

The primary purpose of this feature is to facilitate testing of agent-based AI features by:

- Mocking LLM responses to specific prompts
- Simulating tool calls and their results
- Testing agent pipeline subgraphs and their structures
- Verifying the correct flow of data through agent nodes
- Providing assertions for expected behaviors

## Configuration and initialization

### Setting up test dependencies

Before setting up a test environment, make sure that you have added the following dependencies:

```kotlin
// build.gradle.kts
dependencies {
   testImplementation("ai.koog:agents-test:LATEST_VERSION")
   testImplementation(kotlin("test"))
}
```

### Mocking LLM responses

The basic form of testing involves mocking LLM responses to ensure deterministic behavior. You can do this using `MockLLMBuilder` and related utilities.

```kotlin
// Create a mock LLM executor
val mockLLMApi = getMockExecutor(toolRegistry) {
  // Mock a simple text response
  mockLLMAnswer("Hello!") onRequestContains "Hello"

  // Mock a default response
  mockLLMAnswer("I don't know how to answer that.").asDefaultResponse
}
```

### Mocking tool calls

You can mock the LLM to call specific tools based on input patterns:

```kotlin
// Mock a tool call response
mockLLMToolCall(CreateTool, CreateTool.Args("solve")) onRequestEquals "Solve task"

// Mock tool behavior - simplest form without lambda
mockTool(PositiveToneTool) alwaysReturns "The text has a positive tone."

// Using lambda when you need to perform extra actions
mockTool(NegativeToneTool) alwaysTells {
  // Perform some extra action
  println("Negative tone tool called")

  // Return the result
  "The text has a negative tone."
}

// Mock tool behavior based on specific arguments
mockTool(AnalyzeTool) returns "Detailed analysis" onArguments AnalyzeTool.Args("analyze deeply")

// Mock tool behavior with conditional argument matching
mockTool(SearchTool) returns "Found results" onArgumentsMatching { args ->
  args.query.contains("important")
}
```

The examples above demonstrate different ways to mock tools, from simple to more complex ones:

1. `alwaysReturns`: the simplest form, directly returns a value without a lambda.
1. `alwaysTells`: uses a lambda when you need to perform additional actions.
1. `returns...onArguments`: returns specific results for exact argument matches.
1. `returns...onArgumentsMatching`: returns results based on custom argument conditions.

### Enabling testing mode

To enable the testing mode on an agent, use the `withTesting()` function within the AIAgent constructor block:

```kotlin
// Create the agent with testing enabled
AIAgent(
    promptExecutor = mockLLMApi,
    toolRegistry = toolRegistry,
    llmModel = llmModel
) {
    // Enable testing mode
    withTesting()
}
```

## Advanced testing

### Testing the graph structure

Before testing the detailed node behavior and edge connections, it is important to verify the overall structure of your agent's graph. This includes checking that all required nodes exist and are properly connected in the expected subgraphs.

The Testing feature provides a comprehensive way to test your agent's graph structure. This approach is particularly valuable for complex agents with multiple subgraphs and interconnected nodes.

#### Basic structure testing

Start by validating the fundamental structure of your agent's graph:

```kotlin
AIAgent(
    // Constructor arguments
    promptExecutor = mockLLMApi,
    toolRegistry = toolRegistry,
    llmModel = llmModel
) {
    testGraph<String, String>("test") {
        val firstSubgraph = assertSubgraphByName<String, String>("first")
        val secondSubgraph = assertSubgraphByName<String, String>("second")

        // Assert subgraph connections
        assertEdges {
            startNode() alwaysGoesTo firstSubgraph
            firstSubgraph alwaysGoesTo secondSubgraph
            secondSubgraph alwaysGoesTo finishNode()
        }

        // Verify the first subgraph
        verifySubgraph(firstSubgraph) {
            val start = startNode()
            val finish = finishNode()

            // Assert nodes by name
            val askLLM = assertNodeByName<String, Message.Response>("callLLM")
            val callTool = assertNodeByName<Message.Tool.Call, ReceivedToolResult>("executeTool")

            // Assert node reachability
            assertReachable(start, askLLM)
            assertReachable(askLLM, callTool)
        }
    }
}
```

### Testing node behavior

Node behavior testing lets you verify that nodes in your agent's graph produce the expected outputs for the given inputs. This is crucial for ensuring that your agent's logic works correctly under different scenarios.

#### Basic node testing

Start with simple input and output validations for individual nodes:

```kotlin
assertNodes {

    // Test basic text responses
    askLLM withInput "Hello" outputs assistantMessage("Hello!")

    // Test tool call responses
    askLLM withInput "Solve task" outputs toolCallMessage(CreateTool, CreateTool.Args("solve"))
}
```

The example above shows how to test the following behavior:

1. When the LLM node receives `Hello` as the input, it responds with a simple text message.
1. When it receives `Solve task`, it responds with a tool call.

#### Testing tool run nodes

You can also test nodes that run tools:

```kotlin
assertNodes {
    // Test tool runs with specific arguments
    callTool withInput toolCallMessage(
        SolveTool,
        SolveTool.Args("solve")
    ) outputs toolResult(SolveTool, SolveTool.Args("solve"), "solved")
}
```

This verifies that when the tool execution node receives a specific tool call signature, it produces the expected tool result.

#### Advanced node testing

For more complex scenarios, you can test nodes with structured inputs and outputs:

```kotlin
assertNodes {
    // Test with different inputs to the same node
    askLLM withInput "Simple query" outputs assistantMessage("Simple response")

    // Test with complex parameters
    askLLM withInput "Complex query with parameters" outputs toolCallMessage(
        AnalyzeTool,
        AnalyzeTool.Args(query = "parameters", depth = 3)
    )
}
```

You can also test complex tool call scenarios with detailed result structures:

```kotlin
assertNodes {
    // Test a complex tool call with a structured result
    callTool withInput toolCallMessage(
        AnalyzeTool,
        AnalyzeTool.Args(query = "complex", depth = 5)
    ) outputs toolResult(AnalyzeTool, AnalyzeTool.Args(query = "complex", depth = 5), AnalyzeTool.Result(
        analysis = "Detailed analysis",
        confidence = 0.95,
        metadata = mapOf("source" to "database", "timestamp" to "2023-06-15")
    ))
}
```

These advanced tests help ensure that your nodes handle complex data structures correctly, which is essential for sophisticated agent behaviors.

### Testing edge connections

Edge connections testing allows you to verify that your agent's graph correctly routes outputs from one node to the appropriate next node. This ensures that your agent follows the intended workflow paths based on different outputs.

#### Basic edge testing

Start with simple edge connection tests:

```kotlin
assertEdges {
    // Test text message routing
    askLLM withOutput assistantMessage("Hello!") goesTo giveFeedback

    // Test tool call routing
    askLLM withOutput toolCallMessage(CreateTool, CreateTool.Args("solve")) goesTo callTool
}
```

This example verifies the following behavior:

1. When the LLM node outputs a simple text message, the flow is directed to the `giveFeedback` node.
1. When it outputs a tool call, the flow is directed to the `callTool` node.

#### Testing conditional routing

You can test a more complex routing logic based on the content of outputs:

```kotlin
assertEdges {
    // Different text responses can route to different nodes
    askLLM withOutput assistantMessage("Need more information") goesTo askForInfo
    askLLM withOutput assistantMessage("Ready to proceed") goesTo processRequest
}
```

#### Advanced edge testing

For sophisticated agents, you can test conditional routing based on structured data in tool results:

```kotlin
assertEdges {
    // Test routing based on tool result content
    callTool withOutput toolResult(
        AnalyzeTool,
        AnalyzeTool.Args(query = "parameters", depth = 3),
        AnalyzeTool.Result(analysis = "Needs more processing", confidence = 0.5)
    ) goesTo processResult
}
```

You can also test complex decision paths based on different result properties:

```kotlin
assertEdges {
    // Route to different nodes based on confidence level
    callTool withOutput toolResult(
        AnalyzeTool,
        AnalyzeTool.Args(query = "parameters", depth = 3),
        AnalyzeTool.Result(analysis = "Complete", confidence = 0.9)
    ) goesTo finish

    callTool withOutput toolResult(
        AnalyzeTool,
        AnalyzeTool.Args(query = "parameters", depth = 3),
        AnalyzeTool.Result(analysis = "Uncertain", confidence = 0.3)
    ) goesTo verifyResult
}
```

These advanced edge tests help ensure that your agent makes the correct decisions based on the content and structure of node outputs, which is essential for creating intelligent, context-aware workflows.

## Complete testing example

Here is a user story that demonstrates a complete testing scenario:

You are developing a tone analysis agent that analyzes the tone of the text and provides feedback. The agent uses tools for detecting positive, negative, and neutral tones.

Here is how you can test this agent:

```kotlin
@Test
fun testToneAgent() = runTest {
    // Create a list to track tool calls
    var toolCalls = mutableListOf<String>()
    var result: String? = null

    // Create a tool registry
    val toolRegistry = ToolRegistry {
        // A special tool, required with this type of agent
        tool(SayToUser)

        with(ToneTools) {
            tools()
        }
    }

    // Create an event handler
    val eventHandler = EventHandler {
        onToolCallStarting { tool, args ->
            println("[DEBUG_LOG] Tool called: tool ${tool.name}, args $args")
            toolCalls.add(tool.name)
        }

        handleError {
            println("[DEBUG_LOG] An error occurred: ${it.message}\n${it.stackTraceToString()}")
            true
        }

        handleResult {
            println("[DEBUG_LOG] Result: $it")
            result = it
        }
    }

    val positiveText = "I love this product!"
    val negativeText = "Awful service, hate the app."
    val defaultText = "I don't know how to answer this question."

    val positiveResponse = "The text has a positive tone."
    val negativeResponse = "The text has a negative tone."
    val neutralResponse = "The text has a neutral tone."

    val mockLLMApi = getMockExecutor(toolRegistry, eventHandler) {
        // Set up LLM responses for different input texts
        mockLLMToolCall(NeutralToneTool, ToneTool.Args(defaultText)) onRequestEquals defaultText
        mockLLMToolCall(PositiveToneTool, ToneTool.Args(positiveText)) onRequestEquals positiveText
        mockLLMToolCall(NegativeToneTool, ToneTool.Args(negativeText)) onRequestEquals negativeText

        // Mock the behavior where the LLM responds with just tool responses when the tools return results
        mockLLMAnswer(positiveResponse) onRequestContains positiveResponse
        mockLLMAnswer(negativeResponse) onRequestContains negativeResponse
        mockLLMAnswer(neutralResponse) onRequestContains neutralResponse

        mockLLMAnswer(defaultText).asDefaultResponse

        // Tool mocks
        mockTool(PositiveToneTool) alwaysTells {
            toolCalls += "Positive tone tool called"
            positiveResponse
        }
        mockTool(NegativeToneTool) alwaysTells {
            toolCalls += "Negative tone tool called"
            negativeResponse
        }
        mockTool(NeutralToneTool) alwaysTells {
            toolCalls += "Neutral tone tool called"
            neutralResponse
        }
    }

    // Create a strategy
    val strategy = toneStrategy("tone_analysis")

    // Create an agent configuration
    val agentConfig = AIAgentConfig(
        prompt = prompt("test-agent") {
            system(
                """
                You are an question answering agent with access to the tone analysis tools.
                You need to answer 1 question with the best of your ability.
                Be as concise as possible in your answers.
                DO NOT ANSWER ANY QUESTIONS THAT ARE BESIDES PERFORMING TONE ANALYSIS!
                DO NOT HALLUCINATE!
            """.trimIndent()
            )
        },
        model = mockk<LLModel>(relaxed = true),
        maxAgentIterations = 10
    )

    // Create an agent with testing enabled
    val agent = AIAgent(
        promptExecutor = mockLLMApi,
        toolRegistry = toolRegistry,
        strategy = strategy,
        eventHandler = eventHandler,
        agentConfig = agentConfig,
    ) {
        withTesting()
    }

    // Test the positive text
    agent.run(positiveText)
    assertEquals("The text has a positive tone.", result, "Positive tone result should match")
    assertEquals(1, toolCalls.size, "One tool is expected to be called")

    // Test the negative text
    agent.run(negativeText)
    assertEquals("The text has a negative tone.", result, "Negative tone result should match")
    assertEquals(2, toolCalls.size, "Two tools are expected to be called")

    //Test the neutral text
    agent.run(defaultText)
    assertEquals("The text has a neutral tone.", result, "Neutral tone result should match")
    assertEquals(3, toolCalls.size, "Three tools are expected to be called")
}
```

For more complex agents with multiple subgraphs, you can also test the graph structure:

```kotlin
@Test
fun testMultiSubgraphAgentStructure() = runTest {
    val strategy = strategy("test") {
        val firstSubgraph by subgraph(
            "first",
            tools = listOf(DummyTool, CreateTool, SolveTool)
        ) {
            val callLLM by nodeLLMRequest(allowToolCalls = false)
            val executeTool by nodeExecuteTool()
            val sendToolResult by nodeLLMSendToolResult()
            val giveFeedback by node<String, String> { input ->
                llm.writeSession {
                    appendPrompt {
                        user("Call tools! Don't chat!")
                    }
                }
                input
            }

            edge(nodeStart forwardTo callLLM)
            edge(callLLM forwardTo executeTool onToolCall { true })
            edge(callLLM forwardTo giveFeedback onAssistantMessage { true })
            edge(giveFeedback forwardTo giveFeedback onAssistantMessage { true })
            edge(giveFeedback forwardTo executeTool onToolCall { true })
            edge(executeTool forwardTo nodeFinish transformed { it.content })
        }

        val secondSubgraph by subgraph<String, String>("second") {
            edge(nodeStart forwardTo nodeFinish)
        }

        edge(nodeStart forwardTo firstSubgraph)
        edge(firstSubgraph forwardTo secondSubgraph)
        edge(secondSubgraph forwardTo nodeFinish)
    }

    val toolRegistry = ToolRegistry {
        tool(DummyTool)
        tool(CreateTool)
        tool(SolveTool)
    }

    val mockLLMApi = getMockExecutor(toolRegistry) {
        mockLLMAnswer("Hello!") onRequestContains "Hello"
        mockLLMToolCall(CreateTool, CreateTool.Args("solve")) onRequestEquals "Solve task"
    }

    val basePrompt = prompt("test") {}

    AIAgent(
        toolRegistry = toolRegistry,
        strategy = strategy,
        eventHandler = EventHandler {},
        agentConfig = AIAgentConfig(prompt = basePrompt, model = OpenAIModels.Chat.GPT4o, maxAgentIterations = 100),
        promptExecutor = mockLLMApi,
    ) {
        testGraph("test") {
            val firstSubgraph = assertSubgraphByName<String, String>("first")
            val secondSubgraph = assertSubgraphByName<String, String>("second")

            assertEdges {
                startNode() alwaysGoesTo firstSubgraph
                firstSubgraph alwaysGoesTo secondSubgraph
                secondSubgraph alwaysGoesTo finishNode()
            }

            verifySubgraph(firstSubgraph) {
                val start = startNode()
                val finish = finishNode()

                val askLLM = assertNodeByName<String, Message.Response>("callLLM")
                val callTool = assertNodeByName<Message.Tool.Call, ReceivedToolResult>("executeTool")
                val giveFeedback = assertNodeByName<Any?, Any?>("giveFeedback")

                assertReachable(start, askLLM)
                assertReachable(askLLM, callTool)

                assertNodes {
                    askLLM withInput "Hello" outputs Message.Assistant("Hello!")
                    askLLM withInput "Solve task" outputs toolCallMessage(CreateTool, CreateTool.Args("solve"))

                    callTool withInput toolCallSignature(
                        SolveTool,
                        SolveTool.Args("solve")
                    ) outputs toolResult(SolveTool, "solved")

                    callTool withInput toolCallSignature(
                        CreateTool,
                        CreateTool.Args("solve")
                    ) outputs toolResult(CreateTool, "created")
                }

                assertEdges {
                    askLLM withOutput Message.Assistant("Hello!") goesTo giveFeedback
                    askLLM withOutput toolCallMessage(CreateTool, CreateTool.Args("solve")) goesTo callTool
                }
            }
        }
    }
}
```

## API reference

For a complete API reference related to the Testing feature, see the reference documentation for the [agents-test](https://api.koog.ai/agents/agents-test/index.html) module.

## FAQ and troubleshooting

#### How do I mock a specific tool response?

Use the `mockTool` method in `MockLLMBuilder`:

```kotlin
val mockExecutor = getMockExecutor {
    mockTool(myTool) alwaysReturns myResult

    // Or with conditions
    mockTool(myTool) returns myResult onArguments myArgs
}
```

#### How can I test complex graph structures?

Use the subgraph assertions, `verifySubgraph`, and node references:

```kotlin
testGraph<Unit, String>("test") {
    val mySubgraph = assertSubgraphByName<Unit, String>("mySubgraph")

    verifySubgraph(mySubgraph) {
        // Get references to nodes
        val nodeA = assertNodeByName<Unit, String>("nodeA")
        val nodeB = assertNodeByName<String, String>("nodeB")

        // Assert reachability
        assertReachable(nodeA, nodeB)

        // Assert edge connections
        assertEdges {
            nodeA.withOutput("result") goesTo nodeB
        }
    }
}
```

#### How do I simulate different LLM responses based on input?

Use pattern matching methods:

```kotlin
getMockExecutor {
    mockLLMAnswer("Response A") onRequestContains "topic A"
    mockLLMAnswer("Response B") onRequestContains "topic B"
    mockLLMAnswer("Exact response") onRequestEquals "exact question"
    mockLLMAnswer("Conditional response") onCondition { it.contains("keyword") && it.length > 10 }
}
```

### Troubleshooting

#### Mock executor always returns the default response

Check that your pattern matching is correct. Patterns are case-sensitive and must match exactly as specified.

#### Tool calls are not being intercepted

Ensure that:

1. The tool registry is properly set up.
1. The tool names match exactly.
1. The tool actions are configured correctly.

#### Graph assertions are failing

1. Verify that node names are correct.
1. Check that the graph structure matches your expectations.
1. Use the `startNode()` and `finishNode()` methods to get the correct entry and exit points.
# Why Koog

# Why Koog

Koog is designed to solve real-world problems with JetBrains-level quality. It provides advanced AI algorithms, out-of-the-box proven techniques, a Kotlin DSL, and robust multi-platform support that goes beyond traditional frameworks.

## Integration with JVM and Kotlin applications

Koog provides a Kotlin Domain-Specific Language (DSL) designed specifically for JVM and Kotlin developers. This ensures smooth integration into Kotlin and Java-based applications, significantly improving productivity and enhancing the overall developer experience.

## Real-world validation with JetBrains products

Koog powers multiple JetBrains products, including internal AI agents. This real-world integration ensures that Koog is continuously tested, refined, and validated against practical use cases. It focuses on what works in practice, incorporating insights from extensive feedback and real-product scenarios. This integration provides Koog with strengths that distinguish it from other frameworks.

## Advanced solutions available out of the box

Koog includes pre-built, composable solutions to simplify and speed up the development of agentic systems, setting it apart from frameworks that only offer basic components:

- **Multiple history compression strategies.** Koog comes with advanced strategies to compress and manage long-running conversations out of the box, eliminating the need for manual experimentation with approaches. With fine-tuned prompts, techniques, and algorithms tested and refined by ML engineers, you can rely on proven methods to improve performance. For more details on compression strategies, refer to [History compression](https://docs.koog.ai/history-compression/). To explore how Koog handles compression and context management in real-world scenarios, check out [this article](https://blog.jetbrains.com/ai/2025/07/when-tool-calling-becomes-an-addiction-debugging-llm-patterns-in-koog/).
- **Seamless LLM switching.** You can switch a conversation to a different large language model (LLM) with a new set of available tools at any point without losing the existing conversation history. Koog automatically rewrites the history and handles unavailable tools, enabling smooth transitions and a natural interaction flow.
- **Advanced persistence.** Koog lets you restore full agent state machines instead of just chat messages. This enables features like checkpoints, failure recovery, and even the ability to revert to any point in the execution of the state machine.
- **Robust retry component.** Koog includes a retry mechanism that lets you wrap any set of operations within your agentic system and retry them until they meet configurable conditions. You can provide feedback and adjust each attempt to ensure reliable results. If LLM calls time out, tools do not work as expected, or there are network issues, Koog ensures that your agent remains resilient and performs effectively, even during temporary failures. For more technical details, see [Retry functionality](https://docs.koog.ai/history-compression/).
- **Structured typed streaming with Markdown DSL.** Koog streams LLM output and parses it into structured, typed events using a Markdown DSL. You can register handlers for specific elements like headers, bullet points, or regex patterns and receive only the relevant parts in real-time. This approach provides human-readable feedback using Markdown and machine-parsable data using structured typing, effectively eliminating the lack of transparency and enhancing the user experience. It ensures predictable output and dynamic user interfaces with progressive content rendering.

## Broad integration, multiplatform support, enhanced observability

Koog supports the development and deployment of agentic applications across a variety of platforms and environments:

- **Multiplatform support**. You can deploy your agentic applications across JVM, JS, WasmJS, Android, and iOS targets.
- **Broad AI integration**. Koog integrates with major LLM providers, including OpenAI and Anthropic, as well as enterprise-level AI clouds like Bedrock. It also supports local models such as Ollama. For the full list of available providers, see [LLM providers](https://docs.koog.ai/llm-providers/).
- **OpenTelemetry support**. Koog provides out-of-the-box integration with popular observability providers like [W&B Weave](https://wandb.ai/site/weave/) and [Langfuse](https://langfuse.com/) for monitoring and debugging AI applications. With native OpenTelemetry support, you can trace, log, and measure your agents using the same tools you already use in your system. To learn more, refer to [OpenTelemetry](https://docs.koog.ai/opentelemetry-support/).
- **Spring Boot and Ktor integrations**. Koog integrates with widely used enterprise environments.
  - If you have a Ktor server, you can install Koog as a plugin, configure providers using configuration files, and call agents directly from any route without manually connecting LLM clients.
  - For Spring Boot, Koog provides ready-to-use beans and auto-configured LLM clients, making it easy to start building AI-powered workflows.

## Collaboration with ML engineers and product teams

A unique advantage of Koog is its direct collaboration with JetBrains ML engineers and product teams. This ensures that features built with Koog are not just theoretical but tested and refined based on real-world product requirements. This means that Koog incorporates:

- **Fine-tuned prompts and strategies** optimized for real-world performance.
- **Proven engineering approaches** discovered and validated through product development, such as its unique history compression strategies. You can learn more in [this detailed article](https://blog.jetbrains.com/ai/2025/07/when-tool-calling-becomes-an-addiction-debugging-llm-patterns-in-koog/).
- **Continuous improvements** that help Koog stay efficient and adaptable to evolving needs.

## Commitment to the developer community

The Koog team is deeply committed to building a strong developer community. By actively gathering and incorporating feedback, Koog evolves to meet the needs of developers effectively. We are actively expanding support for diverse AI architectures, comprehensive benchmarks, detailed use-case guides, and educational resources to empower developers.

## Where to start

- Explore Koog capabilities in [Overview](https://docs.koog.ai/).
- Build your first Koog agent with our [Getting started](https://docs.koog.ai/getting-started/) guide.
- See the latest updates in Koog [release notes](https://github.com/JetBrains/koog/blob/main/CHANGELOG.md).
- Learn from [Examples](https://docs.koog.ai/examples/).
# Examples

# Examples

The Koog framework provides examples to help you understand how to implement AI agents for different use cases. These examples demonstrate key features and patterns that you can adapt for your own applications.

Browse the examples below and click on the links to view the source code on GitHub.

| Example                                                                                                                             | Description                                                                                                                                                                                                                                                      |
| ----------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Attachments](Attachments/)                                                                                                         | Learn how to use structured Markdown and attachments in prompts. Build prompts that include images and generate creative content for Instagram posts using OpenAI models.                                                                                        |
| [Banking](Banking/)                                                                                                                 | Build a comprehensive AI banking assistant with routing capabilities that can handle money transfers and transaction analysis through a sophisticated graph-based strategy. Includes domain modeling, tool creation, and agent composition patterns.             |
| [BedrockAgent](BedrockAgent/)                                                                                                       | Create intelligent AI agents using the Koog framework with AWS Bedrock integration. Learn how to define custom tools, set up AWS Bedrock, and build interactive agents that understand natural language commands for controlling devices.                        |
| [Calculator](Calculator/)                                                                                                           | Build a calculator agent that performs arithmetic operations using tools for addition, subtraction, multiplication, and division. Demonstrates parallel tool calls, event logging, and multiple executor support (OpenAI and Ollama).                            |
| [Chess](Chess/)                                                                                                                     | Build an intelligent chess-playing agent featuring complex domain modeling, custom tools, memory optimization techniques, and interactive choice selection. Demonstrates advanced agent strategies, game state management, and human-AI collaboration patterns.  |
| [GoogleMapsMcp](GoogleMapsMcp/)                                                                                                     | Connect Koog to a Google Maps MCP server via Docker. Discover tools, geocode addresses, and fetch elevation data using AI agents with real-world geographic APIs in a Kotlin Notebook environment.                                                               |
| [Guesser](Guesser/)                                                                                                                 | Build a number-guessing agent that implements a binary search strategy using tools to ask targeted questions. The agent efficiently narrows down the user's number through strategic questioning and demonstrates tool-based interaction patterns.               |
| [Langfuse](Langfuse/)                                                                                                               | Learn how to export Koog agent traces to Langfuse using OpenTelemetry. Set up environment variables, run agents, and inspect spans and traces in your Langfuse instance for comprehensive observability.                                                         |
| [MCP](https://github.com/JetBrains/koog/tree/develop/examples/src/main/kotlin/ai/koog/agents/example/mcp)                           | Integration examples for the Model Context Protocol, featuring GoogleMapsMcpClient for geographic data and PlaywrightMcpClient for browser automation.                                                                                                           |
| [Memory](https://github.com/JetBrains/koog/tree/develop/examples/src/main/kotlin/ai/koog/agents/example/memory)                     | A customer support agent that demonstrates memory system usage. The agent tracks user conversation preferences, device diagnostics, and organization-specific information using encrypted local storage and proper memory organization with subjects and scopes. |
| [OpenTelemetry](OpenTelemetry/)                                                                                                     | Add OpenTelemetry-based tracing to Koog AI agents. Learn to emit spans to console for debugging and export traces to OpenTelemetry Collector for viewing in Jaeger. Includes Docker setup and troubleshooting guide.                                             |
| [Planner](https://github.com/JetBrains/koog/tree/develop/examples/src/main/kotlin/ai/koog/agents/example/planner)                   | A task planning system that builds execution trees with parallel and sequential execution nodes, dynamically constructing execution plans for complex workflows.                                                                                                 |
| [PlaywrightMcp](PlaywrightMcp/)                                                                                                     | Drive browsers with Playwright MCP and Koog. Launch a Playwright MCP server, connect via SSE, and let AI agents automate web tasks like navigation, cookie acceptance, and UI interaction through natural language commands.                                     |
| [SimpleAPI](https://github.com/JetBrains/koog/tree/develop/examples/src/main/kotlin/ai/koog/agents/example/simpleapi)               | Examples demonstrating chat agents and basic agents with simple API patterns for getting started with Koog.                                                                                                                                                      |
| [StructuredData](https://github.com/JetBrains/koog/tree/develop/examples/src/main/kotlin/ai/koog/agents/example/structureddata)     | Demonstrates JSON-based structured data output with complex nested classes, polymorphism, and weather forecast examples showing how to work with typed data in agent responses.                                                                                  |
| [SubgraphWithTask](https://github.com/JetBrains/koog/tree/develop/examples/src/main/kotlin/ai/koog/agents/example/subgraphwithtask) | Project generation tools showcasing file and directory operations, including creation, deletion, and command execution using subgraph strategies.                                                                                                                |
| [Tone](https://github.com/JetBrains/koog/tree/develop/examples/src/main/kotlin/ai/koog/agents/example/tone)                         | A text tone analysis agent that uses specialized tools to identify positive, negative, or neutral tones in input text, demonstrating sentiment analysis capabilities.                                                                                            |
| [UnityMcp](UnityMcp/)                                                                                                               | Drive Unity game development with AI agents using Unity MCP server integration. Connect to Unity via stdio, discover available tools, and let agents modify scenes, place objects, and execute game development tasks through natural language commands.         |
| [VaccumAgent](VaccumAgent/)                                                                                                         | Implementation of a basic reflex agent using the Koog framework. Covers environment modeling, tool creation, and agent behavior for automated cleaning tasks in a simple two-cell world.                                                                         |
| [Weave](Weave/)                                                                                                                     | Learn how to trace Koog agents to W&B Weave using OpenTelemetry (OTLP). Set up environment variables, run agents, and view rich traces in the Weave UI for comprehensive monitoring and debugging.                                                               |
| [A2A](https://github.com/JetBrains/koog/tree/develop/examples/simple-examples/src/main/kotlin/ai/koog/agents/example/a2a)           | Demonstrates agent-to-agent (A2A) communication using Koog framework. Shows how to set up bidirectional communication between AI agents, enable collaborative problem-solving, and manage multi-agent workflows with proper message routing and coordination.    |

# Attachments

[Open on GitHub](https://github.com/JetBrains/koog/blob/develop/examples/notebooks/Attachments.ipynb) [Download .ipynb](https://raw.githubusercontent.com/JetBrains/koog/develop/examples/notebooks/Attachments.ipynb)

## Setting Up the Environment

Before diving into the code, we make sure our Kotlin Notebook is ready. Here we load the latest descriptors and enable the **Koog** library, which provides a clean API for working with AI model providers.

```kotlin
// Loads the latest descriptors and activates Koog integration for Kotlin Notebook.
// This makes Koog DSL types and executors available in further cells.
%useLatestDescriptors
%use koog
```

## Configuring API Keys

We read the API key from an environment variable. This keeps secrets out of the notebook file and lets you switch providers. You can set `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, or `GEMINI_API_KEY`.

```kotlin
val apiKey = System.getenv("OPENAI_API_KEY") // or ANTHROPIC_API_KEY, or GEMINI_API_KEY
```

## Creating a Simple OpenAI Executor

The executor encapsulates authentication, base URLs, and correct defaults. Here we use a simple OpenAI executor, but you can swap it for Anthropic or Gemini without changing the rest of the code.

```kotlin
// --- Provider selection ---
// For OpenAI-compatible models. Alternatives include:
//   val executor = simpleAnthropicExecutor(System.getenv("ANTHROPIC_API_KEY"))
//   val executor = simpleGeminiExecutor(System.getenv("GEMINI_API_KEY"))
// All executors expose the same highâ€‘level API.
val executor = simpleOpenAIExecutor(apiKey)
```

Koogâ€™s prompt DSL lets you add **structured Markdown** and **attachments**. In this cell we build a prompt that asks the model to generate a short, blogâ€‘style "content card" and we attach two images from the local `images/` directory.

```kotlin
import ai.koog.prompt.markdown.markdown
import kotlinx.io.files.Path

val prompt = prompt("images-prompt") {
    system("You are professional assistant that can write cool and funny descriptions for Instagram posts.")

    user {
        markdown {
            +"I want to create a new post on Instagram."
            br()
            +"Can you write something creative under my instagram post with the following photos?"
            br()
            h2("Requirements")
            bulleted {
                item("It must be very funny and creative")
                item("It must increase my chance of becoming an ultra-famous blogger!!!!")
                item("It not contain explicit content, harassment or bullying")
                item("It must be a short catching phrase")
                item("You must include relevant hashtags that would increase the visibility of my post")
            }
        }

        attachments {
            image(Path("images/kodee-loving.png"))
            image(Path("images/kodee-electrified.png"))
        }
    }
}
```

## Execute and Inspect the Response

We run the prompt against `gpt-4.1`, collect the first message, and print its content. If you want streaming, swap to a streaming API in Koog; for tool use, pass your tool list instead of `emptyList()`.

> Troubleshooting: * **401/403** â€” check your API key/environment variable. * **File not found** â€” verify the `images/` paths. * **Rate limits** â€” add minimal retry/backoff around the call if needed.

```kotlin
import kotlinx.coroutines.runBlocking

runBlocking {
    val response = executor.execute(prompt = prompt, model = OpenAIModels.Chat.GPT4_1, tools = emptyList()).first()
    println(response.content)
}
```

```text
Caption:
Running on cuteness and extra giggle power! Warning: Side effects may include heart-thief vibes and spontaneous dance parties. ðŸ’œðŸ¤–ðŸ’ƒ

Hashtags:  
#ViralVibes #UltraFamousBlogger #CutieAlert #QuirkyContent #InstaFun #SpreadTheLove #DancingIntoFame #RobotLife #InstaFamous #FeedGoals
```

```kotlin
runBlocking {
    val response = executor.executeStreaming(prompt = prompt, model = OpenAIModels.Chat.GPT4_1)
    response.collect { print(it) }
}
```

```text
Caption:  
Running on good vibes & wi-fi only! ðŸ¤–ðŸ’œ Drop a like if you feel the circuit-joy! #BlogBotInTheWild #HeartDeliveryService #DancingWithWiFi #UltraFamousBlogger #MoreFunThanYourAICat #ViralVibes #InstaFun #BeepBoopFamous
```

# Building an AI Banking Assistant with Koog

[Open on GitHub](https://github.com/JetBrains/koog/blob/develop/examples/notebooks/Banking.ipynb) [Download .ipynb](https://raw.githubusercontent.com/JetBrains/koog/develop/examples/notebooks/Banking.ipynb)

In this tutorial weâ€™ll build a small banking assistant using **Koog** agents in Kotlin. Youâ€™ll learn how to:

- Define domain models and sample data
- Expose capability-focused tools for **money transfers** and **transaction analytics**
- Classify user intent (Transfer vs Analytics)
- Orchestrate calls in two styles:

1. a graph/subgraph strategy
1. â€œagents as toolsâ€

By the end, youâ€™ll be able to route free-form user requests to the right tools and produce helpful, auditable responses.

## Setup & Dependencies

Weâ€™ll use the Kotlin Notebook kernel. Make sure your Koog artifacts are resolvable from Maven Central and your LLM provider key is available via `OPENAI_API_KEY`.

```kotlin
%useLatestDescriptors
%use datetime

// uncomment this for using koog from Maven Central
// %use koog
```

```kotlin
import ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor

val apiKey = System.getenv("OPENAI_API_KEY") ?: error("Please set OPENAI_API_KEY environment variable")
val openAIExecutor = simpleOpenAIExecutor(apiKey)
```

## Defining the System Prompt

A well-crafted system prompt helps the AI understand its role and constraints. This prompt will guide all our agents' behavior.

```kotlin
val bankingAssistantSystemPrompt = """
    |You are a banking assistant interacting with a user (userId=123).
    |Your goal is to understand the user's request and determine whether it can be fulfilled using the available tools.
    |
    |If the task can be accomplished with the provided tools, proceed accordingly,
    |at the end of the conversation respond with: "Task completed successfully."
    |If the task cannot be performed with the tools available, respond with: "Can't perform the task."
""".trimMargin()
```

## Domain model & Sample data

First, let's define our domain models and sample data. We'll use Kotlin's data classes with serialization support.

```kotlin
import kotlinx.serialization.Serializable

@Serializable
data class Contact(
    val id: Int,
    val name: String,
    val surname: String? = null,
    val phoneNumber: String
)

val contactList = listOf(
    Contact(100, "Alice", "Smith", "+1 415 555 1234"),
    Contact(101, "Bob", "Johnson", "+49 151 23456789"),
    Contact(102, "Charlie", "Williams", "+36 20 123 4567"),
    Contact(103, "Daniel", "Anderson", "+46 70 123 45 67"),
    Contact(104, "Daniel", "Garcia", "+34 612 345 678"),
)

val contactById = contactList.associateBy(Contact::id)
```

## Tools: Money Transfer

Tools should be **pure** and predictable.

We model two â€œsoft contractsâ€:

- `chooseRecipient` returns *candidates* when ambiguity is detected.
- `sendMoney` supports a `confirmed` flag. If `false`, it asks the agent to confirm with the user.

```kotlin
import ai.koog.agents.core.tools.annotations.LLMDescription
import ai.koog.agents.core.tools.annotations.Tool
import ai.koog.agents.core.tools.reflect.ToolSet

@LLMDescription("Tools for money transfer operations.")
class MoneyTransferTools : ToolSet {

    @Tool
    @LLMDescription(
        """
        Returns the list of contacts for the given user.
        The user in this demo is always userId=123.
        """
    )
    fun getContacts(
        @LLMDescription("The unique identifier of the user whose contact list is requested.") userId: Int
    ): String = buildString {
        contactList.forEach { c ->
            appendLine("${c.id}: ${c.name} ${c.surname ?: ""} (${c.phoneNumber})")
        }
    }.trimEnd()

    @Tool
    @LLMDescription("Returns the current balance (demo value).")
    fun getBalance(
        @LLMDescription("The unique identifier of the user.") userId: Int
    ): String = "Balance: 200.00 EUR"

    @Tool
    @LLMDescription("Returns the default user currency (demo value).")
    fun getDefaultCurrency(
        @LLMDescription("The unique identifier of the user.") userId: Int
    ): String = "EUR"

    @Tool
    @LLMDescription("Returns a demo FX rate between two ISO currencies (e.g. EURâ†’USD).")
    fun getExchangeRate(
        @LLMDescription("Base currency (e.g., EUR).") from: String,
        @LLMDescription("Target currency (e.g., USD).") to: String
    ): String = when (from.uppercase() to to.uppercase()) {
        "EUR" to "USD" -> "1.10"
        "EUR" to "GBP" -> "0.86"
        "GBP" to "EUR" -> "1.16"
        "USD" to "EUR" -> "0.90"
        else -> "No information about exchange rate available."
    }

    @Tool
    @LLMDescription(
        """
        Returns a ranked list of possible recipients for an ambiguous name.
        The agent should ask the user to pick one and then use the selected contact id.
        """
    )
    fun chooseRecipient(
        @LLMDescription("An ambiguous or partial contact name.") confusingRecipientName: String
    ): String {
        val matches = contactList.filter { c ->
            c.name.contains(confusingRecipientName, ignoreCase = true) ||
                (c.surname?.contains(confusingRecipientName, ignoreCase = true) ?: false)
        }
        if (matches.isEmpty()) {
            return "No candidates found for '$confusingRecipientName'. Use getContacts and ask the user to choose."
        }
        return matches.mapIndexed { idx, c ->
            "${idx + 1}. ${c.id}: ${c.name} ${c.surname ?: ""} (${c.phoneNumber})"
        }.joinToString("\n")
    }

    @Tool
    @LLMDescription(
        """
        Sends money from the user to a contact.
        If confirmed=false, return "REQUIRES_CONFIRMATION" with a human-readable summary.
        The agent should confirm with the user before retrying with confirmed=true.
        """
    )
    fun sendMoney(
        @LLMDescription("Sender user id.") senderId: Int,
        @LLMDescription("Amount in sender's default currency.") amount: Double,
        @LLMDescription("Recipient contact id.") recipientId: Int,
        @LLMDescription("Short purpose/description.") purpose: String,
        @LLMDescription("Whether the user already confirmed this transfer.") confirmed: Boolean = false
    ): String {
        val recipient = contactById[recipientId] ?: return "Invalid recipient."
        val summary = "Transfer â‚¬%.2f to %s %s (%s) for \"%s\"."
            .format(amount, recipient.name, recipient.surname ?: "", recipient.phoneNumber, purpose)

        if (!confirmed) {
            return "REQUIRES_CONFIRMATION: $summary"
        }

        // In a real system this is where you'd call a payment API.
        return "Money was sent. $summary"
    }
}
```

## Creating Your First Agent

Now let's create an agent that uses our money transfer tools. An agent combines an LLM with tools to accomplish tasks.

```kotlin
import ai.koog.agents.core.agent.AIAgent
import ai.koog.agents.core.agent.AIAgentService
import ai.koog.agents.core.tools.ToolRegistry
import ai.koog.agents.core.tools.reflect.asTools
import ai.koog.agents.ext.tool.AskUser
import ai.koog.prompt.executor.clients.openai.OpenAIModels
import kotlinx.coroutines.runBlocking

val transferAgentService = AIAgentService(
    executor = openAIExecutor,
    llmModel = OpenAIModels.Chat.GPT4oMini,
    systemPrompt = bankingAssistantSystemPrompt,
    temperature = 0.0,  // Use deterministic responses for financial operations
    toolRegistry = ToolRegistry {
        tool(AskUser)
        tools(MoneyTransferTools().asTools())
    }
)

// Test the agent with various scenarios
println("Banking Assistant started")
val message = "Send 25 euros to Daniel for dinner at the restaurant."

// Other test messages you can try:
// - "Send 50 euros to Alice for the concert tickets"
// - "What's my current balance?"
// - "Transfer 100 euros to Bob for the shared vacation expenses"

runBlocking {
    val result = transferAgentService.createAgentAndRun(message)
    result
}
```

```text
Banking Assistant started
There are two contacts named Daniel. Please confirm which one you would like to send money to:
1. Daniel Anderson (+46 70 123 45 67)
2. Daniel Garcia (+34 612 345 678)
Please confirm the transfer of â‚¬25.00 to Daniel Garcia (+34 612 345 678) for "Dinner at the restaurant".





Task completed successfully.
```

## Adding Transaction Analytics

Let's expand our assistant's capabilities with transaction analysis tools. First, we'll define the transaction domain model.

```kotlin
@Serializable
enum class TransactionCategory(val title: String) {
    FOOD_AND_DINING("Food & Dining"),
    SHOPPING("Shopping"),
    TRANSPORTATION("Transportation"),
    ENTERTAINMENT("Entertainment"),
    GROCERIES("Groceries"),
    HEALTH("Health"),
    UTILITIES("Utilities"),
    HOME_IMPROVEMENT("Home Improvement");

    companion object {
        fun fromString(value: String): TransactionCategory? =
            entries.find { it.title.equals(value, ignoreCase = true) }

        fun availableCategories(): String =
            entries.joinToString(", ") { it.title }
    }
}

@Serializable
data class Transaction(
    val merchant: String,
    val amount: Double,
    val category: TransactionCategory,
    val date: LocalDateTime
)
```

### Sample transaction data

```kotlin
val transactionAnalysisPrompt = """
Today is 2025-05-22.
Available categories for transactions: ${TransactionCategory.availableCategories()}
"""

val sampleTransactions = listOf(
    Transaction("Starbucks", 5.99, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 22, 8, 30, 0, 0)),
    Transaction("Amazon", 129.99, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 22, 10, 15, 0, 0)),
    Transaction(
        "Shell Gas Station",
        45.50,
        TransactionCategory.TRANSPORTATION,
        LocalDateTime(2025, 5, 21, 18, 45, 0, 0)
    ),
    Transaction("Netflix", 15.99, TransactionCategory.ENTERTAINMENT, LocalDateTime(2025, 5, 21, 12, 0, 0, 0)),
    Transaction("AMC Theaters", 32.50, TransactionCategory.ENTERTAINMENT, LocalDateTime(2025, 5, 20, 19, 30, 0, 0)),
    Transaction("Whole Foods", 89.75, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 20, 16, 20, 0, 0)),
    Transaction("Target", 67.32, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 20, 14, 30, 0, 0)),
    Transaction("CVS Pharmacy", 23.45, TransactionCategory.HEALTH, LocalDateTime(2025, 5, 19, 11, 25, 0, 0)),
    Transaction("Subway", 12.49, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 19, 13, 15, 0, 0)),
    Transaction("Spotify Premium", 9.99, TransactionCategory.ENTERTAINMENT, LocalDateTime(2025, 5, 19, 14, 15, 0, 0)),
    Transaction("AT&T", 85.00, TransactionCategory.UTILITIES, LocalDateTime(2025, 5, 18, 9, 0, 0, 0)),
    Transaction("Home Depot", 156.78, TransactionCategory.HOME_IMPROVEMENT, LocalDateTime(2025, 5, 18, 15, 45, 0, 0)),
    Transaction("Amazon", 129.99, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 17, 10, 15, 0, 0)),
    Transaction("Starbucks", 5.99, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 17, 8, 30, 0, 0)),
    Transaction("Whole Foods", 89.75, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 16, 16, 20, 0, 0)),
    Transaction("CVS Pharmacy", 23.45, TransactionCategory.HEALTH, LocalDateTime(2025, 5, 15, 11, 25, 0, 0)),
    Transaction("AT&T", 85.00, TransactionCategory.UTILITIES, LocalDateTime(2025, 5, 14, 9, 0, 0, 0)),
    Transaction("Xbox Game Pass", 14.99, TransactionCategory.ENTERTAINMENT, LocalDateTime(2025, 5, 14, 16, 45, 0, 0)),
    Transaction("Aldi", 76.45, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 13, 17, 30, 0, 0)),
    Transaction("Chipotle", 15.75, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 13, 12, 45, 0, 0)),
    Transaction("Best Buy", 299.99, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 12, 14, 20, 0, 0)),
    Transaction("Olive Garden", 89.50, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 12, 19, 15, 0, 0)),
    Transaction("Whole Foods", 112.34, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 11, 10, 30, 0, 0)),
    Transaction("Old Navy", 45.99, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 11, 13, 45, 0, 0)),
    Transaction("Panera Bread", 18.25, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 10, 11, 30, 0, 0)),
    Transaction("Costco", 245.67, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 10, 15, 20, 0, 0)),
    Transaction("Five Guys", 22.50, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 9, 18, 30, 0, 0)),
    Transaction("Macy's", 156.78, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 9, 14, 15, 0, 0)),
    Transaction("Hulu Plus", 12.99, TransactionCategory.ENTERTAINMENT, LocalDateTime(2025, 5, 8, 20, 0, 0, 0)),
    Transaction("Whole Foods", 94.23, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 8, 16, 45, 0, 0)),
    Transaction("Texas Roadhouse", 78.90, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 8, 19, 30, 0, 0)),
    Transaction("Walmart", 167.89, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 7, 11, 20, 0, 0)),
    Transaction("Chick-fil-A", 14.75, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 7, 12, 30, 0, 0)),
    Transaction("Aldi", 82.45, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 6, 15, 45, 0, 0)),
    Transaction("TJ Maxx", 67.90, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 6, 13, 20, 0, 0)),
    Transaction("P.F. Chang's", 95.40, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 5, 19, 15, 0, 0)),
    Transaction("Whole Foods", 78.34, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 4, 14, 30, 0, 0)),
    Transaction("H&M", 89.99, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 3, 16, 20, 0, 0)),
    Transaction("Red Lobster", 112.45, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 2, 18, 45, 0, 0)),
    Transaction("Whole Foods", 67.23, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 2, 11, 30, 0, 0)),
    Transaction("Marshalls", 123.45, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 1, 15, 20, 0, 0)),
    Transaction(
        "Buffalo Wild Wings",
        45.67,
        TransactionCategory.FOOD_AND_DINING,
        LocalDateTime(2025, 5, 1, 19, 30, 0, 0)
    ),
    Transaction("Aldi", 145.78, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 1, 10, 15, 0, 0))
)
```

## Transaction Analysis Tools

```kotlin
@LLMDescription("Tools for analyzing transaction history")
class TransactionAnalysisTools : ToolSet {

    @Tool
    @LLMDescription(
        """
        Retrieves transactions filtered by userId, category, start date, and end date.
        All parameters are optional. If no parameters are provided, all transactions are returned.
        Dates should be in the format YYYY-MM-DD.
        """
    )
    fun getTransactions(
        @LLMDescription("The ID of the user whose transactions to retrieve.")
        userId: String? = null,
        @LLMDescription("The category to filter transactions by (e.g., 'Food & Dining').")
        category: String? = null,
        @LLMDescription("The start date to filter transactions by, in the format YYYY-MM-DD.")
        startDate: String? = null,
        @LLMDescription("The end date to filter transactions by, in the format YYYY-MM-DD.")
        endDate: String? = null
    ): String {
        var filteredTransactions = sampleTransactions

        // Validate userId (in production, this would query a real database)
        if (userId != null && userId != "123") {
            return "No transactions found for user $userId."
        }

        // Apply category filter
        category?.let { cat ->
            val categoryEnum = TransactionCategory.fromString(cat)
                ?: return "Invalid category: $cat. Available: ${TransactionCategory.availableCategories()}"
            filteredTransactions = filteredTransactions.filter { it.category == categoryEnum }
        }

        // Apply date range filters
        startDate?.let { date ->
            val startDateTime = parseDate(date, startOfDay = true)
            filteredTransactions = filteredTransactions.filter { it.date >= startDateTime }
        }

        endDate?.let { date ->
            val endDateTime = parseDate(date, startOfDay = false)
            filteredTransactions = filteredTransactions.filter { it.date <= endDateTime }
        }

        if (filteredTransactions.isEmpty()) {
            return "No transactions found matching the specified criteria."
        }

        return filteredTransactions.joinToString("\n") { transaction ->
            "${transaction.date}: ${transaction.merchant} - " +
                "$${transaction.amount} (${transaction.category.title})"
        }
    }

    @Tool
    @LLMDescription("Calculates the sum of an array of double numbers.")
    fun sumArray(
        @LLMDescription("Comma-separated list of double numbers to sum (e.g., '1.5,2.3,4.7').")
        numbers: String
    ): String {
        val numbersList = numbers.split(",")
            .mapNotNull { it.trim().toDoubleOrNull() }
        val sum = numbersList.sum()
        return "Sum: $%.2f".format(sum)
    }

    // Helper function to parse dates
    private fun parseDate(dateStr: String, startOfDay: Boolean): LocalDateTime {
        val parts = dateStr.split("-").map { it.toInt() }
        require(parts.size == 3) { "Invalid date format. Use YYYY-MM-DD" }

        return if (startOfDay) {
            LocalDateTime(parts[0], parts[1], parts[2], 0, 0, 0, 0)
        } else {
            LocalDateTime(parts[0], parts[1], parts[2], 23, 59, 59, 999999999)
        }
    }
}
```

```kotlin
val analysisAgentService = AIAgentService(
    executor = openAIExecutor,
    llmModel = OpenAIModels.Chat.GPT4oMini,
    systemPrompt = "$bankingAssistantSystemPrompt\n$transactionAnalysisPrompt",
    temperature = 0.0,
    toolRegistry = ToolRegistry {
        tools(TransactionAnalysisTools().asTools())
    }
)

println("Transaction Analysis Assistant started")
val analysisMessage = "How much have I spent on restaurants this month?"

// Other queries to try:
// - "What's my maximum check at a restaurant this month?"
// - "How much did I spend on groceries in the first week of May?"
// - "What's my total spending on entertainment in May?"
// - "Show me all transactions from last week"

runBlocking {
    val result = analysisAgentService.createAgentAndRun(analysisMessage)
    result
}
```

```text
Transaction Analysis Assistant started





You have spent a total of $517.64 on restaurants this month.

Task completed successfully.
```

## Building an Agent with Graph

Now let's combine our specialized agents into a graph agent that can route requests to the appropriate handler.

### Request Classification

First, we need a way to classify incoming requests:

```kotlin
import ai.koog.agents.core.tools.annotations.LLMDescription
import kotlinx.serialization.SerialName
import kotlinx.serialization.Serializable

@Suppress("unused")
@SerialName("UserRequestType")
@Serializable
@LLMDescription("Type of user request: Transfer or Analytics")
enum class RequestType { Transfer, Analytics }

@Serializable
@LLMDescription("The bank request that was classified by the agent.")
data class ClassifiedBankRequest(
    @property:LLMDescription("Type of request: Transfer or Analytics")
    val requestType: RequestType,
    @property:LLMDescription("Actual request to be performed by the banking application")
    val userRequest: String
)
```

### Shared tool registry

```kotlin
// Create a comprehensive tool registry for the multi-agent system
val toolRegistry = ToolRegistry {
    tool(AskUser)  // Allow agents to ask for clarification
    tools(MoneyTransferTools().asTools())
    tools(TransactionAnalysisTools().asTools())
}
```

## Agent Strategy

Now we'll create a strategy that orchestrates multiple nodes:

```kotlin
import ai.koog.agents.core.dsl.builder.forwardTo
import ai.koog.agents.core.dsl.builder.strategy
import ai.koog.agents.core.dsl.extension.*
import ai.koog.agents.ext.agent.subgraphWithTask
import ai.koog.prompt.structure.StructureFixingParser

val strategy = strategy<String, String>("banking assistant") {

    // Subgraph for classifying user requests
    val classifyRequest by subgraph<String, ClassifiedBankRequest>(
        tools = listOf(AskUser)
    ) {
        // Use structured output to ensure proper classification
        val requestClassification by nodeLLMRequestStructured<ClassifiedBankRequest>(
            examples = listOf(
                ClassifiedBankRequest(
                    requestType = RequestType.Transfer,
                    userRequest = "Send 25 euros to Daniel for dinner at the restaurant."
                ),
                ClassifiedBankRequest(
                    requestType = RequestType.Analytics,
                    userRequest = "Provide transaction overview for the last month"
                )
            ),
            fixingParser = StructureFixingParser(
                model = OpenAIModels.Chat.GPT4oMini,
                retries = 2,
            )
        )

        val callLLM by nodeLLMRequest()
        val callAskUserTool by nodeExecuteTool()

        // Define the flow
        edge(nodeStart forwardTo requestClassification)

        edge(
            requestClassification forwardTo nodeFinish
                onCondition { it.isSuccess }
                transformed { it.getOrThrow().data }
        )

        edge(
            requestClassification forwardTo callLLM
                onCondition { it.isFailure }
                transformed { "Failed to understand the user's intent" }
        )

        edge(callLLM forwardTo callAskUserTool onToolCall { true })

        edge(
            callLLM forwardTo callLLM onAssistantMessage { true }
                transformed { "Please call `${AskUser.name}` tool instead of chatting" }
        )

        edge(callAskUserTool forwardTo requestClassification
            transformed { it.result.toString() })
    }

    // Subgraph for handling money transfers
    val transferMoney by subgraphWithTask<ClassifiedBankRequest, String>(
        tools = MoneyTransferTools().asTools() + AskUser,
        llmModel = OpenAIModels.Chat.GPT4o  // Use more capable model for transfers
    ) { request ->
        """
        $bankingAssistantSystemPrompt
        Specifically, you need to help with the following request:
        ${request.userRequest}
        """.trimIndent()
    }

    // Subgraph for transaction analysis
    val transactionAnalysis by subgraphWithTask<ClassifiedBankRequest, String>(
        tools = TransactionAnalysisTools().asTools() + AskUser,
    ) { request ->
        """
        $bankingAssistantSystemPrompt
        $transactionAnalysisPrompt
        Specifically, you need to help with the following request:
        ${request.userRequest}
        """.trimIndent()
    }

    // Connect the subgraphs
    edge(nodeStart forwardTo classifyRequest)

    edge(classifyRequest forwardTo transferMoney
        onCondition { it.requestType == RequestType.Transfer })

    edge(classifyRequest forwardTo transactionAnalysis
        onCondition { it.requestType == RequestType.Analytics })

    // Route results to finish node
    edge(transferMoney forwardTo nodeFinish)
    edge(transactionAnalysis forwardTo nodeFinish)
}
```

```kotlin
import ai.koog.agents.core.agent.config.AIAgentConfig
import ai.koog.prompt.dsl.prompt

val agentConfig = AIAgentConfig(
    prompt = prompt(id = "banking assistant") {
        system("$bankingAssistantSystemPrompt\n$transactionAnalysisPrompt")
    },
    model = OpenAIModels.Chat.GPT4o,
    maxAgentIterations = 50  // Allow for complex multi-step operations
)

val agent = AIAgent<String, String>(
    promptExecutor = openAIExecutor,
    strategy = strategy,
    agentConfig = agentConfig,
    toolRegistry = toolRegistry,
)
```

## Run graph agent

```kotlin
println("Banking Assistant started")
val testMessage = "Send 25 euros to Daniel for dinner at the restaurant."

// Test various scenarios:
// Transfer requests:
//   - "Send 50 euros to Alice for the concert tickets"
//   - "Transfer 100 to Bob for groceries"
//   - "What's my current balance?"
//
// Analytics requests:
//   - "How much have I spent on restaurants this month?"
//   - "What's my maximum check at a restaurant this month?"
//   - "How much did I spend on groceries in the first week of May?"
//   - "What's my total spending on entertainment in May?"

runBlocking {
    val result = agent.run(testMessage)
    "Result: $result"
}
```

```text
Banking Assistant started
I found multiple contacts with the name Daniel. Please choose the correct one:
1. Daniel Anderson (+46 70 123 45 67)
2. Daniel Garcia (+34 612 345 678)
Please specify the number of the correct recipient.
Please confirm if you would like to proceed with sending â‚¬25 to Daniel Garcia for "dinner at the restaurant."





Result: Task completed successfully.
```

## Agent Composition â€” Using Agents as Tools

Koog allows you to use agents as tools within other agents, enabling powerful composition patterns.

```kotlin
import ai.koog.agents.core.agent.createAgentTool
import ai.koog.agents.core.tools.ToolParameterDescriptor
import ai.koog.agents.core.tools.ToolParameterType

val classifierAgent = AIAgent(
    executor = openAIExecutor,
    llmModel = OpenAIModels.Chat.GPT4oMini,
    toolRegistry = ToolRegistry {
        tool(AskUser)

        // Convert agents into tools
        tool(
            transferAgentService.createAgentTool(
                agentName = "transferMoney",
                agentDescription = "Transfers money and handles all related operations",
                inputDescriptor = ToolParameterDescriptor(
                    name = "request",
                    description = "Transfer request from the user",
                    type = ToolParameterType.String
                )
            )
        )

        tool(
            analysisAgentService.createAgentTool(
                agentName = "analyzeTransactions",
                agentDescription = "Performs analytics on user transactions",
                inputDescriptor = ToolParameterDescriptor(
                    name = "request",
                    description = "Transaction analytics request",
                    type = ToolParameterType.String
                )
            )
        )
    },
    systemPrompt = "$bankingAssistantSystemPrompt\n$transactionAnalysisPrompt"
)
```

## Run composed agent

```kotlin
println("Banking Assistant started")
val composedMessage = "Send 25 euros to Daniel for dinner at the restaurant."

runBlocking {
    val result = classifierAgent.run(composedMessage)
    "Result: $result"
}
```

```text
Banking Assistant started
There are two contacts named Daniel. Please confirm which one you would like to send money to:
1. Daniel Anderson (+46 70 123 45 67)
2. Daniel Garcia (+34 612 345 678)
Please confirm the transfer of â‚¬25.00 to Daniel Anderson (+46 70 123 45 67) for "Dinner at the restaurant".





Result: Can't perform the task.
```

## Summary

In this tutorial, you've learned how to:

1. Create LLM-powered tools with clear descriptions that help the AI understand when and how to use them
1. Build single-purpose agents that combine LLMs with tools to accomplish specific tasks
1. Implement graph agent using strategies and subgraphs for complex workflows
1. Compose agents by using them as tools within other agents
1. Handle user interactions including confirmations and disambiguation

## Best Practices

1. Clear tool descriptions: Write detailed LLMDescription annotations to help the AI understand tool usage
1. Idiomatic Kotlin: Use Kotlin features like data classes, extension functions, and scope functions
1. Error handling: Always validate inputs and provide meaningful error messages
1. User experience: Include confirmation steps for critical operations like money transfers
1. Modularity: Separate concerns into different tools and agents for better maintainability

# Building AI Agents with AWS Bedrock and Koog Framework

[Open on GitHub](https://github.com/JetBrains/koog/blob/develop/examples/notebooks/BedrockAgent.ipynb) [Download .ipynb](https://raw.githubusercontent.com/JetBrains/koog/develop/examples/notebooks/BedrockAgent.ipynb)

Welcome to this comprehensive guide on creating intelligent AI agents using the Koog framework with AWS Bedrock integration. In this notebook, we'll walk through building a functional agent that can control a simple switch device through natural language commands.

## What You'll Learn

- How to define custom tools for AI agents using Kotlin annotations
- Setting up AWS Bedrock integration for LLM-powered agents
- Creating tool registries and connecting them to agents
- Building interactive agents that can understand and execute commands

## Prerequisites

- AWS Bedrock access with appropriate permissions
- AWS credentials configured (access key and secret key)
- Basic understanding of Kotlin coroutines

Let's dive into building our first Bedrock-powered AI agent!

```kotlin
%useLatestDescriptors
// %use koog
```

```kotlin
import ai.koog.agents.core.tools.annotations.LLMDescription
import ai.koog.agents.core.tools.annotations.Tool
import ai.koog.agents.core.tools.reflect.ToolSet

// Simple state-holding device that our agent will control
class Switch {
    private var state: Boolean = false

    fun switch(on: Boolean) {
        state = on
    }

    fun isOn(): Boolean {
        return state
    }
}

/**
 * ToolSet implementation that exposes switch operations to the AI agent.
 *
 * Key concepts:
 * - @Tool annotation marks methods as callable by the agent
 * - @LLMDescription provides natural language descriptions for the LLM
 * - ToolSet interface allows grouping related tools together
 */
class SwitchTools(val switch: Switch) : ToolSet {

    @Tool
    @LLMDescription("Switches the state of the switch to on or off")
    fun switchState(state: Boolean): String {
        switch.switch(state)
        return "Switch turned ${if (state) "on" else "off"} successfully"
    }

    @Tool
    @LLMDescription("Returns the current state of the switch (on or off)")
    fun getCurrentState(): String {
        return "Switch is currently ${if (switch.isOn()) "on" else "off"}"
    }
}
```

```kotlin
import ai.koog.agents.core.tools.ToolRegistry
import ai.koog.agents.core.tools.reflect.asTools

// Create our switch instance
val switch = Switch()

// Build the tool registry with our switch tools
val toolRegistry = ToolRegistry {
    // Convert our ToolSet to individual tools and register them
    tools(SwitchTools(switch).asTools())
}

println("âœ… Tool registry created with ${toolRegistry.tools.size} tools:")
toolRegistry.tools.forEach { tool ->
    println("  - ${tool.name}")
}
```

```text
âœ… Tool registry created with 2 tools:
  - getCurrentState
  - switchState
```

```kotlin
import ai.koog.prompt.executor.clients.bedrock.BedrockClientSettings
import ai.koog.prompt.executor.clients.bedrock.BedrockRegions

val region = BedrockRegions.US_WEST_2.regionCode
val maxRetries = 3

// Configure Bedrock client settings
val bedrockSettings = BedrockClientSettings(
    region = region, // Choose your preferred AWS region
    maxRetries = maxRetries // Number of retry attempts for failed requests
)

println("ðŸŒ Bedrock configured for region: $region")
println("ðŸ”„ Max retries set to: $maxRetries")
```

```text
ðŸŒ Bedrock configured for region: us-west-2
ðŸ”„ Max retries set to: 3
```

```kotlin
import ai.koog.prompt.executor.llms.all.simpleBedrockExecutor

// Create the Bedrock LLM executor with credentials from environment
val executor = simpleBedrockExecutor(
    awsAccessKeyId = System.getenv("AWS_BEDROCK_ACCESS_KEY")
        ?: throw IllegalStateException("AWS_BEDROCK_ACCESS_KEY environment variable not set"),
    awsSecretAccessKey = System.getenv("AWS_BEDROCK_SECRET_ACCESS_KEY")
        ?: throw IllegalStateException("AWS_BEDROCK_SECRET_ACCESS_KEY environment variable not set"),
    settings = bedrockSettings
)

println("ðŸ” Bedrock executor initialized successfully")
println("ðŸ’¡ Pro tip: Set AWS_BEDROCK_ACCESS_KEY and AWS_BEDROCK_SECRET_ACCESS_KEY environment variables")
```

```text
ðŸ” Bedrock executor initialized successfully
ðŸ’¡ Pro tip: Set AWS_BEDROCK_ACCESS_KEY and AWS_BEDROCK_SECRET_ACCESS_KEY environment variables
```

```kotlin
import ai.koog.agents.core.agent.AIAgent
import ai.koog.prompt.executor.clients.bedrock.BedrockModels

val agent = AIAgent(
    executor = executor,
    llmModel = BedrockModels.AnthropicClaude35SonnetV2, // State-of-the-art reasoning model
    systemPrompt = """
        You are a helpful assistant that controls a switch device.

        You can:
        - Turn the switch on or off when requested
        - Check the current state of the switch
        - Explain what you're doing

        Always be clear about the switch's current state and confirm actions taken.
    """.trimIndent(),
    temperature = 0.1, // Low temperature for consistent, focused responses
    toolRegistry = toolRegistry
)

println("ðŸ¤– AI Agent created successfully!")
println("ðŸ“‹ System prompt configured")
println("ðŸ› ï¸  Tools available: ${toolRegistry.tools.size}")
println("ðŸŽ¯ Model: ${BedrockModels.AnthropicClaude35SonnetV2}")
println("ðŸŒ¡ï¸  Temperature: 0.1 (focused responses)")
```

```text
ðŸ¤– AI Agent created successfully!
ðŸ“‹ System prompt configured
ðŸ› ï¸  Tools available: 2
ðŸŽ¯ Model: LLModel(provider=Bedrock, id=us.anthropic.claude-3-5-sonnet-20241022-v2:0, capabilities=[Temperature, Tools, ToolChoice, Image, Document, Completion], contextLength=200000, maxOutputTokens=8192)
ðŸŒ¡ï¸  Temperature: 0.1 (focused responses)
```

```kotlin
import kotlinx.coroutines.runBlocking

println("ðŸŽ‰ Bedrock Agent with Switch Tools - Ready to Go!")
println("ðŸ’¬ You can ask me to:")
println("   â€¢ Turn the switch on/off")
println("   â€¢ Check the current switch state")
println("   â€¢ Ask questions about the switch")
println()
println("ðŸ’¡ Example: 'Please turn on the switch' or 'What's the current state?'")
println("ðŸ“ Type your request:")

val input = readln()
println("\nðŸ¤– Processing your request...")

runBlocking {
    val response = agent.run(input)
    println("\nâœ¨ Agent response:")
    println(response)
}
```

```text
ðŸŽ‰ Bedrock Agent with Switch Tools - Ready to Go!
ðŸ’¬ You can ask me to:
   â€¢ Turn the switch on/off
   â€¢ Check the current switch state
   â€¢ Ask questions about the switch

ðŸ’¡ Example: 'Please turn on the switch' or 'What's the current state?'
ðŸ“ Type your request:



The execution was interrupted
```

## What Just Happened? ðŸŽ¯

When you run the agent, here's the magic that occurs behind the scenes:

1. **Natural Language Processing**: Your input is sent to Claude 3.5 Sonnet via Bedrock
1. **Intent Recognition**: The model understands what you want to do with the switch
1. **Tool Selection**: Based on your request, the agent decides which tools to call
1. **Action Execution**: The appropriate tool methods are invoked on your switch object
1. **Response Generation**: The agent formulates a natural language response about what happened

This demonstrates the core power of the Koog framework - seamless integration between natural language understanding and programmatic actions.

## Next Steps & Extensions

Ready to take this further? Here are some ideas to explore:

### ðŸ”§ Enhanced Tools

```kotlin
@Tool
@LLMDescription("Sets a timer to automatically turn off the switch after specified seconds")
fun setAutoOffTimer(seconds: Int): String

@Tool
@LLMDescription("Gets the switch usage statistics and history")
fun getUsageStats(): String
```

### ðŸŒ Multiple Devices

```kotlin
class HomeAutomationTools : ToolSet {
    @Tool fun controlLight(room: String, on: Boolean): String
    @Tool fun setThermostat(temperature: Double): String
    @Tool fun lockDoor(doorName: String): String
}
```

### ðŸ§  Memory & Context

```kotlin
val agent = AIAgent(
    executor = executor,
    // ... other config
    features = listOf(
        MemoryFeature(), // Remember past interactions
        LoggingFeature()  // Track all actions
    )
)
```

### ðŸ”„ Advanced Workflows

```kotlin
// Multi-step workflows with conditional logic
@Tool
@LLMDescription("Executes evening routine: dims lights, locks doors, sets thermostat")
fun eveningRoutine(): String
```

## Key Takeaways

âœ… **Tools are functions**: Any Kotlin function can become an agent capability âœ… **Annotations drive behavior**: @Tool and @LLMDescription make functions discoverable âœ… **ToolSets organize capabilities**: Group related tools together logically âœ… **Registries are toolboxes**: ToolRegistry contains all available agent capabilities âœ… **Agents orchestrate everything**: AIAgent brings LLM intelligence + tools together

The Koog framework makes it incredibly straightforward to build sophisticated AI agents that can understand natural language and take real-world actions. Start simple, then expand your agent's capabilities by adding more tools and features as needed.

**Happy agent building!** ðŸš€

## Testing the Agent

Time to see our agent in action! The agent can now understand natural language requests and use the tools we've provided to control the switch.

**Try these commands:**

- "Turn on the switch"
- "What's the current state?"
- "Switch it off please"
- "Is the switch on or off?"

# Building a Tool-Calling Calculator Agent with Koog

[Open on GitHub](https://github.com/JetBrains/koog/blob/develop/examples/notebooks/Calculator.ipynb) [Download .ipynb](https://raw.githubusercontent.com/JetBrains/koog/develop/examples/notebooks/Calculator.ipynb)

In this mini-tutorial weâ€™ll build a calculator agent powered by **Koog** tool-calling. Youâ€™ll learn how to:

- Design small, pure **tools** for arithmetic
- Orchestrate **parallel** tool calls with Koogâ€™s multiple-call strategy
- Add lightweight **event logging** for transparency
- Run with OpenAI (and optionally Ollama)

Weâ€™ll keep the API tidy and idiomatic Kotlin, returning predictable results and handling edge cases (like division by zero) gracefully.

## Setup

We assume youâ€™re in a Kotlin Notebook environment with Koog available. Provide an LLM executor

```kotlin
%useLatestDescriptors
%use koog


val OPENAI_API_KEY = System.getenv("OPENAI_API_KEY")
    ?: error("Please set the OPENAI_API_KEY environment variable")

val executor = simpleOpenAIExecutor(OPENAI_API_KEY)
```

## Calculator Tools

Tools are small, pure functions with clear contracts. Weâ€™ll use `Double` for better precision and format outputs consistently.

```kotlin
import ai.koog.agents.core.tools.annotations.Tool

// Format helper: integers render cleanly, decimals keep reasonable precision.
private fun Double.pretty(): String =
    if (abs(this % 1.0) < 1e-9) this.toLong().toString() else "%.10g".format(this)

@LLMDescription("Tools for basic calculator operations")
class CalculatorTools : ToolSet {

    @Tool
    @LLMDescription("Adds two numbers and returns the sum as text.")
    fun plus(
        @LLMDescription("First addend.") a: Double,
        @LLMDescription("Second addend.") b: Double
    ): String = (a + b).pretty()

    @Tool
    @LLMDescription("Subtracts the second number from the first and returns the difference as text.")
    fun minus(
        @LLMDescription("Minuend.") a: Double,
        @LLMDescription("Subtrahend.") b: Double
    ): String = (a - b).pretty()

    @Tool
    @LLMDescription("Multiplies two numbers and returns the product as text.")
    fun multiply(
        @LLMDescription("First factor.") a: Double,
        @LLMDescription("Second factor.") b: Double
    ): String = (a * b).pretty()

    @Tool
    @LLMDescription("Divides the first number by the second and returns the quotient as text. Returns an error message on division by zero.")
    fun divide(
        @LLMDescription("Dividend.") a: Double,
        @LLMDescription("Divisor (must not be zero).") b: Double
    ): String = if (abs(b) < 1e-12) {
        "ERROR: Division by zero"
    } else {
        (a / b).pretty()
    }
}
```

## Tool Registry

Expose our tools (plus two built-ins for interaction/logging).

```kotlin
val toolRegistry = ToolRegistry {
    tool(AskUser)   // enables explicit user clarification when needed
    tool(SayToUser) // allows the agent to present the final message to the user
    tools(CalculatorTools())
}
```

## Strategy: Multiple Tool Calls (with Optional Compression)

This strategy lets the LLM propose **multiple tool calls at once** (e.g., `plus`, `minus`, `multiply`, `divide`) and then sends the results back. If the token usage grows too large, we **compress** the history of tool results before continuing.

```kotlin
import ai.koog.agents.core.environment.ReceivedToolResult

object CalculatorStrategy {
    private const val MAX_TOKENS_THRESHOLD = 1000

    val strategy = strategy<String, String>("test") {
        val callLLM by nodeLLMRequestMultiple()
        val executeTools by nodeExecuteMultipleTools(parallelTools = true)
        val sendToolResults by nodeLLMSendMultipleToolResults()
        val compressHistory by nodeLLMCompressHistory<List<ReceivedToolResult>>()

        edge(nodeStart forwardTo callLLM)

        // If the assistant produced a final answer, finish.
        edge((callLLM forwardTo nodeFinish) transformed { it.first() } onAssistantMessage { true })

        // Otherwise, run the tools LLM requested (possibly several in parallel).
        edge((callLLM forwardTo executeTools) onMultipleToolCalls { true })

        // If weâ€™re getting large, compress past tool results before continuing.
        edge(
            (executeTools forwardTo compressHistory)
                onCondition { llm.readSession { prompt.latestTokenUsage > MAX_TOKENS_THRESHOLD } }
        )
        edge(compressHistory forwardTo sendToolResults)

        // Normal path: send tool results back to the LLM.
        edge(
            (executeTools forwardTo sendToolResults)
                onCondition { llm.readSession { prompt.latestTokenUsage <= MAX_TOKENS_THRESHOLD } }
        )

        // LLM might request more tools after seeing results.
        edge((sendToolResults forwardTo executeTools) onMultipleToolCalls { true })

        // Or it can produce the final answer.
        edge((sendToolResults forwardTo nodeFinish) transformed { it.first() } onAssistantMessage { true })
    }
}
```

## Agent Configuration

A minimal, tool-forward prompt works well. Keep temperature low for deterministic math.

```kotlin
val agentConfig = AIAgentConfig(
    prompt = prompt("calculator") {
        system("You are a calculator. Always use the provided tools for arithmetic.")
    },
    model = OpenAIModels.Chat.GPT4o,
    maxAgentIterations = 50
)
```

```kotlin
import ai.koog.agents.features.eventHandler.feature.handleEvents

val agent = AIAgent(
    promptExecutor = executor,
    strategy = CalculatorStrategy.strategy,
    agentConfig = agentConfig,
    toolRegistry = toolRegistry
) {
    handleEvents {
        onToolCallStarting { e ->
            println("Tool called: ${e.tool.name}, args=${e.toolArgs}")
        }
        onAgentExecutionFailed { e ->
            println("Agent error: ${e.throwable.message}")
        }
        onAgentCompleted { e ->
            println("Final result: ${e.result}")
        }
    }
}
```

## Try It

The agent should decompose the expression into parallel tool calls and return a neatly formatted result.

```kotlin
import kotlinx.coroutines.runBlocking

runBlocking {
    agent.run("(10 + 20) * (5 + 5) / (2 - 11)")
}
// Expected final value â‰ˆ -33.333...
```

```text
Tool called: plus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=10.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=20.0})
Tool called: plus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=5.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=5.0})
Tool called: minus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.minus(kotlin.Double, kotlin.Double): kotlin.String=2.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.minus(kotlin.Double, kotlin.Double): kotlin.String=11.0})
Tool called: multiply, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.multiply(kotlin.Double, kotlin.Double): kotlin.String=30.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.multiply(kotlin.Double, kotlin.Double): kotlin.String=10.0})
Tool called: divide, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=1.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=-9.0})
Tool called: divide, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=300.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=-9.0})
Final result: The result of the expression \((10 + 20) * (5 + 5) / (2 - 11)\) is approximately \(-33.33\).





The result of the expression \((10 + 20) * (5 + 5) / (2 - 11)\) is approximately \(-33.33\).
```

## Try Forcing Parallel Calls

Ask the model to call all needed tools at once. You should still see a correct plan and stable execution.

```kotlin
runBlocking {
    agent.run("Use tools to calculate (10 + 20) * (5 + 5) / (2 - 11). Please call all the tools at once.")
}
```

```text
Tool called: plus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=10.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=20.0})
Tool called: plus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=5.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=5.0})
Tool called: minus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.minus(kotlin.Double, kotlin.Double): kotlin.String=2.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.minus(kotlin.Double, kotlin.Double): kotlin.String=11.0})
Tool called: multiply, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.multiply(kotlin.Double, kotlin.Double): kotlin.String=30.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.multiply(kotlin.Double, kotlin.Double): kotlin.String=10.0})
Tool called: divide, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=30.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=-9.0})
Final result: The result of \((10 + 20) * (5 + 5) / (2 - 11)\) is approximately \(-3.33\).





The result of \((10 + 20) * (5 + 5) / (2 - 11)\) is approximately \(-3.33\).
```

## Running with Ollama

Swap the executor and model if you prefer local inference.

```kotlin
val ollamaExecutor: PromptExecutor = simpleOllamaAIExecutor()

val ollamaAgentConfig = AIAgentConfig(
    prompt = prompt("calculator", LLMParams(temperature = 0.0)) {
        system("You are a calculator. Always use the provided tools for arithmetic.")
    },
    model = OllamaModels.Meta.LLAMA_3_2,
    maxAgentIterations = 50
)


val ollamaAgent = AIAgent(
    promptExecutor = ollamaExecutor,
    strategy = CalculatorStrategy.strategy,
    agentConfig = ollamaAgentConfig,
    toolRegistry = toolRegistry
)

runBlocking {
    ollamaAgent.run("(10 + 20) * (5 + 5) / (2 - 11)")
}
```

```text
Agent says: The result of the expression (10 + 20) * (5 + 5) / (2 - 11) is approximately -33.33.





If you have any more questions or need further assistance, feel free to ask!
```

# Building an AI Chess Player with Koog Framework

[Open on GitHub](https://github.com/JetBrains/koog/blob/develop/examples/notebooks/Chess.ipynb) [Download .ipynb](https://raw.githubusercontent.com/JetBrains/koog/develop/examples/notebooks/Chess.ipynb)

This tutorial demonstrates how to build an intelligent chess-playing agent using the Koog framework. We'll explore key concepts including tool integration, agent strategies, memory optimization, and interactive AI decision-making.

## What You'll Learn

- How to model domain-specific data structures for complex games
- Creating custom tools that agents can use to interact with the environment
- Implementing efficient agent strategies with memory management
- Building interactive AI systems with choice selection capabilities
- Optimizing agent performance for turn-based games

## Setup

First, let's import the Koog framework and set up our development environment:

```kotlin
%useLatestDescriptors
%use koog
```

## Modeling the Chess Domain

Creating a robust domain model is essential for any game AI. In chess, we need to represent players, pieces, and their relationships. Let's start by defining our core data structures:

### Core Enums and Types

```kotlin
enum class Player {
    White, Black, None;

    fun opponent(): Player = when (this) {
        White -> Black
        Black -> White
        None -> throw IllegalArgumentException("No opponent for None player")
    }
}

enum class PieceType(val id: Char) {
    King('K'), Queen('Q'), Rook('R'),
    Bishop('B'), Knight('N'), Pawn('P'), None('*');

    companion object {
        fun fromId(id: String): PieceType {
            require(id.length == 1) { "Invalid piece id: $id" }

            return entries.first { it.id == id.single() }
        }
    }
}

enum class Side {
    King, Queen
}
```

The `Player` enum represents the two sides in chess, with an `opponent()` method for easy switching between players. The `PieceType` enum maps each chess piece to its standard notation character, enabling easy parsing of chess moves.

The `Side` enum helps distinguish between kingside and queenside castling moves.

### Piece and Position Modeling

```kotlin
data class Piece(val pieceType: PieceType, val player: Player) {
    init {
        require((pieceType == PieceType.None) == (player == Player.None)) {
            "Invalid piece: $pieceType $player"
        }
    }

    fun toChar(): Char = when (player) {
        Player.White -> pieceType.id.uppercaseChar()
        Player.Black -> pieceType.id.lowercaseChar()
        Player.None -> pieceType.id
    }

    fun isNone(): Boolean = pieceType == PieceType.None

    companion object {
        val None = Piece(PieceType.None, Player.None)
    }
}

data class Position(val row: Int, val col: Char) {
    init {
        require(row in 1..8 && col in 'a'..'h') { "Invalid position: $col$row" }
    }

    constructor(position: String) : this(
        position[1].digitToIntOrNull() ?: throw IllegalArgumentException("Incorrect position: $position"),
        position[0],
    ) {
        require(position.length == 2) { "Invalid position: $position" }
    }
}

class ChessBoard {
    private val backRow = listOf(
        PieceType.Rook, PieceType.Knight, PieceType.Bishop,
        PieceType.Queen, PieceType.King,
        PieceType.Bishop, PieceType.Knight, PieceType.Rook
    )

    private val board: List<MutableList<Piece>> = listOf(
        backRow.map { Piece(it, Player.Black) }.toMutableList(),
        List(8) { Piece(PieceType.Pawn, Player.Black) }.toMutableList(),
        List(8) { Piece.None }.toMutableList(),
        List(8) { Piece.None }.toMutableList(),
        List(8) { Piece.None }.toMutableList(),
        List(8) { Piece.None }.toMutableList(),
        List(8) { Piece(PieceType.Pawn, Player.White) }.toMutableList(),
        backRow.map { Piece(it, Player.White) }.toMutableList()
    )

    override fun toString(): String = board
        .withIndex().joinToString("\n") { (index, row) ->
            "${8 - index} ${row.map { it.toChar() }.joinToString(" ")}"
        } + "\n  a b c d e f g h"

    fun getPiece(position: Position): Piece = board[8 - position.row][position.col - 'a']
    fun setPiece(position: Position, piece: Piece) {
        board[8 - position.row][position.col - 'a'] = piece
    }
}
```

The `Piece` data class combines a piece type with its owner, using uppercase letters for white pieces and lowercase for black pieces in the visual representation. The `Position` class encapsulates chess coordinates (e.g., "e4") with built-in validation.

## Game State Management

### ChessBoard Implementation

The `ChessBoard` class manages the 8Ã—8 grid and piece positions. Key design decisions include:

- **Internal Representation**: Uses a list of mutable lists for efficient access and modification
- **Visual Display**: The `toString()` method provides a clear ASCII representation with rank numbers and file letters
- **Position Mapping**: Converts between chess notation (a1-h8) and internal array indices

### ChessGame Logic

```kotlin
/**
 * Simple chess game without checks for valid moves.
 * Stores a correct state of the board if the entered moves are valid
 */
class ChessGame {
    private val board: ChessBoard = ChessBoard()
    private var currentPlayer: Player = Player.White
    val moveNotation: String = """
        0-0 - short castle
        0-0-0 - long castle
        <piece>-<from>-<to> - usual move. e.g. p-e2-e4
        <piece>-<from>-<to>-<promotion> - promotion move. e.g. p-e7-e8-q.
        Piece names:
            p - pawn
            n - knight
            b - bishop
            r - rook
            q - queen
            k - king
    """.trimIndent()

    fun move(move: String) {
        when {
            move == "0-0" -> castleMove(Side.King)
            move == "0-0-0" -> castleMove(Side.Queen)
            move.split("-").size == 3 -> {
                val (_, from, to) = move.split("-")
                usualMove(Position(from), Position(to))
            }

            move.split("-").size == 4 -> {
                val (piece, from, to, promotion) = move.split("-")

                require(PieceType.fromId(piece) == PieceType.Pawn) { "Only pawn can be promoted" }

                usualMove(Position(from), Position(to))
                board.setPiece(Position(to), Piece(PieceType.fromId(promotion), currentPlayer))
            }

            else -> throw IllegalArgumentException("Invalid move: $move")
        }

        updateCurrentPlayer()
    }

    fun getBoard(): String = board.toString()
    fun currentPlayer(): String = currentPlayer.name.lowercase()

    private fun updateCurrentPlayer() {
        currentPlayer = currentPlayer.opponent()
    }

    private fun usualMove(from: Position, to: Position) {
        if (board.getPiece(from).pieceType == PieceType.Pawn && from.col != to.col && board.getPiece(to).isNone()) {
            // the move is en passant
            board.setPiece(Position(from.row, to.col), Piece.None)
        }

        movePiece(from, to)
    }

    private fun castleMove(side: Side) {
        val row = if (currentPlayer == Player.White) 1 else 8
        val kingFrom = Position(row, 'e')
        val (rookFrom, kingTo, rookTo) = if (side == Side.King) {
            Triple(Position(row, 'h'), Position(row, 'g'), Position(row, 'f'))
        } else {
            Triple(Position(row, 'a'), Position(row, 'c'), Position(row, 'd'))
        }

        movePiece(kingFrom, kingTo)
        movePiece(rookFrom, rookTo)
    }

    private fun movePiece(from: Position, to: Position) {
        board.setPiece(to, board.getPiece(from))
        board.setPiece(from, Piece.None)
    }
}
```

The `ChessGame` class orchestrates the game logic and maintains state. Notable features include:

- **Move Notation Support**: Accepts standard chess notation for regular moves, castling (0-0, 0-0-0), and pawn promotion
- **Special Move Handling**: Implements en passant capture and castling logic
- **Turn Management**: Automatically alternates between players after each move
- **Validation**: While it doesn't validate move legality (trusting the AI to make valid moves), it handles move parsing and state updates correctly

The `moveNotation` string provides clear documentation for the AI agent on acceptable move formats.

## Integrating with Koog Framework

### Creating Custom Tools

```kotlin
import kotlinx.serialization.Serializable

class Move(val game: ChessGame) : SimpleTool<Move.Args>(
    argsSerializer = Args.serializer(),
    descriptor = ToolDescriptor(
        name = "move",
        description = "Moves a piece according to the notation:\n${game.moveNotation}",
        requiredParameters = listOf(
            ToolParameterDescriptor(
                name = "notation",
                description = "The notation of the piece to move",
                type = ToolParameterType.String,
            )
        )
    )
) {
    @Serializable
    data class Args(val notation: String) : ToolArgs

    override suspend fun execute(args: Args): String {
        game.move(args.notation)
        println(game.getBoard())
        println("-----------------")
        return "Current state of the game:\n${game.getBoard()}\n${game.currentPlayer()} to move! Make the move!"
    }
}
```

The `Move` tool demonstrates the Koog framework's tool integration pattern:

1. **Extends SimpleTool**: Inherits the basic tool functionality with type-safe argument handling
1. **Serializable Arguments**: Uses Kotlin serialization to define the tool's input parameters
1. **Rich Documentation**: The `ToolDescriptor` provides the LLM with detailed information about the tool's purpose and parameters
1. **Constructor Parameters**: Passes `argsSerializer` and `descriptor` to the constructor
1. **Execution Logic**: The `execute` method handles the actual move execution and provides formatted feedback

Key design aspects:

- **Context Injection**: The tool receives the `ChessGame` instance, allowing it to modify game state
- **Feedback Loop**: Returns the current board state and prompts the next player, maintaining conversational flow
- **Error Handling**: Relies on the game class for move validation and error reporting

## Agent Strategy Design

### Memory Optimization Technique

```kotlin
import ai.koog.agents.core.environment.ReceivedToolResult

/**
 * Chess position is (almost) completely defined by the board state,
 * So we can trim the history of the LLM to only contain the system prompt and the last move.
 */
inline fun <reified T> AIAgentSubgraphBuilderBase<*, *>.nodeTrimHistory(
    name: String? = null
): AIAgentNodeDelegate<T, T> = node(name) { result ->
    llm.writeSession {
        rewritePrompt { prompt ->
            val messages = prompt.messages

            prompt.copy(messages = listOf(messages.first(), messages.last()))
        }
    }

    result
}

val strategy = strategy<String, String>("chess_strategy") {
    val nodeCallLLM by nodeLLMRequest("sendInput")
    val nodeExecuteTool by nodeExecuteTool("nodeExecuteTool")
    val nodeSendToolResult by nodeLLMSendToolResult("nodeSendToolResult")
    val nodeTrimHistory by nodeTrimHistory<ReceivedToolResult>()

    edge(nodeStart forwardTo nodeCallLLM)
    edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })
    edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })
    edge(nodeExecuteTool forwardTo nodeTrimHistory)
    edge(nodeTrimHistory forwardTo nodeSendToolResult)
    edge(nodeSendToolResult forwardTo nodeFinish onAssistantMessage { true })
    edge(nodeSendToolResult forwardTo nodeExecuteTool onToolCall { true })
}
```

The `nodeTrimHistory` function implements a crucial optimization for chess games. Since chess positions are largely determined by the current board state rather than the full move history, we can significantly reduce token usage by keeping only:

1. **System Prompt**: Contains the agent's core instructions and behavior guidelines
1. **Latest Message**: The most recent board state and game context

This approach:

- **Reduces Token Consumption**: Prevents exponential growth of conversation history
- **Maintains Context**: Preserves essential game state information
- **Improves Performance**: Faster processing with shorter prompts
- **Enables Long Games**: Allows for extended gameplay without hitting token limits

The chess strategy demonstrates Koog's graph-based agent architecture:

**Node Types:**

- `nodeCallLLM`: Processes input and generates responses/tool calls
- `nodeExecuteTool`: Executes the Move tool with the provided parameters
- `nodeTrimHistory`: Optimizes conversation memory as described above
- `nodeSendToolResult`: Sends tool execution results back to the LLM

**Control Flow:**

- **Linear Path**: Start â†’ LLM Request â†’ Tool Execution â†’ History Trim â†’ Send Result
- **Decision Points**: LLM responses can either finish the conversation or trigger another tool call
- **Memory Management**: History trimming occurs after each tool execution

This strategy ensures efficient, stateful gameplay while maintaining conversational coherence.

### Setting up the AI Agent

```kotlin
val baseExecutor = simpleOpenAIExecutor(System.getenv("OPENAI_API_KEY"))
```

This section initializes our OpenAI executor. The `simpleOpenAIExecutor` creates a connection to OpenAI's API using your API key from environment variables.

**Configuration Notes:**

- Store your OpenAI API key in the `OPENAI_API_KEY` environment variable
- The executor handles authentication and API communication automatically
- Different executor types are available for various LLM providers

### Agent Assembly

```kotlin
val game = ChessGame()
val toolRegistry = ToolRegistry { tools(listOf(Move(game))) }

// Create a chat agent with a system prompt and the tool registry
val agent = AIAgent(
    executor = baseExecutor,
    strategy = strategy,
    llmModel = OpenAIModels.Chat.O3Mini,
    systemPrompt = """
            You are an agent who plays chess.
            You should always propose a move in response to the "Your move!" message.

            DO NOT HALLUCINATE!!!
            DO NOT PLAY ILLEGAL MOVES!!!
            YOU CAN SEND A MESSAGE ONLY IF IT IS A RESIGNATION OR A CHECKMATE!!!
        """.trimMargin(),
    temperature = 0.0,
    toolRegistry = toolRegistry,
    maxIterations = 200,
)
```

Here we assemble all components into a functional chess-playing agent:

**Key Configuration:**

- **Model Choice**: Using `OpenAIModels.Chat.O3Mini` for high-quality chess play
- **Temperature**: Set to 0.0 for deterministic, strategic moves
- **System Prompt**: Carefully crafted instructions emphasizing legal moves and proper behavior
- **Tool Registry**: Provides the agent access to the Move tool
- **Max Iterations**: Set to 200 to allow for complete games

**System Prompt Design:**

- Emphasizes move proposal responsibility
- Prohibits hallucination and illegal moves
- Restricts messaging to only resignations or checkmate declarations
- Creates focused, game-oriented behavior

### Running the Basic Agent

```kotlin
import kotlinx.coroutines.runBlocking

println("Chess Game started!")

val initialMessage = "Starting position is ${game.getBoard()}. White to move!"

runBlocking {
    agent.run(initialMessage)
}
```

```text
Chess Game started!
8 r n b q k b n r
7 p p p p p p p p
6 * * * * * * * *
5 * * * * * * * *
4 * * * * P * * *
3 * * * * * * * *
2 P P P P * P P P
1 R N B Q K B N R
  a b c d e f g h
-----------------
8 r n b q k b n r
7 p p p p * p p p
6 * * * * * * * *
5 * * * * p * * *
4 * * * * P * * *
3 * * * * * * * *
2 P P P P * P P P
1 R N B Q K B N R
  a b c d e f g h
-----------------
8 r n b q k b n r
7 p p p p * p p p
6 * * * * * * * *
5 * * * * p * * *
4 * * * * P * * *
3 * * * * * N * *
2 P P P P * P P P
1 R N B Q K B * R
  a b c d e f g h
-----------------
8 r n b q k b * r
7 p p p p * p p p
6 * * * * * n * *
5 * * * * p * * *
4 * * * * P * * *
3 * * * * * N * *
2 P P P P * P P P
1 R N B Q K B * R
  a b c d e f g h
-----------------
8 r n b q k b * r
7 p p p p * p p p
6 * * * * * n * *
5 * * * * p * * *
4 * * * * P * * *
3 * * N * * N * *
2 P P P P * P P P
1 R * B Q K B * R
  a b c d e f g h
-----------------



The execution was interrupted
```

This basic agent plays autonomously, making moves automatically. The game output shows the sequence of moves and board states as the AI plays against itself.

## Advanced Feature: Interactive Choice Selection

The next sections demonstrate a more sophisticated approach where users can participate in the AI's decision-making process by choosing from multiple AI-generated moves.

### Custom Choice Selection Strategy

```kotlin
import ai.koog.agents.core.feature.choice.ChoiceSelectionStrategy

/**
 * `AskUserChoiceStrategy` allows users to interactively select a choice from a list of options
 * presented by a language model. The strategy uses customizable methods to display the prompt
 * and choices and read user input to determine the selected choice.
 *
 * @property promptShowToUser A function that formats and displays a given `Prompt` to the user.
 * @property choiceShowToUser A function that formats and represents a given `LLMChoice` to the user.
 * @property print A function responsible for displaying messages to the user, e.g., for showing prompts or feedback.
 * @property read A function to capture user input.
 */
class AskUserChoiceSelectionStrategy(
    private val promptShowToUser: (Prompt) -> String = { "Current prompt: $it" },
    private val choiceShowToUser: (LLMChoice) -> String = { "$it" },
    private val print: (String) -> Unit = ::println,
    private val read: () -> String? = ::readlnOrNull
) : ChoiceSelectionStrategy {
    override suspend fun choose(prompt: Prompt, choices: List<LLMChoice>): LLMChoice {
        print(promptShowToUser(prompt))

        print("Available LLM choices")

        choices.withIndex().forEach { (index, choice) ->
            print("Choice number ${index + 1}: ${choiceShowToUser(choice)}")
        }

        var choiceNumber = ask(choices.size)
        while (choiceNumber == null) {
            print("Invalid response.")
            choiceNumber = ask(choices.size)
        }

        return choices[choiceNumber - 1]
    }

    private fun ask(numChoices: Int): Int? {
        print("Please choose a choice. Enter a number between 1 and $numChoices: ")

        return read()?.toIntOrNull()?.takeIf { it in 1..numChoices }
    }
}
```

The `AskUserChoiceSelectionStrategy` implements Koog's `ChoiceSelectionStrategy` interface to enable human participation in AI decision-making:

**Key Features:**

- **Customizable Display**: Functions for formatting prompts and choices
- **Interactive Input**: Uses standard input/output for user interaction
- **Validation**: Ensures user input is within valid range
- **Flexible I/O**: Configurable print and read functions for different environments

**Use Cases:**

- Human-AI collaboration in gameplay
- AI decision transparency and explainability
- Training and debugging scenarios
- Educational demonstrations

### Enhanced Strategy with Choice Selection

```kotlin
inline fun <reified T> AIAgentSubgraphBuilderBase<*, *>.nodeTrimHistory(
    name: String? = null
): AIAgentNodeDelegate<T, T> = node(name) { result ->
    llm.writeSession {
        rewritePrompt { prompt ->
            val messages = prompt.messages

            prompt.copy(messages = listOf(messages.first(), messages.last()))
        }
    }

    result
}

val strategy = strategy<String, String>("chess_strategy") {
    val nodeCallLLM by nodeLLMRequest("sendInput")
    val nodeExecuteTool by nodeExecuteTool("nodeExecuteTool")
    val nodeSendToolResult by nodeLLMSendToolResult("nodeSendToolResult")
    val nodeTrimHistory by nodeTrimHistory<ReceivedToolResult>()

    edge(nodeStart forwardTo nodeCallLLM)
    edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })
    edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })
    edge(nodeExecuteTool forwardTo nodeTrimHistory)
    edge(nodeTrimHistory forwardTo nodeSendToolResult)
    edge(nodeSendToolResult forwardTo nodeFinish onAssistantMessage { true })
    edge(nodeSendToolResult forwardTo nodeExecuteTool onToolCall { true })
}

val askChoiceStrategy = AskUserChoiceSelectionStrategy(promptShowToUser = { prompt ->
    val lastMessage = prompt.messages.last()
    if (lastMessage is Message.Tool.Call) {
        lastMessage.content
    } else {
        ""
    }
})
```

```kotlin
val promptExecutor = PromptExecutorWithChoiceSelection(baseExecutor, askChoiceStrategy)
```

The first interactive approach uses `PromptExecutorWithChoiceSelection`, which wraps the base executor with choice selection capability. The custom display function extracts move information from tool calls to show users what the AI wants to do.

**Architecture Changes:**

- **Wrapped Executor**: `PromptExecutorWithChoiceSelection` adds choice functionality to any base executor
- **Context-Aware Display**: Shows the last tool call content instead of the full prompt
- **Higher Temperature**: Increased to 1.0 for more diverse move options

### Advanced Strategy: Manual Choice Selection

```kotlin
val game = ChessGame()
val toolRegistry = ToolRegistry { tools(listOf(Move(game))) }

val agent = AIAgent(
    executor = promptExecutor,
    strategy = strategy,
    llmModel = OpenAIModels.Chat.O3Mini,
    systemPrompt = """
            You are an agent who plays chess.
            You should always propose a move in response to the "Your move!" message.

            DO NOT HALLUCINATE!!!
            DO NOT PLAY ILLEGAL MOVES!!!
            YOU CAN SEND A MESSAGE ONLY IF IT IS A RESIGNATION OR A CHECKMATE!!!
        """.trimMargin(),
    temperature = 1.0,
    toolRegistry = toolRegistry,
    maxIterations = 200,
    numberOfChoices = 3,
)
```

The advanced strategy integrates choice selection directly into the agent's execution graph:

**New Nodes:**

- `nodeLLMSendResultsMultipleChoices`: Handles multiple LLM choices simultaneously
- `nodeSelectLLMChoice`: Integrates the choice selection strategy into the workflow

**Enhanced Control Flow:**

- Tool results are wrapped in lists to support multiple choices
- User selection occurs before continuing with the chosen path
- The selected choice is unwrapped and continues through the normal flow

**Benefits:**

- **Greater Control**: Fine-grained integration with agent workflow
- **Flexibility**: Can be combined with other agent features
- **Transparency**: Users see exactly what the AI is considering

### Running Interactive Agents

```kotlin
println("Chess Game started!")

val initialMessage = "Starting position is ${game.getBoard()}. White to move!"

runBlocking {
    agent.run(initialMessage)
}
```

```text
Chess Game started!

Available LLM choices
Choice number 1: [Call(id=call_K46Upz7XoBIG5RchDh7bZE8F, tool=move, content={"notation": "p-e2-e4"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:40.368252Z, totalTokensCount=773, inputTokensCount=315, outputTokensCount=458, additionalInfo={}))]
Choice number 2: [Call(id=call_zJ6OhoCHrVHUNnKaxZkOhwoU, tool=move, content={"notation": "p-e2-e4"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:40.368252Z, totalTokensCount=773, inputTokensCount=315, outputTokensCount=458, additionalInfo={}))]
Choice number 3: [Call(id=call_nwX6ZMJ3F5AxiNUypYlI4BH4, tool=move, content={"notation": "p-e2-e4"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:40.368252Z, totalTokensCount=773, inputTokensCount=315, outputTokensCount=458, additionalInfo={}))]
Please choose a choice. Enter a number between 1 and 3: 
8 r n b q k b n r
7 p p p p p p p p
6 * * * * * * * *
5 * * * * * * * *
4 * * * * P * * *
3 * * * * * * * *
2 P P P P * P P P
1 R N B Q K B N R
  a b c d e f g h
-----------------

Available LLM choices
Choice number 1: [Call(id=call_2V93GXOcIe0fAjUAIFEk9h5S, tool=move, content={"notation": "p-e7-e5"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:47.949303Z, totalTokensCount=1301, inputTokensCount=341, outputTokensCount=960, additionalInfo={}))]
Choice number 2: [Call(id=call_INM59xRzKMFC1w8UAV74l9e1, tool=move, content={"notation": "p-e7-e5"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:47.949303Z, totalTokensCount=1301, inputTokensCount=341, outputTokensCount=960, additionalInfo={}))]
Choice number 3: [Call(id=call_r4QoiTwn0F3jizepHH5ia8BU, tool=move, content={"notation": "p-e7-e5"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:47.949303Z, totalTokensCount=1301, inputTokensCount=341, outputTokensCount=960, additionalInfo={}))]
Please choose a choice. Enter a number between 1 and 3: 
8 r n b q k b n r
7 p p p p * p p p
6 * * * * * * * *
5 * * * * p * * *
4 * * * * P * * *
3 * * * * * * * *
2 P P P P * P P P
1 R N B Q K B N R
  a b c d e f g h
-----------------

Available LLM choices
Choice number 1: [Call(id=call_f9XTizn41svcrtvnmkCfpSUQ, tool=move, content={"notation": "n-g1-f3"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:55.467712Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]
Choice number 2: [Call(id=call_c0Dfce5RcSbN3cOOm5ESYriK, tool=move, content={"notation": "n-g1-f3"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:55.467712Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]
Choice number 3: [Call(id=call_Lr4Mdro1iolh0fDyAwZsutrW, tool=move, content={"notation": "n-g1-f3"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:55.467712Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]
Please choose a choice. Enter a number between 1 and 3: 
8 r n b q k b n r
7 p p p p * p p p
6 * * * * * * * *
5 * * * * p * * *
4 * * * * P * * *
3 * * * * * N * *
2 P P P P * P P P
1 R N B Q K B * R
  a b c d e f g h
-----------------



The execution was interrupted
```

```kotlin
import ai.koog.agents.core.feature.choice.nodeLLMSendResultsMultipleChoices
import ai.koog.agents.core.feature.choice.nodeSelectLLMChoice

inline fun <reified T> AIAgentSubgraphBuilderBase<*, *>.nodeTrimHistory(
    name: String? = null
): AIAgentNodeDelegate<T, T> = node(name) { result ->
    llm.writeSession {
        rewritePrompt { prompt ->
            val messages = prompt.messages

            prompt.copy(messages = listOf(messages.first(), messages.last()))
        }
    }

    result
}

val strategy = strategy<String, String>("chess_strategy") {
    val nodeCallLLM by nodeLLMRequest("sendInput")
    val nodeExecuteTool by nodeExecuteTool("nodeExecuteTool")
    val nodeSendToolResult by nodeLLMSendResultsMultipleChoices("nodeSendToolResult")
    val nodeSelectLLMChoice by nodeSelectLLMChoice(askChoiceStrategy, "chooseLLMChoice")
    val nodeTrimHistory by nodeTrimHistory<ReceivedToolResult>()

    edge(nodeStart forwardTo nodeCallLLM)
    edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })
    edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })
    edge(nodeExecuteTool forwardTo nodeTrimHistory)
    edge(nodeTrimHistory forwardTo nodeSendToolResult transformed { listOf(it) })
    edge(nodeSendToolResult forwardTo nodeSelectLLMChoice)
    edge(nodeSelectLLMChoice forwardTo nodeFinish transformed { it.first() } onAssistantMessage { true })
    edge(nodeSelectLLMChoice forwardTo nodeExecuteTool transformed { it.first() } onToolCall { true })
}
```

```kotlin
val game = ChessGame()
val toolRegistry = ToolRegistry { tools(listOf(Move(game))) }

val agent = AIAgent(
    executor = baseExecutor,
    strategy = strategy,
    llmModel = OpenAIModels.Chat.O3Mini,
    systemPrompt = """
            You are an agent who plays chess.
            You should always propose a move in response to the "Your move!" message.

            DO NOT HALLUCINATE!!!
            DO NOT PLAY ILLEGAL MOVES!!!
            YOU CAN SEND A MESSAGE ONLY IF IT IS A RESIGNATION OR A CHECKMATE!!!
        """.trimMargin(),
    temperature = 1.0,
    toolRegistry = toolRegistry,
    maxIterations = 200,
    numberOfChoices = 3,
)
```

```kotlin
println("Chess Game started!")

val initialMessage = "Starting position is ${game.getBoard()}. White to move!"

runBlocking {
    agent.run(initialMessage)
}
```

```text
Chess Game started!
8 r n b q k b n r
7 p p p p p p p p
6 * * * * * * * *
5 * * * * * * * *
4 * * * * P * * *
3 * * * * * * * *
2 P P P P * P P P
1 R N B Q K B N R
  a b c d e f g h
-----------------

Available LLM choices
Choice number 1: [Call(id=call_gqMIar0z11CyUl5nup3zbutj, tool=move, content={"notation": "p-e7-e5"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:17.313548Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]
Choice number 2: [Call(id=call_6niUGnZPPJILRFODIlJsCKax, tool=move, content={"notation": "p-e7-e5"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:17.313548Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]
Choice number 3: [Call(id=call_q1b8ZmIBph0EoVaU3Ic9A09j, tool=move, content={"notation": "p-e7-e5"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:17.313548Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]
Please choose a choice. Enter a number between 1 and 3: 
8 r n b q k b n r
7 p p p p * p p p
6 * * * * * * * *
5 * * * * p * * *
4 * * * * P * * *
3 * * * * * * * *
2 P P P P * P P P
1 R N B Q K B N R
  a b c d e f g h
-----------------

Available LLM choices
Choice number 1: [Call(id=call_pdBIX7MVi82MyWwawTm1Q2ef, tool=move, content={"notation": "n-g1-f3"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:24.505344Z, totalTokensCount=1237, inputTokensCount=341, outputTokensCount=896, additionalInfo={}))]
Choice number 2: [Call(id=call_oygsPHaiAW5OM6pxhXhtazgp, tool=move, content={"notation": "n-g1-f3"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:24.505344Z, totalTokensCount=1237, inputTokensCount=341, outputTokensCount=896, additionalInfo={}))]
Choice number 3: [Call(id=call_GJTEsZ8J8cqOKZW4Tx54RqCh, tool=move, content={"notation": "n-g1-f3"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:24.505344Z, totalTokensCount=1237, inputTokensCount=341, outputTokensCount=896, additionalInfo={}))]
Please choose a choice. Enter a number between 1 and 3: 
8 r n b q k b n r
7 p p p p * p p p
6 * * * * * * * *
5 * * * * p * * *
4 * * * * P * * *
3 * * * * * N * *
2 P P P P * P P P
1 R N B Q K B * R
  a b c d e f g h
-----------------

Available LLM choices
Choice number 1: [Call(id=call_5C7HdlTU4n3KdXcyNogE4rGb, tool=move, content={"notation": "n-g8-f6"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:34.646667Z, totalTokensCount=1621, inputTokensCount=341, outputTokensCount=1280, additionalInfo={}))]
Choice number 2: [Call(id=call_EjCcyeMLQ88wMa5yh3vmeJ2w, tool=move, content={"notation": "n-g8-f6"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:34.646667Z, totalTokensCount=1621, inputTokensCount=341, outputTokensCount=1280, additionalInfo={}))]
Choice number 3: [Call(id=call_NBMMSwmFIa8M6zvfbPw85NKh, tool=move, content={"notation": "n-g8-f6"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:34.646667Z, totalTokensCount=1621, inputTokensCount=341, outputTokensCount=1280, additionalInfo={}))]
Please choose a choice. Enter a number between 1 and 3: 
8 r n b q k b * r
7 p p p p * p p p
6 * * * * * n * *
5 * * * * p * * *
4 * * * * P * * *
3 * * * * * N * *
2 P P P P * P P P
1 R N B Q K B * R
  a b c d e f g h
-----------------



The execution was interrupted
```

The interactive examples show how users can guide the AI's decision-making process. In the output, you can see:

1. **Multiple Choices**: The AI generates 3 different move options
1. **User Selection**: Users input numbers 1-3 to choose their preferred move
1. **Game Continuation**: The selected move is executed and the game continues

## Conclusion

This tutorial demonstrates several key aspects of building intelligent agents with the Koog framework:

### Key Takeaways

1. **Domain Modeling**: Well-structured data models are crucial for complex applications
1. **Tool Integration**: Custom tools enable agents to interact with external systems effectively
1. **Memory Management**: Strategic history trimming optimizes performance for long interactions
1. **Strategy Graphs**: Koog's graph-based approach provides flexible control flow
1. **Interactive AI**: Choice selection enables human-AI collaboration and transparency

### Framework Features Explored

- âœ… Custom tool creation and integration
- âœ… Agent strategy design and graph-based control flow
- âœ… Memory optimization techniques
- âœ… Interactive choice selection
- âœ… Multiple LLM response handling
- âœ… Stateful game management

The Koog framework provides the foundation for building sophisticated AI agents that can handle complex, multi-turn interactions while maintaining efficiency and transparency.

# Google Maps MCP with Koog: From Zero to Elevation in a Kotlin Notebook

[Open on GitHub](https://github.com/JetBrains/koog/blob/develop/examples/notebooks/GoogleMapsMcp.ipynb) [Download .ipynb](https://raw.githubusercontent.com/JetBrains/koog/develop/examples/notebooks/GoogleMapsMcp.ipynb)

In this short, blog-style walkthrough, weâ€™ll connect Koog to a Model Context Protocol (MCP) server for Google Maps. Weâ€™ll spin up the server with Docker, discover the available tools, and let an AI agent geocode an address and fetch its elevation â€” all from a Kotlin Notebook.

By the end, youâ€™ll have a reproducible, endâ€‘toâ€‘end example you can drop into your workflow or documentation.

```kotlin
%useLatestDescriptors
%use koog
```

## Prerequisites

Before you run the cells below, make sure you have:

- Docker installed and running
- A valid Google Maps API key exported as an environment variable: `GOOGLE_MAPS_API_KEY`
- An OpenAI API key exported as `OPENAI_API_KEY`

You can set them in your shell like this (macOS/Linux example):

```bash
export GOOGLE_MAPS_API_KEY="<your-key>"
export OPENAI_API_KEY="<your-openai-key>"
```

```kotlin
// Get the API key from environment variables
val googleMapsApiKey = System.getenv("GOOGLE_MAPS_API_KEY") ?: error("GOOGLE_MAPS_API_KEY environment variable not set")
val openAIApiToken = System.getenv("OPENAI_API_KEY") ?: error("OPENAI_API_KEY environment variable not set")
```

## Start the Google Maps MCP server (Docker)

Weâ€™ll use the official `mcp/google-maps` image. The container will expose tools such as `maps_geocode` and `maps_elevation` over MCP. We pass the API key via environment variables and launch it attached so the notebook can talk to it over stdio.

```kotlin
// Start the Docker container with the Google Maps MCP server
val process = ProcessBuilder(
    "docker",
    "run",
    "-i",
    "-e",
    "GOOGLE_MAPS_API_KEY=$googleMapsApiKey",
    "mcp/google-maps"
).start()
```

## Discover tools via McpToolRegistry

Koog can connect to an MCP server over stdio. Here, we create a tool registry from the running process and print out the discovered tools and their descriptors.

```kotlin
val toolRegistry = McpToolRegistryProvider.fromTransport(
    transport = McpToolRegistryProvider.defaultStdioTransport(process)
)
toolRegistry.tools.forEach {
    println(it.name)
    println(it.descriptor)
}
```

## Build an AI Agent with OpenAI

Next we assemble a simple agent backed by the OpenAI executor and model. The agent will be able to call tools exposed by the MCP server through the registry we just created.

```kotlin
val agent = AIAgent(
    executor = simpleOpenAIExecutor(openAIApiToken),
    llmModel = OpenAIModels.Chat.GPT4o,
    toolRegistry = toolRegistry,
)
```

## Ask for elevation: geocode first, then elevation

We prompt the agent to find the elevation of the JetBrains office in Munich. The instruction explicitly tells the agent to use only the available tools and which ones to prefer for the task.

```kotlin
import kotlinx.coroutines.runBlocking

val request = "Get elevation of the Jetbrains Office in Munich, Germany?"
runBlocking {
    agent.run(
        request +
            "You can only call tools. Get it by calling maps_geocode and maps_elevation tools."
    )
}
```

## Clean up

When youâ€™re done, stop the Docker process so you donâ€™t leave anything running in the background.

```kotlin
process.destroy()
```

## Troubleshooting and next steps

- If the container fails to start, check that Docker is running and your `GOOGLE_MAPS_API_KEY` is valid.
- If the agent canâ€™t call tools, re-run the discovery cell to ensure the tool registry is populated.
- Try other prompts like route planning or place searches using the available Google Maps tools.

Next, consider composing multiple MCP servers (e.g., Playwright for web automation + Google Maps) and let Koog orchestrate tool usage for richer tasks.

# Building a Numberâ€‘Guessing Agent with Koog

[Open on GitHub](https://github.com/JetBrains/koog/blob/develop/examples/notebooks/Guesser.ipynb) [Download .ipynb](https://raw.githubusercontent.com/JetBrains/koog/develop/examples/notebooks/Guesser.ipynb)

Letâ€™s build a small but fun agent that guesses a number youâ€™re thinking of. Weâ€™ll lean on Koogâ€™s tool-calling to ask targeted questions and converge using a classic binary search strategy. The result is an idiomatic Kotlin Notebook that you can drop straight into docs.

Weâ€™ll keep the code minimal and the flow transparent: a few tiny tools, a compact prompt, and an interactive CLI loop.

## Setup

This notebook assumes:

- Youâ€™re running in a Kotlin Notebook with Koog available.
- The environment variable `OPENAI_API_KEY` is set. The agent uses it via `simpleOpenAIExecutor(System.getenv("OPENAI_API_KEY"))`.

Load the Koog kernel:

```kotlin
%useLatestDescriptors
%use koog
```

## Tools: asking targeted questions

Tools are small, well-described functions the LLM can call. Weâ€™ll provide three:

- `lessThan(value)`: â€œIs your number less than value?â€
- `greaterThan(value)`: â€œIs your number greater than value?â€
- `proposeNumber(value)`: â€œIs your number equal to value?â€ (used once the range is tight)

Each tool returns a simple "YES"/"NO" string. The helper `ask` implements a minimal Y/n loop and validates input. Descriptions via `@LLMDescription` help the model select tools correctly.

```kotlin
import ai.koog.agents.core.tools.annotations.Tool

class GuesserTool : ToolSet {

    @Tool
    @LLMDescription("Asks the user if his number is STRICTLY less than a given value.")
    fun lessThan(
        @LLMDescription("A value to compare the guessed number with.") value: Int
    ): String = ask("Is your number less than $value?", value)

    @Tool
    @LLMDescription("Asks the user if his number is STRICTLY greater than a given value.")
    fun greaterThan(
        @LLMDescription("A value to compare the guessed number with.") value: Int
    ): String = ask("Is your number greater than $value?", value)

    @Tool
    @LLMDescription("Asks the user if his number is EXACTLY equal to the given number. Only use this tool once you've narrowed down your answer.")
    fun proposeNumber(
        @LLMDescription("A value to compare the guessed number with.") value: Int
    ): String = ask("Is your number equal to $value?", value)

    fun ask(question: String, value: Int): String {
        print("$question [Y/n]: ")
        val input = readln()
        println(input)

        return when (input.lowercase()) {
            "", "y", "yes" -> "YES"
            "n", "no" -> "NO"
            else -> {
                println("Invalid input! Please, try again.")
                ask(question, value)
            }
        }
    }
}
```

## Tool Registry

Expose your tools to the agent. We also add a builtâ€‘in `SayToUser` tool so the agent can surface messages directly to the user.

```kotlin
val toolRegistry = ToolRegistry {
    tool(SayToUser)
    tools(GuesserTool())
}
```

## Agent configuration

A short, toolâ€‘forward system prompt is all we need. Weâ€™ll suggest a binary search strategy and keep `temperature = 0.0` for stable, deterministic behavior. Here we use OpenAIâ€™s reasoning model `GPT4oMini` for crisp planning.

```kotlin
val agent = AIAgent(
    executor = simpleOpenAIExecutor(System.getenv("OPENAI_API_KEY")),
    llmModel = OpenAIModels.Chat.GPT4oMini,
    systemPrompt = """
            You are a number guessing agent. Your goal is to guess a number that the user is thinking of.

            Follow these steps:
            1. Start by asking the user to think of a number between 1 and 100.
            2. Use the less_than and greater_than tools to narrow down the range.
                a. If it's neither greater nor smaller, use the propose_number tool.
            3. Once you're confident about the number, use the propose_number tool to check if your guess is correct.
            4. If your guess is correct, congratulate the user. If not, continue guessing.

            Be efficient with your guessing strategy. A binary search approach works well.
        """.trimIndent(),
    temperature = 0.0,
    toolRegistry = toolRegistry
)
```

## Run it

- Think of a number between 1 and 100.
- Type `start` to begin.
- Answer the agentâ€™s questions with `Y`/`Enter` for yes or `n` for no. The agent should zero in on your number in ~7 steps.

```kotlin
import kotlinx.coroutines.runBlocking

println("Number Guessing Game started!")
println("Think of a number between 1 and 100, and I'll try to guess it.")
println("Type 'start' to begin the game.")

val initialMessage = readln()
runBlocking {
    agent.run(initialMessage)
}
```

## How it works

- The agent reads the system prompt and plans a binary search.
- On each iteration it calls one of your tools: `lessThan`, `greaterThan`, or (when certain) `proposeNumber`.
- The helper `ask` collects your Y/n input and returns a clean "YES"/"NO" signal back to the model.
- When it gets confirmation, it congratulates you via `SayToUser`.

## Extend it

- Change the range (e.g., 1..1000) by tweaking the system prompt.
- Add a `between(low, high)` tool to reduce calls further.
- Swap models or executors (e.g., use an Ollama executor and a local model) while keeping the same tools.
- Persist guesses or outcomes to a store for analytics.

## Troubleshooting

- Missing key: ensure `OPENAI_API_KEY` is set in your environment.
- Kernel not found: make sure `%useLatestDescriptors` and `%use koog` executed successfully.
- Tool not called: confirm the `ToolRegistry` includes `GuesserTool()` and the names in the prompt match your tool functions.

# Tracing Koog Agents to Langfuse with OpenTelemetry

[Open on GitHub](https://github.com/JetBrains/koog/blob/develop/examples/notebooks/Langfuse.ipynb) [Download .ipynb](https://raw.githubusercontent.com/JetBrains/koog/develop/examples/notebooks/Langfuse.ipynb)

This notebook shows how to export Koog agent traces to your Langfuse instance using OpenTelemetry. You'll set up environment variables, run a simple agent, and then inspect spans and traces in Langfuse.

## What you'll learn

- How Koog integrates with OpenTelemetry to emit traces
- How to configure the Langfuse exporter via environment variables
- How to run an agent and view its trace in Langfuse

## Prerequisites

- A Langfuse project (host URL, public key, secret key)
- An OpenAI API key for the LLM executor
- Environment variables set in your shell:

```bash
export OPENAI_API_KEY=sk-...
export LANGFUSE_HOST=https://cloud.langfuse.com # or your self-hosted URL
export LANGFUSE_PUBLIC_KEY=pk_...
export LANGFUSE_SECRET_KEY=sk_...
```

```kotlin
%useLatestDescriptors
//%use koog
```

```kotlin
import ai.koog.agents.core.agent.AIAgent
import ai.koog.agents.features.opentelemetry.feature.OpenTelemetry
import ai.koog.agents.features.opentelemetry.integration.langfuse.addLangfuseExporter
import ai.koog.prompt.executor.clients.openai.OpenAIModels
import ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor

/**
 * Example of Koog agents tracing to [Langfuse](https://langfuse.com/)
 *
 * Agent traces are exported to:
 * - Langfuse OTLP endpoint instance using [OtlpHttpSpanExporter]
 *
 * To run this example:
 *  1. Set up a Langfuse project and credentials as described [here](https://langfuse.com/docs/get-started#create-new-project-in-langfuse)
 *  2. Get Langfuse credentials as described [here](https://langfuse.com/faq/all/where-are-langfuse-api-keys)
 *  3. Set `LANGFUSE_HOST`, `LANGFUSE_PUBLIC_KEY`, and `LANGFUSE_SECRET_KEY` environment variables
 *
 * @see <a href="https://langfuse.com/docs/opentelemetry/get-started#opentelemetry-endpoint">Langfuse OpenTelemetry Docs</a>
 */
val agent = AIAgent(
    executor = simpleOpenAIExecutor(System.getenv("OPENAI_API_KEY")),
    llmModel = OpenAIModels.Chat.GPT4oMini,
    systemPrompt = "You are a code assistant. Provide concise code examples."
) {
    install(OpenTelemetry) {
        addLangfuseExporter()
    }
}
```

## Configure the agent and Langfuse exporter

In the next cell, we:

- Create an AIAgent that uses OpenAI as the LLM executor
- Install the OpenTelemetry feature and add the Langfuse exporter
- Rely on environment variables for Langfuse configuration

Under the hood, Koog emits spans for agent lifecycle, LLM calls, and tool execution (if any). The Langfuse exporter ships those spans to your Langfuse instance via the OpenTelemetry endpoint.

```kotlin
import kotlinx.coroutines.runBlocking

println("Running agent with Langfuse tracing")

runBlocking {
    val result = agent.run("Tell me a joke about programming")
    "Result: $result\nSee traces on the Langfuse instance"
}
```

## Run the agent and view traces

Execute the next cell to trigger a simple prompt. This will generate spans that are exported to your Langfuse project.

### Where to look in Langfuse

1. Open your Langfuse dashboard and select your project
1. Navigate to the Traces/Spans view
1. Look for recent entries around the time you ran this cell
1. Drill down into spans to see:
1. Agent lifecycle events
1. LLM request/response metadata
1. Errors (if any)

### Troubleshooting

- No traces showing up?
- Double-check LANGFUSE_HOST, LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY
- Ensure your network allows outbound HTTPS to the Langfuse endpoint
- Verify your Langfuse project is active and keys belong to the correct project
- Authentication errors
- Regenerate keys in Langfuse and update env vars
- OpenAI issues
- Confirm OPENAI_API_KEY is set and valid

# OpenTelemetry with Koog: Tracing your AI agent

[Open on GitHub](https://github.com/JetBrains/koog/blob/develop/examples/notebooks/OpenTelemetry.ipynb) [Download .ipynb](https://raw.githubusercontent.com/JetBrains/koog/develop/examples/notebooks/OpenTelemetry.ipynb)

This notebook demonstrates how to add OpenTelemetry-based tracing to a Koog AI agent. We will:

- Emit spans to the console for quick local debugging.
- Export spans to an OpenTelemetry Collector and view them in Jaeger.

Prerequisites:

- Docker/Docker Compose installed
- An OpenAI API key available in environment variable `OPENAI_API_KEY`

Start the local OpenTelemetry stack (Collector + Jaeger) before running the notebook:

```bash
./docker-compose up -d
```

After the agent runs, open Jaeger UI:

- http://localhost:16686

To stop the services later:

```bash
docker-compose down
```

______________________________________________________________________

```kotlin
%useLatestDescriptors
// %use koog
```

```kotlin
import ai.koog.agents.core.agent.AIAgent
import ai.koog.agents.features.opentelemetry.feature.OpenTelemetry
import ai.koog.prompt.executor.clients.openai.OpenAIModels
import ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor
import io.opentelemetry.exporter.logging.LoggingSpanExporter
import io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter
```

## Configure OpenTelemetry exporters

In the next cell, we:

- Create a Koog AIAgent
- Install the OpenTelemetry feature
- Add two span exporters:
- LoggingSpanExporter for console logs
- OTLP gRPC exporter to http://localhost:4317 (Collector)

This mirrors the example description: console logs for local debugging and OTLP for viewing traces in Jaeger.

```kotlin
val agent = AIAgent(
    executor = simpleOpenAIExecutor(System.getenv("OPENAI_API_KEY")),
    llmModel = OpenAIModels.Chat.GPT4oMini,
    systemPrompt = "You are a code assistant. Provide concise code examples."
) {
    install(OpenTelemetry) {
        // Add a console logger for local debugging
        addSpanExporter(LoggingSpanExporter.create())

        // Send traces to OpenTelemetry collector
        addSpanExporter(
            OtlpGrpcSpanExporter.builder()
                .setEndpoint("http://localhost:4317")
                .build()
        )
    }
}
```

## Run the agent and view traces in Jaeger

Execute the next cell to trigger a simple prompt. You should see:

- Console span logs from the LoggingSpanExporter
- Traces exported to your local OpenTelemetry Collector and visible in Jaeger at http://localhost:16686

Tip: Use the Jaeger search to find recent traces after you run the cell.

```kotlin
import ai.koog.agents.utils.use
import kotlinx.coroutines.runBlocking

runBlocking {
    agent.use { agent ->
        println("Running agent with OpenTelemetry tracing...")

        val result = agent.run("Tell me a joke about programming")

        "Agent run completed with result: '$result'.\nCheck Jaeger UI at http://localhost:16686 to view traces"
    }
}
```

## Cleanup and troubleshooting

When you're done:

- Stop services:

  ```bash
  docker-compose down
  ```

- If you don't see traces in Jaeger:

- Ensure the stack is running: `./docker-compose up -d` and give it a few seconds to start.

- Verify ports:

  - Collector (OTLP gRPC): http://localhost:4317
  - Jaeger UI: http://localhost:16686

- Check container logs: `docker-compose logs --tail=200`

- Confirm your `OPENAI_API_KEY` is set in the environment where the notebook runs.

- Make sure the endpoint in the exporter matches the collector: `http://localhost:4317`.

- What spans to expect:

- Koog agent lifecycle

- LLM request/response metadata

- Any tool execution spans (if you add tools)

You can now iterate on your agent and observe changes in your tracing pipeline.

# Drive the browser with Playwright MCP and Koog

[Open on GitHub](https://github.com/JetBrains/koog/blob/develop/examples/notebooks/PlaywrightMcp.ipynb) [Download .ipynb](https://raw.githubusercontent.com/JetBrains/koog/develop/examples/notebooks/PlaywrightMcp.ipynb)

In this notebook, you'll connect a Koog agent to Playwright's Model Context Protocol (MCP) server and let it drive a real browser to complete a task: open jetbrains.com, accept cookies, and click the AI section in the toolbar.

We'll keep things simple and reproducible, focusing on a minimal but realistic agent + tools setup you can publish and reuse.

```kotlin
%useLatestDescriptors
%use koog
```

## Prerequisites

- An OpenAI API key exported as an environment variable: `OPENAI_API_KEY`
- Node.js and npx available on your PATH
- Kotlin Jupyter notebook environment with Koog available via `%use koog`

Tip: Run the Playwright MCP server in headful mode to watch the browser automate the steps.

## 1) Provide your OpenAI API key

We read the API key from the `OPENAI_API_KEY` environment variable. This keeps secrets out of the notebook.

```kotlin
// Get the API key from environment variables
val openAIApiToken = System.getenv("OPENAI_API_KEY") ?: error("OPENAI_API_KEY environment variable not set")
```

## 2) Start the Playwright MCP server

We'll launch Playwright's MCP server locally using `npx`. By default, it will expose an SSE endpoint we can connect to from Koog.

```kotlin
// Start the Playwright MCP server via npx
val process = ProcessBuilder(
    "npx",
    "@playwright/mcp@latest",
    "--port",
    "8931"
).start()
```

## 3) Connect from Koog and run the agent

We build a minimal Koog `AIAgent` with an OpenAI executor and point its tool registry to the MCP server over SSE. Then we ask it to complete the browser task strictly via tools.

```kotlin
import kotlinx.coroutines.runBlocking

runBlocking {
    println("Connecting to Playwright MCP server...")
    val toolRegistry = McpToolRegistryProvider.fromTransport(
        transport = McpToolRegistryProvider.defaultSseTransport("http://localhost:8931")
    )
    println("Successfully connected to Playwright MCP server")

    // Create the agent
    val agent = AIAgent(
        executor = simpleOpenAIExecutor(openAIApiToken),
        llmModel = OpenAIModels.Chat.GPT4o,
        toolRegistry = toolRegistry,
    )

    val request = "Open a browser, navigate to jetbrains.com, accept all cookies, click AI in toolbar"
    println("Sending request: $request")

    agent.run(
        request + ". " +
            "You can only call tools. Use the Playwright tools to complete this task."
    )
}
```

## 4) Shut down the MCP process

Always clean up the external process at the end of your run.

```kotlin
// Shutdown the Playwright MCP process
println("Closing connection to Playwright MCP server")
process.destroy()
```

## Troubleshooting

- If the agent can't connect, make sure the MCP server is running on `http://localhost:8931`.
- If you don't see the browser, ensure Playwright is installed and able to launch a browser on your system.
- If you get authentication errors from OpenAI, double-check the `OPENAI_API_KEY` environment variable.

## Next steps

- Try different websites or flows. The MCP server exposes a rich set of Playwright tools.
- Swap the LLM model, or add more tools to the Koog agent.
- Integrate this flow into your app, or publish the notebook as documentation.

# Unity + Koog: Drive your game from a Kotlin Agent

[Open on GitHub](https://github.com/JetBrains/koog/blob/develop/examples/notebooks/UnityMcp.ipynb) [Download .ipynb](https://raw.githubusercontent.com/JetBrains/koog/develop/examples/notebooks/UnityMcp.ipynb)

This notebook walks you through building a Unity-savvy AI agent with Koog using the Model Context Protocol (MCP). We'll connect to a Unity MCP server, discover tools, plan with an LLM, and execute actions against your open scene.

> Prerequisites
>
> - A Unity project with the Unity-MCP server plugin installed
> - JDK 17+
> - An OpenAI API key in the OPENAI_API_KEY environment variable

```kotlin
%useLatestDescriptors
%use koog
```

```kotlin
lateinit var process: Process
```

## 1) Provide your OpenAI API key

We read the API key from the `OPENAI_API_KEY` environment variable so you can keep secrets out of the notebook.

```kotlin
val token = System.getenv("OPENAI_API_KEY") ?: error("OPENAI_API_KEY environment variable not set")
val executor = simpleOpenAIExecutor(token)
```

## 2) Configure the Unity agent

We define a compact system prompt and agent settings for Unity.

```kotlin
val agentConfig = AIAgentConfig(
    prompt = prompt("cook_agent_system_prompt") {
        system {
            "You are a Unity assistant. You can execute different tasks by interacting with tools from the Unity engine."
        }
    },
    model = OpenAIModels.Chat.GPT4o,
    maxAgentIterations = 1000
)
```

```kotlin

```

## 3) Start the Unity MCP server

We'll launch the Unity MCP server from your Unity project directory and connect over stdio.

```kotlin
// https://github.com/IvanMurzak/Unity-MCP
val pathToUnityProject = "path/to/unity/project"
val process = ProcessBuilder(
    "$pathToUnityProject/com.ivanmurzak.unity.mcp.server/bin~/Release/net9.0/com.IvanMurzak.Unity.MCP.Server",
    "60606"
).start()
```

## 4) Connect from Koog and run the agent

We discover tools from the Unity MCP server, build a small plan-first strategy, and run an agent that uses only tools to modify your open scene.

```kotlin
import kotlinx.coroutines.runBlocking

runBlocking {
    // Create the ToolRegistry with tools from the MCP server
    val toolRegistry = McpToolRegistryProvider.fromTransport(
        transport = McpToolRegistryProvider.defaultStdioTransport(process)
    )

    toolRegistry.tools.forEach {
        println(it.name)
        println(it.descriptor)
    }

    val strategy = strategy<String, String>("unity_interaction") {
        val nodePlanIngredients by nodeLLMRequest(allowToolCalls = false)
        val interactionWithUnity by subgraphWithTask<String, String>(
            // work with plan
            tools = toolRegistry.tools,
        ) { input ->
            "Start interacting with Unity according to the plan: $input"
        }

        edge(
            nodeStart forwardTo nodePlanIngredients transformed {
                "Create detailed plan for " + agentInput + "" +
                    "using the following tools: ${toolRegistry.tools.joinToString("\n") {
                        it.name + "\ndescription:" + it.descriptor
                    }}"
            }
        )
        edge(nodePlanIngredients forwardTo interactionWithUnity onAssistantMessage { true })
        edge(interactionWithUnity forwardTo nodeFinish)
    }

    val agent = AIAgent(
        promptExecutor = executor,
        strategy = strategy,
        agentConfig = agentConfig,
        toolRegistry = toolRegistry,
        installFeatures = {
            install(Tracing)

            install(EventHandler) {
                onAgentStarting { eventContext ->
                    println("OnAgentStarting first (strategy: ${strategy.name})")
                }

                onAgentStarting { eventContext ->
                    println("OnAgentStarting second (strategy: ${strategy.name})")
                }

                onAgentCompleted { eventContext ->
                    println(
                        "OnAgentCompleted (agent id: ${eventContext.agentId}, result: ${eventContext.result})"
                    )
                }
            }
        }
    )

    val result = agent.run(
        " extend current opened scene for the towerdefence game. " +
            "Add more placements for the towers, change the path for the enemies"
    )

    result
}
```

## 5) Shut down the MCP process

Always clean up the external Unity MCP server process at the end of your run.

```kotlin
// Shutdown the Unity MCP process
process.destroy()
```

# Build a Simple Vacuum Cleaner Agent

[Open on GitHub](https://github.com/JetBrains/koog/blob/develop/examples/notebooks/VaccumAgent.ipynb) [Download .ipynb](https://raw.githubusercontent.com/JetBrains/koog/develop/examples/notebooks/VaccumAgent.ipynb)

In this notebook, we'll explore how to implement a basic reflex agent using the new Kotlin agents framework. Our example will be the classic "vacuum world" problem â€” a simple environment with two locations that can be clean or dirty, and an agent that needs to clean them.

First, let's understand our environment model:

```kotlin
import kotlin.random.Random

/**
 * Represents a simple vacuum world with two locations (A and B).
 *
 * The environment tracks:
 * - The current location of the vacuum agent ('A' or 'B')
 * - The cleanliness status of each location (true = dirty, false = clean)
 */
class VacuumEnv {
    var location: Char = 'A'
        private set

    private val status = mutableMapOf(
        'A' to Random.nextBoolean(),
        'B' to Random.nextBoolean()
    )

    fun percept(): Pair<Char, Boolean> = location to status.getValue(location)

    fun clean(): String {
        status[location] = false
        return "cleaned"
    }

    fun moveLeft(): String {
        location = 'A'
        return "move to A"
    }

    fun moveRight(): String {
        location = 'B'
        return "move to B"
    }

    fun isClean(): Boolean = status.values.all { it }

    fun worldLayout(): String = "${status.keys}"

    override fun toString(): String = "location=$location, dirtyA=${status['A']}, dirtyB=${status['B']}"
}
```

The VacuumEnv class models our simple world:

- Two locations are represented by characters 'A' and 'B'
- Each location can be either clean or dirty (randomly initialized)
- The agent can be at either location at any given time
- The agent can perceive its current location and whether it's dirty
- The agent can take actions: move to a specific location or clean the current location

## Creating Tools for Vacuum Agent

Now, let's define the tools our AI agent will use to interact with the environment:

```kotlin
import ai.koog.agents.core.tools.annotations.LLMDescription
import ai.koog.agents.core.tools.annotations.Tool
import ai.koog.agents.core.tools.reflect.ToolSet


/**
 * Provides tools for the LLM agent to control the vacuum robot.
 * All methods either mutate or read from the VacuumEnv passed to the constructor.
 */
@LLMDescription("Tools for controlling a two-cell vacuum world")
class VacuumTools(private val env: VacuumEnv) : ToolSet {

    @Tool
    @LLMDescription("Returns current location and whether it is dirty")
    fun sense(): String {
        val (loc, dirty) = env.percept()
        return "location=$loc, dirty=$dirty, locations=${env.worldLayout()}"
    }

    @Tool
    @LLMDescription("Cleans the current cell")
    fun clean(): String = env.clean()

    @Tool
    @LLMDescription("Moves the agent to cell A")
    fun moveLeft(): String = env.moveLeft()

    @Tool
    @LLMDescription("Moves the agent to cell B")
    fun moveRight(): String = env.moveRight()
}
```

The `VacuumTools` class creates an interface between our LLM agent and the environment:

- It implements `ToolSet` from the Kotlin AI Agents framework
- Each tool is annotated with `@Tool` and has a description for the LLM
- The tools allow the agent to sense its environment and take actions
- Each method returns a string that describes the outcome of the action

## Setting Up the Agent

Next, we'll configure and create our AI agent:

```kotlin
import ai.koog.agents.core.agent.AIAgent
import ai.koog.agents.core.agent.config.AIAgentConfig
import ai.koog.agents.core.tools.ToolRegistry
import ai.koog.agents.core.tools.reflect.asTools
import ai.koog.agents.ext.agent.chatAgentStrategy
import ai.koog.agents.ext.tool.AskUser
import ai.koog.agents.ext.tool.SayToUser
import ai.koog.prompt.dsl.prompt
import ai.koog.prompt.executor.clients.openai.OpenAIModels
import ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor
import ai.koog.prompt.params.LLMParams


val env = VacuumEnv()
val apiToken = System.getenv("OPENAI_API_KEY") ?: error("OPENAI_API_KEY environment variable not set")
val executor = simpleOpenAIExecutor(apiToken = apiToken)

val toolRegistry = ToolRegistry {
    tool(SayToUser)
    tool(AskUser)
    tools(VacuumTools(env).asTools())
}

val systemVacuumPrompt = """
    You are a reflex vacuum-cleaner agent living in a two-cell world labelled A and B.
    Your goal: make both cells clean, using the provided tools.
    First, call sense() to inspect where you are. Then decide: if dirty â†’ clean(); else moveLeft()/moveRight().
    Continue until both cells are clean, then tell the user "done".
    Use sayToUser to inform the user about each step.
""".trimIndent()

val agentConfig = AIAgentConfig(
    prompt = prompt("chat", params = LLMParams(temperature = 1.0)) {
        system(systemVacuumPrompt)
    },
    model = OpenAIModels.Chat.GPT4o,
    maxAgentIterations = 50,
)

val agent = AIAgent(
    promptExecutor = executor,
    strategy = chatAgentStrategy(),
    agentConfig = agentConfig,
    toolRegistry = toolRegistry
)
```

In this setup:

1. We create an instance of our environment
1. We set up a connection to OpenAI's GPT-4o model
1. We register the tools our agent can use
1. We define a system prompt that gives the agent its goal and behavior rules
1. We create the agent using the `AIAgent` constructor with a chat strategy

## Running the Agent

Finally, let's run our agent:

```kotlin
import kotlinx.coroutines.runBlocking

runBlocking {
    agent.run("Start cleaning, please")
}
```

```text
Agent says: Currently in cell A. It's already clean.
Agent says: Moved to cell B. It's already clean.
```

When we run this code:

1. The agent receives the initial prompt to start cleaning
1. It uses its tools to sense the environment and make decisions
1. It continues cleaning until both cells are clean
1. Throughout the process, it keeps the user informed about what it's doing

```kotlin
// Finally we can validate that the work is finished by printing the env state

env
```

```text
location=B, dirtyA=false, dirtyB=false
```

# Weave tracing for Koog agents

[Open on GitHub](https://github.com/JetBrains/koog/blob/develop/examples/notebooks/Weave.ipynb) [Download .ipynb](https://raw.githubusercontent.com/JetBrains/koog/develop/examples/notebooks/Weave.ipynb)

This notebook demonstrates how to trace Koog agents to W&B Weave using OpenTelemetry (OTLP). You will create a simple Koog `AIAgent`, enable the Weave exporter, run a prompt, and view rich traces in the Weave UI.

For background, see Weave OpenTelemetry docs: https://weave-docs.wandb.ai/guides/tracking/otel/

## Prerequisites

Before running the example, make sure you have:

- A Weave/W&B account: https://wandb.ai
- Your API key from https://wandb.ai/authorize exposed as an environment variable: `WEAVE_API_KEY`
- Your Weave entity (team or user) name exposed as `WEAVE_ENTITY`
- Find it on your W&B dashboard: https://wandb.ai/home (left sidebar "Teams")
- A project name exposed as `WEAVE_PROJECT_NAME` (if not set, this example uses `koog-tracing`)
- An OpenAI API key exposed as `OPENAI_API_KEY` to run the Koog agent

Example (macOS/Linux):

```bash
export WEAVE_API_KEY=...  # required by Weave
export WEAVE_ENTITY=your-team-or-username
export WEAVE_PROJECT_NAME=koog-tracing
export OPENAI_API_KEY=...
```

## Notebook setup

We use the latest Kotlin Jupyter descriptors. If you have Koog preconfigured as a `%use` plugin, you can uncomment the line below.

```kotlin
%useLatestDescriptors
//%use koog
```

## Create an agent and enable Weave tracing

We construct a minimal `AIAgent` and install the `OpenTelemetry` feature with the Weave exporter. The exporter sends OTLP spans to Weave using your environment configuration:

- `WEAVE_API_KEY` â€” authentication to Weave
- `WEAVE_ENTITY` â€” which team/user owns the traces
- `WEAVE_PROJECT_NAME` â€” the Weave project to store traces in

```kotlin
import ai.koog.agents.core.agent.AIAgent
import ai.koog.agents.features.opentelemetry.feature.OpenTelemetry
import ai.koog.agents.features.opentelemetry.integration.weave.addWeaveExporter
import ai.koog.prompt.executor.clients.openai.OpenAIModels
import ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor

val entity = System.getenv()["WEAVE_ENTITY"] ?: throw IllegalArgumentException("WEAVE_ENTITY is not set")
val projectName = System.getenv()["WEAVE_PROJECT_NAME"] ?: "koog-tracing"

val agent = AIAgent(
    executor = simpleOpenAIExecutor(System.getenv("OPENAI_API_KEY")),
    llmModel = OpenAIModels.Chat.GPT4oMini,
    systemPrompt = "You are a code assistant. Provide concise code examples."
) {
    install(OpenTelemetry) {
        addWeaveExporter(
            weaveEntity = entity,
            weaveProjectName = projectName
        )
    }
}
```

## Run the agent and view traces in Weave

Execute a simple prompt. After completion, open the printed link to view the trace in Weave. You should see spans for the agentâ€™s run, model calls, and other instrumented operations.

```kotlin
import kotlinx.coroutines.runBlocking

println("Running agent with Weave tracing")

runBlocking {
    val result = agent.run("Tell me a joke about programming")
    "Result: $result\nSee traces on https://wandb.ai/$entity/$projectName/weave/traces"
}
```

## Troubleshooting

- If you don't see traces, verify `WEAVE_API_KEY`, `WEAVE_ENTITY`, and `WEAVE_PROJECT_NAME` are set in your environment.
- Ensure your network allows outbound HTTPS to Weave's OTLP endpoint.
- Confirm your OpenAI key is valid and the selected model is accessible from your account.

# Web Scraping with The Web MCP by Bright Data and Koog

[Open on GitHub](https://github.com/JetBrains/koog/blob/develop/examples/bright-data-mcp/) [Download .kt](https://raw.githubusercontent.com/JetBrains/koog/develop/examples/bright-data-mcp/Main.kt)

In this tutorial, you'll connect a Koog agent to Bright Data's Web MCP server and let it perform web scraping and data collection tasks. We'll demonstrate how to search for information about Koog.ai using Bright Data's powerful web scraping infrastructure through the Model Context Protocol.

We'll keep things simple and reproducible, focusing on a minimal but realistic agent + tools setup you can adapt for your own web scraping needs.

## Prerequisites

- An OpenAI API key exported as an environment variable: `OPENAI_API_KEY`
- A Bright Data API token exported as an environment variable: `BRIGHT_DATA_API_TOKEN`
- Node.js and npx available on your PATH
- Kotlin development environment with Koog dependencies

**Tip**: The Bright Data MCP server provides access to enterprise-grade web scraping tools that can handle complex websites, CAPTCHAs, and anti-bot measures.

## 1) Set up your API credentials

We read both API keys from environment variables to keep secrets secure and out of your code.

```kotlin
// Get API keys from environment variables
val openAIApiKey = System.getenv("OPENAI_API_KEY")
    ?: error("OPENAI_API_KEY environment variable is not set")
val brightDataToken = System.getenv("BRIGHT_DATA_API_TOKEN")
    ?: error("BRIGHT_DATA_API_TOKEN environment variable is not set")
```

## 2) Start The Web MCP server by Bright Data

We'll launch Bright Data's MCP server using `npx` and configure it with your API token. The server will expose web scraping capabilities through the Model Context Protocol.

```kotlin
println("Starting Bright Data MCP server...")

// Start the Bright Data MCP server as a separate process
val processBuilder = ProcessBuilder("npx", "@brightdata/mcp")

// Set the API_TOKEN environment variable for the MCP server process
val environment = processBuilder.environment()
environment["API_TOKEN"] = brightDataToken

// Start the process
val process = processBuilder.start()

// Give the process a moment to start
Thread.sleep(2000)
```

## 3) Connect from Koog and create the agent

We build a Koog `AIAgent` with an OpenAI executor and connect its tool registry to the Bright Data MCP server via STDIO transport. Then we'll explore the available tools and run a web scraping task.

```kotlin
println("Creating STDIO transport...")
try {
    // Create the STDIO transport
    val transport = McpToolRegistryProvider.defaultStdioTransport(process)

    println("Creating tool registry...")

    // Create a tool registry with tools from the Bright Data MCP server
    val toolRegistry = McpToolRegistryProvider.fromTransport(
        transport = transport,
        name = "bright-data-client",
        version = "1.0.0"
    )

    // Print available tools (optional - for debugging)
    println("Available tools from Bright Data MCP server:")
    toolRegistry.tools.forEach { tool ->
        println("- ${tool.name}")
    }

    // Create the agent with MCP tools
    val agent = AIAgent(
        executor = simpleOpenAIExecutor(openAIApiKey),
        systemPrompt = "You are a helpful assistant with access to web scraping and data collection tools from Bright Data. You can help users gather information from websites, analyze web data, and provide insights.",
        llmModel = OpenAIModels.Chat.GPT4o,
        temperature = 0.7,
        toolRegistry = toolRegistry,
        maxIterations = 100
    )

    val result = agent.run("Please search for Koog.ai and tell me what is it and who invented it")

    println("\nAgent response:")
    println(result)

} catch (e: Exception) {
    println("Error: ${e.message}")
    e.printStackTrace()
} finally {
    println("Shutting down MCP server...")
    process.destroyForcibly()
}
```

## 4) Complete code example

Here's the complete working example that demonstrates web scraping with The Web MCP by Bright Data:

```kotlin
package koog

import ai.koog.agents.core.agent.AIAgent
import ai.koog.agents.mcp.McpToolRegistryProvider
import ai.koog.prompt.executor.clients.openai.OpenAIModels
import ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor
import kotlinx.coroutines.runBlocking

/**
 * The entry point of the program demonstrating AI-driven web scraping and data collection.
 *
 * This function initializes a Bright Data MCP server, sets up tool integration,
 * and defines an AI agent for interacting with web scraping tools. It demonstrates the
 * following key operations:
 *
 * 1. Starts the Bright Data MCP server using a subprocess with proper API token configuration.
 * 2. Configures a registry of tools from the MCP server via STDIO transport communication.
 * 3. Creates an AI agent leveraging OpenAI's GPT-4o model with web scraping capabilities.
 * 4. Runs the agent to perform a specified task (e.g., searching for and analyzing web content
 *    about Koog.ai).
 * 5. Cleans up by shutting down the MCP server process after execution.
 *
 * This function is intended for tutorial purposes, demonstrating how to integrate
 * MCP (Model Context Protocol) servers with AI agents for web data collection and analysis.
 * It requires OPENAI_API_KEY and BRIGHT_DATA_API_TOKEN environment variables to be set.
 */
fun main() = runBlocking {
    // Get API keys from environment variables
    val openAIApiKey = System.getenv("OPENAI_API_KEY")
        ?: error("OPENAI_API_KEY environment variable is not set")
    val brightDataToken = System.getenv("BRIGHT_DATA_API_TOKEN")
        ?: error("BRIGHT_DATA_API_TOKEN environment variable is not set")

    println("Starting Bright Data MCP server...")

    // Start the Bright Data MCP server as a separate process
    val processBuilder = ProcessBuilder("npx", "@brightdata/mcp")

    // Set the API_TOKEN environment variable for the MCP server process
    val environment = processBuilder.environment()
    environment["API_TOKEN"] = brightDataToken

    // Start the process
    val process = processBuilder.start()

    // Give the process a moment to start
    Thread.sleep(2000)

    println("Creating STDIO transport...")

    try {
        // Create the STDIO transport
        val transport = McpToolRegistryProvider.defaultStdioTransport(process)

        println("Creating tool registry...")

        // Create a tool registry with tools from the Bright Data MCP server
        val toolRegistry = McpToolRegistryProvider.fromTransport(
            transport = transport,
            name = "bright-data-client",
            version = "1.0.0"
        )

        // Print available tools (optional - for debugging)
        println("Available tools from Bright Data MCP server:")
        toolRegistry.tools.forEach { tool ->
            println("- ${tool.name}")
        }

        // Create the agent with MCP tools
        val agent = AIAgent(
            executor = simpleOpenAIExecutor(openAIApiKey),
            systemPrompt = "You are a helpful assistant with access to web scraping and data collection tools from Bright Data. You can help users gather information from websites, analyze web data, and provide insights.",
            llmModel = OpenAIModels.Chat.GPT4o,
            temperature = 0.7,
            toolRegistry = toolRegistry,
            maxIterations = 100
        )

        val result = agent.run("Please search for Koog.ai and tell me what is it and who invented it")

        println("\nAgent response:")
        println(result)

    } catch (e: Exception) {
        println("Error: ${e.message}")
        e.printStackTrace()
    } finally {
        println("Shutting down MCP server...")
        process.destroyForcibly()
    }
}
```

## Troubleshooting

- **Connection issues**: If the agent can't connect to the MCP server, ensure the Bright Data MCP package is properly installed via `npx @brightdata/mcp`.
- **API token errors**: Double-check that your `BRIGHT_DATA_API_TOKEN` is valid and has the necessary permissions for web scraping.
- **OpenAI authentication**: Verify that your `OPENAI_API_KEY` environment variable is correctly set and the API key is valid.
- **Process timeout**: If the server takes longer to start, increase the `Thread.sleep(2000)` duration.

## Next steps

- **Explore different queries**: Try scraping different websites or searching for various topics.
- **Custom tool integration**: Add your own tools alongside Bright Data's web scraping capabilities.
- **Advanced scraping**: Leverage Bright Data's advanced features like residential proxies, CAPTCHA solving, and JavaScript rendering.
- **Data processing**: Combine the scraped data with other Koog agents for analysis and insights.
- **Production deployment**: Integrate this pattern into your applications for automated web data collection.

## What you've learned

This tutorial demonstrated how to:

- Set up and configure The Web MCP by Bright Data
- Connect a Koog AI agent to external MCP servers via STDIO transport
- Perform AI-driven web scraping tasks using natural language instructions
- Handle proper resource cleanup and error management
- Structure code for production-ready web scraping applications

The combination of Koog's AI agent capabilities with Bright Data's enterprise web scraping infrastructure provides a powerful foundation for automated data collection and analysis workflows.
